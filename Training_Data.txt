
Contents
Preface pagexxv
Installation xxxiv
Notation xxxvii
1 Introduction 1
1.1 A Motivating Example 2
1.2 Key Components 4
1.3 Kinds of Machine Learning Problems 7
1.4 Roots 20
1.5 The Road to Deep Learning 22
1.6 Success Stories 25
1.7 The Essence of Deep Learning 27
1.8 Summary 29
1.9 Exercises 29
2 Preliminaries 30
2.1 Data Manipulation 30
2.1.1 Getting Started 30
2.1.2 Indexing and Slicing 33
2.1.3 Operations 34
2.1.4 Broadcasting 35
2.1.5 Saving Memory 36
2.1.6 Conversion to Other Python Objects 37
2.1.7 Summary 37
2.1.8 Exercises 38
2.2 Data Preprocessing 38
2.2.1 Reading the Dataset 38
2.2.2 Data Preparation 39
2.2.3 Conversion to the Tensor Format 40
2.2.4 Discussion 40
2.2.5 Exercises 40
2.3 Linear Algebra 41
2.3.1 Scalars 41
iii
2.3.2 Vectors 42
2.3.3 Matrices 43
2.3.4 Tensors 44
2.3.5 Basic Properties of Tensor Arithmetic 45
2.3.6 Reduction 46
2.3.7 Non-Reduction Sum 47
2.3.8 Dot Products 48
2.3.9 Matrix‚ÄìVector Products 48
2.3.10 Matrix‚ÄìMatrix Multiplication 49
2.3.11 Norms 50
2.3.12 Discussion 52
2.3.13 Exercises 53
2.4 Calculus 54
2.4.1 Derivatives and Differentiation 54
2.4.2 Visualization Utilities 56
2.4.3 Partial Derivatives and Gradients 58
2.4.4 Chain Rule 58
2.4.5 Discussion 59
2.4.6 Exercises 59
2.5 Automatic Differentiation 60
2.5.1 A Simple Function 60
2.5.2 Backward for Non-Scalar Variables 61
2.5.3 Detaching Computation 62
2.5.4 Gradients and Python Control Flow 63
2.5.5 Discussion 64
2.5.6 Exercises 64
2.6 Probability and Statistics 65
2.6.1 A Simple Example: Tossing Coins 66
2.6.2 A More Formal Treatment 68
2.6.3 Random Variables 69
2.6.4 Multiple Random Variables 70
2.6.5 An Example 73
2.6.6 Expectations 74
2.6.7 Discussion 76
2.6.8 Exercises 77
2.7 Documentation 78
2.7.1 Functions and Classes in a Module 78
2.7.2 Specific Functions and Classes 79
3 Linear Neural Networks for Regression 82
3.1 Linear Regression 82
3.1.1 Basics 83
3.1.2 Vectorization for Speed 88
3.1.3 The Normal Distribution and Squared Loss 88
3.1.4 Linear Regression as a Neural Network 90
iv
3.1.5 Summary 91
3.1.6 Exercises 92
3.2 Object-Oriented Design for Implementation 93
3.2.1 Utilities 94
3.2.2 Models 96
3.2.3 Data 97
3.2.4 Training 97
3.2.5 Summary 98
3.2.6 Exercises 98
3.3 Synthetic Regression Data 99
3.3.1 Generating the Dataset 99
3.3.2 Reading the Dataset 100
3.3.3 Concise Implementation of the Data Loader 101
3.3.4 Summary 102
3.3.5 Exercises 102
3.4 Linear Regression Implementation from Scratch 103
3.4.1 Defining the Model 103
3.4.2 Defining the Loss Function 104
3.4.3 Defining the Optimization Algorithm 104
3.4.4 Training 105
3.4.5 Summary 107
3.4.6 Exercises 107
3.5 Concise Implementation of Linear Regression 108
3.5.1 Defining the Model 109
3.5.2 Defining the Loss Function 109
3.5.3 Defining the Optimization Algorithm 110
3.5.4 Training 110
3.5.5 Summary 111
3.5.6 Exercises 111
3.6 Generalization 112
3.6.1 Training Error and Generalization Error 113
3.6.2 Underfitting or Overfitting? 115
3.6.3 Model Selection 116
3.6.4 Summary 117
3.6.5 Exercises 117
3.7 Weight Decay 118
3.7.1 Norms and Weight Decay 119
3.7.2 High-Dimensional Linear Regression 120
3.7.3 Implementation from Scratch 121
3.7.4 Concise Implementation 122
3.7.5 Summary 124
3.7.6 Exercises 124
4 Linear Neural Networks for ClassiÔ¨Åcation 125
4.1 Softmax Regression 125
v
4.1.1 Classification 126
4.1.2 Loss Function 129
4.1.3 Information Theory Basics 130
4.1.4 Summary and Discussion 131
4.1.5 Exercises 132
4.2 The Image Classification Dataset 134
4.2.1 Loading the Dataset 134
4.2.2 Reading a Minibatch 135
4.2.3 Visualization 136
4.2.4 Summary 137
4.2.5 Exercises 137
4.3 The Base Classification Model 138
4.3.1 TheClassifier Class 138
4.3.2 Accuracy 138
4.3.3 Summary 139
4.3.4 Exercises 139
4.4 Softmax Regression Implementation from Scratch 140
4.4.1 The Softmax 140
4.4.2 The Model 141
4.4.3 The Cross-Entropy Loss 141
4.4.4 Training 142
4.4.5 Prediction 143
4.4.6 Summary 143
4.4.7 Exercises 144
4.5 Concise Implementation of Softmax Regression 144
4.5.1 Defining the Model 145
4.5.2 Softmax Revisited 145
4.5.3 Training 146
4.5.4 Summary 146
4.5.5 Exercises 147
4.6 Generalization in Classification 147
4.6.1 The Test Set 148
4.6.2 Test Set Reuse 150
4.6.3 Statistical Learning Theory 151
4.6.4 Summary 153
4.6.5 Exercises 154
4.7 Environment and Distribution Shift 154
4.7.1 Types of Distribution Shift 155
4.7.2 Examples of Distribution Shift 157
4.7.3 Correction of Distribution Shift 159
4.7.4 A Taxonomy of Learning Problems 163
4.7.5 Fairness, Accountability, and Transparency in Machine
Learning 164
4.7.6 Summary 165
4.7.7 Exercises 166
vi
5 Multilayer Perceptrons 167
5.1 Multilayer Perceptrons 167
5.1.1 Hidden Layers 167
5.1.2 Activation Functions 171
5.1.3 Summary and Discussion 174
5.1.4 Exercises 175
5.2 Implementation of Multilayer Perceptrons 176
5.2.1 Implementation from Scratch 176
5.2.2 Concise Implementation 177
5.2.3 Summary 178
5.2.4 Exercises 179
5.3 Forward Propagation, Backward Propagation, and Computational Graphs 180
5.3.1 Forward Propagation 180
5.3.2 Computational Graph of Forward Propagation 181
5.3.3 Backpropagation 181
5.3.4 Training Neural Networks 183
5.3.5 Summary 183
5.3.6 Exercises 183
5.4 Numerical Stability and Initialization 184
5.4.1 Vanishing and Exploding Gradients 184
5.4.2 Parameter Initialization 187
5.4.3 Summary 188
5.4.4 Exercises 189
5.5 Generalization in Deep Learning 189
5.5.1 Revisiting Overfitting and Regularization 190
5.5.2 Inspiration from Nonparametrics 191
5.5.3 Early Stopping 192
5.5.4 Classical Regularization Methods for Deep Networks 193
5.5.5 Summary 193
5.5.6 Exercises 194
5.6 Dropout 194
5.6.1 Dropout in Practice 195
5.6.2 Implementation from Scratch 196
5.6.3 Concise Implementation 197
5.6.4 Summary 198
5.6.5 Exercises 198
5.7 Predicting House Prices on Kaggle 199
5.7.1 Downloading Data 199
5.7.2 Kaggle 200
5.7.3 Accessing and Reading the Dataset 201
5.7.4 Data Preprocessing 201
5.7.5 Error Measure 203
5.7.6 ùêæ-Fold Cross-Validation 204
5.7.7 Model Selection 204
5.7.8 Submitting Predictions on Kaggle 205
vii
5.7.9 Summary and Discussion 206
5.7.10 Exercises 206
6 Builders‚Äô Guide 207
6.1 Layers and Modules 207
6.1.1 A Custom Module 209
6.1.2 The Sequential Module 211
6.1.3 Executing Code in the Forward Propagation Method 211
6.1.4 Summary 213
6.1.5 Exercises 213
6.2 Parameter Management 213
6.2.1 Parameter Access 214
6.2.2 Tied Parameters 215
6.2.3 Summary 216
6.2.4 Exercises 216
6.3 Parameter Initialization 216
6.3.1 Built-in Initialization 217
6.3.2 Summary 219
6.3.3 Exercises 219
6.4 Lazy Initialization 219
6.4.1 Summary 220
6.4.2 Exercises 221
6.5 Custom Layers 221
6.5.1 Layers without Parameters 221
6.5.2 Layers with Parameters 222
6.5.3 Summary 223
6.5.4 Exercises 223
6.6 File I/O 223
6.6.1 Loading and Saving Tensors 224
6.6.2 Loading and Saving Model Parameters 225
6.6.3 Summary 226
6.6.4 Exercises 226
6.7 GPUs 226
6.7.1 Computing Devices 227
6.7.2 Tensors and GPUs 228
6.7.3 Neural Networks and GPUs 230
6.7.4 Summary 231
6.7.5 Exercises 231
7 Convolutional Neural Networks 233
7.1 From Fully Connected Layers to Convolutions 234
7.1.1 Invariance 234
7.1.2 Constraining the MLP 235
7.1.3 Convolutions 237
7.1.4 Channels 238
viii
7.1.5 Summary and Discussion 239
7.1.6 Exercises 239
7.2 Convolutions for Images 240
7.2.1 The Cross-Correlation Operation 240
7.2.2 Convolutional Layers 242
7.2.3 Object Edge Detection in Images 242
7.2.4 Learning a Kernel 244
7.2.5 Cross-Correlation and Convolution 245
7.2.6 Feature Map and Receptive Field 245
7.2.7 Summary 246
7.2.8 Exercises 247
7.3 Padding and Stride 247
7.3.1 Padding 248
7.3.2 Stride 250
7.3.3 Summary and Discussion 251
7.3.4 Exercises 251
7.4 Multiple Input and Multiple Output Channels 252
7.4.1 Multiple Input Channels 252
7.4.2 Multiple Output Channels 253
7.4.3 11Convolutional Layer 255
7.4.4 Discussion 256
7.4.5 Exercises 256
7.5 Pooling 257
7.5.1 Maximum Pooling and Average Pooling 258
7.5.2 Padding and Stride 260
7.5.3 Multiple Channels 261
7.5.4 Summary 261
7.5.5 Exercises 262
7.6 Convolutional Neural Networks (LeNet) 262
7.6.1 LeNet 263
7.6.2 Training 265
7.6.3 Summary 266
7.6.4 Exercises 266
8 Modern Convolutional Neural Networks 268
8.1 Deep Convolutional Neural Networks (AlexNet) 269
8.1.1 Representation Learning 270
8.1.2 AlexNet 273
8.1.3 Training 276
8.1.4 Discussion 276
8.1.5 Exercises 277
8.2 Networks Using Blocks (VGG) 278
8.2.1 VGG Blocks 279
8.2.2 VGG Network 279
8.2.3 Training 281
ix
8.2.4 Summary 282
8.2.5 Exercises 282
8.3 Network in Network (NiN) 283
8.3.1 NiN Blocks 283
8.3.2 NiN Model 284
8.3.3 Training 285
8.3.4 Summary 286
8.3.5 Exercises 286
8.4 Multi-Branch Networks (GoogLeNet) 287
8.4.1 Inception Blocks 287
8.4.2 GoogLeNet Model 288
8.4.3 Training 291
8.4.4 Discussion 291
8.4.5 Exercises 292
8.5 Batch Normalization 292
8.5.1 Training Deep Networks 293
8.5.2 Batch Normalization Layers 295
8.5.3 Implementation from Scratch 297
8.5.4 LeNet with Batch Normalization 298
8.5.5 Concise Implementation 299
8.5.6 Discussion 300
8.5.7 Exercises 301
8.6 Residual Networks (ResNet) and ResNeXt 302
8.6.1 Function Classes 302
8.6.2 Residual Blocks 304
8.6.3 ResNet Model 306
8.6.4 Training 308
8.6.5 ResNeXt 308
8.6.6 Summary and Discussion 310
8.6.7 Exercises 311
8.7 Densely Connected Networks (DenseNet) 312
8.7.1 From ResNet to DenseNet 312
8.7.2 Dense Blocks 313
8.7.3 Transition Layers 314
8.7.4 DenseNet Model 315
8.7.5 Training 315
8.7.6 Summary and Discussion 316
8.7.7 Exercises 316
8.8 Designing Convolution Network Architectures 317
8.8.1 The AnyNet Design Space 318
8.8.2 Distributions and Parameters of Design Spaces 320
8.8.3 RegNet 322
8.8.4 Training 323
8.8.5 Discussion 323
8.8.6 Exercises 324
x
9 Recurrent Neural Networks 325
9.1 Working with Sequences 327
9.1.1 Autoregressive Models 328
9.1.2 Sequence Models 330
9.1.3 Training 331
9.1.4 Prediction 333
9.1.5 Summary 335
9.1.6 Exercises 335
9.2 Converting Raw Text into Sequence Data 336
9.2.1 Reading the Dataset 336
9.2.2 Tokenization 337
9.2.3 Vocabulary 337
9.2.4 Putting It All Together 338
9.2.5 Exploratory Language Statistics 339
9.2.6 Summary 341
9.2.7 Exercises 342
9.3 Language Models 342
9.3.1 Learning Language Models 343
9.3.2 Perplexity 345
9.3.3 Partitioning Sequences 346
9.3.4 Summary and Discussion 347
9.3.5 Exercises 348
9.4 Recurrent Neural Networks 348
9.4.1 Neural Networks without Hidden States 349
9.4.2 Recurrent Neural Networks with Hidden States 349
9.4.3 RNN-Based Character-Level Language Models 351
9.4.4 Summary 352
9.4.5 Exercises 352
9.5 Recurrent Neural Network Implementation from Scratch 352
9.5.1 RNN Model 353
9.5.2 RNN-Based Language Model 354
9.5.3 Gradient Clipping 356
9.5.4 Training 357
9.5.5 Decoding 358
9.5.6 Summary 359
9.5.7 Exercises 359
9.6 Concise Implementation of Recurrent Neural Networks 360
9.6.1 Defining the Model 360
9.6.2 Training and Predicting 361
9.6.3 Summary 362
9.6.4 Exercises 362
9.7 Backpropagation Through Time 362
9.7.1 Analysis of Gradients in RNNs 362
9.7.2 Backpropagation Through Time in Detail 365
9.7.3 Summary 368
xi
9.7.4 Exercises 368
10 Modern Recurrent Neural Networks 369
10.1 Long Short-Term Memory (LSTM) 370
10.1.1 Gated Memory Cell 370
10.1.2 Implementation from Scratch 373
10.1.3 Concise Implementation 375
10.1.4 Summary 376
10.1.5 Exercises 376
10.2 Gated Recurrent Units (GRU) 376
10.2.1 Reset Gate and Update Gate 377
10.2.2 Candidate Hidden State 378
10.2.3 Hidden State 378
10.2.4 Implementation from Scratch 379
10.2.5 Concise Implementation 380
10.2.6 Summary 381
10.2.7 Exercises 381
10.3 Deep Recurrent Neural Networks 382
10.3.1 Implementation from Scratch 383
10.3.2 Concise Implementation 384
10.3.3 Summary 385
10.3.4 Exercises 385
10.4 Bidirectional Recurrent Neural Networks 385
10.4.1 Implementation from Scratch 387
10.4.2 Concise Implementation 387
10.4.3 Summary 388
10.4.4 Exercises 388
10.5 Machine Translation and the Dataset 388
10.5.1 Downloading and Preprocessing the Dataset 389
10.5.2 Tokenization 390
10.5.3 Loading Sequences of Fixed Length 391
10.5.4 Reading the Dataset 392
10.5.5 Summary 393
10.5.6 Exercises 394
10.6 The Encoder Decoder Architecture 394
10.6.1 Encoder 394
10.6.2 Decoder 395
10.6.3 Putting the Encoder and Decoder Together 395
10.6.4 Summary 396
10.6.5 Exercises 396
10.7 Sequence-to-Sequence Learning for Machine Translation 396
10.7.1 Teacher Forcing 397
10.7.2 Encoder 397
10.7.3 Decoder 399
10.7.4 Encoder‚ÄìDecoder for Sequence-to-Sequence Learning 400
xii
10.7.5 Loss Function with Masking 401
10.7.6 Training 401
10.7.7 Prediction 402
10.7.8 Evaluation of Predicted Sequences 403
10.7.9 Summary 404
10.7.10 Exercises 404
10.8 Beam Search 405
10.8.1 Greedy Search 405
10.8.2 Exhaustive Search 407
10.8.3 Beam Search 407
10.8.4 Summary 408
10.8.5 Exercises 408
11 Attention Mechanisms and Transformers 409
11.1 Queries, Keys, and Values 411
11.1.1 Visualization 413
11.1.2 Summary 414
11.1.3 Exercises 414
11.2 Attention Pooling by Similarity 415
11.2.1 Kernels and Data 415
11.2.2 Attention Pooling via Nadaraya‚ÄìWatson Regression 417
11.2.3 Adapting Attention Pooling 418
11.2.4 Summary 419
11.2.5 Exercises 420
11.3 Attention Scoring Functions 420
11.3.1 Dot Product Attention 421
11.3.2 Convenience Functions 421
11.3.3 Scaled Dot Product Attention 423
11.3.4 Additive Attention 424
11.3.5 Summary 426
11.3.6 Exercises 426
11.4 The Bahdanau Attention Mechanism 427
11.4.1 Model 428
11.4.2 Defining the Decoder with Attention 428
11.4.3 Training 430
11.4.4 Summary 431
11.4.5 Exercises 432
11.5 Multi-Head Attention 432
11.5.1 Model 433
11.5.2 Implementation 433
11.5.3 Summary 435
11.5.4 Exercises 435
11.6 Self-Attention and Positional Encoding 435
11.6.1 Self-Attention 436
11.6.2 Comparing CNNs, RNNs, and Self-Attention 436
xiii
11.6.3 Positional Encoding 437
11.6.4 Summary 440
11.6.5 Exercises 440
11.7 The Transformer Architecture 440
11.7.1 Model 441
11.7.2 Positionwise Feed-Forward Networks 442
11.7.3 Residual Connection and Layer Normalization 443
11.7.4 Encoder 444
11.7.5 Decoder 445
11.7.6 Training 447
11.7.7 Summary 451
11.7.8 Exercises 451
11.8 Transformers for Vision 451
11.8.1 Model 452
11.8.2 Patch Embedding 453
11.8.3 Vision Transformer Encoder 453
11.8.4 Putting It All Together 454
11.8.5 Training 455
11.8.6 Summary and Discussion 455
11.8.7 Exercises 456
11.9 Large-Scale Pretraining with Transformers 456
11.9.1 Encoder-Only 457
11.9.2 Encoder‚ÄìDecoder 459
11.9.3 Decoder-Only 461
11.9.4 Scalability 463
11.9.5 Large Language Models 465
11.9.6 Summary and Discussion 466
11.9.7 Exercises 467
12 Optimization Algorithms 468
12.1 Optimization and Deep Learning 468
12.1.1 Goal of Optimization 469
12.1.2 Optimization Challenges in Deep Learning 469
12.1.3 Summary 473
12.1.4 Exercises 473
12.2 Convexity 474
12.2.1 Definitions 474
12.2.2 Properties 476
12.2.3 Constraints 479
12.2.4 Summary 481
12.2.5 Exercises 482
12.3 Gradient Descent 482
12.3.1 One-Dimensional Gradient Descent 482
12.3.2 Multivariate Gradient Descent 486
12.3.3 Adaptive Methods 488
xiv
12.3.4 Summary 492
12.3.5 Exercises 492
12.4 Stochastic Gradient Descent 493
12.4.1 Stochastic Gradient Updates 493
12.4.2 Dynamic Learning Rate 495
12.4.3 Convergence Analysis for Convex Objectives 496
12.4.4 Stochastic Gradients and Finite Samples 498
12.4.5 Summary 499
12.4.6 Exercises 499
12.5 Minibatch Stochastic Gradient Descent 500
12.5.1 Vectorization and Caches 500
12.5.2 Minibatches 503
12.5.3 Reading the Dataset 504
12.5.4 Implementation from Scratch 504
12.5.5 Concise Implementation 507
12.5.6 Summary 509
12.5.7 Exercises 509
12.6 Momentum 510
12.6.1 Basics 510
12.6.2 Practical Experiments 514
12.6.3 Theoretical Analysis 516
12.6.4 Summary 518
12.6.5 Exercises 519
12.7 Adagrad 519
12.7.1 Sparse Features and Learning Rates 519
12.7.2 Preconditioning 520
12.7.3 The Algorithm 521
12.7.4 Implementation from Scratch 523
12.7.5 Concise Implementation 524
12.7.6 Summary 524
12.7.7 Exercises 525
12.8 RMSProp 525
12.8.1 The Algorithm 526
12.8.2 Implementation from Scratch 526
12.8.3 Concise Implementation 528
12.8.4 Summary 528
12.8.5 Exercises 529
12.9 Adadelta 529
12.9.1 The Algorithm 529
12.9.2 Implementation 530
12.9.3 Summary 531
12.9.4 Exercises 532
12.10 Adam 532
12.10.1 The Algorithm 532
12.10.2 Implementation 533
xv
12.10.3 Yogi 534
12.10.4 Summary 535
12.10.5 Exercises 536
12.11 Learning Rate Scheduling 536
12.11.1 Toy Problem 537
12.11.2 Schedulers 539
12.11.3 Policies 540
12.11.4 Summary 545
12.11.5 Exercises 545
13 Computational Performance 547
13.1 Compilers and Interpreters 547
13.1.1 Symbolic Programming 548
13.1.2 Hybrid Programming 549
13.1.3 Hybridizing the Sequential Class 550
13.1.4 Summary 552
13.1.5 Exercises 552
13.2 Asynchronous Computation 552
13.2.1 Asynchrony via Backend 553
13.2.2 Barriers and Blockers 554
13.2.3 Improving Computation 555
13.2.4 Summary 555
13.2.5 Exercises 555
13.3 Automatic Parallelism 555
13.3.1 Parallel Computation on GPUs 556
13.3.2 Parallel Computation and Communication 557
13.3.3 Summary 558
13.3.4 Exercises 559
13.4 Hardware 559
13.4.1 Computers 560
13.4.2 Memory 561
13.4.3 Storage 562
13.4.4 CPUs 563
13.4.5 GPUs and other Accelerators 566
13.4.6 Networks and Buses 569
13.4.7 More Latency Numbers 570
13.4.8 Summary 571
13.4.9 Exercises 571
13.5 Training on Multiple GPUs 572
13.5.1 Splitting the Problem 573
13.5.2 Data Parallelism 574
13.5.3 A Toy Network 575
13.5.4 Data Synchronization 576
13.5.5 Distributing Data 577
13.5.6 Training 578
xvi
13.5.7 Summary 580
13.5.8 Exercises 580
13.6 Concise Implementation for Multiple GPUs 581
13.6.1 A Toy Network 581
13.6.2 Network Initialization 582
13.6.3 Training 582
13.6.4 Summary 583
13.6.5 Exercises 584
13.7 Parameter Servers 584
13.7.1 Data-Parallel Training 584
13.7.2 Ring Synchronization 586
13.7.3 Multi-Machine Training 588
13.7.4 Key‚ÄìValue Stores 589
13.7.5 Summary 591
13.7.6 Exercises 591
14 Computer Vision 592
14.1 Image Augmentation 592
14.1.1 Common Image Augmentation Methods 593
14.1.2 Training with Image Augmentation 596
14.1.3 Summary 599
14.1.4 Exercises 599
14.2 Fine-Tuning 600
14.2.1 Steps 600
14.2.2 Hot Dog Recognition 601
14.2.3 Summary 605
14.2.4 Exercises 606
14.3 Object Detection and Bounding Boxes 606
14.3.1 Bounding Boxes 607
14.3.2 Summary 609
14.3.3 Exercises 609
14.4 Anchor Boxes 609
14.4.1 Generating Multiple Anchor Boxes 610
14.4.2 Intersection over Union (IoU) 612
14.4.3 Labeling Anchor Boxes in Training Data 613
14.4.4 PredictingBoundingBoxeswithNon-MaximumSuppression 619
14.4.5 Summary 622
14.4.6 Exercises 623
14.5 Multiscale Object Detection 623
14.5.1 Multiscale Anchor Boxes 623
14.5.2 Multiscale Detection 625
14.5.3 Summary 626
14.5.4 Exercises 626
14.6 The Object Detection Dataset 627
14.6.1 Downloading the Dataset 627
xvii
14.6.2 Reading the Dataset 627
14.6.3 Demonstration 629
14.6.4 Summary 629
14.6.5 Exercises 630
14.7 Single Shot Multibox Detection 630
14.7.1 Model 630
14.7.2 Training 636
14.7.3 Prediction 638
14.7.4 Summary 639
14.7.5 Exercises 640
14.8 Region-based CNNs (R-CNNs) 642
14.8.1 R-CNNs 642
14.8.2 Fast R-CNN 643
14.8.3 Faster R-CNN 645
14.8.4 Mask R-CNN 646
14.8.5 Summary 647
14.8.6 Exercises 647
14.9 Semantic Segmentation and the Dataset 648
14.9.1 Image Segmentation and Instance Segmentation 648
14.9.2 The Pascal VOC2012 Semantic Segmentation Dataset 648
14.9.3 Summary 654
14.9.4 Exercises 654
14.10 Transposed Convolution 654
14.10.1 Basic Operation 654
14.10.2 Padding, Strides, and Multiple Channels 656
14.10.3 Connection to Matrix Transposition 657
14.10.4 Summary 659
14.10.5 Exercises 659
14.11 Fully Convolutional Networks 659
14.11.1 The Model 660
14.11.2 Initializing Transposed Convolutional Layers 662
14.11.3 Reading the Dataset 663
14.11.4 Training 664
14.11.5 Prediction 664
14.11.6 Summary 666
14.11.7 Exercises 666
14.12 Neural Style Transfer 666
14.12.1 Method 666
14.12.2 Reading the Content and Style Images 668
14.12.3 Preprocessing and Postprocessing 668
14.12.4 Extracting Features 669
14.12.5 Defining the Loss Function 670
14.12.6 Initializing the Synthesized Image 672
14.12.7 Training 673
14.12.8 Summary 674
xviii
14.12.9 Exercises 674
14.13 Image Classification (CIFAR-10) on Kaggle 674
14.13.1 Obtaining and Organizing the Dataset 675
14.13.2 Image Augmentation 678
14.13.3 Reading the Dataset 678
14.13.4 Defining the Model 679
14.13.5 Defining the Training Function 679
14.13.6 Training and Validating the Model 680
14.13.7 Classifying the Testing Set and Submitting Results on Kaggle 680
14.13.8 Summary 681
14.13.9 Exercises 682
14.14 Dog Breed Identification (ImageNet Dogs) on Kaggle 682
14.14.1 Obtaining and Organizing the Dataset 682
14.14.2 Image Augmentation 684
14.14.3 Reading the Dataset 685
14.14.4 Fine-Tuning a Pretrained Model 685
14.14.5 Defining the Training Function 686
14.14.6 Training and Validating the Model 687
14.14.7 Classifying the Testing Set and Submitting Results on Kaggle 688
14.14.8 Summary 688
14.14.9 Exercises 689
15 Natural Language Processing: Pretraining 690
15.1 Word Embedding (word2vec) 691
15.1.1 One-Hot Vectors Are a Bad Choice 691
15.1.2 Self-Supervised word2vec 691
15.1.3 The Skip-Gram Model 692
15.1.4 The Continuous Bag of Words (CBOW) Model 694
15.1.5 Summary 695
15.1.6 Exercises 695
15.2 Approximate Training 696
15.2.1 Negative Sampling 696
15.2.2 Hierarchical Softmax 698
15.2.3 Summary 699
15.2.4 Exercises 699
15.3 The Dataset for Pretraining Word Embeddings 699
15.3.1 Reading the Dataset 699
15.3.2 Subsampling 700
15.3.3 Extracting Center Words and Context Words 702
15.3.4 Negative Sampling 703
15.3.5 Loading Training Examples in Minibatches 704
15.3.6 Putting It All Together 705
15.3.7 Summary 706
15.3.8 Exercises 706
15.4 Pretraining word2vec 707
xix
15.4.1 The Skip-Gram Model 707
15.4.2 Training 708
15.4.3 Applying Word Embeddings 711
15.4.4 Summary 711
15.4.5 Exercises 711
15.5 Word Embedding with Global Vectors (GloVe) 711
15.5.1 Skip-Gram with Global Corpus Statistics 712
15.5.2 The GloVe Model 713
15.5.3 Interpreting GloVe from the Ratio of Co-occurrence
Probabilities 713
15.5.4 Summary 715
15.5.5 Exercises 715
15.6 Subword Embedding 715
15.6.1 The fastText Model 715
15.6.2 Byte Pair Encoding 716
15.6.3 Summary 719
15.6.4 Exercises 719
15.7 Word Similarity and Analogy 720
15.7.1 Loading Pretrained Word Vectors 720
15.7.2 Applying Pretrained Word Vectors 722
15.7.3 Summary 724
15.7.4 Exercises 724
15.8 Bidirectional Encoder Representations from Transformers (BERT) 724
15.8.1 From Context-Independent to Context-Sensitive 724
15.8.2 From Task-Specific to Task-Agnostic 725
15.8.3 BERT: Combining the Best of Both Worlds 725
15.8.4 Input Representation 726
15.8.5 Pretraining Tasks 728
15.8.6 Putting It All Together 731
15.8.7 Summary 732
15.8.8 Exercises 733
15.9 The Dataset for Pretraining BERT 733
15.9.1 Defining Helper Functions for Pretraining Tasks 734
15.9.2 Transforming Text into the Pretraining Dataset 736
15.9.3 Summary 738
15.9.4 Exercises 739
15.10 Pretraining BERT 739
15.10.1 Pretraining BERT 739
15.10.2 Representing Text with BERT 741
15.10.3 Summary 742
15.10.4 Exercises 743
16 Natural Language Processing: Applications 744
16.1 Sentiment Analysis and the Dataset 745
16.1.1 Reading the Dataset 745
xx
16.1.2 Preprocessing the Dataset 746
16.1.3 Creating Data Iterators 747
16.1.4 Putting It All Together 747
16.1.5 Summary 748
16.1.6 Exercises 748
16.2 Sentiment Analysis: Using Recurrent Neural Networks 748
16.2.1 Representing Single Text with RNNs 749
16.2.2 Loading Pretrained Word Vectors 750
16.2.3 Training and Evaluating the Model 751
16.2.4 Summary 751
16.2.5 Exercises 752
16.3 Sentiment Analysis: Using Convolutional Neural Networks 752
16.3.1 One-Dimensional Convolutions 753
16.3.2 Max-Over-Time Pooling 754
16.3.3 The textCNN Model 755
16.3.4 Summary 758
16.3.5 Exercises 758
16.4 Natural Language Inference and the Dataset 759
16.4.1 Natural Language Inference 759
16.4.2 The Stanford Natural Language Inference (SNLI) Dataset 760
16.4.3 Summary 763
16.4.4 Exercises 763
16.5 Natural Language Inference: Using Attention 763
16.5.1 The Model 764
16.5.2 Training and Evaluating the Model 768
16.5.3 Summary 770
16.5.4 Exercises 770
16.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications 771
16.6.1 Single Text Classification 771
16.6.2 Text Pair Classification or Regression 772
16.6.3 Text Tagging 773
16.6.4 Question Answering 773
16.6.5 Summary 774
16.6.6 Exercises 774
16.7 Natural Language Inference: Fine-Tuning BERT 775
16.7.1 Loading Pretrained BERT 775
16.7.2 The Dataset for Fine-Tuning BERT 776
16.7.3 Fine-Tuning BERT 778
16.7.4 Summary 779
16.7.5 Exercises 779
17 Reinforcement Learning 781
17.1 Markov Decision Process (MDP) 782
17.1.1 Definition of an MDP 782
17.1.2 Return and Discount Factor 783
xxi
17.1.3 Discussion of the Markov Assumption 784
17.1.4 Summary 785
17.1.5 Exercises 785
17.2 Value Iteration 785
17.2.1 Stochastic Policy 785
17.2.2 Value Function 786
17.2.3 Action-Value Function 786
17.2.4 Optimal Stochastic Policy 787
17.2.5 Principle of Dynamic Programming 787
17.2.6 Value Iteration 788
17.2.7 Policy Evaluation 788
17.2.8 Implementation of Value Iteration 789
17.2.9 Summary 790
17.2.10 Exercises 791
17.3 Q-Learning 791
17.3.1 The Q-Learning Algorithm 791
17.3.2 An Optimization Problem Underlying Q-Learning 791
17.3.3 Exploration in Q-Learning 793
17.3.4 The ‚ÄúSelf-correcting‚Äù Property of Q-Learning 793
17.3.5 Implementation of Q-Learning 794
17.3.6 Summary 795
17.3.7 Exercises 796
18 Gaussian Processes 797
18.1 Introduction to Gaussian Processes 798
18.1.1 Summary 807
18.1.2 Exercises 808
18.2 Gaussian Process Priors 809
18.2.1 Definition 809
18.2.2 A Simple Gaussian Process 810
18.2.3 From Weight Space to Function Space 811
18.2.4 The Radial Basis Function (RBF) Kernel 811
18.2.5 The Neural Network Kernel 813
18.2.6 Summary 814
18.2.7 Exercises 814
18.3 Gaussian Process Inference 815
18.3.1 Posterior Inference for Regression 815
18.3.2 Equations for Making Predictions and Learning Kernel
Hyperparameters in GP Regression 817
18.3.3 Interpreting Equations for Learning and Predictions 817
18.3.4 Worked Example from Scratch 818
18.3.5 Making Life Easy with GPyTorch 822
18.3.6 Summary 825
18.3.7 Exercises 826
xxii
19 Hyperparameter Optimization 828
19.1 What Is Hyperparameter Optimization? 828
19.1.1 The Optimization Problem 829
19.1.2 Random Search 832
19.1.3 Summary 834
19.1.4 Exercises 835
19.2 Hyperparameter Optimization API 836
19.2.1 Searcher 836
19.2.2 Scheduler 837
19.2.3 Tuner 837
19.2.4 Bookkeeping the Performance of HPO Algorithms 838
19.2.5 Example: Optimizing the Hyperparameters of a Convolu-
tional Neural Network 839
19.2.6 Comparing HPO Algorithms 841
19.2.7 Summary 842
19.2.8 Exercises 842
19.3 Asynchronous Random Search 843
19.3.1 Objective Function 844
19.3.2 Asynchronous Scheduler 845
19.3.3 Visualize the Asynchronous Optimization Process 851
19.3.4 Summary 852
19.3.5 Exercises 853
19.4 Multi-Fidelity Hyperparameter Optimization 853
19.4.1 Successive Halving 855
19.4.2 Summary 866
19.5 Asynchronous Successive Halving 867
19.5.1 Objective Function 869
19.5.2 Asynchronous Scheduler 870
19.5.3 Visualize the Optimization Process 879
19.5.4 Summary 879
20 Generative Adversarial Networks 880
20.1 Generative Adversarial Networks 880
20.1.1 Generate Some ‚ÄúReal‚Äù Data 882
20.1.2 Generator 883
20.1.3 Discriminator 883
20.1.4 Training 883
20.1.5 Summary 885
20.1.6 Exercises 885
20.2 Deep Convolutional Generative Adversarial Networks 886
20.2.1 The Pokemon Dataset 886
20.2.2 The Generator 887
20.2.3 Discriminator 889
20.2.4 Training 891
20.2.5 Summary 892
xxiii
xxiv Contents
20.2.6 Exercises 892
21 Recommender Systems 893
21.1 Overview of Recommender Systems 893
21.1.1 Collaborative Filtering 894
21.1.2 Explicit Feedback and Implicit Feedback 895
21.1.3 Recommendation Tasks 895
21.1.4 Summary 895
21.1.5 Exercises 895
Appendix A Mathematics for Deep Learning 897
Appendix B Tools for Deep Learning 1035
References 1089
Preface
Just a few years ago, there were no legions of deep learning scientists developing intelli-
gent products and services at major companies and startups. When we entered the field,
machinelearningdidnotcommandheadlinesindailynewspapers. Ourparentshadnoidea
whatmachinelearningwas, letalonewhywemightpreferittoacareerinmedicineorlaw.
Machine learning was a blue skies academic discipline whose industrial significance was
limited to a narrow set of real-world applications, including speech recognition and com-
puter vision. Moreover, many of these applications required so much domain knowledge
that they were often regarded as entirely separate areas for which machine learning was
one small component. At that time, neural networks‚Äîthe predecessors of the deep learn-
ing methods that we focus on in this book‚Äîwere generally regarded as outmoded.
Yet in just few years, deep learning has taken the world by surprise, driving rapid progress
in such diverse fields as computer vision, natural language processing, automatic speech
recognition, reinforcement learning, and biomedical informatics. Moreover, the success
of deep learning in so many tasks of practical interest has even catalyzed developments in
theoreticalmachinelearningandstatistics. Withtheseadvancesinhand, wecannowbuild
cars that drive themselves with more autonomy than ever before (though less autonomy
thansomecompaniesmighthaveyoubelieve),dialoguesystemsthatdebugcodebyasking
clarifying questions, and software agents beating the best human players in the world at
boardgamessuchas Go, a featonce thought tobe decades away. Already, thesetools exert
ever-wider influence on industry and society, changing the way movies are made, diseases
are diagnosed, and playing a growing role in basic sciences‚Äîfrom astrophysics, to climate
modeling, to weather prediction, to biomedicine.
AboutThis Book
This book represents our attempt to make deep learning approachable, teaching you the
concepts , thecontext, and thecode.
One Medium Combining Code, Math, and HTML
For any computing technology to reach its full impact, it must be well understood, well
documented, and supported by mature, well-maintained tools. The key ideas should be
clearly distilled, minimizing the onboarding time needed to bring new practitioners up to
xxv
xxvi Preface
1
2date. Mature libraries should automate common tasks, and exemplar code should make
it easy for practitioners to modify, apply, and extend common applications to suit their
needs.
As an example, take dynamic web applications. Despite a large number of companies,
such as Amazon, developing successful database-driven web applications in the 1990s, the
potentialofthistechnologytoaidcreativeentrepreneurswasrealizedtoafargreaterdegree
only in the past ten years, owing in part to the development of powerful, well-documented
frameworks.
Testing the potential of deep learning presents unique challenges because any single appli-
cationbringstogethervariousdisciplines. Applyingdeeplearningrequiressimultaneously
understanding (i) the motivations for casting a problem in a particular way; (ii) the math-
ematical form of a given model; (iii) the optimization algorithms for fitting the models to
data;(iv)thestatisticalprinciplesthattelluswhenweshouldexpectourmodelstogeneral-
ize to unseen data and practical methods for certifying that they have, in fact, generalized;
and (v) the engineering techniques required to train models efficiently, navigating the pit-
falls of numerical computing and getting the most out of available hardware. Teaching the
critical thinking skills required to formulate problems, the mathematics to solve them, and
the software tools to implement those solutions all in one place presents formidable chal-
lenges. Ourgoalinthisbookistopresentaunifiedresourcetobringwould-bepractitioners
up to speed.
Whenwestartedthisbookproject,therewerenoresourcesthatsimultaneously(i)remained
up to date; (ii) covered the breadth of modern machine learning practices with sufficient
technical depth; and (iii) interleaved exposition of the quality one expects of a textbook
with the clean runnable code that one expects of a hands-on tutorial. We found plenty of
code examples illustrating how to use a given deep learning framework (e.g., how to do
basic numerical computing with matrices in TensorFlow) or for implementing particular
techniques (e.g., code snippets for LeNet, AlexNet, ResNet, etc.) scattered across various
blog posts and GitHub repositories. However, these examples typically focused on howto
implement a given approach, but left out the discussion of whycertain algorithmic deci-
sionsaremade. Whilesomeinteractiveresourceshavepoppedupsporadicallytoaddressa
particulartopic,e.g.,theengagingblogpostspublishedonthewebsite Distill1,orpersonal
blogs,theyonlycoveredselectedtopicsindeeplearning, andoftenlackedassociatedcode.
On the other hand, while several deep learning textbooks have emerged‚Äîe.g., Goodfellow
et al.(2016), which offers a comprehensive survey on the basics of deep learning‚Äîthese
resources do not marry the descriptions to realizations of the concepts in code, sometimes
leaving readers clueless as to how to implement them. Moreover, too many resources are
hidden behind the paywalls of commercial course providers.
Wesetouttocreatearesourcethatcould(i)befreelyavailableforeveryone; (ii)offersuffi-
cienttechnicaldepthtoprovideastartingpointonthepathtoactuallybecominganapplied
machine learning scientist; (iii) include runnable code, showing readers howto solve prob-
lemsinpractice;(iv)allowforrapidupdates,bothbyusandalsobythecommunityatlarge;
and (v) be complemented by a forum2for interactive discussion of technical details and to
answer questions.
xxvii Preface
Thesegoalswereofteninconflict. Equations,theorems,andcitationsarebestmanagedand
laid out in LaTeX. Code is best described in Python. And webpages are native in HTML
andJavaScript. Furthermore,wewantthecontenttobeaccessiblebothasexecutablecode,
asaphysicalbook,asadownloadablePDF,andontheInternetasawebsite. Noworkflows
seemed suited to these demands, so we decided to assemble our own ( Section B.6 ). We
settled on GitHub to share the source and to facilitate community contributions; Jupyter
notebooksformixingcode,equationsandtext;Sphinxasarenderingengine;andDiscourse
asadiscussionplatform. Whileoursystemisnotperfect,thesechoicesstrikeacompromise
amongthecompetingconcerns. Webelievethat DiveintoDeepLearning mightbethefirst
book published using such an integrated workflow.
LearningbyDoing
Many textbooks present concepts in succession, covering each in exhaustive detail. For
example, the excellent textbook of Bishop ( 2006), teaches each topic so thoroughly that
getting to the chapter on linear regression requires a nontrivial amount of work. While
expertslovethisbookpreciselyforitsthoroughness,fortruebeginners,thispropertylimits
its usefulness as an introductory text.
In this book, we teach most concepts just in time . In other words, you will learn concepts
at the very moment that they are needed to accomplish some practical end. While we
take some time at the outset to teach fundamental preliminaries, like linear algebra and
probability,wewantyoutotastethesatisfactionoftrainingyourfirstmodelbeforeworrying
about more esoteric concepts.
Aside from a few preliminary notebooks that provide a crash course in the basic mathe-
matical background, each subsequent chapter both introduces a reasonable number of new
concepts and provides several self-contained working examples, using real datasets. This
presented an organizational challenge. Some models might logically be grouped together
in a single notebook. And some ideas might be best taught by executing several models
in succession. By contrast, there is a big advantage to adhering to a policy of one working
example,onenotebook : Thismakesitaseasyaspossibleforyoutostartyourownresearch
projects by leveraging our code. Just copy a notebook and start modifying it.
Throughout, we interleave the runnable code with background material as needed. In gen-
eral, we err on the side of making tools available before explaining them fully (often filling
in the background later). For instance, we might use stochastic gradient descent before
explaining why it is useful or offering some intuition for why it works. This helps to give
practitionersthenecessaryammunitiontosolveproblemsquickly, attheexpenseofrequir-
ing the reader to trust us with some curatorial decisions.
This book teaches deep learning concepts from scratch. Sometimes, we delve into fine
details about models that would typically be hidden from users by modern deep learning
frameworks. This comes up especially in the basic tutorials, where we want you to un-
derstand everything that happens in a given layer or optimizer. In these cases, we often
present two versions of the example: one where we implement everything from scratch,
relying only on NumPy-like functionality and automatic differentiation, and a more prac-
xxviii Preface
tical example, where we write succinct code using the high-level APIs of deep learning
frameworks. After explaining how some component works, we rely on the high-level API
in subsequent tutorials.
Content and Structure
Thebookcanbedividedintoroughlythreeparts,dealingwithpreliminaries,deeplearning
techniques, and advanced topics focused on real systems and applications ( Fig. 1).
tFig. 1 Book structure.
Part1: BasicsandPreliminaries .Chapter 1 is an introduction to deep learning. Then,
inChapter2 ,wequicklybringyouuptospeedontheprerequisitesrequiredforhands-
on deep learning, such as how to store and manipulate data, and how to apply vari-
ousnumericaloperationsbasedonelementaryconceptsfromlinearalgebra, calculus,
and probability. Chapter 3 andChapter 5 cover the most fundamental concepts and
techniques in deep learning, including regression and classification; linear models;
multilayer perceptrons; and overfitting and regularization.
Part 2: Modern Deep Learning Techniques .Chapter 6 describes the key computa-
tional components of deep learning systems and lays the groundwork for our sub-
sequent implementations of more complex models. Next, Chapter 7 andChapter 8
present convolutional neural networks (CNNs), powerful tools that form the back-
bone of most modern computer vision systems. Similarly, Chapter 9 andChapter 10
introducerecurrentneuralnetworks(RNNs),modelsthatexploitsequential(e.g.,tem-
poral) structure in data and are commonly used for natural language processing and
time series prediction. In Chapter 11 , we describe a relatively new class of models,
based on so-called attention mechanisms , that has displaced RNNs as the dominant
architecture for most natural language processing tasks. These sections will bring
you up to speed on the most powerful and general tools that are widely used by deep
learning practitioners.
xxix Preface
3Part3: Scalability,EÔ¨Äiciency,andApplications (available online3). InChapter12,we
discuss several common optimization algorithms used to train deep learning models.
Next, in Chapter 13, we examine several key factors that influence the computational
performance of deep learning code. Then, in Chapter 14, we illustrate major applica-
tions of deep learning in computer vision. Finally, in Chapter 15 and Chapter 16, we
demonstratehowtopretrainlanguagerepresentationmodelsandapplythemtonatural
language processing tasks.
Code
Mostsectionsofthisbookfeatureexecutablecode. Webelievethatsomeintuitionsarebest
developed via trial and error, tweaking the code in small ways and observing the results.
Ideally, an elegant mathematical theory might tell us precisely how to tweak our code to
achieveadesiredresult. However,deeplearningpractitionerstodaymustoftentreadwhere
no solid theory provides guidance. Despite our best attempts, formal explanations for the
efficacy of various techniques are still lacking, for a variety of reasons: the mathematics to
characterize these models can be so difficult; the explanation likely depends on properties
of the data that currently lack clear definitions; and serious inquiry on these topics has
only recently kicked into high gear. We are hopeful that as the theory of deep learning
progresses,eachfutureeditionofthisbookwillprovideinsightsthateclipsethosepresently
available.
Toavoidunnecessaryrepetition,wecapturesomeofourmostfrequentlyimportedandused
functions and classes in the d2lpackage. Throughout, we mark blocks of code (such as
functions,classes,orcollectionofimportstatements)with #@savetoindicatethattheywill
be accessed later via the d2lpackage. We offer a detailed overview of these classes and
functions in Section B.8 . The d2lpackage is lightweight and only requires the following
dependencies:
#@save
import collections
import hashlib
import inspect
import math
import os
import random
import re
import shutil
import sys
import tarfile
import time
import zipfile
from collections import defaultdict
import pandas aspd
import requests
from IPython import display
from matplotlib import pyplot asplt
from matplotlib_inline import backend_inline
d2l =sys.modules[ __name__ ]
xxx Preface
4
5
6
7
8
9
10Most of the code in this book is based on PyTorch, a popular open-source framework that
has been enthusiastically embraced by the deep learning research community. All of the
code in this book has passed tests under the latest stable version of PyTorch. However, due
to the rapid development of deep learning, some code in the print edition may not work
properly in future versions of PyTorch. We plan to keep the online version up to date.
In case you encounter any problems, please consult Installation (page xxxiv) to update
your code and runtime environment. Below lists dependencies in our PyTorch implemen-
tation.
#@save
import numpy asnp
import torch
import torchvision
from PIL import Image
from scipy .spatial import distance_matrix
from torch import nn
from torch .nnimport functional asF
from torchvision import transforms
TargetAudience
Thisbookisforstudents(undergraduateorgraduate),engineers,andresearchers,whoseek
a solid grasp of the practical techniques of deep learning. Because we explain every con-
ceptfromscratch,nopreviousbackgroundindeeplearningormachinelearningisrequired.
Fully explaining the methods of deep learning requires some mathematics and program-
ming, but we will only assume that you enter with some basics, including modest amounts
of linear algebra, calculus, probability, and Python programming. Just in case you have
forgotten anything, the online Appendix4provides a refresher on most of the mathematics
youwillfindinthisbook. Usually, wewillprioritizeintuitionandideasovermathematical
rigor. If you would like to extend these foundations beyond the prerequisites to understand
our book, we happily recommend some other terrific resources: Linear Analysis by Bol-
lob√°s (1999) covers linear algebra and functional analysis in great depth. All of Statistics
(Wasserman, 2013 ) provides a marvelous introduction to statistics. Joe Blitzstein‚Äôs books5
andcourses6on probability and inference are pedagogical gems. And if you have not used
Python before, you may want to peruse this Python tutorial7.
Notebooks,Website,GitHub, and Forum
All of our notebooks are available for download on the D2L.ai website8and onGitHub9.
Associated with this book, we have launched a discussion forum, located at discuss.d2l.ai
10. Whenever you have questions on any section of the book, you can find a link to the
associated discussion page at the end of each notebook.
xxxi Preface
Acknowledgments
WeareindebtedtothehundredsofcontributorsforboththeEnglishandtheChinesedrafts.
They helped improve the content and offered valuable feedback. This book was originally
implemented with MXNet as the primary framework. We thank Anirudh Dagar and Yuan
Tang for adapting a majority part of earlier MXNet code into PyTorch and TensorFlow im-
plementations, respectively. Since July 2021, we have redesigned and reimplemented this
book in PyTorch, MXNet, and TensorFlow, choosing PyTorch as the primary framework.
We thank Anirudh Dagar for adapting a majority part of more recent PyTorch code into
JAX implementations. We thank Gaosheng Wu, Liujun Hu, Ge Zhang, and Jiehang Xie
from Baidu for adapting a majority part of more recent PyTorch code into PaddlePaddle
implementations in the Chinese draft. We thank Shuai Zhang for integrating the LaTeX
style from the press into the PDF building.
On GitHub, we thank every contributor of this English draft for making it better for ev-
eryone. Their GitHub IDs or names are (in no particular order): alxnorden, avinashingit,
bowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarro-
man, gkutiel, JohnMitro, LiangPu, RahulAgarwal, MohamedAliJamaoui, Michael(Stu)
Stewart, Mike M√ºller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sun-
deepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB,
Yuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen,
pmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Ab-
hishek Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Bud-
dareddygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete L√ºer, Surbhi
Vijayvargeeya, Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pe-
dro Larroy, lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gard-
ner, Maximilian B√∂ther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong,
Steve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas, ati-ozgur,
qbaza, dchoi77, Adam Gerson, Phuc Le, Mark Atwood, christabella, vn09, Haibin Lin,
jjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil, Peter
Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep
Vu,sfilip,mlxd,Kale-abTessera,SanjarAdilov,MatteoFerrara,hsneto,KatarzynaBiesial-
ska, Gregory Bruss, Duy‚ÄìThanh Doan, paulaurel, graytowne, Duc Pham, sl7423, Jaedong
Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums, Cuong V.
Nguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic,
the-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, bia-
giom, abhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang
Kang, StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl,
SportsTHU,HoaNguyen,manuel-arno-korfmann-webentwicklung,aterzis-personal,nxby,
Xiaoting He, Josiah Yoder, mathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi,
vkramdev,simonwardjones,LakshKD,TalNeoran,djliden,Nikhil95,OrenBarkan,guoweis,
haozhu233, pratikhack, YueYing, tayfununal, steinsag, charleybeller, AndrewLumsdaine,
Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates, Adriaan Tijsseling, Ron
xxxii Preface
11Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua Broyde, jake221,
jonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hongshen Yang, Vin-
ney Cavallo, yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald, Shivam Upad-
hyay, Quanshangze Du, Biswajit Sahoo, Parthe Pandit, Ishan Kumar, HomunculusK, Lane
Schwartz,varadgunjal,JasonWiener,ArminGholampoor,Shreshtha13,eigen-arnav,Hyeong-
gyu Kim, EmilyOng, B√°lint Mucs√°nyi, Chase DuBois, Juntian Tao, Wenxiang Xu, Lifu
Huang, filevich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov, Francesco
‚ÄúFuma‚ÄùFumagalli,PeilinSun,VincentGurgul,qingfengtommy,JanmeyShukla,MoShan,
KaanSancak,regob,AlexSauer,GopalakrishnaRamachandra,TobiasUelwer,ChaoWang,
TianCao,NicolasCorthorn,akash5474,kxxt,zxydi1992,JacobBritton,ShuangchiHe,zh-
mou, krahets, Jie-Han Chen, Atishay Garg, Marcel Flygare, adtygan, Nik Vaessen, bolded,
LouisSchlessinger,BalajiVaratharajan,atgctg,KaixinLi,VictorBarbaros,RiccardoMusto,
Elizabeth Ho, azimjonn, Guilherme Miotto, Alessandro Finamore, Joji Joseph, Anthony
Biel,ZemingZhao,shjustinbaek,gab-chen,nantekoto,YutaroNishiyama,OrenAmsalem,
Tian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen, Matthew
Caseres, Andrew Walsh, pggPL, RohanKarthikeyan, Ryan Choi, and Likun Lei.
We thank Amazon Web Services, especially Wen-Ming Ye, George Karypis, Swami Siva-
subramanian,PeterDeSantis,AdamSelipsky,andAndrewJassyfortheirgeneroussupport
in writing this book. Without the available time, resources, discussions with colleagues,
and continuous encouragement, this book would not have happened. During the prepara-
tion of the book for publication, Cambridge University Press has offered excellent support.
WethankourcommissioningeditorDavidTranahforhishelpandprofessionalism.
Summary
Deep learning has revolutionized pattern recognition, introducing technology that now
powers a wide range of technologies, in such diverse fields as computer vision, natural
language processing, and automatic speech recognition. To successfully apply deep learn-
ing, you must understand how to cast a problem, the basic mathematics of modeling, the
algorithms for fitting your models to data, and the engineering techniques to implement it
all. This book presents a comprehensive resource, including prose, figures, mathematics,
and code, all in one place.
Exercises
1.Register an account on the discussion forum of this book discuss.d2l.ai11.
2.Install Python on your computer.
xxxiii Preface
123.Follow the links at the bottom of the section to the forum, where you will be able to
seek out help and discuss the book and find answers to your questions by engaging the
authors and broader community.
Discussions12.
13
14
Installation
Inordertogetupandrunning,wewillneedanenvironmentforrunningPython,theJupyter
Notebook, the relevant libraries, and the code needed to run the book itself.
Installing Miniconda
Yoursimplestoptionistoinstall Miniconda13. NotethatthePython3.xversionisrequired.
You can skip the following steps if your machine already has conda installed.
Visit the Miniconda website and determine the appropriate version for your system based
on your Python 3.x version and machine architecture. Suppose that your Python version is
3.9(ourtestedversion). IfyouareusingmacOS,youwoulddownloadthebashscriptwhose
name contains the strings ‚ÄúMacOSX‚Äù, navigate to the download location, and execute the
installation as follows (taking Intel Macs as an example):
# The file name is subject to changes
shMiniconda3-py39_4.12.0-MacOSX-x86_64.sh -b
ALinuxuserwoulddownloadthefilewhosenamecontainsthestrings‚ÄúLinux‚Äùandexecute
the following at the download location:
# The file name is subject to changes
shMiniconda3-py39_4.12.0-Linux-x86_64.sh -b
AWindowsuserwoulddownloadandinstallMinicondabyfollowingits onlineinstructions
14. On Windows, you may search for cmdto open the Command Prompt (command-line
interpreter) for running commands.
Next, initialize the shell so we can run condadirectly.
~/miniconda3/bin/conda init
Then close and reopen your current shell. You should be able to create a new environment
as follows:
xxxiv
xxxv Installation
15
16conda create --name d2l python =3.9-y
Now we can activate the d2lenvironment:
conda activate d2l
Installing the Deep Learning Frameworkand the
d2lPackage
Before installing any deep learning framework, please first check whether or not you have
proper GPUs on your machine (the GPUs that power the display on a standard laptop are
not relevant for our purposes). For example, if your computer has NVIDIA GPUs and has
installed CUDA15, then you are all set. If your machine does not house any GPU, there
is no need to worry just yet. Your CPU provides more than enough horsepower to get you
through the first few chapters. Just remember that you will want to access GPUs before
running larger models.
Youcan installPyTorch(the specified versionsare testedat the time ofwriting) with either
CPU or GPU support as follows:
pip install torch ==2.0.0 torchvision ==0.15.1
Our next step is to install the d2lpackage that we developed in order to encapsulate fre-
quently used functions and classes found throughout this book:
pip install d2l==1.0.3
Downloadingand Runningthe Code
Next, you will want to download the notebooks so that you can run each of the book‚Äôs
code blocks. Simply click on the ‚ÄúNotebooks‚Äù tab at the top of any HTML page on the
D2L.ai website16to download the code and then unzip it. Alternatively, you can fetch the
notebooks from the command line as follows:
mkdir d2l-en &&cdd2l-en
curl https://d2l.ai/d2l-en-1.0.3.zip -od2l-en.zip
unzip d2l-en.zip &&rmd2l-en.zip
cdpytorch
xxxvi Installation
17If you do not already have unzipinstalled, first run sudo apt-get install unzip . Now
we can start the Jupyter Notebook server by running:
jupyter notebook
Atthispoint,youcanopen http://localhost:8888 (itmayhavealreadyopenedautomatically)
in your web browser. Then we can run the code for each section of the book. Whenever
you open a new command line window, you will need to execute conda activate d2l
to activate the runtime environment before running the D2L notebooks, or updating your
packages(eitherthedeeplearningframeworkorthe d2lpackage). Toexittheenvironment,
runconda deactivate .
Discussions17.
Notation
Throughout this book, we adhere to the following notational conventions. Note that some
of these symbols are placeholders, while others refer to specific objects. As a general rule
of thumb, the indefinite article ‚Äúa‚Äù often indicates that the symbol is a placeholder and that
similarly formatted symbols can denote other objects of the same type. For example, ‚Äú ùë•: a
scalar‚Äù means that lowercased letters generally represent scalar values, but ‚Äú Z: the set of
integers‚Äù refers specifically to the symbol Z.
NumericalObjects
ùë•: a scalar
x: a vector
X: a matrix
X: a general tensor
I: the identity matrix (of some given dimension), i.e., a square matrix with 1on all
diagonal entries and 0on all off-diagonals
ùë•ùëñ,¬ªx¬ºùëñ: theùëñthelement of vector x
ùë•ùëñùëó,ùë•ùëñ,ùëó,¬ªX¬ºùëñùëó,¬ªX¬ºùëñ,ùëó: the element of matrix Xat rowùëñand column ùëó.
SetTheory
X: a set
Z: the set of integers
Z¬∏: the set of positive integers
R: the set of real numbers
Rùëõ: the set ofùëõ-dimensional vectors of real numbers
xxxvii
xxxviii Notation
Rùëéùëè: The set of matrices of real numbers with ùëérows andùëècolumns
jXj: cardinality (number of elements) of set X
A[B: union of setsAandB
A\B: intersection of sets AandB
AnB: set subtraction of BfromA(contains only those elements of Athat do not
belong toB)
Functions and Operators
ùëì¬π¬∫: a function
log¬π¬∫: the natural logarithm (base ùëí)
log2¬π¬∫: logarithm to base 2
exp¬π¬∫: the exponential function
1¬π¬∫: the indicator function; evaluates to 1if the boolean argument is true, and 0other-
wise
1X¬πùëß¬∫: the set-membership indicator function; evaluates to 1if the element ùëßbelongs to
the setXand0otherwise
¬π¬∫>: transpose of a vector or a matrix
X 1: inverse of matrix X
: Hadamard (elementwise) product
¬ª,¬º: concatenation
kkùëù:‚Ñìùëùnorm
kk:‚Ñì2norm
hx,yi: inner (dot) product of vectors xandy
√ç: summation over a collection of elements
√é: product over a collection of elements
def=: an equality asserted as a definition of the symbol on the left-hand side
xxxix Notation
18Calculus
ùëëùë¶
ùëëùë•: derivative of ùë¶with respect to ùë•
ùúïùë¶
ùúïùë•: partial derivative of ùë¶with respect to ùë•
rxùë¶: gradient of ùë¶with respect to x
¬Øùëè
ùëéùëì¬πùë•¬∫ùëëùë•: definite integral of ùëìfromùëétoùëèwith respect to ùë•
¬Ø
ùëì¬πùë•¬∫ùëëùë•: indefinite integral of ùëìwith respect to ùë•
Probabilityand InformationTheory
ùëã: a random variable
ùëÉ: a probability distribution
ùëãùëÉ: the random variable ùëãfollows distribution ùëÉ
ùëÉ¬πùëã=ùë•¬∫: the probability assigned to the event where random variable ùëãtakes valueùë•
ùëÉ¬πùëãjùëå¬∫: the conditional probability distribution of ùëãgivenùëå
ùëù¬π¬∫: a probability density function (PDF) associated with distribution ùëÉ
ùê∏¬ªùëã¬º: expectation of a random variable ùëã
ùëã?ùëå: random variables ùëãandùëåare independent
ùëã?ùëåjùëç: random variables ùëãandùëåare conditionally independent given ùëç
ùúéùëã: standard deviation of random variable ùëã
Var¬πùëã¬∫: variance of random variable ùëã, equal toùúé2
ùëã
Cov¬πùëã,ùëå¬∫: covariance of random variables ùëãandùëå
ùúå¬πùëã,ùëå¬∫: the Pearson correlation coefficient between ùëãandùëå, equalsCov¬πùëã,ùëå¬∫
ùúéùëãùúéùëå
ùêª¬πùëã¬∫: entropy of random variable ùëã
ùê∑KL¬πùëÉkùëÑ¬∫: the KL-divergence (or relative entropy) from distribution ùëÑto distribution
ùëÉ
Discussions18.

1 Introduction
Untilrecently,nearlyeverycomputerprogramthatyoumighthaveinteractedwithduringan
ordinarydaywascodedupasarigidsetofrulesspecifyingpreciselyhowitshouldbehave.
Say that we wanted to write an application to manage an e-commerce platform. After
huddling around a whiteboard for a few hours to ponder the problem, we might settle on
the broad strokes of a working solution, for example: (i) users interact with the application
through an interface running in a web browser or mobile application; (ii) our application
interacts with a commercial-grade database engine to keep track of each user‚Äôs state and
maintain records of historical transactions; and (iii) at the heart of our application, the
business logic (you might say, the brains) of our application spells out a set of rules that
map every conceivable circumstance to the corresponding action that our program should
take.
To build the brains of our application, we might enumerate all the common events that our
program should handle. For example, whenever a customer clicks to add an item to their
shoppingcart,ourprogramshouldaddanentrytotheshoppingcartdatabasetable,associ-
ating that user‚Äôs ID with the requested product‚Äôs ID. We might then attempt to step through
every possible corner case, testing the appropriateness of our rules and making any neces-
sary modifications. What happens if a user initiates a purchase with an empty cart? While
few developers ever get it completely right the first time (it might take some test runs to
work out the kinks), for the most part we can write such programs and confidently launch
thembeforeever seeing a real customer. Our ability to manually design automated sys-
tems that drive functioning products and systems, often in novel situations, is a remarkable
cognitive feat. And when you are able to devise solutions that work 100%of the time, you
typically should not be worrying about machine learning.
Fortunately for the growing community of machine learning scientists, many tasks that we
wouldliketoautomatedonotbendsoeasilytohumaningenuity. Imaginehuddlingaround
the whiteboard with the smartest minds you know, but this time you are tackling one of the
following problems:
Writeaprogramthatpredictstomorrow‚Äôsweathergivengeographicinformation,satellite
images, and a trailing window of past weather.
Writeaprogramthattakesinafactoidquestion,expressedinfree-formtext,andanswers
it correctly.
Write a program that, given an image, identifies every person depicted in it and draws
outlines around each.
1
2 Introduction
Write a program that presents users with products that they are likely to enjoy but un-
likely, in the natural course of browsing, to encounter.
For these problems, even elite programmers would struggle to code up solutions from
scratch. The reasons can vary. Sometimes the program that we are looking for follows
a pattern that changes over time, so there is no fixed right answer! In such cases, any
successful solution must adapt gracefully to a changing world. At other times, the rela-
tionship (say between pixels, and abstract categories) may be too complicated, requiring
thousands or millions of computations and following unknown principles. In the case of
image recognition, the precise steps required to perform the task lie beyond our conscious
understanding, even though our subconscious cognitive processes execute the task effort-
lessly.
Machinelearning is the study of algorithms that can learn from experience. As a machine
learning algorithm accumulates more experience, typically in the form of observational
data or interactions with an environment, its performance improves. Contrast this with
our deterministic e-commerce platform, which follows the same business logic, no matter
how much experience accrues, until the developers themselves learn and decide that it is
time to update the software. In this book, we will teach you the fundamentals of machine
learning, focusing in particular on deep learning , a powerful set of techniques driving in-
novations in areas as diverse as computer vision, natural language processing, healthcare,
and genomics.
1.1AMotivatingExample
Before beginning writing, the authors of this book, like much of the work force, had to
becomecaffeinated. Wehoppedinthecarandstarteddriving. UsinganiPhone,Alexcalled
out ‚ÄúHey Siri‚Äù, awakening the phone‚Äôs voice recognition system. Then Mu commanded
‚Äúdirections to Blue Bottle coffee shop‚Äù. The phone quickly displayed the transcription of
his command. It also recognized that we wereasking for directions and launched the Maps
application (app) to fulfill our request. Once launched, the Maps app identified a number
of routes. Next to each route, the phone displayed a predicted transit time. While this story
was fabricated for pedagogical convenience, it demonstrates that in the span of just a few
seconds,oureverydayinteractionswithasmartphonecanengageseveralmachinelearning
models.
Imagine just writing a program to respond to a wakeword such as ‚ÄúAlexa‚Äù, ‚ÄúOK Google‚Äù,
and ‚ÄúHey Siri‚Äù. Try coding it up in a room by yourself with nothing but a computer and
a code editor, as illustrated in Fig. 1.1.1 . How would you write such a program from first
principles? Think about it‚Ä¶ the problem is hard. Every second, the microphone will col-
lect roughly 44,000 samples. Each sample is a measurement of the amplitude of the sound
wave. What rule could map reliably from a snippet of raw audio to confident predictions
fyes,nogaboutwhetherthesnippetcontainsthewakeword? Ifyouarestuck,donotworry.
3 A Motivating Example
We do not know how to write such a program from scratch either. That is why we use ma-
chine learning.
tFig. 1.1.1 Identify a wake word.
Here is the trick. Often, even when we do not know how to tell a computer explicitly how
to map from inputs to outputs, we are nonetheless capable of performing the cognitive feat
ourselves. In other words, even if you do not know how to program a computer to rec-
ognize the word ‚ÄúAlexa‚Äù, you yourself are able to recognize it. Armed with this ability,
we can collect a huge datasetcontaining examples of audio snippets and associated labels,
indicating which snippets contain the wake word. In the currently dominant approach to
machinelearning,wedonotattempttodesignasystem explicitly torecognizewakewords.
Instead, we define a flexible program whose behavior is determined by a number of pa-
rameters . Then we use the dataset to determine the best possible parameter values, i.e.,
those that improve the performance of our program with respect to a chosen performance
measure.
You can think of the parameters as knobs that we can turn, manipulating the behavior of
the program. Once the parameters are fixed, we call the program a model. The set of all
distinct programs (input‚Äìoutput mappings) that we can produce just by manipulating the
parameters is called a familyof models. And the ‚Äúmeta-program‚Äù that uses our dataset to
choose the parameters is called a learning algorithm .
Before we can go ahead and engage the learning algorithm, we have to define the problem
precisely, pinning down the exact nature of the inputs and outputs, and choosing an ap-
propriate model family. In this case, our model receives a snippet of audio as input, and
the model generates a selection among fyes,nogasoutput. If all goes according to plan
the model‚Äôs guesses will typically be correct as to whether the snippet contains the wake
word.
If we choose the right family of models, there should exist one setting of the knobs such
thatthemodelfires‚Äúyes‚Äùeverytimeithearstheword‚ÄúAlexa‚Äù. Becausetheexactchoiceof
the wake word is arbitrary, we will probably need a model family sufficiently rich that, via
another setting of the knobs, it could fire ‚Äúyes‚Äù only upon hearing the word ‚ÄúApricot‚Äù. We
expectthatthesamemodelfamilyshouldbesuitablefor‚ÄúAlexa‚Äùrecognitionand‚ÄúApricot‚Äù
recognition because they seem, intuitively, to be similar tasks. However, we might need a
different family of models entirely if we want to deal with fundamentally different inputs
or outputs, say if we wanted to map from images to captions, or from English sentences to
Chinese sentences.
As you might guess, if we just set all of the knobs randomly, it is unlikely that our model
will recognize ‚ÄúAlexa‚Äù, ‚ÄúApricot‚Äù, or any other English word. In machine learning, the
learning is the process by which we discover the right setting of the knobs for coercing the
4 Introduction
desired behavior from our model. In other words, we trainour model with data. As shown
inFig. 1.1.2 , the training process usually looks like the following:
1.Start off with a randomly initialized model that cannot do anything useful.
2.Grab some of your data (e.g., audio snippets and corresponding fyes,noglabels).
3.Tweak the knobs to make the model perform better as assessed on those examples.
4.Repeat Steps 2 and 3 until the model is awesome.
tFig. 1.1.2 A typical training process.
To summarize, rather than code up a wakeword recognizer, wecode up a program that can
learnto recognize wake words, if presented with a large labeled dataset. You can think of
thisactofdeterminingaprogram‚Äôsbehaviorbypresentingitwithadatasetas programming
withdata . Thatistosay,wecan‚Äúprogram‚Äùacatdetectorbyprovidingourmachinelearning
system with many examples of cats and dogs. This way the detector will eventually learn
to emit a very large positive number if it is a cat, a very large negative number if it is a
dog, andsomethingclosertozeroifitisnotsure. Thisbarelyscratchesthesurfaceofwhat
machine learning can do. Deep learning, which we will explain in greater detail later, is
just one among many popular methods for solving machine learning problems.
1.2KeyComponents
In our wake word example, we described a dataset consisting of audio snippets and binary
labels, and we gave a hand-wavy sense of how we might train a model to approximate a
mapping from snippets to classifications. This sort of problem, where we try to predict a
designated unknown label based on known inputs given a dataset consisting of examples
forwhichthelabelsareknown, iscalled supervisedlearning . Thisisjustoneamongmany
kinds of machine learning problems. Before we explore other varieties, we would like to
shed more light on some core components that will follow us around, no matter what kind
of machine learning problem we tackle:
1.Thedatathat we can learn from.
2.Amodelof how to transform the data.
3.Anobjectivefunction that quantifies how well (or badly) the model is doing.
4.Analgorithm to adjust the model‚Äôs parameters to optimize the objective function.
5 Key Components
1.2.1Data
It might go without saying that you cannot do data science without data. We could lose
hundreds of pages pondering what precisely data is, but for now, we will focus on the key
propertiesofthedatasetsthatwewillbeconcernedwith. Generally,weareconcernedwith
a collection of examples. In order to work with data usefully, we typically need to come
up with a suitable numerical representation. Each example (ordata point ,data instance ,
sample)typicallyconsistsofasetofattributescalled features (sometimescalled covariates
orinputs), based on which the model must make its predictions. In supervised learning
problems, our goal is to predict the value of a special attribute, called the label(ortarget),
that is not part of the model‚Äôs input.
If we were working with image data, each example might consist of an individual photo-
graph(thefeatures)andanumberindicatingthecategorytowhichthephotographbelongs
(the label). The photograph would be represented numerically as three grids of numerical
values representing the brightness of red, green, and blue light at each pixel location. For
example, a 200200pixel color photograph would consist of 2002003=120000
numerical values.
Alternatively, we might work with electronic health record data and tackle the task of pre-
dicting the likelihood that a given patient will survive the next 30 days. Here, our features
might consist of a collection of readily available attributes and frequently recorded mea-
surements, including age, vital signs, comorbidities, current medications, and recent pro-
cedures. The label available for training would be a binary value indicating whether each
patient in the historical data survived within the 30-day window.
In such cases, when every example is characterized by the same number of numerical fea-
tures, we say that the inputs are fixed-length vectors and we call the (constant) length of
the vectors the dimensionality of the data. As you might imagine, fixed-length inputs can
be convenient, giving us one less complication to worry about. However, not all data can
easilyberepresentedas fixed-length vectors. Whilewemightexpectmicroscopeimagesto
comefromstandardequipment,wecannotexpectimagesminedfromtheInternetalltohave
the same resolution or shape. For images, we might consider cropping them to a standard
size, but that strategy only gets us so far. We risk losing information in the cropped-out
portions. Moreover, text data resists fixed-length representations even more stubbornly.
Consider the customer reviews left on e-commerce sites such as Amazon, IMDb, and Tri-
pAdvisor. Some are short: ‚Äúit stinks!‚Äù. Others ramble for pages. One major advantage of
deeplearningovertraditionalmethodsisthecomparativegracewithwhichmodernmodels
can handle varying-length data.
Generally,themoredatawehave,theeasierourjobbecomes. Whenwehavemoredata,we
can train more powerful models and rely less heavily on preconceived assumptions. The
regime change from (comparatively) small to big data is a major contributor to the success
of modern deep learning. To drive the point home, many of the most exciting models in
deep learning do not work without large datasets. Some others might work in the small
data regime, but are no better than traditional approaches.
Finally, it is not enough to have lots of data and to process it cleverly. We need the right
6 Introduction
data. If the data is full of mistakes, or if the chosen features are not predictive of the target
quantity of interest, learning is going to fail. The situation is captured well by the clich√©:
garbage in, garbage out . Moreover, poor predictive performance is not the only poten-
tial consequence. In sensitive applications of machine learning, like predictive policing,
resumescreening, andriskmodelsusedforlending, wemustbeespeciallyalerttothecon-
sequencesofgarbagedata. Onecommonlyoccurringfailuremodeconcernsdatasetswhere
somegroupsofpeopleareunrepresentedinthetrainingdata. Imagineapplyingaskincan-
cer recognition system that had never seen black skin before. Failure can also occur when
thedatadoesnotonlyunder-representsomegroupsbutreflectssocietalprejudices. Forex-
ample,ifpasthiringdecisionsareusedtotrainapredictivemodelthatwillbeusedtoscreen
resumesthenmachinelearningmodelscouldinadvertentlycaptureandautomatehistorical
injustices. Note that this can all happen without the data scientist actively conspiring, or
even being aware.
1.2.2Models
Most machine learning involves transforming the data in some sense. We might want to
buildasystemthatingestsphotosandpredictssmiley-ness. Alternatively,wemightwantto
ingest a set of sensor readings and predict how normal vs. anomalous the readings are. By
model, we denote the computational machinery for ingesting data of one type, and spitting
out predictions of a possibly different type. In particular, we are interested in statistical
modelsthat can be estimated from data. While simple models are perfectly capable of ad-
dressing appropriately simple problems, the problems that we focus on in this book stretch
the limits of classical methods. Deep learning is differentiated from classical approaches
principally by the set of powerful models that it focuses on. These models consist of many
successive transformations of the data that are chained together top to bottom, thus the
namedeep learning . On our way to discussing deep models, we will also discuss some
more traditional methods.
1.2.3ObjectiveFunctions
Earlier,weintroducedmachinelearningaslearningfromexperience. By learning here,we
meanimprovingatsometaskovertime. Butwhoistosaywhatconstitutesanimprovement?
You might imagine that we could propose updating our model, and some people might
disagree on whether our proposal constituted an improvement or not.
In order to develop a formal mathematical system of learning machines, we need to have
formal measures of how good (or bad) our models are. In machine learning, and optimiza-
tion more generally, we call these objective functions . By convention, we usually define
objective functions so that lower is better. This is merely a convention. You can take any
functionforwhichhigheris better, and turn itinto a newfunction that is qualitativelyiden-
tical but for which lower is better by flipping the sign. Because we choose lower to be
better, these functions are sometimes called loss functions .
When trying to predict numerical values, the most common loss function is squarederror ,
i.e., the square of the difference between the prediction and the ground truth target. For
classification, the most common objective is to minimize error rate, i.e., the fraction of
7 Kinds of Machine Learning Problems
examples on which our predictions disagree with the ground truth. Some objectives (e.g.,
squared error) are easy to optimize, while others (e.g., error rate) are difficult to optimize
directly,owingtonon-differentiabilityorothercomplications. Inthesecases,itiscommon
instead to optimize a surrogateobjective .
Duringoptimization, wethinkofthelossasafunctionofthemodel‚Äôsparameters, andtreat
the training dataset as a constant. We learn the best values of our model‚Äôs parameters by
minimizing the loss incurred on a set consisting of some number of examples collected for
training. However, doing well on the training data does not guarantee that we will do well
on unseen data. So we will typically want to split the available data into two partitions:
thetraining dataset (ortraining set ), for learning model parameters; and the test dataset
(ortest set), which is held out for evaluation. At the end of the day, we typically report
how our models perform on both partitions. You could think of training performance as
analogous to the scores that a student achieves on the practice exams used to prepare for
some real final exam. Even if the results are encouraging, that does not guarantee success
on the final exam. Over the course of studying, the student might begin to memorize the
practice questions, appearing to master the topic but faltering when faced with previously
unseen questions on the actual final exam. When a model performs well on the training set
butfailstogeneralizetounseendata,wesaythatitis overfitting tothetrainingdata.
1.2.4OptimizationAlgorithms
Once we have got some data source and representation, a model, and a well-defined objec-
tive function, we need an algorithm capable of searching for the best possible parameters
for minimizing the loss function. Popular optimization algorithms for deep learning are
based on an approach called gradient descent . In brief, at each step, this method checks
to see, for each parameter, how that training set loss would change if you perturbed that
parameter by just a small amount. It would then update the parameter in the direction that
lowers the loss.
1.3Kinds of MachineLearning Problems
The wake word problem in our motivating example is just one among many that machine
learning can tackle. To motivate the reader further and provide us with some common
language that will follow us throughout the book, we now provide a broad overview of the
landscape of machine learning problems.
1.3.1SupervisedLearning
Supervised learning describes tasks where we are given a dataset containing both features
and labels and asked to produce a model that predicts the labels when given input features.
Each feature‚Äìlabel pair is called an example. Sometimes, when the context is clear, we
may use the term examples to refer to a collection of inputs, even when the corresponding
8 Introduction
labels are unknown. The supervision comes into play because, for choosing the parame-
ters, we (the supervisors) provide the model with a dataset consisting of labeled examples.
In probabilistic terms, we typically are interested in estimating the conditional probability
of a label given input features. While it is just one among several paradigms, supervised
learning accounts for the majority of successful applications of machine learning in indus-
try. Partly that is because many important tasks can be described crisply as estimating the
probability of something unknown given a particular set of available data:
Predict cancer vs. not cancer, given a computer tomography image.
Predict the correct translation in French, given a sentence in English.
Predict the price of a stock next month based on this month‚Äôs financial reporting data.
While all supervised learning problems are captured by the simple description ‚Äúpredicting
thelabelsgiveninputfeatures‚Äù,supervisedlearningitselfcantakediverseformsandrequire
tons of modeling decisions, depending on (among other considerations) the type, size, and
quantity of the inputs and outputs. For example, we use different models for processing
sequences of arbitrary lengths and fixed-length vector representations. We will visit many
of these problems in depth throughout this book.
Informally, the learning process looks something like the following. First, grab a big col-
lectionofexamplesforwhichthefeaturesareknownandselectfromthemarandomsubset,
acquiring the ground truth labels for each. Sometimes these labels might be available data
that have already been collected (e.g., did a patient die within the following year?) and
other times we might need to employ human annotators to label the data, (e.g., assigning
images to categories). Together, these inputs and corresponding labels comprise the train-
ing set. We feed the training dataset into a supervised learning algorithm, a function that
takes as input a dataset and outputs another function: the learned model. Finally, we can
feed previously unseen inputs to the learned model, using its outputs as predictions of the
corresponding label. The full process is drawn in Fig. 1.3.1 .
tFig. 1.3.1 Supervised learning.
Regression
Perhapsthesimplestsupervisedlearningtasktowrapyourheadaroundis regression . Con-
sider, for example, a set of data harvested from a database of home sales. We might con-
struct a table, in which each row corresponds to a different house, and each column cor-
responds to some relevant attribute, such as the square footage of a house, the number of
bedrooms, the number of bathrooms, and the number of minutes (walking) to the center
of town. In this dataset, each example would be a specific house, and the corresponding
9 Kinds of Machine Learning Problems
19featurevectorwouldbeonerowinthetable. IfyouliveinNewYorkorSanFrancisco, and
you are not the CEO of Amazon, Google, Microsoft, or Facebook, the (sq. footage, no. of
bedrooms, no. of bathrooms, walking distance) feature vector for your home might look
something like:¬ª600,1,1,60¬º. However, if you live in Pittsburgh, it might look more like
¬ª3000,4,3,10¬º. Fixed-lengthfeaturevectorslikethisareessentialformostclassicmachine
learning algorithms.
Whatmakesaproblemaregressionisactuallytheformofthetarget. Saythatyouareinthe
market for a new home. You might want to estimate the fair market value of a house, given
somefeaturessuchasabove. Thedataheremightconsistofhistoricalhomelistingsandthe
labels might be the observed sales prices. When labels take on arbitrary numerical values
(even within some interval), we call this a regression problem. The goal is to produce a
model whose predictions closely approximate the actual label values.
Lotsofpracticalproblemsareeasilydescribedasregressionproblems. Predictingtherating
that a user will assign to a movie can be thought of as a regression problem and if you
designed a great algorithm to accomplish this feat in 2009, you might have won the 1-
million-dollar Netflix prize19. Predicting the length of stay for patients in the hospital is
also a regression problem. A good rule of thumb is that any how much? orhow many?
problem is likely to be regression. For example:
How many hours will this surgery take?
How much rainfall will this town have in the next six hours?
Even if you have never worked with machine learning before, you have probably worked
througharegressionprobleminformally. Imagine,forexample,thatyouhadyourdrainsre-
pairedandthatyourcontractorspent3hoursremovinggunkfromyoursewagepipes. Then
they sent you a bill of 350 dollars. Now imagine that your friend hired the same contractor
for 2 hours and received a bill of 250 dollars. If someone then asked you how much to
expect on their upcoming gunk-removal invoice you might make some reasonable assump-
tions, such as more hours worked costs more dollars. You might also assume that there is
some base charge and that the contractor then charges per hour. If these assumptions held
true,thengiventhesetwodataexamples,youcouldalreadyidentifythecontractor‚Äôspricing
structure: 100 dollars per hour plus 50 dollars to show up at your house. If you followed
thatmuch,thenyoualreadyunderstandthehigh-levelideabehind linearregression.
In this case, we could produce the parameters that exactly matched the contractor‚Äôs prices.
Sometimes this is not possible, e.g., if some of the variation arises from factors beyond
your two features. In these cases, we will try to learn models that minimize the distance
between our predictions and the observed values. In most of our chapters, we will focus on
minimizing the squared error loss function. As we will see later, this loss corresponds to
the assumption that our data were corrupted by Gaussian noise.
Classification
Whileregressionmodelsaregreatforaddressing howmany? questions,lotsofproblemsdo
not fit comfortably in this template. Consider, for example, a bank that wants to develop a
10 Introduction
check scanning feature for its mobile app. Ideally, the customer would simply snap a photo
of a check and the app would automatically recognize the text from the image. Assuming
that we had some ability to segment out image patches corresponding to each handwritten
character, then the primary remaining task would be to determine which character among
some known set is depicted in each image patch. These kinds of which one? problems
are called classification and require a different set of tools from those used for regression,
although many techniques will carry over.
Inclassification , we want our model to look at features, e.g., the pixel values in an image,
and then predict to which category (sometimes called a class) among some discrete set
of options, an example belongs. For handwritten digits, we might have ten classes, corre-
sponding to the digits 0 through 9. The simplest form of classification is when there are
only two classes, a problem which we call binary classification . For example, our dataset
couldconsistofimagesofanimalsandourlabelsmightbetheclasses{cat, dog}. Whereas
in regression we sought a regressor to output a numerical value, in classification we seek a
classifier, whose output is the predicted class assignment.
For reasons that we will get into as the book gets more technical, it can be difficult to opti-
mizeamodelthatcanonlyoutputa firmcategoricalassignment,e.g.,either‚Äúcat‚Äùor‚Äúdog‚Äù.
In these cases, it is usually much easier to express our model in the language of probabili-
ties. Given features of an example, our model assigns a probability to each possible class.
Returning to our animal classification example where the classes are {cat, dog}, a classi-
fier might see an image and output the probability that the image is a cat as 0.9. We can
interpret this number by saying that the classifier is 90% sure that the image depicts a cat.
The magnitude of the probability for the predicted class conveys a notion of uncertainty.
It is not the only one available and we will discuss others in chapters dealing with more
advanced topics.
Whenwehavemorethantwopossibleclasses,wecalltheproblem multiclassclassification .
Commonexamplesincludehandwrittencharacterrecognition{0, 1, 2, ... 9, a, b, c, ...}. While
we attacked regression problems by trying to minimize the squared error loss function, the
commonlossfunctionforclassificationproblemsiscalled cross-entropy , whosenamewill
be demystified when we introduce information theory in later chapters.
Note that the most likely class is not necessarily the one that you are going to use for your
decision. Assume that you find a beautiful mushroom in your backyard as shown in Fig.
1.3.2.
Now, assumethatyoubuiltaclassifierandtrainedittopredictwhetheramushroomispoi-
sonous based on a photograph. Say our poison-detection classifier outputs that the proba-
bility that Fig. 1.3.2 shows a death cap is 0.2. In other words, the classifier is 80% sure that
ourmushroomisnotadeathcap. Still,youwouldhavetobeafooltoeatit. Thatisbecause
the certain benefit of a delicious dinner is not worth a 20% risk of dying from it. In other
words, the effect of the uncertain risk outweighs the benefit by far. Thus, in order to make
a decision about whether to eat the mushroom, we need to compute the expected detriment
associated with each action which depends both on the likely outcomes and the benefits or
harms associated with each. In this case, the detriment incurred by eating the mushroom
11 Kinds of Machine Learning Problems
tFig. 1.3.2 Death cap - do not eat!
20might be 0.21¬∏ 0.80=1, whereas the loss of discarding it is 0.20¬∏0.81=0.8.
Our caution was justified: as any mycologist would tell us, the mushroom in Fig. 1.3.2 is
actually a death cap.
Classification can get much more complicated than just binary or multiclass classification.
For instance, there are some variants of classification addressing hierarchically structured
classes. Insuchcasesnotallerrorsareequal‚Äîifwemusterr,wemightprefertomisclassify
to a related class rather than a distant class. Usually, this is referred to as hierarchical
classification . For inspiration, you might think of Linnaeus20, who organized fauna in a
hierarchy.
Inthecaseofanimalclassification,itmightnotbesobadtomistakeapoodleforaschnauzer,
but our model would pay a huge penalty if it confused a poodle with a dinosaur. Which
hierarchy is relevant might depend on how you plan to use the model. For example, rat-
tlesnakes and garter snakes might be close on the phylogenetic tree, but mistaking a rattler
for a garter could have fatal consequences.
Tagging
Some classification problems fit neatly into the binary or multiclass classification setups.
Forexample,wecouldtrainanormalbinaryclassifiertodistinguishcatsfromdogs. Given
thecurrentstateofcomputervision,wecandothiseasily,withoff-the-shelftools. Nonethe-
less, no matter how accurate our model gets, we might find ourselves in trouble when the
classifier encounters an image of the Town Musicians of Bremen , a popular German fairy
tale featuring four animals ( Fig. 1.3.3 ).
As you can see, the photo features a cat, a rooster, a dog, and a donkey, with some trees in
thebackground. Ifweanticipateencounteringsuchimages, multiclassclassificationmight
not be the right problem formulation. Instead, we might want to give the model the option
of saying the image depicts a cat, a dog, a donkey, anda rooster.
12 Introduction
tFig. 1.3.3 A donkey, a dog, a cat, and a rooster.
21The problem of learning to predict classes that are not mutually exclusive is called multi-
label classification . Auto-tagging problems are typically best described in terms of multi-
label classification. Think of the tags people might apply to posts on a technical blog, e.g.,
‚Äúmachine learning‚Äù, ‚Äútechnology‚Äù, ‚Äúgadgets‚Äù, ‚Äúprogramming languages‚Äù, ‚ÄúLinux‚Äù, ‚Äúcloud
computing‚Äù, ‚ÄúAWS‚Äù. A typical article might have 5‚Äì10 tags applied. Typically, tags will
exhibit some correlation structure. Posts about ‚Äúcloud computing‚Äù are likely to mention
‚ÄúAWS‚Äù and posts about ‚Äúmachine learning‚Äù are likely to mention ‚ÄúGPUs‚Äù.
Sometimes such tagging problems draw on enormous label sets. The National Library of
Medicineemploysmanyprofessionalannotatorswhoassociateeacharticletobeindexedin
PubMed with a set of tags drawn from the Medical Subject Headings (MeSH) ontology, a
collection of roughly 28,000 tags. Correctly tagging articles is important because it allows
researchers to conduct exhaustive reviews of the literature. This is a time-consuming pro-
cessand typicallythere isa one-yearlagbetweenarchivingand tagging. Machinelearning
can provide provisional tags until each article has a proper manual review. Indeed, for
several years, the BioASQ organization has hosted competitions21for this task.
13 Kinds of Machine Learning Problems
22Search
In the field of information retrieval, we often impose ranks on sets of items. Take web
searchforexample. Thegoalislesstodetermine whether aparticularpageisrelevantfora
query,butratherwhich,amongasetofrelevantresults,shouldbeshownmostprominently
to a particular user. One way of doing this might be to first assign a score to every element
in the set and then to retrieve the top-rated elements. PageRank22, the original secret
sauce behind the Google search engine, was an early example of such a scoring system.
Weirdly, the scoring provided by PageRank did not depend on the actual query. Instead,
they relied on a simple relevance filter to identify the set of relevant candidates and then
used PageRank to prioritize the more authoritative pages. Nowadays, search engines use
machinelearningandbehavioralmodelstoobtainquery-dependentrelevancescores. There
are entire academic conferences devoted to this subject.
Recommender Systems
Recommender systems are another problem setting that is related to search and ranking.
The problems are similar insofar as the goal is to display a set of items relevant to the user.
The main difference is the emphasis on personalization to specific users in the context of
recommender systems. For instance, for movie recommendations, the results page for a
science fiction fan and the results page for a connoisseur of Peter Sellers comedies might
differ significantly. Similar problems pop up in other recommendation settings, e.g., for
retail products, music, and news recommendation.
Insomecases,customersprovideexplicitfeedback,communicatinghowmuchtheylikeda
particularproduct(e.g.,theproductratingsandreviewsonAmazon,IMDb,orGoodreads).
In other cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which
might indicate dissatisfaction or maybe just indicate that the song was inappropriate in
context. In the simplest formulations, these systems are trained to estimate some score,
suchasanexpectedstarratingortheprobabilitythatagivenuserwillpurchaseaparticular
item.
Given such a model, for any given user, we could retrieve the set of objects with the largest
scores, which could then be recommended to the user. Production systems are consider-
ably more advanced and take detailed user activity and item characteristics into account
whencomputingsuchscores. Fig.1.3.4 displaysthedeeplearningbooksrecommendedby
Amazon based on personalization algorithms tuned to capture Aston‚Äôs preferences.
Despite their tremendous economic value, recommender systems naively built on top of
predictivemodelssuffersomeseriousconceptualflaws. Tostart,weonlyobserve censored
feedback : users preferentially rate movies that they feel strongly about. For example, on
a five-point scale, you might notice that items receive many one- and five-star ratings but
that there are conspicuously few three-star ratings. Moreover, current purchase habits are
often a result of the recommendation algorithm currently in place, but learning algorithms
do not always take this detail into account. Thus it is possible for feedback loops to form
where a recommender system preferentially pushes an item that is then taken to be better
(due to greater purchases) and in turn is recommended even more frequently. Many of
14 Introduction
tFig. 1.3.4 Deep learning books recommended by Amazon.
these problems‚Äîabout how to deal with censoring, incentives, and feedback loops‚Äîare
important open research questions.
SequenceLearning
Sofar,wehavelookedatproblemswherewehavesomefixednumberofinputsandproduce
a fixed number of outputs. For example, we considered predicting house prices given a
fixed set of features: square footage, number of bedrooms, number of bathrooms, and the
transit time to downtown. We also discussed mapping from an image (of fixed dimension)
to the predicted probabilities that it belongs to each among a fixed number of classes and
predictingstarratingsassociatedwithpurchasesbasedontheuserIDandproductIDalone.
In these cases, once our model is trained, after each test example is fed into our model, it
is immediately forgotten. We assumed that successive observations were independent and
thus there was no need to hold on to this context.
But how should we deal with video snippets? In this case, each snippet might consist of
a different number of frames. And our guess of what is going on in each frame might be
muchstrongerifwetakeintoaccountthepreviousorsucceedingframes. Thesamegoesfor
language. Forexample,onepopulardeeplearningproblemismachinetranslation: thetask
of ingesting sentences in some source language and predicting their translations in another
language.
Such problems also occur in medicine. We might want a model to monitor patients in the
intensive care unit and to fire off alerts whenever their risk of dying in the next 24 hours
exceeds some threshold. Here, we would not throw away everything that we know about
15 Kinds of Machine Learning Problems
the patient history every hour, because we might not want to make predictions based only
on the most recent measurements.
Questions like these are among the most exciting applications of machine learning and
they are instances of sequence learning . They require a model either to ingest sequences
of inputs or to emit sequences of outputs (or both). Specifically, sequence-to-sequence
learning considers problems where both inputs and outputs consist of variable-length se-
quences. Examples include machine translation and speech-to-text transcription. While it
is impossible to consider all types of sequence transformations, the following special cases
are worth mentioning.
Tagging and Parsing . This involves annotating a text sequence with attributes. Here,
the inputs and outputs are aligned, i.e., they are of the same number and occur in a corre-
sponding order. For instance, in part-of-speech (PoS) tagging , we annotate every word in
a sentence with the corresponding part of speech, i.e., ‚Äúnoun‚Äù or ‚Äúdirect object‚Äù. Alterna-
tively, we might want to know which groups of contiguous words refer to named entities,
likepeople,places, ororganizations . In the cartoonishly simple example below, we might
just want to indicate whether or not any word in the sentence is part of a named entity
(tagged as ‚ÄúEnt‚Äù).
Tom has dinner in Washington with Sally
Ent - - - Ent - Ent
Automatic Speech Recognition . With speech recognition, the input sequence is an audio
recording of a speaker ( Fig. 1.3.5 ), and the output is a transcript of what the speaker said.
The challenge is that there are many more audio frames (sound is typically sampled at
8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text,
since thousands of samples may correspond to a single spoken word. These are sequence-
to-sequence learning problems, where the output is much shorter than the input. While
humans are remarkably good at recognizing speech, even from low-quality audio, getting
computers to perform the same feat is a formidable challenge.
tFig. 1.3.5 -D-e-e-p- L-ea-r-ni-ng- in an audio recording.
TexttoSpeech . This is the inverse of automatic speech recognition. Here, the input is text
andtheoutputisanaudiofile. Inthiscase,theoutputismuchlongerthantheinput.
MachineTranslation . Unlike the case of speech recognition, where corresponding inputs
and outputs occur in the same order, in machine translation, unaligned data poses a new
challenge. Here the input and output sequences can have different lengths, and the corre-
16 Introduction
sponding regions of the respective sequences may appear in a different order. Consider the
followingillustrativeexampleofthepeculiartendencyofGermanstoplacetheverbsatthe
end of sentences:
German: Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
English: Have you already looked at this excellent textbook?
Wrong alignment: Have you yourself already this excellent textbook looked at?
Many related problems pop up in other learning tasks. For instance, determining the order
in which a user reads a webpage is a two-dimensional layout analysis problem. Dialogue
problemsexhibitallkindsofadditionalcomplications,wheredeterminingwhattosaynext
requires taking into account real-world knowledge and the prior state of the conversation
across long temporal distances. Such topics are active areas of research.
1.3.2Unsupervisedand Self-Supervised Learning
The previous examples focused on supervised learning, where we feed the model a giant
dataset containing both the features and corresponding label values. You could think of
the supervised learner as having an extremely specialized job and an extremely dictatorial
boss. Thebossstandsoverthelearner‚Äôsshoulderandtellsthemexactlywhattodoinevery
situationuntiltheylearntomapfromsituationstoactions. Workingforsuchabosssounds
pretty lame. On the other hand, pleasing such a boss is pretty easy. You just recognize the
pattern as quickly as possible and imitate the boss‚Äôs actions.
Considering the opposite situation, it could be frustrating to work for a boss who has no
idea what they want you to do. However, if you plan to be a data scientist, you had better
get used to it. The boss might just hand you a giant dump of data and tell you to do some
datasciencewithit! This sounds vague because it is vague. We call this class of problems
unsupervisedlearning , and the type and number of questions we can ask is limited only by
ourcreativity. Wewilladdressunsupervisedlearningtechniquesinlaterchapters. Towhet
your appetite for now, we describe a few of the following questions you might ask.
Can we find a small number of prototypes that accurately summarize the data? Given a
setofphotos,canwegroupthemintolandscapephotos,picturesofdogs,babies,cats,
and mountain peaks? Likewise, given a collection of users‚Äô browsing activities, can
we group them into users with similar behavior? This problem is typically known as
clustering .
Canwefindasmallnumberofparametersthataccuratelycapturetherelevantproperties
of the data? The trajectories of a ball are well described by velocity, diameter, and
mass of the ball. Tailors have developed a small number of parameters that describe
human body shape fairlyaccuratelyforthe purpose of fitting clothes. These problems
arereferredtoas subspaceestimation . Ifthedependenceislinear,itiscalled principal
componentanalysis .
Is there a representation of (arbitrarily structured) objects in Euclidean space such that
symbolic properties can be well matched? This can be used to describe entities and
their relations, such as ‚ÄúRome‚Äù  ‚ÄúItaly‚Äù¬∏‚ÄúFrance‚Äù =‚ÄúParis‚Äù.
17 Kinds of Machine Learning Problems
Isthereadescriptionoftherootcausesofmuchofthedatathatweobserve? Forinstance,
ifwehavedemographicdataabouthouseprices,pollution,crime,location,education,
and salaries, can we discover how they are related simply based on empirical data?
The fields concerned with causality andprobabilistic graphical models tackle such
questions.
Another important and exciting recent development in unsupervised learning is the ad-
vent ofdeep generativemodels . These models estimate the density of the data, either
explicitly or implicitly . Once trained, we can use a generative model either to score
examples according to how likely they are, or to sample synthetic examples from the
learned distribution. Early deep learning breakthroughs in generative modeling came
with the invention of variational autoencoders (Kingma and Welling, 2014 ,Rezende
et al., 2014) and continued with the development of generative adversarial networks
(Goodfellow et al., 2014). More recent advances include normalizing flows ( Dinhet
al., 2014,Dinhet al., 2017) and diffusion models ( Hoet al., 2020,Sohl-Dickstein et
al., 2015,Song and Ermon, 2019 ,Songetal., 2021).
A further development in unsupervised learning has been the rise of self-supervisedlearn-
ing, techniques that leverage some aspect of the unlabeled data to provide supervision. For
text, we can train models to ‚Äúfill in the blanks‚Äù by predicting randomly masked words us-
ing their surrounding words (contexts) in big corpora without any labeling effort ( Devlin
et al., 2018)! For images, we may train models to tell the relative position between two
croppedregionsofthesameimage( Doerschetal., 2015), topredictanoccludedpartofan
image based on the remaining portions of the image, or to predict whether two examples
are perturbed versions of the same underlying image. Self-supervised models often learn
representationsthataresubsequentlyleveragedbyfine-tuningtheresultingmodelsonsome
downstream task of interest.
1.3.3Interactingwith an Environment
So far, we have not discussed where data actually comes from, or what actually happens
when a machine learning model generates an output. That is because supervised learning
and unsupervised learning do not address these issues in a very sophisticated way. In each
case,wegrababigpileofdataupfront,thensetourpatternrecognitionmachinesinmotion
without ever interacting with the environment again. Because all the learning takes place
after the algorithm is disconnected from the environment, this is sometimes called offline
learning . Forexample,supervisedlearningassumesthesimpleinteractionpatterndepicted
inFig. 1.3.6 .
This simplicity of offline learning has its charms. The upside is that we can worry about
pattern recognition in isolation, with no concern about complications arising from interac-
tions with a dynamic environment. But this problem formulation is limiting. If you grew
up reading Asimov‚Äôs Robot novels, then you probably picture artificially intelligent agents
capable not only of making predictions, but also of taking actions in the world. We want
to think about intelligent agents, not just predictive models. This means that we need to
think about choosing actions, not just making predictions. In contrast to mere predictions,
actions actually impact the environment. If we want to train an intelligent agent, we must
18 Introduction
tFig. 1.3.6 Collecting data for supervised learning from an environment.
account for the way its actions might impact the future observations of the agent, and so
offline learning is inappropriate.
Considering the interaction with an environment opens a whole set of new modeling ques-
tions. The following are just a few examples.
Does the environment remember what we did previously?
Does the environment want to help us, e.g., a user reading text into a speech recognizer?
Does the environment want to beat us, e.g., spammers adapting their emails to evade
spam filters?
Does the environment have shifting dynamics? For example, would future data always
resemble the past or would the patterns change over time, either naturally or in re-
sponse to our automated tools?
These questions raise the problem of distribution shift , where training and test data are
different. An example of this, that many of us may have met, is when taking exams written
by a lecturer, while the homework was composed by their teaching assistants. Next, we
briefly describe reinforcement learning, a rich framework for posing learning problems in
which an agent interacts with an environment.
1.3.4ReinforcementLearning
If you are interested in using machine learning to develop an agent that interacts with an
environment and takes actions, then you are probably going to wind up focusing on re-
inforcement learning . This might include applications to robotics, to dialogue systems,
and even to developing artificial intelligence (AI) for video games. Deep reinforcement
learning , which applies deep learning to reinforcement learning problems, has surged in
popularity. The breakthroughdeepQ-network, thatbeathumans atAtari gamesusingonly
the visual input ( Mnihetal., 2015), and the AlphaGo program, which dethroned the world
champion at the board game Go ( Silveretal., 2016), are two prominent examples.
Reinforcementlearninggivesaverygeneralstatementofaprobleminwhichanagentinter-
acts with an environment over a series of time steps. At each time step, the agent receives
someobservation from the environment and must choose an actionthat is subsequently
transmitted back to the environment via some mechanism (sometimes called an actuator ),
when, after each loop, the agent receives a reward from the environment. This process is
19 Kinds of Machine Learning Problems
illustrated in Fig. 1.3.7 . The agent then receives a subsequent observation, and chooses a
subsequent action, and so on. The behavior of a reinforcement learning agent is governed
by apolicy. In brief, a policyis just a function that maps from observations of the environ-
ment to actions. The goal of reinforcement learning is to produce good policies.
tFig. 1.3.7 The interaction between reinforcement learning and an environment.
Itishardtooverstatethegeneralityofthereinforcementlearningframework. Forexample,
supervised learning can be recast as reinforcement learning. Say we had a classification
problem. We could create a reinforcement learning agent with one action corresponding
to each class. We could then create an environment which gave a reward that was exactly
equal to the loss function from the original supervised learning problem.
Further, reinforcement learning can also address many problems that supervised learning
cannot. Forexample,insupervisedlearning,wealwaysexpectthatthetraininginputcomes
associated with the correct label. But in reinforcement learning, we do not assume that,
for each observation the environment tells us the optimal action. In general, we just get
some reward. Moreover, the environment may not even tell us which actions led to the
reward.
Considerthegameofchess. Theonlyrealrewardsignalcomesattheendofthegamewhen
we either win, earning a reward of, say, 1, or when we lose, receiving a reward of, say,
 1. So reinforcement learners must deal with the creditassignment problem: determining
which actions to credit or blame for an outcome. The same goes for an employee who gets
apromotiononOctober11. Thatpromotionlikelyreflectsanumberofwell-chosenactions
over the previous year. Getting promoted in the future requires figuring out which actions
along the way led to the earlier promotions.
Reinforcement learners may also have to deal with the problem of partial observability.
That is, the current observation might not tell you everything about your current state. Say
your cleaning robot found itself trapped in one of many identical closets in your house.
Rescuing the robot involves inferring its precise location which might require considering
earlier observations prior to it entering the closet.
Finally, at any given point, reinforcement learners might know of one good policy, but
there might be many other better policies that the agent has never tried. The reinforcement
learner must constantly choose whether to exploitthe best (currently) known strategy as a
policy, or to explorethe space of strategies, potentially giving up some short-term reward
in exchange for knowledge.
The general reinforcement learning problem has a very general setting. Actions affect sub-
20 Introduction
23
24
25
26sequent observations. Rewards are only observed when they correspond to the chosen ac-
tions. The environment may be either fully or partially observed. Accounting for all this
complexity at once may be asking too much. Moreover, not every practical problem ex-
hibits all this complexity. As a result, researchers have studied a number of special cases
of reinforcement learning problems.
When the environment is fully observed, we call the reinforcement learning problem a
Markovdecisionprocess . When the state does not depend on the previous actions, we call
it acontextual bandit problem . When there is no state, just a set of available actions with
initially unknown rewards, we have the classic multi-armed bandit problem .
1.4Roots
We have just reviewed a small subset of problems that machine learning can address. For
a diverse set of machine learning problems, deep learning provides powerful tools for their
solution. Although many deep learning methods are recent inventions, the core ideas be-
hind learning from data have been studied for centuries. In fact, humans have held the
desire to analyze data and to predict future outcomes for ages, and it is this desire that is
at the root of much of natural science and mathematics. Two examples are the Bernoulli
distribution, named after Jacob Bernoulli (1655‚Äì1705)23, and the Gaussian distribution
discovered by Carl Friedrich Gauss (1777‚Äì1855)24. Gauss invented, for instance, the least
mean squares algorithm, which is still used today for a multitude of problems from insur-
ance calculations to medical diagnostics. Such tools enhanced the experimental approach
in the natural sciences‚Äîfor instance, Ohm‚Äôs law relating current and voltage in a resistor
is perfectly described by a linear model.
Even in the middle ages, mathematicians had a keen intuition of estimates. For instance,
the geometry book of Jacob K√∂bel (1460‚Äì1533)25illustrates averaging the length of 16
adult men‚Äôs feet to estimate the typical foot length in the population ( Fig. 1.4.1 ).
As a group of individuals exited a church, 16 adult men were asked to line up in a row
and have their feet measured. The sum of these measurements was then divided by 16 to
obtain an estimate for what now is called one foot. This ‚Äúalgorithm‚Äù was later improved to
deal with misshapen feet; The two men with the shortest and longest feet were sent away,
averaging only over the remainder. This is among the earliest examples of a trimmed mean
estimate.
Statistics really took off with the availability and collection of data. One of its pioneers,
Ronald Fisher (1890‚Äì1962)26, contributed significantly to its theory and also its applica-
tions in genetics. Many of his algorithms (such as linear discriminant analysis) and con-
cepts (such as the Fisher information matrix) still hold a prominent place in the founda-
tions of modern statistics. Even his data resources had a lasting impact. The Iris dataset
that Fisher released in 1936 is still sometimes used to demonstrate machine learning algo-
rithms. Fisher was also a proponent of eugenics, which should remind us that the morally
21 Roots
tFig. 1.4.1 Estimating the length of a foot.
27
28
29dubious use of data science has as long and enduring a history as its productive use in
industry and the natural sciences.
Other influences for machine learning came from the information theory of Claude Shan-
non (1916‚Äì2001)27and the theory of computation proposed by Alan Turing (1912‚Äì1954)
28. Turing posed the question ‚Äúcan machines think?‚Äù in his famous paper Computing Ma-
chineryandIntelligence (Turing, 1950 ). Describing what is now known as the Turing test,
he proposed that a machine can be considered intelligent if it is difficult for a human evalu-
ator to distinguish between the replies from a machine and those of a human, based purely
on textual interactions.
Further influences came from neuroscience and psychology. After all, humans clearly ex-
hibit intelligent behavior. Many scholars have asked whether one could explain and pos-
sibly reverse engineer this capacity. One of the first biologically inspired algorithms was
formulated by Donald Hebb (1904‚Äì1985)29. In his groundbreaking book The Organiza-
tion of Behavior (Hebb, 1949 ), he posited that neurons learn by positive reinforcement.
This became known as the Hebbian learning rule. These ideas inspired later work, such
as Rosenblatt‚Äôs perceptron learning algorithm, and laid the foundations of many stochastic
gradient descent algorithms that underpin deep learning today: reinforce desirable behav-
ior and diminish undesirable behavior to obtain good settings of the parameters in a neural
network.
22 Introduction
Biological inspiration is what gave neuralnetworks their name. For over a century (dating
back to the models of Alexander Bain, 1873, and James Sherrington, 1890), researchers
havetriedtoassemblecomputationalcircuitsthatresemblenetworksofinteractingneurons.
Over time, the interpretation of biology has become less literal, but the name stuck. At its
heart lie a few key principles that can be found in most networks today:
The alternation of linear and nonlinear processing units, often referred to as layers.
The use of the chain rule (also known as backpropagation ) for adjusting parameters in
the entire network at once.
Afterinitialrapidprogress, researchinneuralnetworkslanguishedfromaround1995until
2005. This was mainly due to two reasons. First, training a network is computationally
very expensive. While random-access memory was plentiful at the end of the past century,
computational power was scarce. Second, datasets were relatively small. In fact, Fisher‚Äôs
Iris dataset from 1936 was still a popular tool for testing the efficacy of algorithms. The
MNIST dataset with its 60,000 handwritten digits was considered huge.
Given the scarcity of data and computation, strong statistical tools such as kernel methods,
decision trees, and graphical models proved empirically superior in many applications.
Moreover, unlike neural networks, they did not require weeks to train and provided pre-
dictable results with strong theoretical guarantees.
1.5The Roadto Deep Learning
Much of this changed with the availabilityof massiveamounts of data, thanks to the World
Wide Web, the advent of companies serving hundreds of millions of users online, a dis-
semination of low-cost, high-quality sensors, inexpensive data storage (Kryder‚Äôs law), and
cheap computation (Moore‚Äôs law). In particular, the landscape of computation in deep
learning was revolutionized by advances in GPUs that were originally engineered for com-
puter gaming. Suddenly algorithms and models that seemed computationally infeasible
were within reach. This is best illustrated in tab_intro_decade .
:Dataset vs. computer memory and computational power
Table 1.5.1: label: tab_intro_decade
23 The Road to Deep Learning
Decade Dataset Mem-
oryFloating point calculations per
second
1970 100 (Iris) 1 KB 100 KF (Intel 8080)
1980 1 K (house prices in Boston) 100
KB1 MF (Intel 80186)
1990 10 K (optical character recog-
nition)10 MB 10 MF (Intel 80486)
2000 10 M (web pages) 100
MB1 GF (Intel Core)
2010 10 G (advertising) 1 GB 1 TF (NVIDIA C2050)
2020 1 T (social network) 100
GB1 PF (NVIDIA DGX-2)
Note that random-access memory has not kept pace with the growth in data. At the same
time, increases in computational power have outpaced the growth in datasets. This means
thatstatisticalmodelsneedtobecomemorememoryefficient,andsotheyarefreetospend
more computer cycles optimizing parameters, thanks to the increased compute budget.
Consequently, the sweet spot in machine learning and statistics moved from (generalized)
linear models and kernel methods to deep neural networks. This is also one of the rea-
sons why many of the mainstays of deep learning, such as multilayer perceptrons ( McCul-
loch and Pitts, 1943 ), convolutional neural networks ( LeCunetal., 1998), long short-term
memory( HochreiterandSchmidhuber,1997 ),andQ-Learning( WatkinsandDayan,1992 ),
were essentially ‚Äúrediscovered‚Äù in the past decade, after lying comparatively dormant for
considerable time.
The recent progress in statistical models, applications, and algorithms has sometimes been
likenedtotheCambrianexplosion: amomentofrapidprogressintheevolutionofspecies.
Indeed, the state of the art is not just a mere consequence of available resources applied
to decades-old algorithms. Note that the list of ideas below barely scratches the surface of
what has helped researchers achieve tremendous progress over the past decade.
Novelmethodsforcapacitycontrol,suchas dropout (Srivastava etal.,2014),havehelped
to mitigate overfitting. Here, noise is injected ( Bishop, 1995 ) throughout the neural
network during training.
Attention mechanisms solved a second problem that had plagued statistics for over a
century: how to increase the memory and complexity of a system without increasing
the number of learnable parameters. Researchers found an elegant solution by using
what can only be viewed as a learnable pointer structure (Bahdanau et al., 2014).
Rather than having to remember an entire text sequence, e.g., for machine translation
in a fixed-dimensional representation, all that needed to be stored was a pointer to the
intermediate state of the translation process. This allowed for significantly increased
accuracyforlongsequences,sincethemodelnolongerneededtoremembertheentire
sequence before commencing the generation of a new one.
Builtsolelyonattentionmechanisms,the Transformer architecture( Vaswanietal.,2017)
24 Introduction
30has demonstrated superior scalingbehavior: it performs better with an increase in
dataset size, model size, and amount of training compute ( Kaplanet al., 2020). This
architecture has demonstrated compelling success in a wide range of areas, such as
naturallanguageprocessing( Brownetal.,2020,Devlinetal.,2018),computervision
(Dosovitskiy et al., 2021,Liuet al., 2021), speech recognition ( Gulatiet al., 2020),
reinforcement learning ( Chenet al., 2021), and graph neural networks ( Dwivedi and
Bresson,2020 ). Forexample,asingleTransformerpretrainedonmodalitiesasdiverse
as text, images, joint torques, and button presses can play Atari, caption images, chat,
and control a robot ( Reedetal., 2022).
Modeling probabilities of text sequences, language models can predict text given other
text. Scaling up the data, model, and compute has unlocked a growing number of
capabilities of language models to perform desired tasks via human-like text genera-
tionbasedoninputtext( Aniletal.,2023,Brownetal.,2020,Chowdhery etal.,2022,
Hoffmann etal.,2022,OpenAI,2023 ,Raeetal.,2021,Touvronetal.,2023a,Touvron
et al., 2023b). For instance, aligning language models with human intent ( Ouyanget
al., 2022), OpenAI‚Äôs ChatGPT30allows users to interact with it in a conversational
way to solve problems, such as code debugging and creative writing.
Multi-stagedesigns,e.g.,viathememorynetworks( Sukhbaatar etal.,2015)andtheneu-
ralprogrammer-interpreter( ReedandDeFreitas,2015 )permittedstatisticalmodelers
todescribeiterativeapproachestoreasoning. Thesetoolsallowforaninternalstateof
the deep neural network to be modified repeatedly, thus carrying out subsequent steps
in a chain of reasoning, just as a processor can modify memory for a computation.
A key development in deep generative modeling was the invention of generative adver-
sarialnetworks (Goodfellow etal.,2014). Traditionally,statisticalmethodsfordensity
estimation and generative models focused on finding proper probability distributions
and (often approximate) algorithms for sampling from them. As a result, these algo-
rithms were largely limited by the lack of flexibility inherent in the statistical models.
The crucial innovation in generative adversarial networks was to replace the sampler
by an arbitrary algorithm with differentiable parameters. These are then adjusted in
such a way that the discriminator (effectively a two-sample test) cannot distinguish
fake from real data. Through the ability to use arbitrary algorithms to generate data,
density estimation was opened up to a wide variety of techniques. Examples of gal-
loping zebras ( Zhuet al., 2017) and of fake celebrity faces ( Karraset al., 2017) are
each testimony to this progress. Even amateur doodlers can produce photorealistic
images just based on sketches describing the layout of a scene ( Parketal., 2019).
Furthermore, while the diffusion process gradually adds random noise to data samples,
diffusionmodels (Hoetal.,2020,Sohl-Dickstein etal.,2015)learnthedenoisingpro-
cess to gradually construct data samples from random noise, reversing the diffusion
process. They have started to replace generative adversarial networks in more recent
deep generative models, such as in DALL-E 2 ( Rameshetal., 2022) and Imagen ( Sa-
hariaetal., 2022) for creative art and image generation based on text descriptions.
In many cases, a single GPU is insufficient for processing the large amounts of data
25 Success Stories
31
32
33
34
35
36
37
38
39
40
41
42availablefortraining. Overthepastdecadetheabilitytobuildparallelanddistributed
trainingalgorithmshasimprovedsignificantly. Oneofthekeychallengesindesigning
scalable algorithms is that the workhorse of deep learning optimization, stochastic
gradient descent, relies on relatively small minibatches of data to be processed. At
the same time, small batches limit the efficiency of GPUs. Hence, training on 1,024
GPUs with a minibatch size of, say, 32 images per batch amounts to an aggregate
minibatch of about 32,000 images. Work, first by Li ( 2017) and subsequently by You
etal.(2017) and Jiaetal.(2018) pushed the size up to 64,000 observations, reducing
training time forthe ResNet-50model on the ImageNetdataset to less than 7 minutes.
By comparison, training times were initially of the order of days.
The ability to parallelize computation has also contributed to progress in reinforcement
learning . This has led to significant progress in computers achieving superhuman
performanceontaskslikeGo,Atarigames,Starcraft,andinphysicssimulations(e.g.,
using MuJoCo) where environment simulators are available. See, e.g., Silver et al.
(2016)foradescriptionofsuchachievementsinAlphaGo. Inanutshell,reinforcement
learningworksbestifplentyof(state,action, reward)tuplesareavailable. Simulation
provides such an avenue.
Deep learning frameworks have played a crucial role in disseminating ideas. The first
generationofopen-sourceframeworksforneuralnetworkmodelingconsistedof Caffe
31,Torch32, andTheano33. Many seminal papers were written using these tools.
These have now been superseded by TensorFlow34(often used via its high-level API
Keras35),CNTK36,Caffe 237, andApache MXNet38. The third generation of
frameworks consists of so-called imperative tools for deep learning, a trend that was
arguably ignited by Chainer39, which used a syntax similar to Python NumPy to
describe models. This idea was adopted by both PyTorch40, theGluon API41of
MXNet, and JAX42.
The division of labor between system researchers building better tools and statistical mod-
elers building better neural networks has greatly simplified things. For instance, training a
linear logistic regression model used to be a nontrivial homework problem, worthy to give
to new machine learning Ph.D. students at Carnegie Mellon University in 2014. By now,
thistaskcanbeaccomplishedwithunder10linesofcode,puttingitfirmlywithinthereach
of any programmer.
1.6Success Stories
Artificial intelligence has a long history of delivering results that would be difficult to ac-
complish otherwise. For instance, mail sorting systems using optical character recognition
have been deployed since the 1990s. This is, after all, the source of the famous MNIST
dataset of handwritten digits. The same applies to reading checks for bank deposits and
scoring creditworthiness of applicants. Financial transactions are checked for fraud auto-
26 Introduction
matically. Thisformsthebackboneofmanye-commercepaymentsystems,suchasPayPal,
Stripe, AliPay, WeChat, Apple, Visa, and MasterCard. Computer programs for chess have
been competitive for decades. Machine learning feeds search, recommendation, personal-
ization, and ranking on the Internet. In other words, machine learning is pervasive, albeit
often hidden from sight.
ItisonlyrecentlythatAIhasbeeninthelimelight,mostlyduetosolutionstoproblemsthat
were considered intractable previously and that are directly related to consumers. Many of
such advances are attributed to deep learning.
Intelligent assistants, such as Apple‚Äôs Siri, Amazon‚Äôs Alexa, and Google‚Äôs assistant, are
able to respond to spoken requests with a reasonable degree of accuracy. This in-
cludes menial jobs, like turning on light switches, and more complex tasks, such as
arranging barber‚Äôs appointments and offering phone support dialog. This is likely the
most noticeable sign that AI is affecting our lives.
A key ingredient in digital assistants is their ability to recognize speech accurately. The
accuracyofsuchsystemshasgraduallyincreasedtothepointofachievingparitywith
humans for certain applications ( Xiongetal., 2018).
Object recognition has likewise come a long way. Identifying the object in a picture was
a fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC
Labs and University of Illinois at Urbana-Champaign achieved a top-five error rate
of 28% ( Linet al., 2010). By 2017, this error rate was reduced to 2.25% ( Huet al.,
2018). Similarly, stunning results have been achieved for identifying birdsong and for
diagnosing skin cancer.
Prowess in games used to provide a measuring stick for human ability. Starting from
TD-Gammon, a program for playing backgammon using temporal difference rein-
forcement learning, algorithmic and computational progress has led to algorithms for
a wide range of applications. Compared with backgammon, chess has a much more
complex state space and set of actions. DeepBlue beat Garry Kasparov using mas-
sive parallelism, special-purpose hardware and efficient search through the game tree
(Campbell etal.,2002). Goismoredifficultstill,duetoitshugestatespace. AlphaGo
reached human parity in 2015, using deep learning combined with Monte Carlo tree
sampling ( Silveretal., 2016). The challenge in Poker was that the state space is large
andonlypartiallyobserved(wedonotknowtheopponents‚Äôcards). Libratusexceeded
human performance in Poker using efficiently structured strategies ( Brown and Sand-
holm, 2017 ).
Another indication of progress in AI is the advent of self-driving vehicles. While full
autonomy is not yet within reach, excellent progress has been made in this direction,
with companies such as Tesla, NVIDIA, and Waymo shipping products that enable
partial autonomy. What makes full autonomy so challenging is that proper driving
requires the ability to perceive, to reason and to incorporate rules into a system. At
present, deep learning is used primarily in the visual aspect of these problems. The
rest is heavily tuned by engineers.
27 The Essence of Deep Learning
This barely scratches the surface of significant applications of machine learning. For in-
stance, robotics, logistics, computational biology, particle physics, and astronomy owe
some of their most impressive recent advances at least in parts to machine learning, which
is thus becoming a ubiquitous tool for engineers and scientists.
Frequently, questions about a coming AI apocalypse and the plausibility of a singularity
have been raised in non-technical articles. The fear is that somehow machine learning
systems will become sentient and make decisions, independently of their programmers,
that directly impact the lives of humans. To some extent, AI already affects the livelihood
of humans in direct ways: creditworthiness is assessed automatically, autopilots mostly
navigate vehicles, decisions about whether to grant bail use statistical data as input. More
frivolously, we can ask Alexa to switch on the coffee machine.
Fortunately, we are far from a sentient AI system that could deliberately manipulate its
human creators. First, AI systems are engineered, trained, and deployed in a specific, goal-
orientedmanner. Whiletheirbehaviormightgivetheillusionofgeneralintelligence,itisa
combination of rules, heuristics and statistical models that underlie the design. Second, at
present, there are simply no tools for artificialgeneralintelligence that are able to improve
themselves,reasonaboutthemselves,andthatareabletomodify,extend,andimprovetheir
own architecture while trying to solve general tasks.
A much more pressing concern is how AI is being used in our daily lives. It is likely that
many routine tasks, currently fulfilled by humans, can and will be automated. Farm robots
will likely reduce the costs for organic farmers but they will also automate harvesting op-
erations. This phase of the industrial revolution may have profound consequences for large
swaths of society, since menial jobs provide much employment in many countries. Fur-
thermore, statistical models, when applied without care, can lead to racial, gender, or age
bias and raise reasonable concerns about procedural fairness if automated to drive conse-
quential decisions. It is important to ensure that these algorithms are used with care. With
what we know today, this strikes us as a much more pressing concern than the potential of
malevolent superintelligence for destroying humanity.
1.7The Essence of Deep Learning
Thusfar,wehavetalkedinbroadtermsaboutmachinelearning. Deeplearningisthesubset
of machine learning concerned with models based on many-layered neural networks. It is
deepinpreciselythesensethatitsmodelslearnmany layersoftransformations. Whilethis
mightsoundnarrow,deeplearninghasgivenrisetoadizzyingarrayofmodels,techniques,
problem formulations, and applications. Many intuitions have been developed to explain
the benefits of depth. Arguably, all machine learning has many layers of computation, the
first consisting of feature processing steps. What differentiates deep learning is that the
operations learned at each of the many layers of representations are learned jointly from
data.
28 Introduction
Theproblemsthatwehavediscussedsofar, suchaslearningfromtherawaudiosignal, the
raw pixel values of images, or mapping between sentences of arbitrary lengths and their
counterparts in foreign languages, are those where deep learning excels and traditional
methods falter. It turns out that these many-layered models are capable of addressing low-
level perceptual data in a way that previous tools could not. Arguably the most significant
commonality in deep learning methods is end-to-end training . That is, rather than assem-
bling a system based on components that are individually tuned, one builds the system and
then tunes their performance jointly. For instance, in computer vision scientists used to
separate the process of feature engineering from the process of building machine learn-
ing models. The Canny edge detector ( Canny, 1987 ) and Lowe‚Äôs SIFT feature extractor
(Lowe, 2004 ) reigned supreme for over a decade as algorithms for mapping images into
feature vectors. In bygone days, the crucial part of applying machine learning to these
problems consisted of coming up with manually-engineered ways of transforming the data
into some form amenable to shallow models. Unfortunately, there is only so much that
humans can accomplish by ingenuity in comparison with a consistent evaluation over mil-
lions of choices carried out automatically by an algorithm. When deep learning took over,
these feature extractors were replaced by automatically tuned filters that yielded superior
accuracy.
Thus, one key advantage of deep learning is that it replaces not only the shallow models at
theendoftraditionallearningpipelines,butalsothelabor-intensiveprocessoffeatureengi-
neering. Moreover,byreplacingmuchofthedomain-specificpreprocessing,deeplearning
has eliminated many of the boundaries that previously separated computer vision, speech
recognition, naturallanguageprocessing, medicalinformatics, andother applicationareas,
thereby offering a unified set of tools for tackling diverse problems.
Beyond end-to-end training, we are experiencing a transition from parametric statistical
descriptions to fullynonparametric models. When datais scarce, one needsto relyonsim-
plifyingassumptionsaboutrealityinordertoobtainusefulmodels. Whendataisabundant,
these can be replaced by nonparametric models that better fit the data. To some extent, this
mirrors the progress that physics experienced in the middle of the previous century with
the availability of computers. Rather than solving by hand parametric approximations of
how electrons behave, one can now resort to numerical simulations of the associated par-
tial differential equations. This has led to much more accurate models, albeit often at the
expense of interpretation.
Another difference from previous work is the acceptance of suboptimal solutions, dealing
with nonconvex nonlinear optimization problems, and the willingness to try things before
proving them. This new-found empiricism in dealing with statistical problems, combined
with a rapid influx of talent has led to rapid progress in the development of practical algo-
rithms, albeit in many cases at the expense of modifying and re-inventing tools that existed
for decades.
Intheend, thedeeplearningcommunitypridesitselfonsharingtoolsacrossacademicand
corporate boundaries, releasing many excellent libraries, statistical models, and trained
networks as open source. It is in this spirit that the notebooks forming this book are freely
available for distribution and use. We have worked hard to lower the barriers of access for
29 Summary
43anyonewishingtolearnaboutdeeplearningandwehopethatourreaderswillbenefitfrom
this.
1.8Summary
Machine learning studies how computer systems can leverage experience (often data) to
improve performance at specific tasks. It combines ideas from statistics, data mining, and
optimization. Often, it is used as a means of implementing AI solutions. As a class of
machine learning, representational learning focuses on how to automatically find the ap-
propriate way to represent data. Considered as multi-level representation learning through
learning many layers of transformations, deep learning replaces not only the shallow mod-
els at the end of traditional machinelearning pipelines, but also the labor-intensiveprocess
of feature engineering. Much of the recent progress in deep learning has been triggered
by an abundance of data arising from cheap sensors and Internet-scale applications, and
by significant progress in computation, mostly through GPUs. Furthermore, the availabil-
ity of efficient deep learning frameworks has made design and implementation of whole
system optimization significantly easier, and this is a key component in obtaining high
performance.
1.9Exercises
1.Which parts of code that you are currently writing could be ‚Äúlearned‚Äù, i.e., improved
by learning and automatically determining design choices that are made in your code?
Does your code include heuristic design choices? What data might you need to learn
the desired behavior?
2.Which problems that you encounter have many examples for their solution, yet no spe-
cificwayforautomatingthem? Thesemaybeprimecandidatesforusingdeeplearning.
3.Describe the relationships between algorithms, data, and computation. How do char-
acteristics of the data and the current available computational resources influence the
appropriateness of various algorithms?
4.Name some settings where end-to-end training is not currently the default approach but
where it might be useful.
Discussions43.
2 Preliminaries
To prepare for your dive into deep learning, you will need a few survival skills: (i) tech-
niques for storing and manipulating data; (ii) libraries for ingesting and preprocessing data
from a variety of sources; (iii) knowledge of the basic linear algebraic operations that we
apply to high-dimensional data elements; (iv) just enough calculus to determine which di-
rection to adjust each parameter in order to decrease the loss function; (v) the ability to
automatically compute derivatives so that you can forget much of the calculus you just
learned; (vi) some basic fluency in probability, our primary language for reasoning under
uncertainty; and(vii)someaptitudeforfindinganswersintheofficialdocumentationwhen
you get stuck.
Inshort, thischapterprovidesarapidintroductiontothebasicsthatyouwillneedtofollow
mostof the technical content in this book.
2.1Data Manipulation
In order to get anything done, we need some way to store and manipulate data. Generally,
there are two important things we need to do with data: (i) acquire them; and (ii) process
them once they are inside the computer. There is no point in acquiring data without some
way to store it, so to start, let‚Äôs get our hands dirty with ùëõ-dimensional arrays, which we
alsocalltensors. IfyoualreadyknowtheNumPyscientificcomputingpackage,thiswillbe
a breeze. For all modern deep learning frameworks, the tensor class (ndarray in MXNet,
Tensorin PyTorch and TensorFlow) resembles NumPy‚Äôs ndarray , with a few killer fea-
tures added. First, the tensor class supports automatic differentiation. Second, it leverages
GPUs to accelerate numerical computation, whereas NumPy only runs on CPUs. These
properties make neural networks both easy to code and fast to run.
2.1.1GettingStarted
To start, we import the PyTorch library. Note that the package name is torch.
import torch
A tensor represents a (possibly multidimensional) array of numerical values. In the one-
dimensionalcase, i.e., whenonlyoneaxisisneededforthedata, atensoriscalleda vector.
30
31 Data Manipulation
With two axes, a tensor is called a matrix. Withùëò > 2axes, we drop the specialized names
and just refer to the object as a ùëòth-ordertensor .
PyTorch provides a variety of functions for creating new tensors prepopulated with values.
Forexample,byinvoking arange(n) ,wecancreateavectorofevenlyspacedvalues,start-
ing at 0 (included) and ending at n(not included). By default, the interval size is 1. Unless
otherwisespecified,newtensorsarestoredinmainmemoryanddesignatedforCPU-based
computation.
x=torch .arange( 12, dtype =torch .float32)
x
tensor([ 0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.])
Each of these values is called an element of the tensor. The tensor xcontains 12 elements.
We can inspect the total number of elements in a tensor via its numelmethod.
x.numel()
12
Wecanaccessatensor‚Äôs shape(thelengthalongeachaxis)byinspectingits shapeattribute.
Because we are dealing with a vector here, the shapecontains just a single element and is
identical to the size.
x.shape
torch .Size([ 12])
Wecanchangetheshapeofatensorwithoutalteringitssizeorvalues,byinvoking reshape .
For example, we can transform our vector xwhose shape is (12,) to a matrix Xwith shape
(3,4). Thisnewtensorretainsallelementsbutreconfiguresthemintoamatrix. Noticethat
theelementsofourvectorarelaidoutonerowatatimeandthus x[3] == X[0, 3] .
X=x.reshape( 3,4)
X
tensor([[ 0.,1.,2.,3.],
[4.,5.,6.,7.],
[8.,9.,10.,11.]])
Note that specifying every shape component to reshape is redundant. Because we already
know our tensor‚Äôs size, we can work out one component of the shape given the rest. For
example, given a tensor of size ùëõand target shape ( ‚Ñé,ùë§), we know that ùë§=ùëõ¬ù‚Ñé. To
32 Preliminaries
automaticallyinferonecomponentoftheshape,wecanplacea -1fortheshapecomponent
thatshouldbeinferredautomatically. Inourcase, insteadofcalling x.reshape(3, 4) , we
could have equivalently called x.reshape(-1, 4) orx.reshape(3, -1) .
Practitioners often need to work with tensors initialized to contain all 0s or 1s. We can
construct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the zerosfunc-
tion.
torch .zeros(( 2,3,4))
tensor([[[ 0.,0.,0.,0.],
[0.,0.,0.,0.],
[0.,0.,0.,0.]],
[[0.,0.,0.,0.],
[0.,0.,0.,0.],
[0.,0.,0.,0.]]])
Similarly, we can create a tensor with all 1s by invoking ones.
torch .ones(( 2,3,4))
tensor([[[ 1.,1.,1.,1.],
[1.,1.,1.,1.],
[1.,1.,1.,1.]],
[[1.,1.,1.,1.],
[1.,1.,1.,1.],
[1.,1.,1.,1.]]])
We often wish to sample each element randomly (and independently) from a given prob-
ability distribution. For example, the parameters of neural networks are often initialized
randomly. The following snippet creates a tensor with elements drawn from a standard
Gaussian (normal) distribution with mean 0 and standard deviation 1.
torch .randn( 3,4)
tensor([[ 0.1351 ,-0.9099 ,-0.2028 ,2.1937 ],
[-0.3200 ,-0.7545 ,0.8086 ,-1.8730 ],
[0.3929 ,0.4931 ,0.9114 ,-0.7072 ]])
Finally, we can construct tensors by supplying the exact values for each element by sup-
plying (possibly nested) Python list(s) containing numerical literals. Here, we construct a
matrix with a list of lists, where the outermost list corresponds to axis 0, and the inner list
corresponds to axis 1.
33 Data Manipulation
torch .tensor([[ 2,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])
tensor([[ 2,1,4,3],
[1,2,3,4],
[4,3,2,1]])
2.1.2Indexingand Slicing
AswithPythonlists,wecanaccesstensorelementsbyindexing(startingwith0). Toaccess
anelementbasedonitspositionrelativetotheendofthelist,wecanusenegativeindexing.
Finally, we can access whole ranges of indices via slicing (e.g., X[start:stop] ), where
the returned value includes the first index ( start)but not the last (stop). Finally, when
only one index (or slice) is specified for a ùëòth-order tensor, it is applied along axis 0. Thus,
in the following code, [-1]selects the last row and [1:3]selects the second and third
rows.
X[-1], X[ 1:3]
(tensor([ 8.,9.,10.,11.]),
tensor([[ 4.,5.,6.,7.],
[8.,9.,10.,11.]]))
Beyondreadingthem,wecanalso writeelementsofamatrixbyspecifyingindices.
X[1,2]=17
X
tensor([[ 0.,1.,2.,3.],
[4.,5.,17.,7.],
[8.,9.,10.,11.]])
If we want to assign multiple elements the same value, we apply the indexing on the left-
handsideoftheassignmentoperation. Forinstance, [:2, :] accessesthefirstandsecond
rows, where :takes all the elements along axis 1 (column). While we discussed indexing
formatrices,thisalsoworksforvectorsandfortensorsofmorethantwodimensions.
X[:2, :] =12
X
tensor([[ 12.,12.,12.,12.],
[12.,12.,12.,12.],
[8.,9.,10.,11.]])
34 Preliminaries
2.1.3Operations
Now that we know how to construct tensors and how to read from and write to their ele-
ments,wecanbegintomanipulatethemwithvariousmathematicaloperations. Amongthe
most useful of these are the elementwise operations. These apply a standard scalar opera-
tion to each element of a tensor. For functions that take two tensors as inputs, elementwise
operations apply some standard binary operator on each pair of corresponding elements.
We can create an elementwise function from any function that maps from a scalar to a
scalar.
In mathematical notation, we denote such unaryscalar operators (taking one input) by the
signatureùëì:R!R. This just means that the function maps from any real number onto
some other real number. Most standard operators, including unary ones like ùëíùë•, can be
applied elementwise.
torch .exp(x)
tensor([ 162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,
162754.7969 ,162754.7969 ,162754.7969 , 2980.9580 , 8103.0840 ,
22026.4648 ,59874.1406 ])
Likewise, we denote binaryscalar operators, which map pairs of real numbers to a (single)
real number via the signature ùëì:R,R!R. Given any two vectors uandvof the
same shape , and a binary operator ùëì, we can produce a vector c=ùêπ¬πu,v¬∫by setting
ùëêùëñ ùëì¬πùë¢ùëñ,ùë£ùëñ¬∫for allùëñ, whereùëêùëñ,ùë¢ùëñ, andùë£ùëñare theùëñthelements of vectors c,u, andv.
Here, we produced the vector-valued ùêπ:Rùëë,Rùëë!Rùëëbyliftingthe scalar function to an
elementwise vector operation. The common standard arithmetic operators for addition ( +),
subtraction( -),multiplication( *),division( /),andexponentiation( **)haveallbeen lifted
to elementwise operations for identically-shaped tensors of arbitrary shape.
x=torch .tensor([ 1.0,2,4,8])
y=torch .tensor([ 2,2,2,2])
x+y, x -y, x *y, x /y, x **y
(tensor([ 3.,4.,6.,10.]),
tensor([ -1.,0.,2.,6.]),
tensor([ 2.,4.,8.,16.]),
tensor([ 0.5000 ,1.0000 ,2.0000 ,4.0000 ]),
tensor([ 1.,4.,16.,64.]))
In addition to elementwise computations, we can also perform linear algebraic operations,
such as dot products and matrix multiplications. We will elaborate on these in Section
2.3.
We can also concatenate multiple tensors, stacking them end-to-end to form a larger one.
Wejustneedtoprovidealistoftensorsandtellthesystemalongwhichaxistoconcatenate.
The example below shows what happens when we concatenate two matrices along rows
35 Data Manipulation
(axis 0) instead of columns (axis 1). We can see that the first output‚Äôs axis-0 length ( 6) is
the sum of the two input tensors‚Äô axis-0 lengths ( 3¬∏3); while the second output‚Äôs axis-1
length ( 8) is the sum of the two input tensors‚Äô axis-1 lengths ( 4¬∏4).
X=torch .arange( 12, dtype =torch .float32) .reshape(( 3,4))
Y=torch .tensor([[ 2.0,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])
torch .cat((X, Y), dim =0), torch .cat((X, Y), dim =1)
(tensor([[ 0.,1.,2.,3.],
[4.,5.,6.,7.],
[8.,9.,10.,11.],
[2.,1.,4.,3.],
[1.,2.,3.,4.],
[4.,3.,2.,1.]]),
tensor([[ 0.,1.,2.,3.,2.,1.,4.,3.],
[4.,5.,6.,7.,1.,2.,3.,4.],
[8.,9.,10.,11.,4.,3.,2.,1.]]))
Sometimes, we wantto construct a binary tensor via logicalstatements . Take X == Yas an
example. Foreachposition i, j,ifX[i, j] andY[i, j] areequal,thenthecorresponding
entry in the result takes value 1, otherwise it takes value 0.
X==Y
tensor([[ False ,True ,False ,True ],
[False ,False ,False ,False ],
[False ,False ,False ,False ]])
Summing all the elements in the tensor yields a tensor with only one element.
X.sum()
tensor( 66.)
2.1.4Broadcasting
By now, you know how to perform elementwise binary operations on two tensors of the
same shape. Under certain conditions, even when shapes differ, we can still perform ele-
mentwisebinaryoperationsbyinvokingthe broadcastingmechanism . Broadcastingworks
according to the following two-step procedure: (i) expand one or both arrays by copying
elementsalongaxeswithlength1sothatafterthistransformation, thetwotensorshavethe
same shape; (ii) perform an elementwise operation on the resulting arrays.
a=torch .arange( 3).reshape(( 3,1))
b=torch .arange( 2).reshape(( 1,2))
a, b
36 Preliminaries
(tensor([[ 0],
[1],
[2]]),
tensor([[ 0,1]]))
Since aandbare31and 12matrices, respectively, their shapes do not match up.
Broadcasting produces a larger 32matrix by replicating matrix aalong the columns and
matrix balong the rows before adding them elementwise.
a+b
tensor([[ 0,1],
[1,2],
[2,3]])
2.1.5SavingMemory
Running operations can cause new memory to be allocated to host results. For example, if
wewrite Y = X + Y ,wedereferencethetensorthat Yusedtopointtoandinsteadpoint Yat
the newly allocated memory. We can demonstrate this issue with Python‚Äôs id()function,
which gives us the exact address of the referenced object in memory. Note that after we
runY = Y + X ,id(Y)points to a different location. That is because Python first evaluates
Y + X, allocating new memory for the result and then points Yto this new location in
memory.
before =id(Y)
Y=Y+X
id(Y) ==before
False
This might be undesirable for two reasons. First, we do not want to run around allocat-
ing memory unnecessarily all the time. In machine learning, we often have hundreds of
megabytes of parameters and update all of them multiple times per second. Whenever
possible, we want to perform these updates in place. Second, we might point at the same
parameters from multiple variables. If we do not update in place, we must be careful to
update all of these references, lest we spring a memory leak or inadvertently refer to stale
parameters.
Fortunately, performing in-place operations is easy. We can assign the result of an oper-
ation to a previously allocated array Yby using slice notation: Y[:] = <expression> .
To illustrate this concept, we overwrite the values of tensor Z, after initializing it, using
zeros_like , to have the same shape as Y.
37 Data Manipulation
Z=torch .zeros_like(Y)
print ('id(Z): ',id(Z))
Z[:] =X+Y
print ('id(Z): ',id(Z))
id(Z): 140381179266448
id(Z): 140381179266448
If the value of Xis not reused in subsequent computations, we can also use X[:] = X + Y
orX += Yto reduce the memory overhead of the operation.
before =id(X)
X+=Y
id(X) ==before
True
2.1.6Conversionto Other Python Objects
Converting to a NumPy tensor ( ndarray ), or vice versa, is easy. The torch tensor and
NumPy array will share their underlying memory, and changing one through an in-place
operation will also change the other.
A=X.numpy()
B=torch .from_numpy(A)
type (A), type (B)
(numpy .ndarray, torch .Tensor)
To convert a size-1 tensor to a Python scalar, we can invoke the itemfunction or Python‚Äôs
built-in functions.
a=torch .tensor([ 3.5])
a, a .item(), float (a), int(a)
(tensor([ 3.5000 ]), 3.5,3.5,3)
2.1.7Summary
The tensor class is the main interface for storing and manipulating data in deep learning li-
braries. Tensorsprovideavarietyoffunctionalitiesincludingconstructionroutines; index-
ingandslicing;basicmathematicsoperations;broadcasting;memory-efficientassignment;
and conversion to and from other Python objects.
38 Preliminaries
44
45
462.1.8Exercises
1.Run the code in this section. Change the conditional statement X == YtoX < YorX >
Y, and then see what kind of tensor you can get.
2.Replace the two tensors that operate by element in the broadcasting mechanism with
other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?
Discussions44.
2.2Data Preprocessing
So far, we have been working with synthetic data that arrived in ready-made tensors. How-
ever, to apply deep learning in the wild we must extract messy data stored in arbitrary
formats, andpreprocessittosuitourneeds. Fortunately,the pandaslibrary45candomuch
of the heavy lifting. This section, while no substitute for a proper pandastutorial46, will
give you a crash course on some of the most common routines.
2.2.1Readingthe Dataset
Comma-separatedvalues(CSV)filesareubiquitousforthestoringoftabular(spreadsheet-
like) data. In them, each line corresponds to one record and consists of several (comma-
separated)fields,e.g.,‚ÄúAlbertEinstein,March141879,Ulm,Federalpolytechnicschool,field
of gravitational physics‚Äù. To demonstrate how to load CSV files with pandas, we create a
CSV file below ../data/house_tiny.csv . This file represents a dataset of homes, where
each row corresponds to a distinct home and the columns correspond to the number of
rooms ( NumRooms ), the roof type ( RoofType ), and the price ( Price).
import os
os.makedirs(os .path .join( '..','data '), exist_ok =True )
data_file =os.path .join( '..','data ','house_tiny.csv ')
with open (data_file, 'w')asf:
f.write( '''NumRooms,RoofType,Price
NA,NA,127500
2,NA,106000
4,Slate,178100
NA,NA,140000 ''')
Now let‚Äôs import pandasand load the dataset with read_csv .
import pandas aspd
data =pd.read_csv(data_file)
print (data)
39 Data Preprocessing
NumRooms RoofType Price
0 NaN NaN 127500
1 2.0 NaN 106000
2 4.0 Slate 178100
3 NaN NaN 140000
2.2.2Data Preparation
In supervised learning, we train models to predict a designated targetvalue, given some
set ofinputvalues. Our first step in processing the dataset is to separate out columns cor-
responding to input versus target values. We can select columns either by name or via
integer-location based indexing ( iloc).
You might have noticed that pandasreplaced all CSV entries with value NAwith a spe-
cialNaN(not a number ) value. This can also happen whenever an entry is empty, e.g.,
‚Äú3‚Äû,270000‚Äù. These are called missingvalues and they are the ‚Äúbed bugs‚Äù of data science,
a persistent menace that you will confront throughout your career. Depending upon the
context, missing values might be handled either via imputation ordeletion. Imputation re-
places missing values with estimates of their values while deletion simply discards either
those rows or those columns that contain missing values.
Herearesomecommonimputationheuristics. Forcategoricalinputfields,wecantreat NaN
asacategory. Sincethe RoofType columntakesvalues SlateandNaN,pandascanconvert
thiscolumnintotwocolumns RoofType_Slate andRoofType_nan . Arowwhoserooftype
isSlatewill set values of RoofType_Slate andRoofType_nan to 1 and 0, respectively.
The converse holds for a row with a missing RoofType value.
inputs, targets =data .iloc[:, 0:2], data .iloc[:, 2]
inputs =pd.get_dummies(inputs, dummy_na =True )
print (inputs)
NumRooms RoofType_Slate RoofType_nan
0 NaN False True
1 2.0 False True
2 4.0 True False
3 NaN False True
For missing numerical values, one common heuristic is to replace the NaNentries with the
mean value of the corresponding column.
inputs =inputs .fillna(inputs .mean())
print (inputs)
NumRooms RoofType_Slate RoofType_nan
0 3.0 False True
1 2.0 False True
(continues on next page)
40 Preliminaries
47
48
49
50
51(continued from previous page)
2 4.0 True False
3 3.0 False True
2.2.3Conversionto the TensorFormat
Now that all the entries in inputsandtargets are numerical, we can load them into a
tensor (recall Section 2.1 ).
import torch
X=torch .tensor(inputs .to_numpy(dtype =float ))
y=torch .tensor(targets .to_numpy(dtype =float ))
X, y
(tensor([[ 3.,0.,1.],
[2.,0.,1.],
[4.,1.,0.],
[3.,0.,1.]], dtype =torch .float64),
tensor([ 127500. ,106000. ,178100. ,140000. ], dtype =torch .float64))
2.2.4Discussion
You now know how to partition data columns, impute missing variables, and load pan-
dasdata into tensors. In Section 5.7 , you will pick up some more data processing skills.
While this crash course kept things simple, data processing can get hairy. For example,
rather than arriving in a single CSV file, our dataset might be spread across multiple files
extractedfromarelationaldatabase. Forinstance, inane-commerceapplication, customer
addressesmightliveinonetableandpurchasedatainanother. Moreover,practitionersface
myriaddatatypesbeyondcategoricalandnumeric, forexample, textstrings, images, audio
data, and point clouds. Oftentimes, advanced tools and efficient algorithms are required
in order to prevent data processing from becoming the biggest bottleneck in the machine
learning pipeline. These problems will arise when we get to computer vision and natural
languageprocessing. Finally,wemustpayattentiontodataquality. Real-worlddatasetsare
often plagued by outliers, faulty measurements from sensors, and recording errors, which
must be addressed before feeding the data into any model. Data visualization tools such as
seaborn47,Bokeh48,ormatplotlib49canhelpyoutomanuallyinspectthedataanddevelop
intuitions about the type of problems you may need to address.
2.2.5Exercises
1.Try loading datasets, e.g., Abalone from the UCI Machine Learning Repository50and
inspect their properties. What fraction of them has missing values? What fraction of
the variables is numerical, categorical, or text?
2.Try indexing and selecting data columns by name rather than by column number. The
pandas documentation on indexing51has further details on how to do this.
41 Linear Algebra
52
53
543.How large a dataset do you think you could load this way? What might be the limita-
tions? Hint: consider the time to read the data, representation, processing, and memory
footprint. Try this out on your laptop. What happens if you try it out on a server?
4.How would you deal with data that has a very large number of categories? What if the
category labels are all unique? Should you include the latter?
5.What alternatives to pandas can you think of? How about loading NumPy tensors from
a file52? Check out Pillow53, the Python Imaging Library.
Discussions54.
2.3LinearAlgebra
By now, we can load datasets into tensors and manipulate these tensors with basic math-
ematical operations. To start building sophisticated models, we will also need a few tools
fromlinearalgebra. Thissectionoffersagentleintroductiontothemostessentialconcepts,
starting from scalar arithmetic and ramping up to matrix multiplication.
import torch
2.3.1Scalars
Most everyday mathematics consists of manipulating numbers one at a time. Formally, we
call these values scalars. For example, the temperature in Palo Alto is a balmy 72degrees
Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the
expressionùëê=5
9¬πùëì 32¬∫, settingùëìto72. In this equation, the values 5,9, and 32are
constant scalars. The variables ùëêandùëìin general represent unknown scalars.
We denote scalars by ordinary lower-cased letters (e.g., ùë•,ùë¶, andùëß) and the space of all
(continuous) real-valued scalars by R. For expedience, we will skip past rigorous defini-
tions ofspaces: just remember that the expression ùë•2Ris a formal way to say that ùë•is
a real-valued scalar. The symbol 2(pronounced ‚Äúin‚Äù) denotes membership in a set. For
example,ùë•,ùë¶2f0,1gindicates that ùë•andùë¶are variables that can only take values 0or
1.
Scalars are implemented as tensors that contain only one element. Below, we assign two
scalars and perform the familiar addition, multiplication, division, and exponentiation op-
erations.
x=torch .tensor( 3.0)
y=torch .tensor( 2.0)
x+y, x *y, x /y, x **y
42 Preliminaries
(tensor( 5.), tensor( 6.), tensor( 1.5000 ), tensor( 9.))
2.3.2Vectors
For current purposes, you can think of a vector as a fixed-length array of scalars. As with
their code counterparts, we call these scalars the elements of the vector (synonyms include
entriesandcomponents ). When vectors represent examples from real-world datasets, their
values hold some real-world significance. For example, if we were training a model to
predicttheriskofaloandefaulting, wemightassociateeachapplicantwithavectorwhose
componentscorrespondtoquantitiesliketheirincome,lengthofemployment,ornumberof
previous defaults. If we were studying the risk of heart attack, each vector might represent
a patient and its components might correspond to their most recent vital signs, cholesterol
levels, minutes of exercise per day, etc. We denote vectors by bold lowercase letters, (e.g.,
x,y, andz).
Vectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary
lengths, subject to memory limitations. Caution: in Python, as in most programming lan-
guages, vector indices start at 0, also known as zero-based indexing , whereas in linear
algebra subscripts begin at 1(one-based indexing).
x=torch .arange( 3)
x
tensor([ 0,1,2])
We can refer to an element of a vector by using a subscript. For example, ùë•2denotes the
secondelementof x. Sinceùë•2isascalar,wedonotboldit. Bydefault,wevisualizevectors
by stacking their elements vertically.
x=2666664ùë•1
...
ùë•ùëõ3777775, (2.3.1)
Hereùë•1,...,ùë•ùëõare elements of the vector. Later on, we will distinguish between such
column vectors androw vectors whose elements are stacked horizontally. Recall that we
access a tensor‚Äôs elements via indexing.
x[2]
tensor( 2)
To indicate that a vector contains ùëõelements, we write x2Rùëõ. Formally, we call ùëõthe
dimensionality ofthevector. Incode,thiscorrespondstothetensor‚Äôslength,accessiblevia
Python‚Äôs built-in lenfunction.
43 Linear Algebra
len(x)
3
We can also access the length via the shapeattribute. The shape is a tuple that indicates
a tensor‚Äôs length along each axis. Tensors with just one axis have shapes with just one
element.
x.shape
torch .Size([ 3])
Oftentimes,theword‚Äúdimension‚Äùgetsoverloadedtomeanboththenumberofaxesandthe
length along a particular axis. To avoid this confusion, we use orderto refer to the number
of axes and dimensionality exclusively to refer to the number of components.
2.3.3Matrices
Justasscalarsare 0th-ordertensorsandvectorsare 1st-ordertensors,matricesare 2nd-order
tensors. Wedenotematricesbyboldcapitalletters(e.g., X,Y, andZ), andrepresentthem
in code by tensors with two axes. The expression A2Rùëöùëõindicates that a matrix A
containsùëöùëõreal-valued scalars, arranged as ùëörows andùëõcolumns. When ùëö=ùëõ, we
say that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an
individualelement,wesubscriptboththerowandcolumnindices,e.g., ùëéùëñùëóisthevaluethat
belongs to A‚Äôsùëñthrow andùëóthcolumn:
A=266666664ùëé11ùëé12ùëé1ùëõ
ùëé21ùëé22ùëé2ùëõ
............
ùëéùëö1ùëéùëö2ùëéùëöùëõ377777775. (2.3.2)
In code, we represent a matrix A2Rùëöùëõby a 2nd-order tensor with shape ( ùëö,ùëõ). We can
convert any appropriately sized ùëöùëõtensor into an ùëöùëõmatrix by passing the desired
shape to reshape :
A=torch .arange( 6).reshape( 3,2)
A
tensor([[ 0,1],
[2,3],
[4,5]])
Sometimes we want to flip the axes. When we exchange a matrix‚Äôs rows and columns, the
result is called its transpose . Formally, we signify a matrix A‚Äôs transpose by A>and if
44 Preliminaries
B=A>, thenùëèùëñùëó=ùëéùëóùëñfor allùëñandùëó. Thus, the transpose of an ùëöùëõmatrix is anùëõùëö
matrix:
A>=266666664ùëé11ùëé21... ùëéùëö1
ùëé12ùëé22... ùëéùëö2
............
ùëé1ùëõùëé2ùëõ... ùëéùëöùëõ377777775. (2.3.3)
In code, we can access any matrix‚Äôs transpose as follows:
A.T
tensor([[ 0,2,4],
[1,3,5]])
Symmetricmatricesarethesubsetofsquarematricesthatareequaltotheirowntransposes:
A=A>. The following matrix is symmetric:
A=torch .tensor([[ 1,2,3], [ 2,0,4], [ 3,4,5]])
A==A.T
tensor([[ True ,True ,True ],
[True ,True ,True ],
[True ,True ,True ]])
Matrices are useful for representing datasets. Typically, rows correspond to individual
records and columns correspond to distinct attributes.
2.3.4Tensors
While you can go far in your machine learning journey with only scalars, vectors, and
matrices, eventually you may need to work with higher-order tensors. Tensors give us
a generic way of describing extensions to ùëõth-order arrays. We call software objects of
thetensor class ‚Äútensors‚Äù precisely because they too can have arbitrary numbers of axes.
While it may be confusing to use the word tensorfor both the mathematical object and its
realization in code, our meaning should usually be clear from context. We denote general
tensors by capital letters with a special font face (e.g., X,Y, andZ) and their indexing
mechanism (e.g., ùë•ùëñùëóùëòand¬ªX¬º1,2ùëñ 1,3) follows naturally from that of matrices.
Tensors will become more important when we start working with images. Each image
arrives as a 3rd-order tensor with axes corresponding to the height, width, and channel. At
eachspatiallocation,theintensitiesofeachcolor(red,green,andblue)arestackedalongthe
channel. Furthermore, a collection of images is represented in code by a 4th-order tensor,
wheredistinctimagesareindexedalongthefirstaxis. Higher-ordertensorsareconstructed,
as were vectors and matrices, by growing the number of shape components.
45 Linear Algebra
torch .arange( 24).reshape( 2,3,4)
tensor([[[ 0,1,2,3],
[4,5,6,7],
[8,9,10,11]],
[[12,13,14,15],
[16,17,18,19],
[20,21,22,23]]])
2.3.5BasicPropertiesof Tensor Arithmetic
Scalars,vectors,matrices,andhigher-ordertensorsallhavesomehandyproperties. Forex-
ample,elementwiseoperationsproduceoutputsthathavethesameshapeastheiroperands.
A=torch .arange( 6, dtype =torch .float32) .reshape( 2,3)
B=A.clone() # Assign a copy of A to B by allocating new memory
A, A +B
(tensor([[ 0.,1.,2.],
[3.,4.,5.]]),
tensor([[ 0.,2.,4.],
[6.,8.,10.]]))
The elementwise product of two matrices is called their Hadamard product (denoted).
WecanspellouttheentriesoftheHadamardproductoftwomatrices A,B2Rùëöùëõ:
AB=266666664ùëé11ùëè11ùëé12ùëè12... ùëé 1ùëõùëè1ùëõ
ùëé21ùëè21ùëé22ùëè22... ùëé 2ùëõùëè2ùëõ
............
ùëéùëö1ùëèùëö1ùëéùëö2ùëèùëö2... ùëéùëöùëõùëèùëöùëõ377777775. (2.3.4)
A*B
tensor([[ 0.,1.,4.],
[9.,16.,25.]])
Adding or multiplying a scalar and a tensor produces a result with the same shape as
the original tensor. Here, each element of the tensor is added to (or multiplied by) the
scalar.
a=2
X=torch .arange( 24).reshape( 2,3,4)
a+X, (a *X).shape
46 Preliminaries
(tensor([[[ 2,3,4,5],
[6,7,8,9],
[10,11,12,13]],
[[14,15,16,17],
[18,19,20,21],
[22,23,24,25]]]),
torch .Size([ 2,3,4]))
2.3.6Reduction
Often, we wish to calculate the sum of a tensor‚Äôs elements. To express the sum of the
elementsinavector xoflengthùëõ,wewrite√çùëõ
ùëñ=1ùë•ùëñ. Thereisasimplefunctionforit:
x=torch .arange( 3, dtype =torch .float32)
x, x .sum()
(tensor([ 0.,1.,2.]), tensor( 3.))
To express sums over the elements of tensors of arbitrary shape, we simply sum over all
its axes. For example, the sum of the elements of an ùëöùëõmatrix Acould be written√çùëö
ùëñ=1√çùëõ
ùëó=1ùëéùëñùëó.
A.shape, A .sum()
(torch .Size([ 2,3]), tensor( 15.))
By default, invoking the sum function reduces a tensor along all of its axes, eventually
producing a scalar. Our libraries also allow us to specify the axes along which the tensor
should be reduced. To sum over all elements along the rows (axis 0), we specify axis=0in
sum. Since the input matrix reduces along axis 0 to generate the output vector, this axis is
missing from the shape of the output.
A.shape, A .sum(axis =0).shape
(torch .Size([ 2,3]), torch .Size([ 3]))
Specifying axis=1will reduce the column dimension (axis 1) by summing up elements of
all the columns.
A.shape, A .sum(axis =1).shape
(torch .Size([ 2,3]), torch .Size([ 2]))
47 Linear Algebra
Reducing a matrix along both rows and columns via summation is equivalent to summing
up all the elements of the matrix.
A.sum(axis =[0,1])==A.sum() # Same as A.sum()
tensor( True )
A related quantity is the mean, also called the average. We calculate the mean by dividing
the sum by the total number of elements. Because computing the mean is so common, it
gets a dedicated library function that works analogously to sum.
A.mean(), A .sum() /A.numel()
(tensor( 2.5000 ), tensor( 2.5000 ))
Likewise, the function for calculating the mean can also reduce a tensor along specific
axes.
A.mean(axis =0), A .sum(axis =0)/A.shape[ 0]
(tensor([ 1.5000 ,2.5000 ,3.5000 ]), tensor([ 1.5000 ,2.5000 ,3.5000 ]))
2.3.7Non-ReductionSum
Sometimes it can be useful to keep the number of axesunchanged when invoking the func-
tion for calculating the sum or mean. This matters when we want to use the broadcast
mechanism.
sum_A =A.sum(axis =1, keepdims =True )
sum_A, sum_A .shape
(tensor([[ 3.],
[12.]]),
torch .Size([ 2,1]))
For instance, since sum_Akeeps its two axes after summing each row, we can divide Aby
sum_Awith broadcasting to create a matrix where each row sums up to 1.
A/sum_A
tensor([[ 0.0000 ,0.3333 ,0.6667 ],
[0.2500 ,0.3333 ,0.4167 ]])
48 Preliminaries
If we want to calculate the cumulative sum of elements of Aalong some axis, say axis=0
(row by row), we can call the cumsumfunction. By design, this function does not reduce
the input tensor along any axis.
A.cumsum(axis =0)
tensor([[ 0.,1.,2.],
[3.,5.,7.]])
2.3.8DotProducts
Sofar,wehaveonlyperformedelementwiseoperations,sums,andaverages. Andifthiswas
allwecoulddo,linearalgebrawouldnotdeserveitsownsection. Fortunately,thisiswhere
things get more interesting. One of the most fundamental operations is the dot product.
Given two vectors x,y2Rùëë, theirdotproduct x>y(also known as innerproduct ,hx,yi)
is a sum over the products of the elements at the same position: x>y=√çùëë
ùëñ=1ùë•ùëñùë¶ùëñ.
y=torch .ones( 3, dtype =torch .float32)
x, y, torch .dot(x, y)
(tensor([ 0.,1.,2.]), tensor([ 1.,1.,1.]), tensor( 3.))
Equivalently,wecancalculatethedotproductoftwovectorsbyperforminganelementwise
multiplication followed by a sum:
torch .sum(x *y)
tensor( 3.)
Dot products are useful in a wide range of contexts. For example, given some set of val-
ues, denoted by a vector x2Rùëõ, and a set of weights, denoted by w2Rùëõ, the weighted
sum of the values in xaccording to the weights wcould be expressed as the dot product
x>w. When the weights are nonnegative and sum to 1, i.e., √çùëõ
ùëñ=1ùë§ùëñ=1, the dot prod-
uct expresses a weighted average . After normalizing two vectors to have unit length, the
dot products express the cosine of the angle between them. Later in this section, we will
formally introduce this notion of length.
2.3.9Matrix‚ÄìVectorProducts
Now that we know how to calculate dot products, we can begin to understand the product
between anùëöùëõmatrix Aand anùëõ-dimensional vector x. To start off, we visualize our
49 Linear Algebra
matrix in terms of its row vectors
A=266666664a>
1
a>
2...
a>
ùëö377777775, (2.3.5)
where each a>
ùëñ2Rùëõis a row vector representing the ùëñthrow of the matrix A.
The matrix‚Äìvector product Axis simply a column vector of length ùëö, whoseùëñthelement
is the dot product a>
ùëñx:
Ax=266666664a>
1
a>
2...
a>
ùëö377777775x=266666664a>
1x
a>
2x
...
a>
ùëöx377777775. (2.3.6)
We can think of multiplication with a matrix A2Rùëöùëõas a transformation that projects
vectorsfrom RùëõtoRùëö. Thesetransformationsareremarkablyuseful. Forexample,wecan
represent rotations as multiplications by certain square matrices. Matrix‚Äìvector products
alsodescribethekeycalculationinvolvedincomputingtheoutputsofeachlayerinaneural
network given the outputs from the previous layer.
To express a matrix‚Äìvector product in code, we use the mvfunction. Note that the column
dimensionof A(itslengthalongaxis1)mustbethesameasthedimensionof x(itslength).
Pythonhasaconvenienceoperator @thatcanexecutebothmatrix‚Äìvectorandmatrix‚Äìmatrix
products (depending on its arguments). Thus we can write A@x.
A.shape, x .shape, torch .mv(A, x), A @x
(torch .Size([ 2,3]), torch .Size([ 3]), tensor([ 5.,14.]), tensor([ 5.,14.]))
2.3.10Matrix‚ÄìMatrixMultiplication
Once you have gotten the hang of dot products and matrix‚Äìvector products, then matrix‚Äì
matrixmultiplication should be straightforward.
Say that we have two matrices A2RùëõùëòandB2Rùëòùëö:
A=266666664ùëé11ùëé12ùëé1ùëò
ùëé21ùëé22ùëé2ùëò
............
ùëéùëõ1ùëéùëõ2ùëéùëõùëò377777775,B=266666664ùëè11ùëè12ùëè1ùëö
ùëè21ùëè22ùëè2ùëö
............
ùëèùëò1ùëèùëò2ùëèùëòùëö377777775. (2.3.7)
Leta>
ùëñ2Rùëòdenotetherowvectorrepresentingthe ùëñthrowofthematrix Aandlet bùëó2Rùëò
50 Preliminaries
denote the column vector from the ùëóthcolumn of the matrix B:
A=266666664a>
1
a>
2...
a>
ùëõ377777775,B=
b1b2bùëö
. (2.3.8)
To form the matrix product C2Rùëõùëö, we simply compute each element ùëêùëñùëóas the dot
product between the ùëñthrow of Aand theùëóthcolumn of B, i.e.,a>
ùëñbùëó:
C=AB=266666664a>
1
a>
2...
a>
ùëõ377777775
b1b2bùëö
=266666664a>
1b1a>
1b2a>
1bùëö
a>
2b1a>
2b2a>
2bùëö
............
a>
ùëõb1a>
ùëõb2a>
ùëõbùëö377777775. (2.3.9)
Wecanthinkofthematrix‚Äìmatrixmultiplication ABasperforming ùëömatrix‚Äìvectorprod-
ucts orùëöùëõdot products and stitching the results together to form an ùëõùëömatrix. In the
following snippet, we perform matrix multiplication on AandB. Here, Ais a matrix with
two rows and three columns, and Bis a matrix with three rows and four columns. After
multiplication, we obtain a matrix with two rows and four columns.
B=torch .ones( 3,4)
torch .mm(A, B), A @B
(tensor([[ 3.,3.,3.,3.],
[12.,12.,12.,12.]]),
tensor([[ 3.,3.,3.,3.],
[12.,12.,12.,12.]]))
The term matrix‚Äìmatrix multiplication is often simplified to matrix multiplication , and
should not be confused with the Hadamard product.
2.3.11Norms
Some of the most useful operators in linear algebra are norms. Informally, the norm of a
vector tells us how bigit is. For instance, the ‚Ñì2norm measures the (Euclidean) length of a
vector. Here, we are employing a notion of sizethat concerns the magnitude of a vector‚Äôs
components (not its dimensionality).
A norm is a function kkthat maps a vector to a scalar and satisfies the following three
properties:
1.Given any vector x, if we scale (all elements of) the vector by a scalar ùõº2R, its norm
scales accordingly:
kùõºxk=jùõºjkxk. (2.3.10)
51 Linear Algebra
2.For any vectors xandy: norms satisfy the triangle inequality:
kx¬∏ykkxk¬∏kyk. (2.3.11)
3.The norm of a vector is nonnegative and it only vanishes if the vector is zero:
kxk>0for allx‚â†0. (2.3.12)
Many functions are valid norms and different norms encode different notions of size. The
Euclidean norm that we all learned in elementary school geometry when calculating the
hypotenuseofarighttriangleisthesquarerootofthesumofsquaresofavector‚Äôselements.
Formally, this is called the ‚Ñì2normand expressed as
kxk2=vtùëõ√ï
ùëñ=1ùë•2
ùëñ. (2.3.13)
The method normcalculates the ‚Ñì2norm.
u=torch .tensor([ 3.0,-4.0])
torch .norm(u)
tensor( 5.)
The‚Ñì1norm is also common and the associated measure is called the Manhattan distance.
By definition, the ‚Ñì1norm sums the absolute values of a vector‚Äôs elements:
kxk1=ùëõ√ï
ùëñ=1jùë•ùëñj. (2.3.14)
Compared to the ‚Ñì2norm, it is less sensitive to outliers. To compute the ‚Ñì1norm, we
compose the absolute value with the sum operation.
torch .abs(u) .sum()
tensor( 7.)
Both the‚Ñì2and‚Ñì1norms are special cases of the more general ‚Ñìùëùnorms:
kxkùëù= ùëõ√ï
ùëñ=1jùë•ùëñjùëù!1¬ùùëù
. (2.3.15)
In the case of matrices, matters are more complicated. After all, matrices can be viewed
bothascollectionsofindividualentries andasobjectsthatoperateonvectorsandtransform
them into other vectors. For instance, we can ask by how much longer the matrix‚Äìvector
product Xvcould be relative to v. This line of thought leads to what is called the spectral
52 Preliminaries
norm. For now, we introduce the Frobenius norm , which is much easier to compute and
defined as the square root of the sum of the squares of a matrix‚Äôs elements:
kXkF=vutùëö√ï
ùëñ=1ùëõ√ï
ùëó=1ùë•2
ùëñùëó. (2.3.16)
The Frobenius norm behaves as if it were an ‚Ñì2norm of a matrix-shaped vector. Invoking
the following function will calculate the Frobenius norm of a matrix.
torch .norm(torch .ones(( 4,9)))
tensor( 6.)
While we do not want to get too far ahead of ourselves, we already can plant some intu-
ition about why these concepts are useful. In deep learning, we are often trying to solve
optimization problems: maximize the probability assigned to observed data; maximize the
revenue associated with a recommender model; minimize the distance between predictions
andthegroundtruthobservations; minimize thedistancebetweenrepresentationsofphotos
ofthesamepersonwhile maximizing thedistancebetweenrepresentationsofphotosofdif-
ferentpeople. Thesedistances,whichconstitutetheobjectivesofdeeplearningalgorithms,
are often expressed as norms.
2.3.12Discussion
In this section, we have reviewed all the linear algebra that you will need to understand a
significant chunk of modern deep learning. There is a lot more to linear algebra, though,
and much of it is useful for machine learning. For example, matrices can be decomposed
into factors, and these decompositions can reveal low-dimensional structure in real-world
datasets. There are entire subfields of machine learning that focus on using matrix decom-
positions and their generalizations to high-order tensors to discover structure in datasets
and solve prediction problems. But this book focuses on deep learning. And we believe
youwillbemoreinclinedtolearnmoremathematicsonceyouhavegottenyourhandsdirty
applyingmachinelearningtorealdatasets. Sowhilewereservetherighttointroducemore
mathematics later on, we wrap up this section here.
If you are eager to learn more linear algebra, there are many excellent books and online
resources. For a more advanced crash course, consider checking out Strang ( 1993), Kolter
(2008), and Petersen and Pedersen ( 2008).
To recap:
Scalars, vectors, matrices, and tensors are the basic mathematical objects used in linear
algebra and have zero, one, two, and an arbitrary number of axes, respectively.
Tensors can be sliced or reduced along specified axes via indexing, or operations such
assumandmean, respectively.
53 Linear Algebra
ElementwiseproductsarecalledHadamardproducts. Bycontrast,dotproducts,matrix‚Äì
vector products, and matrix‚Äìmatrix products are not elementwise operations and in
general return objects having shapes that are different from the the operands.
Compared to Hadamard products, matrix‚Äìmatrix products take considerably longer to
compute (cubic rather than quadratic time).
Norms capture various notions of the magnitude of a vector (or matrix), and are com-
monly applied to the difference of two vectors to measure their distance apart.
Common vector norms include the ‚Ñì1and‚Ñì2norms, and common matrix norms include
thespectral andFrobenius norms.
2.3.13Exercises
1.Prove that the transpose of the transpose of a matrix is the matrix itself: ¬πA>¬∫>=A.
2.Given two matrices AandB, show that sum and transposition commute: A>¬∏B>=
¬πA¬∏B¬∫>.
3.Given any square matrix A, isA¬∏A>always symmetric? Can you prove the result by
using only the results of the previous two exercises?
4.We defined the tensor Xof shape (2, 3, 4) in this section. What is the output of len(X)?
Write your answer without implementing any code, then check your answer using code.
5.For a tensor Xof arbitrary shape, does len(X) always correspond to the length of a
certain axis of X? What is that axis?
6.Run A / A.sum(axis=1) and see what happens. Can you analyze the results?
7.When traveling between two points in downtown Manhattan, what is the distance that
you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can
you travel diagonally?
8.Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs
along axes 0, 1, and 2?
9.Feed a tensor with three or more axes to the linalg.norm function and observe its
output. What does this function compute for tensors of arbitrary shape?
10.Consider three large matrices, say A2R210216,B2R21625andC2R25214, ini-
tialized with Gaussian random variables. You want to compute the product ABC. Is
thereanydifferenceinmemoryfootprintandspeed,dependingonwhetheryoucompute
¬πAB¬∫CorA¬πBC¬∫. Why?
11.Consider three large matrices, say A2R210216,B2R21625andC2R25216. Is there
any difference in speed depending on whether you compute ABorAC>? Why? What
changes if you initialize C=B>without cloning memory? Why?
12.Consider three matrices, say A,B,C2R100200. Construct a tensor with three axes by
54 Preliminaries
55stacking¬ªA,B,C¬º. What is the dimensionality? Slice out the second coordinate of the
third axis to recover B. Check that your answer is correct.
Discussions55.
2.4Calculus
For a long time, how to calculate the area of a circle remained a mystery. Then, in Ancient
Greece, the mathematician Archimedes came up with the clever idea to inscribe a series of
polygons with increasing numbers of vertices on the inside of a circle ( Fig. 2.4.1 ). For a
polygon with ùëõvertices, we obtain ùëõtriangles. The height of each triangle approaches the
radiusùëüaswepartitionthecirclemorefinely. Atthesametime,itsbaseapproaches 2ùúãùëü¬ùùëõ,
since the ratio between arc and secant approaches 1 for a large number of vertices. Thus,
the area of the polygon approaches ùëõùëü1
2¬π2ùúãùëü¬ùùëõ¬∫=ùúãùëü2.
tFig. 2.4.1 Finding the area of a circle as a limit procedure.
Thislimitingprocedureisattherootofboth differentialcalculus andintegralcalculus . The
former can tell us how to increase or decrease a function‚Äôs value by manipulating its argu-
ments. This comes in handy for the optimization problems that we face in deep learning,
where we repeatedly update our parameters in order to decrease the loss function. Opti-
mizationaddresseshowtofitourmodelstotrainingdata,andcalculusisitskeyprerequisite.
However, do not forget that our ultimate goal is to perform well on previouslyunseen data.
That problem is called generalization and will be a key focus of other chapters.
%matplotlib inline
import numpy asnp
from matplotlib_inline import backend_inline
from d2l import torch asd2l
2.4.1Derivativesand Differentiation
Put simply, a derivative is the rate of change in a function with respect to changes in its
arguments. Derivatives can tell us how rapidly a loss function would increase or decrease
were we to increase ordecrease each parameter by an infinitesimally small amount. For-
mally, for functions ùëì:R!R, that map from scalars to scalars, the derivative ofùëìat a
pointùë•is defined as
ùëì0¬πùë•¬∫=lim
‚Ñé!0ùëì¬πùë•¬∏‚Ñé¬∫ ùëì¬πùë•¬∫
‚Ñé. (2.4.1)
55 Calculus
This term on the right hand side is called a limitand it tells us what happens to the value of
an expression as a specified variable approaches a particular value. This limit tells us what
the ratio between a perturbation ‚Ñéand the change in the function value ùëì¬πùë•¬∏‚Ñé¬∫ ùëì¬πùë•¬∫
converges to as we shrink its size to zero.
Whenùëì0¬πùë•¬∫exists,ùëìis said to be differentiable atùë•; and whenùëì0¬πùë•¬∫exists for all ùë•on a
set, e.g., the interval ¬ªùëé,ùëè¬º, we say that ùëìis differentiable on this set. Not all functions are
differentiable,includingmanythatwewishtooptimize,suchasaccuracyandtheareaunder
the receiving operating characteristic (AUC). However, because computing the derivative
of the loss is a crucial step in nearly all algorithms for training deep neural networks, we
often optimize a differentiable surrogate instead.
We can interpret the derivative ùëì0¬πùë•¬∫as theinstantaneous rate of change of ùëì¬πùë•¬∫with
respect toùë•. Let‚Äôs develop some intuition with an example. Define ùë¢=ùëì¬πùë•¬∫=3ùë•2 
4ùë•.
def f(x):
return 3*x**2-4*x
Settingùë•=1, we see thatùëì¬πùë•¬∏‚Ñé¬∫ ùëì¬πùë•¬∫
‚Ñéapproaches 2as‚Ñéapproaches 0. While this ex-
periment lacks the rigor of a mathematical proof, we can quickly see that indeed ùëì0¬π1¬∫=
2.
for hin10.0 **np.arange( -1,-6,-1):
print (f'h={h:.5f}, numerical limit= {(f(1+h)-f(1))/h:.5f}')
h=0.10000 , numerical limit =2.30000
h=0.01000 , numerical limit =2.03000
h=0.00100 , numerical limit =2.00300
h=0.00010 , numerical limit =2.00030
h=0.00001 , numerical limit =2.00003
There are several equivalent notational conventions for derivatives. Given ùë¶=ùëì¬πùë•¬∫, the
following expressions are equivalent:
ùëì0¬πùë•¬∫=ùë¶0=ùëëùë¶
ùëëùë•=ùëëùëì
ùëëùë•=ùëë
ùëëùë•ùëì¬πùë•¬∫=ùê∑ùëì¬πùë•¬∫=ùê∑ùë•ùëì¬πùë•¬∫, (2.4.2)
where the symbolsùëë
ùëëùë•andùê∑aredifferentiation operators . Below, we present the deriva-
tives of some common functions:
ùëë
ùëëùë•ùê∂=0 for any constant ùê∂
ùëë
ùëëùë•ùë•ùëõ=ùëõùë•ùëõ 1forùëõ‚â†0
ùëë
ùëëùë•ùëíùë•=ùëíùë•
ùëë
ùëëùë•lnùë•=ùë• 1.(2.4.3)
56 Preliminaries
Functions composed from differentiable functions are often themselves differentiable. The
following rules come in handy for working with compositions of any differentiable func-
tionsùëìandùëî, and constant ùê∂.
ùëë
ùëëùë•¬ªùê∂ùëì¬πùë•¬∫¬º=ùê∂ùëë
ùëëùë•ùëì¬πùë•¬∫ Constant multiple rule
ùëë
ùëëùë•¬ªùëì¬πùë•¬∫¬∏ùëî¬πùë•¬∫¬º=ùëë
ùëëùë•ùëì¬πùë•¬∫¬∏ùëë
ùëëùë•ùëî¬πùë•¬∫ Sum rule
ùëë
ùëëùë•¬ªùëì¬πùë•¬∫ùëî¬πùë•¬∫¬º=ùëì¬πùë•¬∫ùëë
ùëëùë•ùëî¬πùë•¬∫¬∏ùëî¬πùë•¬∫ùëë
ùëëùë•ùëì¬πùë•¬∫Product rule
ùëë
ùëëùë•ùëì¬πùë•¬∫
ùëî¬πùë•¬∫=ùëî¬πùë•¬∫ùëë
ùëëùë•ùëì¬πùë•¬∫ ùëì¬πùë•¬∫ùëë
ùëëùë•ùëî¬πùë•¬∫
ùëî2¬πùë•¬∫Quotient rule(2.4.4)
Using this, we can apply the rules to find the derivative of 3ùë•2 4ùë•via
ùëë
ùëëùë•¬ª3ùë•2 4ùë•¬º=3ùëë
ùëëùë•ùë•2 4ùëë
ùëëùë•ùë•=6ùë• 4. (2.4.5)
Plugging in ùë•=1shows that, indeed, the derivative equals 2at this location. Note that
derivatives tell us the slopeof a function at a particular location.
2.4.2VisualizationUtilities
We can visualize the slopes of functions using the matplotlib library. We need to de-
fine a few functions. As its name indicates, use_svg_display tells matplotlib to output
graphicsinSVGformatforcrisperimages. Thecomment #@saveisaspecialmodifierthat
allows us to save any function, class, or other code block to the d2lpackage so that we can
invoke it later without repeating the code, e.g., via d2l.use_svg_display() .
def use_svg_display (): #@save
"""Use the svg format to display a plot in Jupyter."""
backend_inline .set_matplotlib_formats( 'svg')
Conveniently, we can set figure sizes with set_figsize . Since the import statement from
matplotlib import pyplot as plt was marked via #@savein the d2lpackage, we can
calld2l.plt .
def set_figsize (figsize =(3.5,2.5)): #@save
"""Set the figure size for matplotlib."""
use_svg_display()
d2l.plt.rcParams[ 'figure.figsize ']=figsize
The set_axes function can associate axes with properties, including labels, ranges, and
scales.
#@save
def set_axes (axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
"""Set the axes for matplotlib."""
axes .set_xlabel(xlabel), axes .set_ylabel(ylabel)
(continues on next page)
57 Calculus
(continued from previous page)
axes .set_xscale(xscale), axes .set_yscale(yscale)
axes .set_xlim(xlim), axes .set_ylim(ylim)
iflegend:
axes .legend(legend)
axes .grid()
Withthesethreefunctions,wecandefinea plotfunctiontooverlaymultiplecurves. Much
of the code here is just ensuring that the sizes and shapes of inputs match.
#@save
def plot (X, Y =None , xlabel =None , ylabel =None , legend =[], xlim =None ,
ylim =None , xscale ='linear ', yscale ='linear ',
fmts =('-','m--','g-.','r:'), figsize =(3.5,2.5), axes =None ):
"""Plot data points."""
def has_one_axis (X): # True if X (tensor or list) has 1 axis
return (hasattr (X, "ndim ")and X.ndim ==1orisinstance (X, list )
and not hasattr (X[0],"__len__ "))
ifhas_one_axis(X): X =[X]
ifYisNone :
X, Y =[[]] *len(X), X
elif has_one_axis(Y):
Y=[Y]
iflen(X) !=len(Y):
X=X*len(Y)
set_figsize(figsize)
ifaxes isNone :
axes =d2l.plt.gca()
axes .cla()
for x, y, fmt inzip(X, Y, fmts):
axes .plot(x,y,fmt) iflen(x) else axes .plot(y,fmt)
set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
Now we can plot the function ùë¢=ùëì¬πùë•¬∫and its tangent line ùë¶=2ùë• 3atùë•=1, where the
coefficient 2is the slope of the tangent line.
x=np.arange( 0,3,0.1)
plot(x, [f(x), 2*x-3],'x','f(x) ', legend =['f(x) ','Tangent line (x=1) '])

58 Preliminaries
2.4.3PartialDerivativesand Gradients
Thus far, we have been differentiating functions of just one variable. In deep learning, we
also need to work with functions of manyvariables. We briefly introduce notions of the
derivative that apply to such multivariate functions.
Letùë¶=ùëì¬πùë•1,ùë•2,...,ùë•ùëõ¬∫be a function with ùëõvariables. The partial derivative ofùë¶with
respect to its ùëñthparameterùë•ùëñis
ùúïùë¶
ùúïùë•ùëñ=lim
‚Ñé!0ùëì¬πùë•1,...,ùë•ùëñ 1,ùë•ùëñ¬∏‚Ñé,ùë•ùëñ¬∏1,...,ùë•ùëõ¬∫ ùëì¬πùë•1,...,ùë•ùëñ,...,ùë•ùëõ¬∫
‚Ñé. (2.4.6)
Tocalculateùúïùë¶
ùúïùë•ùëñ,wecantreat ùë•1,...,ùë•ùëñ 1,ùë•ùëñ¬∏1,...,ùë•ùëõasconstantsandcalculatethederiva-
tive ofùë¶with respect to ùë•ùëñ. The following notational conventions for partial derivatives are
all common and all mean the same thing:
ùúïùë¶
ùúïùë•ùëñ=ùúïùëì
ùúïùë•ùëñ=ùúïùë•ùëñùëì=ùúïùëñùëì=ùëìùë•ùëñ=ùëìùëñ=ùê∑ùëñùëì=ùê∑ùë•ùëñùëì. (2.4.7)
We can concatenate partial derivatives of a multivariate function with respect to all its
variables to obtain a vector that is called the gradient of the function. Suppose that the
input of function ùëì:Rùëõ!Ris anùëõ-dimensional vector x=¬ªùë•1,ùë•2,...,ùë•ùëõ¬º>and the
output is a scalar. The gradient of the function ùëìwith respect to xis a vector of ùëõpartial
derivatives:
rxùëì¬πx¬∫=
ùúïùë•1ùëì¬πx¬∫,ùúïùë•2ùëì¬πx¬∫,...ùúïùë•ùëõùëì¬πx¬∫>. (2.4.8)
When there is no ambiguity, rxùëì¬πx¬∫is typically replaced by rùëì¬πx¬∫. The following rules
come in handy for differentiating multivariate functions:
For all A2Rùëöùëõwe haverxAx=A>andrxx>A=A.
For square matrices A2Rùëõùëõwe have thatrxx>Ax=¬πA¬∏A>¬∫xand in particular
rxkxk2=rxx>x=2x.
Similarly, for any matrix X, we haverXkXk2
F=2X.
2.4.4Chain Rule
In deep learning, the gradients of concern are often difficult to calculate because we are
workingwithdeeplynestedfunctions(offunctions(offunctions‚Ä¶)). Fortunately,the chain
ruletakescareofthis. Returningtofunctionsofasinglevariable,supposethat ùë¶=ùëì¬πùëî¬πùë•¬∫¬∫
and that the underlying functions ùë¶=ùëì¬πùë¢¬∫andùë¢=ùëî¬πùë•¬∫are both differentiable. The chain
rule states that
ùëëùë¶
ùëëùë•=ùëëùë¶
ùëëùë¢ùëëùë¢
ùëëùë•. (2.4.9)
Turningbacktomultivariatefunctions,supposethat ùë¶=ùëì¬πu¬∫hasvariables ùë¢1,ùë¢2,...,ùë¢ùëö,
where eachùë¢ùëñ=ùëîùëñ¬πx¬∫has variables ùë•1,ùë•2,...,ùë•ùëõ, i.e.,u=ùëî¬πx¬∫. Then the chain rule
states that
ùúïùë¶
ùúïùë•ùëñ=ùúïùë¶
ùúïùë¢1ùúïùë¢1
ùúïùë•ùëñ¬∏ùúïùë¶
ùúïùë¢2ùúïùë¢2
ùúïùë•ùëñ¬∏...¬∏ùúïùë¶
ùúïùë¢ùëöùúïùë¢ùëö
ùúïùë•ùëñand sorxùë¶=Aruùë¶, (2.4.10)
59 Calculus
56whereA2Rùëõùëöis amatrixthat contains the derivative of vector uwith respect to vector
x. Thus, evaluating the gradient requires computing a vector‚Äìmatrix product. This is one
of the key reasons why linear algebra is such an integral building block in building deep
learning systems.
2.4.5Discussion
Whilewehavejustscratchedthesurfaceofadeeptopic,anumberofconceptsalreadycome
intofocus: first, thecompositionrulesfordifferentiationcanbeappliedroutinely, enabling
us to compute gradients automatically . This task requires no creativity and thus we can
focus our cognitive powers elsewhere. Second, computing the derivatives of vector-valued
functions requires us to multiply matrices as we trace the dependency graph of variables
from output to input. In particular, this graph is traversed in a forward direction when
we evaluate a function and in a backwards direction when we compute gradients. Later
chapters will formally introduce backpropagation, a computational procedure for applying
the chain rule.
From the viewpoint of optimization, gradients allow us to determine how to move the pa-
rametersofamodelinordertolowertheloss, andeachstepoftheoptimizationalgorithms
used throughout this book will require calculating the gradient.
2.4.6Exercises
1.Sofarwetooktherulesforderivativesforgranted. Usingthedefinitionandlimitsprove
the properties for (i) ùëì¬πùë•¬∫=ùëê, (ii)ùëì¬πùë•¬∫=ùë•ùëõ, (iii)ùëì¬πùë•¬∫=ùëíùë•and (iv)ùëì¬πùë•¬∫=logùë•.
2.In the same vein, prove the product, sum, and quotient rule from first principles.
3.Prove that the constant multiple rule follows as a special case of the product rule.
4.Calculate the derivative of ùëì¬πùë•¬∫=ùë•ùë•.
5.What does it mean that ùëì0¬πùë•¬∫=0for someùë•? Give an example of a function ùëìand a
locationùë•for which this might hold.
6.Plot the function ùë¶=ùëì¬πùë•¬∫=ùë•3 1
ùë•and plot its tangent line at ùë•=1.
7.Find the gradient of the function ùëì¬πx¬∫=3ùë•2
1¬∏5ùëíùë•2.
8.What is the gradient of the function ùëì¬πx¬∫=kxk2? What happens for x=0?
9.Can you write out the chain rule for the case where ùë¢=ùëì¬πùë•,ùë¶,ùëß¬∫andùë•=ùë•¬πùëé,ùëè¬∫,
ùë¶=ùë¶¬πùëé,ùëè¬∫, andùëß=ùëß¬πùëé,ùëè¬∫?
10.Given a function ùëì¬πùë•¬∫that is invertible, compute the derivative of its inverse ùëì 1¬πùë•¬∫.
Here we have that ùëì 1¬πùëì¬πùë•¬∫¬∫=ùë•and conversely ùëì¬πùëì 1¬πùë¶¬∫¬∫=ùë¶. Hint: use these
properties in your derivation.
Discussions56.
60 Preliminaries
2.5AutomaticDifferentiation
Recallfrom Section2.4 thatcalculatingderivativesisthecrucialstepinalltheoptimization
algorithms that we will use to train deep networks. While the calculations are straightfor-
ward,workingthemoutbyhandcanbetediousanderror-prone,andtheseissuesonlygrow
as our models become more complex.
Fortunately all modern deep learning frameworks take this work off our plates by offering
automatic differentiation (often shortened to autograd ). As we pass data through each
successivefunction,theframeworkbuildsa computationalgraph thattrackshoweachvalue
depends on others. To calculate derivatives, automatic differentiation works backwards
through this graph applying the chain rule. The computational algorithm for applying the
chain rule in this fashion is called backpropagation .
While autograd libraries have become a hot concern over the past decade, they have a
long history. In fact the earliest references to autograd date back over half of a century
(Wengert, 1964 ). The core ideas behind modern backpropagation date to a PhD thesis
from 1980 ( Speelpenning, 1980 ) and were further developed in the late 1980s ( Griewank,
1989). While backpropagation has become the default method for computing gradients,
it is not the only option. For instance, the Julia programming language employs forward
propagation ( Revelsetal., 2016). Before exploring methods, let‚Äôs first master the autograd
package.
import torch
2.5.1ASimpleFunction
Let‚Äôs assume that we are interested in differentiating the function ùë¶=2x>xwith respect to
the column vector x. To start, we assign xan initial value.
x=torch .arange( 4.0)
x
tensor([ 0.,1.,2.,3.])
Before we calculate the gradient of ùë¶with respect to x, we need a place to store it. In
general, we avoid allocating new memory every time we take a derivative because deep
learning requires successively computing derivatives with respect to the same parameters
a great many times, and we might risk running out of memory. Note that the gradient of
a scalar-valued function with respect to a vector xis vector-valued with the same shape as
x.
61 Automatic Differentiation
# Can also create x = torch.arange(4.0, requires_grad=True)
x.requires_grad_( True )
x.grad # The gradient is None by default
We now calculate our function of xand assign the result to y.
y=2*torch .dot(x, x)
y
tensor( 28., grad_fn =<MulBackward0 >)
We can now take the gradient of ywith respect to xby calling its backward method. Next,
we can access the gradient via x‚Äôsgradattribute.
y.backward()
x.grad
tensor([ 0.,4.,8.,12.])
We already know that the gradient of the function ùë¶=2x>xwith respect to xshould be
4x. Wecannowverify thattheautomaticgradientcomputationandtheexpectedresultare
identical.
x.grad ==4*x
tensor([ True ,True ,True ,True ])
Now let‚Äôs calculate another function of xand take its gradient. Note that PyTorch does not
automatically reset the gradient buffer when we record a new gradient. Instead, the new
gradient is added to the already-stored gradient. This behavior comes in handy when we
want to optimize the sum of multiple objective functions. To reset the gradient buffer, we
can call x.grad.zero_() as follows:
x.grad .zero_() # Reset the gradient
y=x.sum()
y.backward()
x.grad
tensor([ 1.,1.,1.,1.])
2.5.2BackwardforNon-ScalarVariables
When yis a vector, the most natural representation of the derivative of ywith respect
to a vector xis a matrix called the Jacobian that contains the partial derivatives of each
62 Preliminaries
57component of ywith respect to each component of x. Likewise, for higher-order yandx,
the result of differentiation could be an even higher-order tensor.
While Jacobians do show up in some advanced machine learning techniques, more com-
monly we want to sum up the gradients of each component of ywith respect to the full
vector x, yielding a vector of the same shape as x. For example, we often have a vector
representing the value of our loss function calculated separately for each example among a
batchof training examples. Here, we just want to sum up the gradients computed individ-
ually for each example.
Because deep learning frameworks vary in how they interpret gradients of non-scalar ten-
sors, PyTorch takes some steps to avoid confusion. Invoking backward on a non-scalar
elicits an error unless we tell PyTorch how to reduce the object to a scalar. More formally,
we need to provide some vector vsuch that backward will compute v>ùúïxyrather than
ùúïxy. This next part may be confusing, but for reasons that will become clear later, this
argument (representing v) is named gradient . For a more detailed description, see Yang
Zhang‚Äôs Medium post57.
x.grad .zero_()
y=x*x
y.backward(gradient =torch .ones( len(y))) # Faster: y.sum().backward()
x.grad
tensor([ 0.,2.,4.,6.])
2.5.3DetachingComputation
Sometimes, we wish to move some calculations outside of the recorded computational
graph. For example, say that we use the input to create some auxiliary intermediate terms
for which we do not want to compute a gradient. In this case, we need to detachthe re-
spective computational graph from the final result. The following toy example makes this
clearer: suppose we have z = x * y andy = x * x but we want to focus on the direct
influence of xonzrather than the influence conveyed via y. In this case, we can create a
new variable uthat takes the same value as ybut whose provenance (how it was created)
hasbeenwipedout. Thus uhasnoancestorsinthegraphandgradientsdonotflowthrough
utox. For example, taking the gradient of z = x * u will yield the result u, (not 3 * x
* xas you might have expected since z = x * x * x ).
x.grad .zero_()
y=x*x
u=y.detach()
z=u*x
z.sum() .backward()
x.grad ==u
63 Automatic Differentiation
tensor([ True ,True ,True ,True ])
Notethatwhilethisproceduredetaches y‚Äôsancestorsfromthegraphleadingto z, thecom-
putational graph leading to ypersists and thus we can calculate the gradient of ywith
respect to x.
x.grad .zero_()
y.sum() .backward()
x.grad ==2*x
tensor([ True ,True ,True ,True ])
2.5.4Gradientsand Python ControlFlow
So far we reviewed cases where the path from input to output was well defined via a func-
tion such as z = x * x * x . Programming offers us a lot more freedom in how we
compute results. For instance, we can make them depend on auxiliary variables or condi-
tion choices on intermediate results. One benefit of using automatic differentiation is that
even if building the computational graph of a function required passing through a maze
of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still
calculate the gradient of the resulting variable. To illustrate this, consider the following
code snippet where the number of iterations of the whileloop and the evaluation of the if
statement both depend on the value of the input a.
def f(a):
b=a*2
while b.norm() <1000 :
b=b*2
ifb.sum() >0:
c=b
else :
c=100 *b
return c
Below, we call this function, passing in a random value, as input. Since the input is a
random variable, we do not know what form the computational graph will take. However,
whenever we execute f(a)on a specific input, we realize a specific computational graph
and can subsequently run backward .
a=torch .randn(size =(), requires_grad =True )
d=f(a)
d.backward()
Even though our function fis, for demonstration purposes, a bit contrived, its dependence
on the input is quite simple: it is a linearfunction of awith piecewise defined scale. As
64 Preliminaries
such, f(a) / a is a vector of constant entries and, moreover, f(a) / a needs to match the
gradient of f(a)with respect to a.
a.grad ==d/a
tensor( True )
Dynamic control flow is very common in deep learning. For instance, when processing
text, the computational graph depends on the length of the input. In these cases, automatic
differentiation becomes vital for statistical modeling since it is impossible to compute the
gradienta priori.
2.5.5Discussion
You have now gotten a taste of the power of automatic differentiation. The development of
libraries for calculating derivatives both automatically and efficiently has been a massive
productivity booster for deep learning practitioners, liberating them so they can focus on
less menial. Moreover, autograd lets us design massive models for which pen and paper
gradient computations would be prohibitively time consuming. Interestingly, while we use
autograd to optimize models (in a statistical sense) the optimization of autograd libraries
themselves(inacomputationalsense)isarichsubjectofvitalinteresttoframeworkdesign-
ers. Here, tools from compilers and graph manipulation are leveraged to compute results
in the most expedient and memory-efficient manner.
For now, try to remember these basics: (i) attach gradients to those variables with respect
to which we desire derivatives; (ii) record the computation of the target value; (iii) execute
the backpropagation function; and (iv) access the resulting gradient.
2.5.6Exercises
1.Whyisthesecondderivativemuchmoreexpensivetocomputethanthefirstderivative?
2.After running the function for backpropagation, immediately run it again and see what
happens. Investigate.
3.In the control flow example where we calculate the derivative of dwith respect to a,
what would happen if we changed the variable ato a random vector or a matrix? At
this point, the result of the calculation f(a)is no longer a scalar. What happens to the
result? How do we analyze this?
4.Letùëì¬πùë•¬∫=sin¬πùë•¬∫. Plot the graph of ùëìand of its derivative ùëì0. Do not exploit the fact
thatùëì0¬πùë•¬∫=cos¬πùë•¬∫but rather use automatic differentiation to get the result.
5.Letùëì¬πùë•¬∫=¬π¬πlogùë•2¬∫sinùë•¬∫¬∏ùë• 1. Write out a dependency graph tracing results from
ùë•toùëì¬πùë•¬∫.
6.Usethechainruletocomputethederivativeùëëùëì
ùëëùë•oftheaforementionedfunction,placing
each term on the dependency graph that you constructed previously.
65 Probability and Statistics
587.Given the graph and the intermediate derivative results, you have a number of options
when computing the gradient. Evaluate the result once starting from ùë•toùëìand once
fromùëìtracing back to ùë•. The path from ùë•toùëìis commonly known as forward differ-
entiation , whereas the path from ùëìtoùë•is known as backward differentiation.
8.When might you want to use forward, and when backward, differentiation? Hint: con-
sider the amount of intermediate data needed, the ability to parallelize steps, and the
size of matrices and vectors involved.
Discussions58.
2.6Probabilityand Statistics
One way or another, machine learning is all about uncertainty. In supervised learning, we
wanttopredictsomethingunknown(the target)givensomethingknown(the features). De-
pending on our objective, we might attempt to predict the most likely value of the target.
Or we might predict the value with the smallest expected distance from the target. And
sometimes we wish not only to predict a specific value but to quantifyouruncertainty . For
example, given some features describing a patient, we might want to know howlikely they
are to suffer a heart attack in the next year. In unsupervised learning, we often care about
uncertainty. To determine whether a set of measurements are anomalous, it helps to know
how likely one is to observe values in a population of interest. Furthermore, in reinforce-
ment learning, we wish to develop agents that act intelligently in various environments.
This requires reasoning about how an environment might be expected to change and what
rewards one might expect to encounter in response to each of the available actions.
Probability is the mathematical field concerned with reasoning under uncertainty. Given a
probabilistic model of some process, we can reason about the likelihood of various events.
Theuseofprobabilitiestodescribethefrequenciesofrepeatableevents(likecointosses)is
fairlyuncontroversial. Infact, frequentist scholarsadheretoaninterpretationofprobability
that applies onlyto such repeatable events. By contrast Bayesian scholars use the language
ofprobabilitymorebroadlytoformalizereasoningunderuncertainty. Bayesianprobability
is characterized by two unique features: (i) assigning degrees of belief to non-repeatable
events, e.g., what is the probability that a dam will collapse?; and (ii) subjectivity. While
Bayesianprobabilityprovidesunambiguousrulesforhowoneshouldupdatetheirbeliefsin
light of new evidence, it allows for different individuals to start off with different priorbe-
liefs.Statistics helps us to reason backwards, starting off with collection and organization
of data and backing out to what inferences we might draw about the process that generated
the data. Whenever we analyze a dataset, hunting for patterns that we hope might charac-
terize a broader population, we are employing statistical thinking. Many courses, majors,
theses, careers, departments, companies, and institutions have been devoted to the study of
probabilityand statistics. While thissection onlyscratchesthe surface, wewill providethe
foundation that you need to begin building models.
66 Preliminaries
%matplotlib inline
import random
import torch
from torch .distributions .multinomial import Multinomial
from d2l import torch asd2l
2.6.1A SimpleExample: TossingCoins
Imagine that we plan to toss a coin and want to quantify how likely we are to see heads
(vs. tails). If the coin is fair, then both outcomes (heads and tails), are equally likely.
Moreover if we plan to toss the coin ùëõtimes then the fraction of heads that we expectto
see should exactly match the expected fraction of tails. One intuitive way to see this is
by symmetry: for every possible outcome with ùëõhheads andùëõt=¬πùëõ ùëõh¬∫tails, there is
an equally likely outcome with ùëõtheads andùëõhtails. Note that this is only possible if on
average we expect to see 1¬ù2of tosses come up heads and 1¬ù2come up tails. Of course, if
you conduct this experiment many times with ùëõ=1000000 tosses each, you might never
see a trial where ùëõh=ùëõtexactly.
Formally, the quantity 1¬ù2is called a probability and here it captures the certainty with
which any given toss will come up heads. Probabilities assign scores between 0and1to
outcomes of interest, called events. Here the event of interest is heads and we denote the
correspondingprobability ùëÉ¬πheads¬∫. Aprobabilityof 1indicatesabsolutecertainty(imag-
ine a trick coin where both sides were heads) and a probability of 0indicates impossibility
(e.g.,ifbothsidesweretails). Thefrequencies ùëõh¬ùùëõandùëõt¬ùùëõarenotprobabilitiesbutrather
statistics . Probabilities are theoretical quantities that underly the data generating process.
Here,theprobability 1¬ù2isapropertyofthecoinitself. Bycontrast,statisticsare empirical
quantities that are computed as functions of the observed data. Our interests in probabilis-
tic and statistical quantities are inextricably intertwined. We often design special statistics
calledestimators that,givenadataset,produce estimates ofmodelparameterssuchasprob-
abilities. Moreover, when those estimators satisfy a nice property called consistency , our
estimates will converge to the corresponding probability. In turn, these inferred probabili-
tiestellaboutthelikelystatisticalpropertiesofdatafromthesamepopulationthatwemight
encounter in the future.
Suppose that we stumbled upon a real coin for which we did not know the true ùëÉ¬πheads¬∫.
To investigate this quantity with statistical methods, we need to (i) collect some data; and
(ii) design an estimator. Data acquisition here is easy; we can toss the coin many times
andrecordalltheoutcomes. Formally, drawingrealizationsfromsomeunderlyingrandom
process is called sampling . As you might have guessed, one natural estimator is the ratio
of the number of observed headsto the total number of tosses.
Now, suppose that the coin was in fact fair, i.e., ùëÉ¬πheads¬∫=0.5. To simulate tosses of a
fair coin, we can invoke any random number generator. There are some easy ways to draw
samples of an event with probability 0.5. For example Python‚Äôs random.random yields
numbers in the interval ¬ª0,1¬ºwhere the probability of lying in any sub-interval ¬ªùëé,ùëè¬º
67 Probability and Statistics
¬ª0,1¬ºis equal toùëè ùëé. Thus we can get out 0and1with probability 0.5each by testing
whether the returned float number is greater than 0.5:
num_tosses =100
heads =sum([random .random() >0.5 for _inrange (num_tosses)])
tails =num_tosses -heads
print ("heads, tails: ", [heads, tails])
heads, tails: [ 44,56]
More generally, we can simulate multiple draws from any variable with a finite number
of possible outcomes (like the toss of a coin or roll of a die) by calling the multinomial
function, setting the first argument to the number of draws and the second as a list of prob-
abilitiesassociatedwitheachofthepossibleoutcomes. Tosimulatetentossesofafaircoin,
weassignprobabilityvector [0.5, 0.5] ,interpretingindex0asheadsandindex1astails.
The function returns a vector with length equal to the number of possible outcomes (here,
2), where the first component tells us the number of occurrences of heads and the second
component tells us the number of occurrences of tails.
fair_probs =torch .tensor([ 0.5,0.5])
Multinomial( 100, fair_probs) .sample()
tensor([ 50.,50.])
Each time you run this sampling process, you will receive a new random value that may
differ from the previous outcome. Dividing by the number of tosses gives us the frequency
ofeachoutcomeinourdata. Notethatthesefrequencies,justliketheprobabilitiesthatthey
are intended to estimate, sum to 1.
Multinomial( 100, fair_probs) .sample() /100
tensor([ 0.4800 ,0.5200 ])
Here, even though our simulated coin is fair (we ourselves set the probabilities [0.5, 0.
5]), the counts of heads and tails may not be identical. That is because we only drew a
relatively small number of samples. If we did not implement the simulation ourselves, and
onlysawtheoutcome,howwouldweknowifthecoinwereslightlyunfairorifthepossible
deviation from 1¬ù2was just an artifact of the small sample size? Let‚Äôs see what happens
when we simulate 10,000 tosses.
counts =Multinomial( 10000 , fair_probs) .sample()
counts /10000
68 Preliminaries
tensor([ 0.4966 ,0.5034 ])
In general, for averages of repeated events (like coin tosses), as the number of repetitions
grows, our estimates are guaranteed to converge to the true underlying probabilities. The
mathematical formulation of this phenomenon is called the law of large numbers and the
central limit theorem tells us that in many situations, as the sample size ùëõgrows, these
errors should go down at a rate of ¬π1¬ùpùëõ¬∫. Let‚Äôs get some more intuition by studying how
our estimate evolves as we grow the number of tosses from 1 to 10,000.
counts =Multinomial( 1, fair_probs) .sample(( 10000 ,))
cum_counts =counts .cumsum(dim =0)
estimates =cum_counts /cum_counts .sum(dim =1, keepdims =True )
estimates =estimates .numpy()
d2l.set_figsize(( 4.5,3.5))
d2l.plt.plot(estimates[:, 0], label =("P(coin=heads) "))
d2l.plt.plot(estimates[:, 1], label =("P(coin=tails) "))
d2l.plt.axhline(y =0.5, color ='black ', linestyle ='dashed ')
d2l.plt.gca() .set_xlabel( 'Samples ')
d2l.plt.gca() .set_ylabel( 'Estimated probability ')
d2l.plt.legend();
Each solid curve corresponds to one of the two values of the coin and gives our estimated
probability that the coin turns up that value after each group of experiments. The dashed
black line gives the true underlying probability. As we get more data by conducting more
experiments, the curves converge towards the true probability. You might already begin to
see the shape of some of the more advanced questions that preoccupy statisticians: How
quickly does this convergence happen? If we had already tested many coins manufactured
at the same plant, how might we incorporate this information?
2.6.2A MoreFormalTreatment
We have already gotten pretty far: posing a probabilistic model, generating synthetic data,
running a statistical estimator, empirically assessing convergence, and reporting error met-
69 Probability and Statistics
rics (checking the deviation). However, to go much further, we will need to be more pre-
cise.
When dealing with randomness, we denote the set of possible outcomes Sand call it the
sample space oroutcome space . Here, each element is a distinct possible outcome . In
the case of rolling a single coin, S=fheads,tailsg. For a single die,S=f1,2,3,4,5,6g.
Whenflippingtwocoins,possibleoutcomesare f¬πheads,heads¬∫,¬πheads,tails¬∫,¬πtails,heads¬∫,¬πtails,tails¬∫g.
Eventsare subsets of the sample space. For instance, the event ‚Äúthe first coin toss comes
up heads‚Äù corresponds to the set f¬πheads,heads¬∫,¬πheads,tails¬∫g. Whenever the outcome
ùëßof a random experiment satisfies ùëß2A, then eventAhas occurred. For a single roll
of a die, we could define the events ‚Äúseeing a 5‚Äù (A=f5g) and ‚Äúseeing an odd number‚Äù
(B=f1,3,5g). Inthiscase,ifthediecameup 5,wewouldsaythatboth AandBoccurred.
On the other hand, if ùëß=3, thenAdid not occur butBdid.
Aprobability function maps events onto real values ùëÉ:AS!¬ª 0,1¬º. The probabil-
ity, denoted ùëÉ¬πA¬∫, of an eventAin the given sample space S, has the following proper-
ties:
The probability of any event Ais a nonnegative real number, i.e., ùëÉ¬πA¬∫ 0;
The probability of the entire sample space is 1, i.e.,ùëÉ¬πS¬∫=1;
For any countable sequence of events A1,A2,...that aremutuallyexclusive (i.e.,Aùëñ\
Aùëó=;for allùëñ‚â†ùëó), the probability that any of them happens is equal to the sum of
their individual probabilities, i.e., ùëÉ¬π√ê1
ùëñ=1Aùëñ¬∫=√ç1
ùëñ=1ùëÉ¬πAùëñ¬∫.
These axioms of probability theory, proposed by Kolmogorov ( 1933), can be applied to
rapidly derive a number of important consequences. For instance, it follows immediately
thattheprobabilityofanyevent AoritscomplementA0occurringis1(because A[A0=
S). We can also prove that ùëÉ¬π;¬∫=0because 1=ùëÉ¬πS[S0¬∫=ùëÉ¬πS[;¬∫ =ùëÉ¬πS¬∫¬∏ùëÉ¬π;¬∫=
1¬∏ùëÉ¬π;¬∫. Consequently, the probability of any event Aandits complementA0occurring
simultaneouslyis ùëÉ¬πA\A0¬∫=0. Informally,thistellsusthatimpossibleeventshavezero
probability of occurring.
2.6.3RandomVariables
When we spoke about events like the roll of a die coming up odds or the first coin toss
coming up heads, we were invoking the idea of a random variable . Formally, random
variablesaremappingsfromanunderlyingsamplespacetoasetof(possiblymany)values.
Youmightwonderhowarandomvariableisdifferentfromthesamplespace,sincebothare
collections of outcomes. Importantly, random variables can be much coarser than the raw
sample space. We can define a binary random variable like ‚Äúgreater than 0.5‚Äù even when
the underlying sample space is infinite, e.g., points on the line segment between 0and1.
Additionally, multiple random variables can share the same underlying sample space. For
example‚Äúwhethermyhomealarmgoesoff‚Äùand‚Äúwhethermyhousewasburgled‚Äùareboth
binaryrandomvariablesthatshareanunderlyingsamplespace. Consequently,knowingthe
value taken by one random variable can tell us something about the likely value of another
70 Preliminaries
random variable. Knowing that the alarm went off, we might suspect that the house was
likely burgled.
Every value taken by a random variable corresponds to a subset of the underlying sample
space. Thus the occurrence where the random variable ùëãtakes valueùë£, denoted by ùëã=ùë£,
is aneventandùëÉ¬πùëã=ùë£¬∫denotes its probability. Sometimes this notation can get clunky,
and we can abuse notation when the context is clear. For example, we might use ùëÉ¬πùëã¬∫to
refer broadly to the distribution ofùëã, i.e., the function that tells us the probability that ùëã
takes any given value. Other times we write expressions like ùëÉ¬πùëã,ùëå¬∫=ùëÉ¬πùëã¬∫ùëÉ¬πùëå¬∫, as a
shorthand to express a statement that is true for all of the values that the random variables
ùëãandùëåcan take, i.e., for all ùëñ,ùëóit holds that ùëÉ¬πùëã=ùëñandùëå=ùëó¬∫=ùëÉ¬πùëã=ùëñ¬∫ùëÉ¬πùëå=ùëó¬∫.
Othertimes, weabusenotationbywriting ùëÉ¬πùë£¬∫whentherandomvariableisclearfromthe
context. Since an event in probability theory is a set of outcomes from the sample space,
wecanspecifyarangeofvaluesforarandomvariabletotake. Forexample, ùëÉ¬π1ùëã3¬∫
denotes the probability of the event f1ùëã3g.
Note that there is a subtle difference between discrete random variables, like flips of a coin
or tosses of a die, and continuous ones, like the weight and the height of a person sampled
at random from the population. In this case we seldom really care about someone‚Äôs exact
height. Moreover, if we took precise enough measurements, we would find that no two
people on the planet have the exact same height. In fact, with fine enough measurements,
you would never have the same height when you wake up and when you go to sleep. There
islittlepointinaskingabouttheexactprobabilitythatsomeoneis1.801392782910287192
meters tall. Instead, we typically care more about being able to say whether someone‚Äôs
heightfallsintoagiveninterval, saybetween1.79and1.81meters. Inthesecaseswework
withprobability densities . Theheightofexactly1.80metershasnoprobability,butnonzero
density. Toworkouttheprobabilityassignedtoaninterval, wemusttakean integralofthe
density over that interval.
2.6.4MultipleRandom Variables
Youmighthavenoticedthatwecouldnotevenmakeitthroughtheprevioussectionwithout
makingstatementsinvolvinginteractionsamongmultiplerandomvariables(recall ùëÉ¬πùëã,ùëå¬∫=
ùëÉ¬πùëã¬∫ùëÉ¬πùëå¬∫). Mostofmachinelearningisconcernedwithsuchrelationships. Here,thesam-
ple space would be the population of interest, say customers who transact with a business,
photographs on the Internet, or proteins known to biologists. Each random variable would
represent the (unknown) value of a different attribute. Whenever we sample an individual
from the population, we observe a realization of each of the random variables. Because
the values taken by random variables correspond to subsets of the sample space that could
be overlapping, partially overlapping, or entirely disjoint, knowing the value taken by one
random variable can cause us to update our beliefs about which values of another random
variable are likely. If a patient walks into a hospital and we observe that they are having
trouble breathing and have lost their sense of smell, then we believe that they are more
likely to have COVID-19 than we might if they had no trouble breathing and a perfectly
ordinary sense of smell.
When working with multiple random variables, we can construct events corresponding to
71 Probability and Statistics
every combination of values that the variables can jointly take. The probability function
thatassignsprobabilitiestoeachofthesecombinations(e.g. ùê¥=ùëéandùêµ=ùëè)iscalledthe
joint probability function and simply returns the probability assigned to the intersection
of the corresponding subsets of the sample space. The joint probability assigned to the
event where random variables ùê¥andùêµtake valuesùëéandùëè, respectively, is denoted ùëÉ¬πùê¥=
ùëé,ùêµ=ùëè¬∫, where the comma indicates ‚Äúand‚Äù. Note that for any values ùëéandùëè, it follows
that
ùëÉ¬πùê¥=ùëé,ùêµ=ùëè¬∫ùëÉ¬πùê¥=ùëé¬∫andùëÉ¬πùê¥=ùëé,ùêµ=ùëè¬∫ùëÉ¬πùêµ=ùëè¬∫, (2.6.1)
since forùê¥=ùëéandùêµ=ùëèto happen,ùê¥=ùëéhas to happen andùêµ=ùëèalso has to
happen. Interestingly,thejointprobabilitytellsusallthatwecanknowabouttheserandom
variables in a probabilistic sense, and can be used to derive many other useful quantities,
including recovering the individual distributions ùëÉ¬πùê¥¬∫andùëÉ¬πùêµ¬∫. To recover ùëÉ¬πùê¥=ùëé¬∫
we simply sum up ùëÉ¬πùê¥=ùëé,ùêµ=ùë£¬∫over all values ùë£that the random variable ùêµcan take:
ùëÉ¬πùê¥=ùëé¬∫=√ç
ùë£ùëÉ¬πùê¥=ùëé,ùêµ=ùë£¬∫.
The ratioùëÉ¬πùê¥=ùëé,ùêµ=ùëè¬∫
ùëÉ¬πùê¥=ùëé¬∫1turns out to be extremely important. It is called the conditional
probability , and is denoted via the ‚Äú j‚Äù symbol:
ùëÉ¬πùêµ=ùëèjùê¥=ùëé¬∫=ùëÉ¬πùê¥=ùëé,ùêµ=ùëè¬∫¬ùùëÉ¬πùê¥=ùëé¬∫. (2.6.2)
It tells us the new probability associated with the event ùêµ=ùëè, once we condition on the
factùê¥=ùëétook place. We can think of this conditional probability as restricting attention
onlytothesubsetofthesamplespaceassociatedwith ùê¥=ùëéandthenrenormalizingsothat
all probabilities sum to 1. Conditional probabilities are in fact just ordinary probabilities
and thus respect all of the axioms, as long as we condition all terms on the same event and
thus restrict attention to the same sample space. For instance, for disjoint events BandB0,
we have that ùëÉ¬πB[B0jùê¥=ùëé¬∫=ùëÉ¬πBjùê¥=ùëé¬∫¬∏ùëÉ¬πB0jùê¥=ùëé¬∫.
Using the definition of conditional probabilities, we can derive the famous result called
Bayes‚Äô theorem . By construction, we have that ùëÉ¬πùê¥,ùêµ¬∫=ùëÉ¬πùêµjùê¥¬∫ùëÉ¬πùê¥¬∫andùëÉ¬πùê¥,ùêµ¬∫=
ùëÉ¬πùê¥jùêµ¬∫ùëÉ¬πùêµ¬∫. Combining both equations yields ùëÉ¬πùêµjùê¥¬∫ùëÉ¬πùê¥¬∫=ùëÉ¬πùê¥jùêµ¬∫ùëÉ¬πùêµ¬∫and
hence
ùëÉ¬πùê¥jùêµ¬∫=ùëÉ¬πùêµjùê¥¬∫ùëÉ¬πùê¥¬∫
ùëÉ¬πùêµ¬∫. (2.6.3)
This simple equation has profound implications because it allows us to reverse the order of
conditioning. Ifweknowhowtoestimate ùëÉ¬πùêµjùê¥¬∫,ùëÉ¬πùê¥¬∫,andùëÉ¬πùêµ¬∫,thenwecanestimate
ùëÉ¬πùê¥jùêµ¬∫. We often find it easier to estimate one term directly but not the other and Bayes‚Äô
theoremcancometotherescuehere. Forinstance,ifweknowtheprevalenceofsymptoms
for a given disease, and the overall prevalences of the disease and symptoms, respectively,
we can determine how likely someone is to have the disease based on their symptoms. In
some cases we might not have direct access to ùëÉ¬πùêµ¬∫, such as the prevalence of symptoms.
In this case a simplified version of Bayes‚Äô theorem comes in handy:
ùëÉ¬πùê¥jùêµ¬∫/ùëÉ¬πùêµjùê¥¬∫ùëÉ¬πùê¥¬∫. (2.6.4)
72 Preliminaries
Since we know that ùëÉ¬πùê¥jùêµ¬∫must be normalized to 1, i.e.,√ç
ùëéùëÉ¬πùê¥=ùëéjùêµ¬∫=1, we can
use it to compute
ùëÉ¬πùê¥jùêµ¬∫=ùëÉ¬πùêµjùê¥¬∫ùëÉ¬πùê¥¬∫√ç
ùëéùëÉ¬πùêµjùê¥=ùëé¬∫ùëÉ¬πùê¥=ùëé¬∫. (2.6.5)
In Bayesian statistics, we think of an observer as possessing some (subjective) prior be-
liefs about the plausibility of the available hypotheses encoded in the priorùëÉ¬πùêª¬∫, and a
likelihood function that says how likely one is to observe any value of the collected evi-
dence for each of the hypotheses in the class ùëÉ¬πùê∏jùêª¬∫. Bayes‚Äô theorem is then interpreted
as telling us how to update the initial priorùëÉ¬πùêª¬∫in light of the available evidence ùê∏to
produceposterior beliefsùëÉ¬πùêªjùê∏¬∫=ùëÉ¬πùê∏jùêª¬∫ùëÉ¬πùêª¬∫
ùëÉ¬πùê∏¬∫. Informally, this can be stated as ‚Äúpos-
terior equals prior times likelihood, divided by the evidence‚Äù. Now, because the evidence
ùëÉ¬πùê∏¬∫is the same for all hypotheses, we can get away with simply normalizing over the
hypotheses.
Note that√ç
ùëéùëÉ¬πùê¥=ùëéjùêµ¬∫=1also allows us to marginalize over random variables.
That is, we can drop variables from a joint distribution such as ùëÉ¬πùê¥,ùêµ¬∫. After all, we have
that
√ï
ùëéùëÉ¬πùêµjùê¥=ùëé¬∫ùëÉ¬πùê¥=ùëé¬∫=√ï
ùëéùëÉ¬πùêµ,ùê¥=ùëé¬∫=ùëÉ¬πùêµ¬∫.(2.6.6)
Independenceisanotherfundamentallyimportantconceptthatformsthebackboneofmany
important ideas in statistics. In short, two variables are independent if conditioning on the
value ofùê¥does not cause any change to the probability distribution associated with ùêµand
vice versa. More formally, independence, denoted ùê¥?ùêµ, requires that ùëÉ¬πùê¥jùêµ¬∫=ùëÉ¬πùê¥¬∫
and, consequently, that ùëÉ¬πùê¥,ùêµ¬∫=ùëÉ¬πùê¥jùêµ¬∫ùëÉ¬πùêµ¬∫=ùëÉ¬πùê¥¬∫ùëÉ¬πùêµ¬∫. Independence is often
an appropriate assumption. For example, if the random variable ùê¥represents the outcome
from tossing one fair coin and the random variable ùêµrepresents the outcome from tossing
another, then knowing whether ùê¥came up heads should not influence the probability of ùêµ
coming up heads.
Independence is especially useful when it holds among the successive draws of our data
from some underlying distribution (allowing us to make strong statistical conclusions) or
whenitholdsamongvariousvariablesinourdata,allowingustoworkwithsimplermodels
that encode this independence structure. On the other hand, estimating the dependencies
amongrandomvariablesisoftentheveryaimoflearning. Wecaretoestimatetheprobabil-
ity of disease given symptoms specifically because we believe that diseases and symptoms
arenotindependent.
Note that because conditional probabilities are proper probabilities, the concepts of inde-
pendenceanddependencealsoapplytothem. Tworandomvariables ùê¥andùêµarecondition-
ally independent given a third variable ùê∂if and only if ùëÉ¬πùê¥,ùêµjùê∂¬∫=ùëÉ¬πùê¥jùê∂¬∫ùëÉ¬πùêµjùê∂¬∫.
Interestingly, two variables can be independent in general but become dependent when
conditioning on a third. This often occurs when the two random variables ùê¥andùêµcor-
respond to causes of some third variable ùê∂. For example, broken bones and lung cancer
might be independent in the general population but if we condition on being in the hospital
then we might find that broken bones are negatively correlated with lung cancer. That is
73 Probability and Statistics
because the broken bone explainsaway why some person is in the hospital and thus lowers
the probability that they are hospitalized because of having lung cancer.
Andconversely,twodependentrandomvariablescanbecomeindependentuponcondition-
ing on a third. This often happens when two otherwise unrelated events have a common
cause. Shoesizeandreadinglevelarehighlycorrelatedamongelementaryschoolstudents,
but this correlation disappears if we condition on age.
2.6.5An Example
Let‚Äôs put our skills to the test. Assume that a doctor administers an HIV test to a patient.
This test is fairly accurate and fails only with 1% probability if the patient is healthy but
reported as diseased, i.e., healthy patients test positive in 1% of cases. Moreover, it never
failstodetectHIVifthepatientactuallyhasit. Weuse ùê∑12f0,1gtoindicatethediagnosis
(0if negative and 1if positive) and ùêª2f0,1gto denote the HIV status.
Conditional probability ùêª=1ùêª=0
ùëÉ¬πùê∑1=1jùêª¬∫ 1 0.01
ùëÉ¬πùê∑1=0jùêª¬∫ 0 0.99
Note that the column sums are all 1 (but the row sums do not), since they are conditional
probabilities. Let‚Äôs compute the probability of the patient having HIV if the test comes
back positive, i.e., ùëÉ¬πùêª=1jùê∑1=1¬∫. Intuitively this is going to depend on how common
the disease is, since it affects the number of false alarms. Assume that the population is
fairly free of the disease, e.g., ùëÉ¬πùêª=1¬∫=0.0015. To apply Bayes‚Äô theorem, we need to
apply marginalization to determine
ùëÉ¬πùê∑1=1¬∫=ùëÉ¬πùê∑1=1,ùêª=0¬∫¬∏ùëÉ¬πùê∑1=1,ùêª=1¬∫
=ùëÉ¬πùê∑1=1jùêª=0¬∫ùëÉ¬πùêª=0¬∫¬∏ùëÉ¬πùê∑1=1jùêª=1¬∫ùëÉ¬πùêª=1¬∫
=0.011485.(2.6.7)
This leads us to
ùëÉ¬πùêª=1jùê∑1=1¬∫=ùëÉ¬πùê∑1=1jùêª=1¬∫ùëÉ¬πùêª=1¬∫
ùëÉ¬πùê∑1=1¬∫=0.1306. (2.6.8)
In other words, there is only a 13.06% chance that the patient actually has HIV, despite the
testbeingprettyaccurate. Aswecansee,probabilitycanbecounterintuitive. Whatshoulda
patientdouponreceivingsuchterrifyingnews? Likely,thepatientwouldaskthephysician
to administer another test to get clarity. The second test has different characteristics and it
is not as good as the first one.
Conditional probability ùêª=1ùêª=0
ùëÉ¬πùê∑2=1jùêª¬∫ 0.98 0.03
ùëÉ¬πùê∑2=0jùêª¬∫ 0.02 0.97
74 Preliminaries
Unfortunately, the second test comes back positive, too. Let‚Äôs calculate the requisite prob-
abilities to invoke Bayes‚Äô theorem by assuming conditional independence:
ùëÉ¬πùê∑1=1,ùê∑2=1jùêª=0¬∫=ùëÉ¬πùê∑1=1jùêª=0¬∫ùëÉ¬πùê∑2=1jùêª=0¬∫=0.0003,
ùëÉ¬πùê∑1=1,ùê∑2=1jùêª=1¬∫=ùëÉ¬πùê∑1=1jùêª=1¬∫ùëÉ¬πùê∑2=1jùêª=1¬∫= 0.98.
(2.6.9)
Now we can apply marginalization to obtain the probability that both tests come back pos-
itive:
ùëÉ¬πùê∑1=1,ùê∑2=1¬∫
=ùëÉ¬πùê∑1=1,ùê∑2=1,ùêª=0¬∫¬∏ùëÉ¬πùê∑1=1,ùê∑2=1,ùêª=1¬∫
=ùëÉ¬πùê∑1=1,ùê∑2=1jùêª=0¬∫ùëÉ¬πùêª=0¬∫¬∏ùëÉ¬πùê∑1=1,ùê∑2=1jùêª=1¬∫ùëÉ¬πùêª=1¬∫
=0.00176955.
(2.6.10)
Finally,theprobabilityofthepatienthavingHIVgiventhatbothtestsarepositiveis
ùëÉ¬πùêª=1jùê∑1=1,ùê∑2=1¬∫=ùëÉ¬πùê∑1=1,ùê∑2=1jùêª=1¬∫ùëÉ¬πùêª=1¬∫
ùëÉ¬πùê∑1=1,ùê∑2=1¬∫=0.8307.(2.6.11)
That is, the second test allowed us to gain much higher confidence that not all is well. De-
spite the second test being considerably less accurate than the first one, it still significantly
improved our estimate. The assumption of both tests being conditionally independent of
each other was crucial for our ability to generate a more accurate estimate. Take the ex-
treme case where we run the same test twice. In this situation we would expect the same
outcomebothtimes,hencenoadditionalinsightisgainedfromrunningthesametestagain.
The astute reader might have noticed that the diagnosis behaved like a classifier hiding in
plain sight where our ability to decide whether a patient is healthy increases as we obtain
more features (test outcomes).
2.6.6Expectations
Often, making decisions requires not just looking at the probabilities assigned to individ-
ual events but composing them together into useful aggregates that can provide us with
guidance. For example, when random variables take continuous scalar values, we often
care about knowing what value to expect on average . This quantity is formally called an
expectation . If we are making investments, the first quantity of interest might be the return
we can expect, averaging over all the possible outcomes (and weighting by the appropri-
ate probabilities). For instance, say that with 50% probability, an investment might fail
altogether, with 40% probability it might provide a 2 return, and with 10% probability
it might provide a 10 return 10. To calculate the expected return, we sum over all re-
turns, multiplying each by the probability that they will occur. This yields the expectation
0.50¬∏0.42¬∏0.110=1.8. Hence the expected return is 1.8 .
In general, the expectation (or average) of the random variable ùëãis defined as
ùê∏¬ªùëã¬º=ùê∏ùë•ùëÉ¬ªùë•¬º=√ï
ùë•ùë•ùëÉ¬πùëã=ùë•¬∫.(2.6.12)
75 Probability and Statistics
Likewise, for densities we obtain ùê∏¬ªùëã¬º=¬Ø
ùë• ùëëùëù¬πùë•¬∫. Sometimes we are interested in the
expected value of some function of ùë•. We can calculate these expectations as
ùê∏ùë•ùëÉ¬ªùëì¬πùë•¬∫¬º=√ï
ùë•ùëì¬πùë•¬∫ùëÉ¬πùë•¬∫andùê∏ùë•ùëÉ¬ªùëì¬πùë•¬∫¬º=¬π
ùëì¬πùë•¬∫ùëù¬πùë•¬∫ùëëùë• (2.6.13)
for discrete probabilities and densities, respectively. Returning to the investment exam-
ple from above, ùëìmight be the utility(happiness) associated with the return. Behavior
economists have long noted that people associate greater disutility with losing money than
the utility gained from earning one dollar relative to their baseline. Moreover, the value
of money tends to be sub-linear. Possessing 100k dollars versus zero dollars can make the
differencebetweenpayingtherent, eatingwell, andenjoyingqualityhealthcareversussuf-
fering through homelessness. On the other hand, the gains due to possessing 200k versus
100k are less dramatic. Reasoning like this motivates the clich√© that ‚Äúthe utility of money
is logarithmic‚Äù.
Iftheutilityassociatedwithatotallosswere  1, andtheutilitiesassociatedwithreturnsof
1,2, and 10were 1,2and4, respectively, then the expected happiness of investing would
be0.5¬π 1¬∫¬∏0.42¬∏0.14=0.7(an expected loss of utility of 30%). If indeed this were
your utility function, you might be best off keeping the money in the bank.
Forfinancialdecisions,wemightalsowanttomeasurehow riskyaninvestmentis. Here,we
care not just about the expected value but how much the actual values tend to varyrelative
to this value. Note that we cannot just take the expectation of the difference between the
actual and expected values. This is because the expectation of a difference is the difference
of the expectations, i.e., ùê∏¬ªùëã ùê∏¬ªùëã¬º¬º=ùê∏¬ªùëã¬º ùê∏¬ªùê∏¬ªùëã¬º¬º=0. However, we can look at
the expectation of any non-negative function of this difference. The variance of a random
variable is calculated by looking at the expected value of the squared differences:
Var¬ªùëã¬º=ùê∏
¬πùëã ùê∏¬ªùëã¬º¬∫2
=ùê∏¬ªùëã2¬º ùê∏¬ªùëã¬º2. (2.6.14)
Here the equality follows by expanding ¬πùëã ùê∏¬ªùëã¬º¬∫2=ùëã2 2ùëãùê∏¬ªùëã¬º¬∏ùê∏¬ªùëã¬º2and taking
expectationsforeachterm. Thesquarerootofthevarianceisanotherusefulquantitycalled
thestandarddeviation . Whilethisandthevarianceconveythesameinformation(eithercan
becalculatedfromtheother),thestandarddeviationhasthenicepropertythatitisexpressed
in the same units as the original quantity represented by the random variable.
Lastly, the variance of a function of a random variable is defined analogously as
Varùë•ùëÉ¬ªùëì¬πùë•¬∫¬º=ùê∏ùë•ùëÉ¬ªùëì2¬πùë•¬∫¬º ùê∏ùë•ùëÉ¬ªùëì¬πùë•¬∫¬º2. (2.6.15)
Returning to our investment example, we can now compute the variance of the investment.
It is given by 0.50¬∏0.422¬∏0.1102 1.82=8.36. For all intents and purposes this
is a risky investment. Note that by mathematical convention mean and variance are often
referenced as ùúáandùúé2. This is particularly the case whenever we use it to parametrize a
Gaussian distribution.
In the same way as we introduced expectations and variance for scalarrandom variables,
we can do so for vector-valued ones. Expectations are easy, since we can apply them el-
ementwise. For instance, ùùÅdef=ùê∏xùëÉ¬ªx¬ºhas coordinates ùúáùëñ=ùê∏xùëÉ¬ªùë•ùëñ¬º.Covariances
76 Preliminaries
59are more complicated. We define them by taking expectations of the outer product of the
difference between random variables and their mean:
ùö∫def=CovxùëÉ¬ªx¬º=ùê∏xùëÉ
¬πx ùùÅ¬∫¬πx ùùÅ¬∫>
. (2.6.16)
This matrix ùö∫is referred to as the covariance matrix. An easy way to see its effect is to
consider some vector vof the same size as x. It follows that
v>ùö∫v=ùê∏xùëÉ
v>¬πx ùùÅ¬∫¬πx ùùÅ¬∫>v
=Varùë•ùëÉ¬ªv>x¬º. (2.6.17)
As such, ùö∫allows us to compute the variance for any linear function of xby a simple
matrix multiplication. The off-diagonal elements tell us how correlated the coordinates
are: a value of 0 means no correlation, where a larger positive value means that they are
more strongly correlated.
2.6.7Discussion
In machine learning, there are many things to be uncertain about! We can be uncertain
about the value of a label given an input. We can be uncertain about the estimated value of
a parameter. We can even be uncertain about whether data arriving at deployment is even
from the same distribution as the training data.
Byaleatoric uncertainty , we mean uncertainty that is intrinsic to the problem, and due to
genuine randomness unaccounted for by the observed variables. By epistemicuncertainty ,
we mean uncertainty over a model‚Äôs parameters, the sort of uncertainty that we can hope
to reduce by collecting more data. We might have epistemic uncertainty concerning the
probability that a coin turns up heads, but even once we know this probability, we are left
with aleatoric uncertainty about the outcome of any future toss. No matter how long we
watch someone tossing a fair coin, we will never be more or less than 50% certain that
the next toss will come up heads. These terms come from mechanical modeling, (see e.g.,
Der Kiureghian and Ditlevsen ( 2009) for a review on this aspect of uncertainty quantifica-
tion59). It is worth noting, however, that these terms constitute a slight abuse of language.
The term epistemic refers to anything concerning knowledge and thus, in the philosophical
sense, all uncertainty is epistemic.
Wesawthatsamplingdatafromsomeunknownprobabilitydistributioncanprovideuswith
information that can be used to estimate the parameters of the data generating distribution.
That said, the rate at which this is possible can be quite slow. In our coin tossing example
(and many others) we can do no better than to design estimators that converge at a rate of
1¬ùpùëõ, whereùëõis the sample size (e.g., the number of tosses). This means that by going
from10to1000observations(usuallyaveryachievabletask)weseeatenfoldreductionof
uncertainty, whereas the next 1000 observations help comparatively little, offering only a
1.41timesreduction. Thisisapersistentfeatureofmachinelearning: whilethereareoften
easy gains, it takes a very large amount of data, and often with it an enormous amount of
computation, to make further gains. For an empirical review of this fact for large scale
language models see Revels etal.(2016).
We also sharpened our language and tools for statistical modeling. In the process of that
77 Probability and Statistics
60
61
62we learned about conditional probabilities and about one of the most important equations
in statistics‚ÄîBayes‚Äô theorem. It is an effective tool for decoupling information conveyed
by data through a likelihood term ùëÉ¬πùêµjùê¥¬∫that addresses how well observations ùêµmatch
a choice of parameters ùê¥, and a prior probability ùëÉ¬πùê¥¬∫which governs how plausible a par-
ticular choice of ùê¥was in the first place. In particular, we saw how this rule can be applied
to assign probabilities to diagnoses, based on the efficacy of the test andthe prevalence of
the disease itself (i.e., our prior).
Lastly, we introduced a first set of nontrivial questions about the effect of a specific proba-
bilitydistribution,namelyexpectationsandvariances. Whiletherearemanymorethanjust
linear and quadratic expectations for a probability distribution, these two already provide
a good deal of knowledge about the possible behavior of the distribution. For instance,
Chebyshev‚Äôs inequality60states thatùëÉ¬πjùëã ùúájùëòùúé¬∫ 1¬ùùëò2, whereùúáis the expecta-
tion,ùúé2is the variance of the distribution, and ùëò > 1is a confidence parameter of our
choosing. It tells us that draws from a distribution lie with at least 50% probability within
a¬ª p
2ùúé,p
2ùúé¬ºinterval centered on the expectation.
2.6.8Exercises
1.Giveanexamplewhereobservingmoredatacanreducetheamountofuncertaintyabout
the outcome to an arbitrarily low level.
2.Giveanexamplewhereobservingmoredatawillonlyreducetheamountofuncertainty
uptoapointandthennofurther. Explainwhythisisthecaseandwhereyouexpectthis
point to occur.
3.We empirically demonstrated convergence to the mean for the toss of a coin. Calculate
thevarianceoftheestimateoftheprobabilitythatweseeaheadafterdrawing ùëõsamples.
1.How does the variance scale with the number of observations?
2.Use Chebyshev‚Äôs inequality to bound the deviation from the expectation.
3.How does it relate to the central limit theorem?
4.Assume that we draw ùëösamplesùë•ùëñfrom a probability distribution with zero mean and
unit variance. Compute the averages ùëßùëödef=ùëö 1√çùëö
ùëñ=1ùë•ùëñ. Can we apply Chebyshev‚Äôs
inequality for every ùëßùëöindependently? Why not?
5.Given two events with probability ùëÉ¬πA¬∫andùëÉ¬πB¬∫, compute upper and lower bounds
onùëÉ¬πA[B¬∫ andùëÉ¬πA\B¬∫ . Hint: graph the situation using a Venn diagram61.
6.Assumethatwehaveasequenceofrandomvariables,say ùê¥,ùêµ,andùê∂,whereùêµonlyde-
pendsonùê¥,andùê∂onlydependson ùêµ,canyousimplifythejointprobability ùëÉ¬πùê¥,ùêµ,ùê∂¬∫?
Hint: this is a Markov chain62.
7.InSection 2.6.5 , assume that the outcomes of the two tests are not independent. In
particular assume that either test on its own has a false positive rate of 10% and a false
negative rate of 1%. That is, assume that ùëÉ¬πùê∑=1jùêª=0¬∫=0.1and thatùëÉ¬πùê∑=
0jùêª=1¬∫=0.01. Moreover, assume that for ùêª=1(infected) the test outcomes are
78 Preliminaries
63
64
65
66conditionally independent, i.e., that ùëÉ¬πùê∑1,ùê∑2jùêª=1¬∫=ùëÉ¬πùê∑1jùêª=1¬∫ùëÉ¬πùê∑2jùêª=
1¬∫but that for healthy patients the outcomes are coupled via ùëÉ¬πùê∑1=ùê∑2=1jùêª=
0¬∫=0.02.
1.Work out the joint probability table for ùê∑1andùê∑2, givenùêª=0based on the infor-
mation you have so far.
2.Derive the probability that the patient is diseased ( ùêª=1) after one test returns
positive. You can assume the same baseline probability ùëÉ¬πùêª=1¬∫=0.0015as
before.
3.Derive the probability that the patient is diseased ( ùêª=1) after both tests return
positive.
8.Assume that you are an asset manager for an investment bank and you have a choice of
stocksùë†ùëñtoinvestin. Yourportfolioneedstoaddupto 1withweights ùõºùëñforeachstock.
The stocks have an average return ùùÅ=ùê∏sùëÉ¬ªs¬ºand covariance ùö∫=CovsùëÉ¬ªs¬º.
1.Compute the expected return for a given portfolio ùú∂.
2.If you wanted to maximize the return of the portfolio, how should you choose your
investment?
3.Compute the variance of the portfolio.
4.Formulateanoptimizationproblemofmaximizingthereturnwhilekeepingthevari-
anceconstrainedtoanupperbound. ThisistheNobel-Prizewinning Markovitzport-
folio63(Mangram, 2013 ). Tosolveityouwillneedaquadraticprogrammingsolver,
something way beyond the scope of this book.
Discussions64.
2.7Documentation
WhilewecannotpossiblyintroduceeverysinglePyTorchfunctionandclass(andtheinfor-
mation might become outdated quickly), the API documentation65and additional tutorials
66and examples provide such documentation. This section provides some guidance for
how to explore the PyTorch API.
import torch
2.7.1Functionsand Classes in a Module
To know which functions and classes can be called in a module, we invoke the dirfunc-
tion. For instance, we can query all properties in the module for generating random num-
bers:
79 Documentation
print (dir(torch .distributions))
['AbsTransform ','AffineTransform ','Bernoulli ','Beta ','Binomial ',
‚Ü©!'CatTransform ','Categorical ','Cauchy ','Chi2 ','ComposeTransform ',
‚Ü©!'ContinuousBernoulli ','CorrCholeskyTransform ',
‚Ü©!'CumulativeDistributionTransform ','Dirichlet ','Distribution ','ExpTransform
‚Ü©!','Exponential ','ExponentialFamily ','FisherSnedecor ','Gamma ','Geometric
‚Ü©!','Gumbel ','HalfCauchy ','HalfNormal ','Independent ','IndependentTransform
‚Ü©!','Kumaraswamy ','LKJCholesky ','Laplace ','LogNormal ','LogisticNormal ',
‚Ü©!'LowRankMultivariateNormal ','LowerCholeskyTransform ','MixtureSameFamily ',
‚Ü©!'Multinomial ','MultivariateNormal ','NegativeBinomial ','Normal ',
‚Ü©!'OneHotCategorical ','OneHotCategoricalStraightThrough ','Pareto ','Poisson ',
‚Ü©!'PositiveDefiniteTransform ','PowerTransform ','RelaxedBernoulli ',
‚Ü©!'RelaxedOneHotCategorical ','ReshapeTransform ','SigmoidTransform ',
‚Ü©!'SoftmaxTransform ','SoftplusTransform ','StackTransform ',
‚Ü©!'StickBreakingTransform ','StudentT ','TanhTransform ','Transform ',
‚Ü©!'TransformedDistribution ','Uniform ','VonMises ','Weibull ','Wishart ','__
‚Ü©!all__ ','__builtins__ ','__cached__ ','__doc__ ','__file__ ','__loader__ ','_
‚Ü©!_name__ ','__package__ ','__path__ ','__spec__ ','bernoulli ','beta ',
‚Ü©!'biject_to ','binomial ','categorical ','cauchy ','chi2 ','constraint_
‚Ü©!registry ','constraints ','continuous_bernoulli ','dirichlet ','distribution
‚Ü©!','exp_family ','exponential ','fishersnedecor ','gamma ','geometric ',
‚Ü©!'gumbel ','half_cauchy ','half_normal ','identity_transform ','independent ',
‚Ü©!'kl','kl_divergence ','kumaraswamy ','laplace ','lkj_cholesky ','log_normal
‚Ü©!','logistic_normal ','lowrank_multivariate_normal ','mixture_same_family ',
‚Ü©!'multinomial ','multivariate_normal ','negative_binomial ','normal ','one_
‚Ü©!hot_categorical ','pareto ','poisson ','register_kl ','relaxed_bernoulli ',
‚Ü©!'relaxed_categorical ','studentT ','transform_to ','transformed_distribution
‚Ü©!','transforms ','uniform ','utils ','von_mises ','weibull ','wishart ']
Generally, we can ignore functions that start and end with __(special objects in Python) or
functionsthatstartwithasingle _(usuallyinternalfunctions). Basedontheremainingfunc-
tionorattributenames,wemighthazardaguessthatthismoduleoffersvariousmethodsfor
generating random numbers, including sampling from the uniform distribution ( uniform ),
normal distribution ( normal), and multinomial distribution ( multinomial ).
2.7.2SpecificFunctions and Classes
For specific instructions on how to use a given function or class, we can invoke the help
function. Asanexample,let‚Äôsexploretheusageinstructionsfortensors‚Äô onesfunction.
help(torch .ones)
Help on built-in function ones in module torch:
ones(...)
ones( *size, *, out=None, dtype=None, layout=torch.strided, device=None,
‚Ü©!requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value 1, with the shape defined
80 Preliminaries
by the variable argument size.
Args:
size (int...): a sequence of integers defining the shape of the ‚ê£
‚Ü©!output tensor.
Can be a variable number of arguments or a collection like a ‚ê£
‚Ü©!list or tuple.
Keyword arguments:
out (Tensor, optional): the output tensor.
dtype (torch.dtype, optional): the desired data type of returned ‚ê£
‚Ü©!tensor.
Default: if None, uses a global default (see torch.set_default_
‚Ü©!tensor_type()).
layout (torch.layout, optional): the desired layout of returned ‚ê£
‚Ü©!Tensor.
Default: torch.strided.
device (torch.device, optional): the desired device of returned ‚ê£
‚Ü©!tensor.
Default: if None, uses the current device for the default tensor ‚ê£
‚Ü©!type
(see torch.set_default_tensor_type()). device will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor ‚ê£
‚Ü©!types.
requires_grad (bool, optional): If autograd should record operations ‚ê£
‚Ü©!on the
returned tensor. Default: False.
Example::
>>> torch.ones(2, 3)
tensor([[ 1., 1., 1.],
[ 1., 1., 1.]])
>>> torch.ones(5)
tensor([ 1., 1., 1., 1., 1.])
From the documentation, we can see that the onesfunction creates a new tensor with the
specified shape and sets all the elements to the value of 1. Whenever possible, you should
run a quick test to confirm your interpretation:
torch .ones( 4)
tensor([ 1.,1.,1.,1.])
81 Documentation
67In the Jupyter notebook, we can use ?to display the document in another window. For
example, list?will create content that is almost identical to help(list) , displaying it
in a new browser window. In addition, if we use two question marks, such as list??, the
Python code implementing the function will also be displayed.
The official documentation provides plenty of descriptions and examples that are beyond
this book. We emphasize important use cases that will get you started quickly with prac-
tical problems, rather than completeness of coverage. We also encourage you to study the
source code of the libraries to see examples of high-quality implementations of production
code. By doing this you will become a better engineer in addition to becoming a better
scientist.
Discussions67.
3 Linear Neural Networks for Regression
Before we worry about making our neural networks deep, it will be helpful to implement
someshallowones,forwhichtheinputsconnectdirectlytotheoutputs. Thiswillproveim-
portant for a few reasons. First, rather than getting distracted by complicated architectures,
we can focus on the basics of neural network training, including parametrizing the output
layer, handling data, specifying a loss function, and training the model. Second, this class
of shallow networks happens to comprise the set of linear models, which subsumes many
classical methods of statistical prediction, including linear and softmax regression. Un-
derstanding these classical tools is pivotal because they are widely used in many contexts
and we will often need to use them as baselines when justifying the use of fancier archi-
tectures. This chapter will focus narrowly on linear regression and the next one will extend
our modeling repertoire by developing linear neural networks for classification.
3.1LinearRegression
Regression problems pop up whenever we want to predict a numerical value. Common ex-
amples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for
patientsinthehospital),forecastingdemand(forretailsales),amongnumerousothers. Not
everypredictionproblemisoneofclassicalregression. Lateron,wewillintroduceclassifi-
cationproblems,wherethegoalistopredictmembershipamongasetofcategories.
As a running example, suppose that we wish to estimate the prices of houses (in dollars)
based on their area (in square feet) and age (in years). To develop a model for predicting
house prices, we need to get our hands on data, including the sales price, area, and age for
each home. In the terminology of machine learning, the dataset is called a trainingdataset
ortraining set , and each row (containing the data corresponding to one sale) is called an
example (ordata point ,instance,sample). The thing we are trying to predict (price) is
called alabel(ortarget). The variables (age and area) upon which the predictions are
based are called features (orcovariates ).
%matplotlib inline
import math
import time
import numpy asnp
(continues on next page)
82
83 Linear Regression
(continued from previous page)
import torch
from d2l import torch asd2l
3.1.1Basics
Linearregression is both the simplest and most popular among the standard tools for tack-
ling regression problems. Dating back to the dawn of the 19th century ( Gauss, 1809 ,Leg-
endre, 1805 ), linear regression flows from a few simple assumptions. First, we assume that
the relationship between features xand targetùë¶is approximately linear, i.e., that the con-
ditional mean ùê∏¬ªùëåjùëã=x¬ºcan be expressed as a weighted sum of the features x. This
setup allows that the target value may still deviate from its expected value on account of
observationnoise. Next,wecanimposetheassumptionthatanysuchnoiseiswellbehaved,
following a Gaussian distribution. Typically, we will use ùëõto denote the number of exam-
ples in our dataset. We use superscripts to enumerate samples and targets, and subscripts
to index coordinates. More concretely, x¬πùëñ¬∫denotes theùëñthsample andùë•¬πùëñ¬∫
ùëódenotes itsùëóth
coordinate.
Model
At the heart of every solution is a model that describes how features can be transformed
into an estimate of the target. The assumption of linearity means that the expected value of
the target(price) can be expressedas a weighted sum of the features(area and age):
price=ùë§areaarea¬∏ùë§ageage¬∏ùëè. (3.1.1)
Hereùë§areaandùë§ageare called weights, andùëèis called a bias(oroffsetorintercept ). The
weights determine the influence of each feature on our prediction. The bias determines the
value of the estimate when all features are zero. Even though we will never see any newly-
built homes with precisely zero area, we still need the bias because it allows us to express
all linear functions of our features (rather than restricting us to lines that pass through the
origin). Strictly speaking, (3.1.1 )is anaÔ¨Äine transformation of input features, which is
characterized by a linear transformation of features via a weighted sum, combined with a
translation via the added bias. Given a dataset, our goal is to choose the weights wand
the biasùëèthat, on average, make our model‚Äôs predictions fit the true prices observed in the
data as closely as possible.
In disciplines where it is common to focus on datasets with just a few features, explicitly
expressing models long-form, as in (3.1.1 ), is common. In machine learning, we usually
work with high-dimensional datasets, where it is more convenient to employ compact lin-
ear algebra notation. When our inputs consist of ùëëfeatures, we can assign each an index
(between 1andùëë) and express our prediction ÀÜùë¶(in general the ‚Äúhat‚Äù symbol denotes an
estimate) as
ÀÜùë¶=ùë§1ùë•1¬∏¬∏ùë§ùëëùë•ùëë¬∏ùëè. (3.1.2)
84 Linear Neural Networks for Regression
Collecting all features into a vector x2Rùëëand all weights into a vector w2Rùëë, we can
express our model compactly via the dot product between wandx:
ÀÜùë¶=w>x¬∏ùëè. (3.1.3)
In(3.1.3 ), the vector xcorresponds to the features of a single example. We will often
find it convenient to refer to features of our entire dataset of ùëõexamples via the design
matrix X2Rùëõùëë. Here, Xcontains one row for every example and one column for every
feature. For a collection of features X, the predictions ÀÜy2Rùëõcan be expressed via the
matrix‚Äìvector product:
ÀÜy=Xw¬∏ùëè, (3.1.4)
where broadcasting ( Section 2.1.4 ) is applied during the summation. Given features of a
training dataset Xand corresponding (known) labels y, the goal of linear regression is to
findtheweightvector wandthebiasterm ùëèsuchthat,givenfeaturesofanewdataexample
sampled from the same distribution as X, the new example‚Äôs label will (in expectation) be
predicted with the smallest error.
Even if we believe that the best model for predicting ùë¶givenxis linear, we would not
expect to find a real-world dataset of ùëõexamples where ùë¶¬πùëñ¬∫exactly equals w>x¬πùëñ¬∫¬∏ùëè
for all 1ùëñùëõ. For example, whatever instruments we use to observe the features X
and labels y, there might be a small amount of measurement error. Thus, even when we
are confident that the underlying relationship is linear, we will incorporate a noise term to
account for such errors.
Before we can go about searching for the best parameters (ormodelparameters )wandùëè,
we will need two more things: (i) a measure of the quality of some given model; and (ii) a
procedure for updating the model to improve its quality.
Loss Function
Naturally, fitting our model to the data requires that we agree on some measure of fitness
(or, equivalently, of unfitness ).Loss functions quantify the distance between the realand
predicted valuesofthetarget. Thelosswillusuallybeanonnegativenumberwheresmaller
valuesarebetterandperfectpredictionsincuralossof0. Forregressionproblems,themost
common loss function is the squared error. When our prediction for an example ùëñisÀÜùë¶¬πùëñ¬∫
and the corresponding true label is ùë¶¬πùëñ¬∫, thesquarederror is given by:
ùëô¬πùëñ¬∫¬πw,ùëè¬∫=1
2
ÀÜùë¶¬πùëñ¬∫ ùë¶¬πùëñ¬∫2
. (3.1.5)
The constant1
2makes no real difference but proves to be notationally convenient, since it
cancels out when we take the derivative of the loss. Because the training dataset is given
to us, and thus is out of our control, the empirical error is only a function of the model
parameters. In Fig. 3.1.1 , we visualize the fit of a linear regression model in a problem
with one-dimensional inputs.
Notethatlargedifferencesbetweenestimates ÀÜùë¶¬πùëñ¬∫andtargetsùë¶¬πùëñ¬∫leadtoevenlargercontri-
butions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword;
85 Linear Regression
tFig. 3.1.1 Fitting a linear regression model to one-dimensional data.
while it encourages the model to avoid large errors it can also lead to excessive sensitivity
to anomalous data). To measure the quality of a model on the entire dataset of ùëõexamples,
we simply average (or equivalently, sum) the losses on the training set:
ùêø¬πw,ùëè¬∫=1
ùëõùëõ√ï
ùëñ=1ùëô¬πùëñ¬∫¬πw,ùëè¬∫=1
ùëõùëõ√ï
ùëñ=11
2
w>x¬πùëñ¬∫¬∏ùëè ùë¶¬πùëñ¬∫2
. (3.1.6)
When training the model, we seek parameters ( w,ùëè) that minimize the total loss across
all training examples:
w,ùëè=argmin
w,ùëèùêø¬πw,ùëè¬∫.(3.1.7)
AnalyticSolution
Unlike most of the models that we will cover, linear regression presents us with a surpris-
ingly easy optimization problem. In particular, we can find the optimal parameters (as
assessed on the training data) analytically by applying a simple formula as follows. First,
we can subsume the bias ùëèinto the parameter wby appending a column to the design ma-
trix consisting of all 1s. Then our prediction problem is to minimize ky Xwk2. As long
as the design matrix Xhas full rank (no feature is linearly dependent on the others), then
there will be just one critical point on the loss surface and it corresponds to the minimum
of the loss over the entire domain. Taking the derivative of the loss with respect to wand
setting it equal to zero yields:
ùúïwky Xwk2=2X>¬πXw y¬∫=0and hence X>y=X>Xw. (3.1.8)
Solving for wprovides us with the optimal solution for the optimization problem. Note
that this solution
w=¬πX>X¬∫ 1X>y (3.1.9)
willonlybeuniquewhenthematrix X>Xisinvertible,i.e.,whenthecolumnsofthedesign
matrix are linearly independent ( Golub and Van Loan, 1996 ).
While simple problems like linear regression may admit analytic solutions, you should
notgetusedtosuchgoodfortune. Althoughanalyticsolutionsallowfornicemathematical
analysis,therequirementofananalyticsolutionissorestrictivethatitwouldexcludealmost
all exciting aspects of deep learning.
86 Linear Neural Networks for Regression
MinibatchStochasticGradient Descent
Fortunately, even in cases where we cannot solve the models analytically, we can still of-
ten train models effectively in practice. Moreover, for many tasks, those hard-to-optimize
models turn out to be so much better that figuring out howto train them ends up being well
worth the trouble.
The key technique for optimizing nearly every deep learning model, and which we will
call upon throughout this book, consists of iteratively reducing the error by updating the
parameters in the direction that incrementally lowers the loss function. This algorithm is
calledgradientdescent .
The most naive application of gradient descent consists of taking the derivative of the loss
function,whichisanaverageofthelossescomputedoneverysingleexampleinthedataset.
In practice, this can be extremely slow: we must pass over the entire dataset before making
a single update, even if the update steps might be very powerful ( Liu and Nocedal, 1989 ).
Even worse, if there is a lot of redundancy in the training data, the benefit of a full update
is limited.
The other extreme is to consider only a single example at a time and to take update steps
based on one observation at a time. The resulting algorithm, stochastic gradient descent
(SGD) can be an effective strategy ( Bottou, 2010 ), even for large datasets. Unfortunately,
SGD has drawbacks, both computational and statistical. One problem arises from the fact
thatprocessorsarealotfastermultiplyingandaddingnumbersthantheyareatmovingdata
from main memory to processor cache. It is up to an order of magnitude more efficient
to perform a matrix‚Äìvector multiplication than a corresponding number of vector‚Äìvector
operations. Thismeansthatitcantakealotlongertoprocessonesampleatatimecompared
to a full batch. A second problem is that some of the layers, such as batch normalization
(to be described in Section 8.5 ), only work well when we have access to more than one
observation at a time.
The solution to both problems is to pick an intermediate strategy: rather than taking a full
batchoronlyasinglesampleatatime,wetakea minibatch ofobservations( Lietal.,2014).
The specific choice of the size of the said minibatch depends on many factors, such as the
amount of memory, the number of accelerators, the choice of layers, and the total dataset
size. Despite all that, a number between 32 and 256, preferably a multiple of a large power
of2, is a good start. This leads us to minibatchstochasticgradientdescent .
Initsmostbasicform, ineachiteration ùë°, wefirstrandomlysampleaminibatch Bùë°consist-
ing of a fixed number jBjof training examples. We then compute the derivative (gradient)
oftheaveragelossontheminibatchwithrespecttothemodelparameters. Finally,wemul-
tiply the gradient by a predetermined small positive value ùúÇ, called the learning rate , and
subtract the resulting term from the current parameter values. We can express the update
as follows:
¬πw,ùëè¬∫ ¬πw,ùëè¬∫ ùúÇ
jBj√ï
ùëñ2Bùë°ùúï¬πw,ùëè¬∫ùëô¬πùëñ¬∫¬πw,ùëè¬∫.(3.1.10)
In summary, minibatch SGD proceeds as follows: (i) initialize the values of the model
87 Linear Regression
parameters, typically at random; (ii) iteratively sample random minibatches from the data,
updating the parameters in the direction of the negative gradient. For quadratic losses and
affine transformations, this has a closed-form expansion:
w w ùúÇ
jBj√ï
ùëñ2Bùë°ùúïwùëô¬πùëñ¬∫¬πw,ùëè¬∫=w ùúÇ
jBj√ï
ùëñ2Bùë°x¬πùëñ¬∫
w>x¬πùëñ¬∫¬∏ùëè ùë¶¬πùëñ¬∫
ùëè ùëè ùúÇ
jBj√ï
ùëñ2Bùë°ùúïùëèùëô¬πùëñ¬∫¬πw,ùëè¬∫=ùëè ùúÇ
jBj√ï
ùëñ2Bùë°
w>x¬πùëñ¬∫¬∏ùëè ùë¶¬πùëñ¬∫
.(3.1.11)
Since we pick a minibatch Bwe need to normalize by its size jBj. Frequently minibatch
size and learning rate are user-defined. Such tunable parameters that are not updated in the
training loop are called hyperparameters . They can be tuned automatically by a number
of techniques, such as Bayesian optimization ( Frazier, 2018 ). In the end, the quality of the
solution is typically assessed on a separate validationdataset (orvalidationset ).
After training for some predetermined number of iterations (or until some other stopping
criterionismet),werecordtheestimatedmodelparameters,denoted ÀÜw,ÀÜùëè. Notethatevenif
ourfunctionistrulylinearandnoiseless, theseparameterswillnotbetheexactminimizers
of the loss, nor even deterministic. Although the algorithm converges slowly towards the
minimizers it typically will not find them exactly in a finite number of steps. Moreover,
the minibatchesBused for updating the parameters are chosen at random. This breaks
determinism.
Linear regression happens to be a learning problem with a global minimum (whenever X
is full rank, or equivalently, whenever X>Xis invertible). However, the loss surfaces for
deep networks contain many saddle points and minima. Fortunately, we typically do not
care about finding an exact set of parameters but merely any set of parameters that leads
to accurate predictions (and thus low loss). In practice, deep learning practitioners seldom
struggle to find parameters that minimize the loss on training sets (Frankle and Carbin,
2018,Izmailov et al., 2018). The more formidable task is to find parameters that lead
to accurate predictions on previously unseen data, a challenge called generalization . We
return to these topics throughout the book.
Predictions
Given the model ÀÜw>x¬∏ÀÜùëè, we can now make predictions for a new example, e.g., pre-
dicting the sales price of a previously unseen house given its area ùë•1and ageùë•2. Deep
learningpractitionershavetakentocallingthepredictionphase inference butthisisabitof
a misnomer‚Äî inference refers broadly to any conclusion reached on the basis of evidence,
including both the values of the parameters and the likely label for an unseen instance. If
anything, in the statistics literature inference more often denotes parameter inference and
this overloading of terminology creates unnecessary confusion when deep learning prac-
titioners talk to statisticians. In the following we will stick to prediction whenever possi-
ble.
3.1.2VectorizationforSpeed
88 Linear Neural Networks for Regression
Whentrainingourmodels, wetypicallywanttoprocesswholeminibatchesofexamplessi-
multaneously. Doingthisefficientlyrequiresthatwevectorizethecalculationsandleverage
fast linear algebra libraries rather than writing costly for-loops in Python.
Toseewhythismatterssomuch,let‚Äôsconsidertwomethodsforaddingvectors. Tostart,we
instantiate two 10,000-dimensional vectors containing all 1s. In the first method, we loop
over the vectors with a Python for-loop. In the second, we rely on a single call to +.
n=10000
a=torch .ones(n)
b=torch .ones(n)
Now we can benchmark the workloads. First, we add them, one coordinate at a time, using
a for-loop.
c=torch .zeros(n)
t=time .time()
for iinrange (n):
c[i] =a[i] +b[i]
f'{time .time() -t:.5f}sec'
'0.17802 sec '
Alternatively, we rely on the reloaded +operator to compute the elementwise sum.
t=time .time()
d=a+b
f'{time .time() -t:.5f}sec'
'0.00036 sec '
Thesecondmethodisdramaticallyfasterthanthefirst. Vectorizingcodeoftenyieldsorder-
of-magnitude speedups. Moreover, we push more of the mathematics to the library so we
do not have to write as many calculations ourselves, reducing the potential for errors and
increasing portability of the code.
3.1.3The NormalDistribution and SquaredLoss
Sofarwehavegivenafairlyfunctionalmotivationofthesquaredlossobjective: theoptimal
parameters return the conditional expectation ùê∏¬ªùëåjùëã¬ºwhenever the underlying pattern
is truly linear, and the loss assigns large penalties for outliers. We can also provide a more
formalmotivationforthesquaredlossobjectivebymakingprobabilisticassumptionsabout
the distribution of noise.
Linear regression was invented at the turn of the 19th century. While it has long been
debated whether Gauss or Legendre first thought up the idea, it was Gauss who also dis-
covered the normal distribution (also called the Gaussian ). It turns out that the normal
89 Linear Regression
distribution and linear regression with squared loss share a deeper connection than com-
mon parentage.
Tobegin,recallthatanormaldistributionwithmean ùúáandvariance ùúé2(standarddeviation
ùúé) is given as
ùëù¬πùë•¬∫=1p
2ùúãùúé2exp
 1
2ùúé2¬πùë• ùúá¬∫2
. (3.1.12)
Below we define a function to compute the normal distribution.
def normal (x, mu, sigma):
p=1/math .sqrt( 2*math .pi*sigma **2)
return p*np.exp( -0.5 *(x-mu)**2/sigma **2)
We can now visualize the normal distributions.
# Use NumPy again for visualization
x=np.arange( -7,7,0.01 )
# Mean and standard deviation pairs
params =[(0,1), ( 0,2), ( 3,1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma inparams], xlabel ='x',
ylabel ='p(x) ', figsize =(4.5,2.5),
legend =[f'mean {mu}, std {sigma }'for mu, sigma inparams])
Note that changing the mean corresponds to a shift along the ùë•-axis, and increasing the
variance spreads the distribution out, lowering its peak.
Onewaytomotivatelinearregressionwithsquaredlossistoassumethatobservationsarise
fromnoisymeasurements,wherethenoise ùúñfollowsthenormaldistribution N¬π0,ùúé2¬∫:
ùë¶=w>x¬∏ùëè¬∏ùúñwhereùúñN¬π 0,ùúé2¬∫. (3.1.13)
Thus, we can now write out the likelihood of seeing a particular ùë¶for a given xvia
ùëÉ¬πùë¶jx¬∫=1p
2ùúãùúé2exp
 1
2ùúé2¬πùë¶ w>x ùëè¬∫2
. (3.1.14)
As such, the likelihood factorizes. According to the principle of maximum likelihood , the
90 Linear Neural Networks for Regression
best values of parameters wandùëèare those that maximize the likelihood of the entire
dataset:
ùëÉ¬πyjX¬∫=ùëõ√ñ
ùëñ=1ùëù¬πùë¶¬πùëñ¬∫jx¬πùëñ¬∫¬∫. (3.1.15)
Theequalityfollowssinceallpairs ¬πx¬πùëñ¬∫,ùë¶¬πùëñ¬∫¬∫weredrawnindependentlyofeachother. Es-
timatorschosenaccordingtotheprincipleofmaximumlikelihoodarecalled maximumlike-
lihood estimators . While, maximizing the product of many exponential functions, might
lookdifficult,wecansimplifythingssignificantly,withoutchangingtheobjective,bymax-
imizing the logarithm of the likelihood instead. For historical reasons, optimizations are
moreoftenexpressedasminimizationratherthanmaximization. So,withoutchangingany-
thing,wecan minimize thenegativelog-likelihood ,whichwecanexpressasfollows:
 logùëÉ¬πyjX¬∫=ùëõ√ï
ùëñ=11
2log¬π2ùúãùúé2¬∫¬∏1
2ùúé2
ùë¶¬πùëñ¬∫ w>x¬πùëñ¬∫ ùëè2
. (3.1.16)
If we assume that ùúéis fixed, we can ignore the first term, because it does not depend on w
orùëè. The second term is identical to the squared error loss introduced earlier, except for
the multiplicative constant1
ùúé2. Fortunately, the solution does not depend on ùúéeither. It
follows that minimizing the mean squared error is equivalent to the maximum likelihood
estimation of a linear model under the assumption of additive Gaussian noise.
3.1.4Linear Regressionas a NeuralNetwork
While linear models are not sufficiently rich to express the many complicated networks
that we will introduce in this book, (artificial) neural networks are rich enough to subsume
linear models as networks in which every feature is represented by an input neuron, all of
which are connected directly to the output.
Fig. 3.1.2 depicts linear regression as a neural network. The diagram highlights the con-
nectivity pattern, such as how each input is connected to the output, but not the specific
values taken by the weights or biases.
tFig. 3.1.2 Linear regression is a single-layer neural network.
Theinputsare ùë•1,...,ùë•ùëë. Wereferto ùëëasthenumberofinputs orthefeaturedimensional-
ityin the input layer. The output of the network is ùëú1. Because we are just trying to predict
asinglenumericalvalue,wehaveonlyoneoutputneuron. Notethattheinputvaluesareall
given. There is just a single computed neuron. In summary, we can think of linear regres-
sion as a single-layer fully connected neural network. We will encounter networks with far
more layers in later chapters.
91 Linear Regression
Biology
Because linear regression predates computational neuroscience, it might seem anachro-
nistic to describe linear regression in terms of neural networks. Nonetheless, they were a
natural place to start when the cyberneticists and neurophysiologists Warren McCulloch
and Walter Pitts began to develop models of artificial neurons. Consider the cartoonish
picture of a biological neuron in Fig. 3.1.3 , consisting of dendrites (input terminals), the
nucleus(CPU),the axon(outputwire),andthe axonterminals (outputterminals),enabling
connections to other neurons via synapses .
Dendrite
Cell bodyNode of
RanvierAxon T erminal
Schwann cell
Myelin sheathAxon
Nucleus
tFig. 3.1.3 The real neuron (source: ‚ÄúAnatomy and Physiology‚Äù by the US National Cancer
Institute‚Äôs Surveillance, Epidemiology and End Results (SEER) Program).
Information ùë•ùëñarriving from other neurons (or environmental sensors) is received in the
dendrites. In particular, that information is weighted by synaptic weights ùë§ùëñ, determining
the effect of the inputs, e.g., activation or inhibition via the product ùë•ùëñùë§ùëñ. The weighted
inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum ùë¶=√ç
ùëñùë•ùëñùë§ùëñ¬∏ùëè, possibly subject to some nonlinear postprocessing via a function ùúé¬πùë¶¬∫. This
information is then sent via the axon to the axon terminals, where it reaches its destination
(e.g., an actuator such as a muscle) or it is fed into another neuron via its dendrites.
Certainly, the high-level idea that many such units could be combined, provided they have
the correct connectivity and learning algorithm, to produce far more interesting and com-
plex behavior than any one neuron alone could express arises from our study of real bi-
ological neural systems. At the same time, most research in deep learning today draws
inspiration from a much wider source. We invoke Russell and Norvig ( 2016) who pointed
out that although airplanes might have been inspired by birds, ornithology has not been
the primary driver of aeronautics innovation for some centuries. Likewise, inspiration in
deep learning these days comes in equal or greater measure from mathematics, linguistics,
psychology, statistics, computer science, and many other fields.
3.1.5Summary
Inthissection,weintroducedtraditionallinearregression,wheretheparametersofalinear
function are chosen to minimize squared loss on the training set. We also motivated this
choice of objective both via some practical considerations and through an interpretation
of linear regression as maximimum likelihood estimation under an assumption of linearity
andGaussiannoise. Afterdiscussingbothcomputationalconsiderationsandconnectionsto
92 Linear Neural Networks for Regression
statistics,weshowedhowsuchlinearmodelscouldbeexpressedassimpleneuralnetworks
where the inputs are directly wired to the output(s). While we will soon move past linear
models altogether, they are sufficient to introduce most of the components that all of our
models require: parametric forms, differentiable objectives, optimization via minibatch
stochastic gradient descent, and ultimately, evaluation on previously unseen data.
3.1.6Exercises
1.Assume that we have some data ùë•1,...,ùë•ùëõ2R. Our goal is to find a constant ùëèsuch
that√ç
ùëñ¬πùë•ùëñ ùëè¬∫2is minimized.
1.Find an analytic solution for the optimal value of ùëè.
2.How does this problem and its solution relate to the normal distribution?
3.Whatifwechangethelossfrom√ç
ùëñ¬πùë•ùëñ ùëè¬∫2to√ç
ùëñjùë•ùëñ ùëèj? Canyoufindtheoptimal
solution forùëè?
2.Prove that the affine functions that can be expressed by x>w¬∏ùëèare equivalent to linear
functions on¬πx,1¬∫.
3.Assume that you want to find quadratic functions of x, i.e.,ùëì¬πx¬∫=ùëè¬∏√ç
ùëñùë§ùëñùë•ùëñ¬∏√ç
ùëóùëñùë§ùëñùëóùë•ùëñùë•ùëó. How would you formulate this in a deep network?
4.Recall that one of the conditions for the linear regression problem to be solvable was
that the design matrix X>Xhas full rank.
1.What happens if this is not the case?
2.How could you fix it? What happens if you add a small amount of coordinate-wise
independent Gaussian noise to all entries of X?
3.What is the expected value of the design matrix X>Xin this case?
4.What happens with stochastic gradient descent when X>Xdoes not have full rank?
5.Assume that the noise model governing the additive noise ùúñis the exponential distribu-
tion. That is, ùëù¬πùúñ¬∫=1
2exp¬π jùúñj¬∫.
1.Write out the negative log-likelihood of the data under the model  logùëÉ¬πyjX¬∫.
2.Can you find a closed form solution?
3.Suggest a minibatch stochastic gradient descent algorithm to solve this problem.
What could possibly go wrong (hint: what happens near the stationary point as we
keep on updating the parameters)? Can you fix this?
6.Assume that we want to design a neural network with two layers by composing two
linear layers. That is, the output of the first layer becomes the input of the second layer.
Why would such a naive composition not work?
7.What happens if you want to use regression for realistic price estimation of houses or
stock prices?
93 Object-Oriented Design for Implementation
68
69
701.Show that the additive Gaussian noise assumption is not appropriate. Hint: can we
have negative prices? What about fluctuations?
2.Whywouldregressiontothelogarithmofthepricebemuchbetter,i.e., ùë¶=logprice?
3.Whatdoyouneedtoworryaboutwhendealingwithpennystock,i.e.,stockwithvery
low prices? Hint: can you trade at all possible prices? Why is this a bigger problem
for cheap stock? For more information review the celebrated Black‚ÄìScholes model
for option pricing ( Black and Scholes, 1973 ).
8.Suppose we want to use regression to estimate the number of apples sold in a grocery
store.
1.What are the problems with a Gaussian additive noise model? Hint: you are selling
apples, not oil.
2.ThePoisson distribution68captures distributions over counts. It is given by ùëù¬πùëòj
ùúÜ¬∫=ùúÜùëòùëí ùúÜ¬ùùëò!. HereùúÜis the rate function and ùëòis the number of events you see.
Prove thatùúÜis the expected value of counts ùëò.
3.Design a loss function associated with the Poisson distribution.
4.Design a loss function for estimating logùúÜinstead.
Discussions69.
3.2Object-Oriented Design forImplementation
In our introduction to linear regression, we walked through various components including
the data, the model, the loss function, and the optimization algorithm. Indeed, linear re-
gressionisoneofthesimplestmachinelearningmodels. Trainingit,however,usesmanyof
the same components that other models in this book require. Therefore, before diving into
the implementation details it is worth designing some of the APIs that we use throughout.
Treating components in deep learning as objects, we can start by defining classes for these
objects and their interactions. This object-oriented design for implementation will greatly
streamline the presentation and you might even want to use it in your projects.
Inspired by open-source libraries such as PyTorch Lightning70, at a high level we wish
to have three classes: (i) Modulecontains models, losses, and optimization methods; (ii)
DataModule provides data loaders for training and validation; (iii) both classes are com-
bined using the Trainer class, which allows us to train models on a variety of hardware
platforms. Most code in this book adapts ModuleandDataModule . We will touch upon
theTrainer class only when we discuss GPUs, CPUs, parallel training, and optimization
algorithms.
94 Linear Neural Networks for Regression
import time
import numpy asnp
import torch
from torch import nn
from d2l import torch asd2l
3.2.1Utilities
Weneedafewutilitiestosimplifyobject-orientedprogramminginJupyternotebooks. One
of the challenges is that class definitions tend to be fairly long blocks of code. Notebook
readability demands short code fragments, interspersed with explanations, a requirement
incompatible with the style of programming common for Python libraries. The first utility
functionallowsustoregisterfunctionsasmethodsinaclass aftertheclasshasbeencreated.
In fact, we can do so evenafter we have created instances of the class! It allows us to split
the implementation of a class into multiple code blocks.
def add_to_class (Class): #@save
"""Register functions as methods in created class."""
def wrapper (obj):
setattr (Class, obj .__name__ , obj)
return wrapper
Let‚Äôs have a quick look at how to use it. We plan to implement a class Awith a method do.
Instead of having code for both Aanddoin the same code block, we can first declare the
class Aand create an instance a.
class A:
def __init__ (self ):
self .b=1
a=A()
Next we define the method doas we normally would, but not in class A‚Äôs scope. Instead,
we decorate this method by add_to_class with class Aas its argument. In doing so, the
method is able to access the member variables of Ajust as we would expect had it been
includedaspartof A‚Äôsdefinition. Let‚Äôsseewhathappenswhenweinvokeitfortheinstance
a.
@add_to_class (A)
def do(self ):
print ('Class attribute "b"is',self .b)
a.do()
Class attribute "b"is1
The second one is a utility class that saves all arguments in a class‚Äôs __init__ method
95 Object-Oriented Design for Implementation
71as class attributes. This allows us to extend constructor call signatures implicitly without
additional code.
class HyperParameters :#@save
"""The base class of hyperparameters."""
def save_hyperparameters (self , ignore =[]):
raise NotImplemented
We defer its implementation into Section B.7 . To use it, we define our class that inherits
from HyperParameters andcalls save_hyperparameters inthe __init__ method.
# Call the fully implemented HyperParameters class saved in d2l
class B(d2l .HyperParameters):
def __init__ (self , a, b, c):
self .save_hyperparameters(ignore =['c'])
print ('self.a = ',self .a,'self.b = ',self .b)
print ('There is no self.c = ',not hasattr (self ,'c'))
b=B(a=1, b=2, c=3)
self .a=1self .b=2
There isnoself .c=True
The final utility allows us to plot experiment progress interactively while it is going on.
In deference to the much more powerful (and complex) TensorBoard71we name it Pro-
gressBoard . The implementation is deferred to Section B.7 . For now, let‚Äôs simply see it
in action.
Thedrawmethod plots a point (x, y) in the figure, with labelspecified in the legend.
The optional every_n smooths the line by only showing 1¬ùùëõpoints in the figure. Their
values are averaged from the ùëõneighbor points in the original figure.
class ProgressBoard (d2l .HyperParameters): #@save
"""The board that plots data points in animation."""
def __init__ (self , xlabel =None , ylabel =None , xlim =None ,
ylim =None , xscale ='linear ', yscale ='linear ',
ls=['-','--','-.',':'], colors =['C0','C1','C2','C3'],
fig=None , axes =None , figsize =(3.5,2.5), display =True ):
self .save_hyperparameters()
def draw (self , x, y, label, every_n =1):
raise NotImplemented
Inthefollowingexample,wedraw sinandcoswithadifferentsmoothness. Ifyourunthis
code block, you will see the lines grow in animation.
board =d2l.ProgressBoard( 'x')
for xinnp.arange( 0,10,0.1):
board .draw(x, np .sin(x), 'sin', every_n =2)
board .draw(x, np .cos(x), 'cos', every_n =10)
96 Linear Neural Networks for Regression
3.2.2Models
The Module class is the base class of all models we will implement. At the very least
we need three methods. The first, __init__ , stores the learnable parameters, the train-
ing_step methodacceptsadatabatchtoreturnthelossvalue,andfinally, configure_optimizers
returns the optimization method, or a list of them, that is used to update the learnable pa-
rameters. Optionally we can define validation_step to report the evaluation measures.
Sometimes we put the code for computing the output into a separate forward method to
make it more reusable.
class Module (nn.Module, d2l .HyperParameters): #@save
"""The base class of models."""
def __init__ (self , plot_train_per_epoch =2, plot_valid_per_epoch =1):
super ().__init__ ()
self .save_hyperparameters()
self .board =ProgressBoard()
def loss (self , y_hat, y):
raise NotImplementedError
def forward (self , X):
assert hasattr (self ,'net'),'Neural network is defined '
return self .net(X)
def plot (self , key, value, train):
"""Plot a point in animation."""
assert hasattr (self ,'trainer '),'Trainer is not inited '
self .board .xlabel ='epoch '
iftrain:
x=self .trainer .train_batch_idx /\
self .trainer .num_train_batches
n=self .trainer .num_train_batches /\
self .plot_train_per_epoch
else :
x=self .trainer .epoch +1
n=self .trainer .num_val_batches /\
self .plot_valid_per_epoch
self .board .draw(x, value .to(d2l .cpu()) .detach() .numpy(),
('train_ 'iftrain else 'val_ ')+key,
every_n =int(n))
(continues on next page)
97 Object-Oriented Design for Implementation
(continued from previous page)
def training_step (self , batch):
l=self .loss( self (*batch[: -1]), batch[ -1])
self .plot( 'loss ', l, train =True )
return l
def validation_step (self , batch):
l=self .loss( self (*batch[: -1]), batch[ -1])
self .plot( 'loss ', l, train =False )
def configure_optimizers (self ):
raise NotImplementedError
You may notice that Moduleis a subclass of nn.Module , the base class of neural networks
in PyTorch. It provides convenient features for handling neural networks. For example, if
we define a forward method, such as forward(self, X) , then for an instance awe can
invoke this method by a(X). This works since it calls the forward method in the built-in
__call__ method. You can find more details and examples about nn.Module inSection
6.1.
3.2.3Data
TheDataModule class is the base class for data. Quite frequently the __init__ method is
used to prepare the data. This includes downloading and preprocessing if needed. The
train_dataloader returns the data loader for the training dataset. A data loader is a
(Python) generator that yields a data batch each time it is used. This batch is then fed
into the training_step method of Module to compute the loss. There is an optional
val_dataloader to return the validation dataset loader. It behaves in the same manner,
except that it yields data batches for the validation_step method in Module.
class DataModule (d2l .HyperParameters): #@save
"""The base class of data."""
def __init__ (self , root ='../data ', num_workers =4):
self .save_hyperparameters()
def get_dataloader (self , train):
raise NotImplementedError
def train_dataloader (self ):
return self .get_dataloader(train =True )
def val_dataloader (self ):
return self .get_dataloader(train =False )
3.2.4Training
TheTrainer class trains the learnable parameters in the Moduleclass with data specified
inDataModule . The key method is fit, which accepts two arguments: model, an instance
ofModule, and data, an instance of DataModule . It then iterates over the entire dataset
98 Linear Neural Networks for Regression
72
73max_epochs times to train the model. As before, we will defer the implementation of this
method to later chapters.
class Trainer (d2l .HyperParameters): #@save
"""The base class for training models with data."""
def __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):
self .save_hyperparameters()
assert num_gpus ==0,'No GPU support yet '
def prepare_data (self , data):
self .train_dataloader =data .train_dataloader()
self .val_dataloader =data .val_dataloader()
self .num_train_batches =len(self .train_dataloader)
self .num_val_batches =(len(self .val_dataloader)
ifself .val_dataloader isnot None else 0)
def prepare_model (self , model):
model .trainer =self
model .board .xlim =[0,self .max_epochs]
self .model =model
def fit(self , model, data):
self .prepare_data(data)
self .prepare_model(model)
self .optim =model .configure_optimizers()
self .epoch =0
self .train_batch_idx =0
self .val_batch_idx =0
for self .epoch inrange (self .max_epochs):
self .fit_epoch()
def fit_epoch (self ):
raise NotImplementedError
3.2.5Summary
To highlight the object-oriented design for our future deep learning implementation, the
above classes simply show how their objects store data and interact with each other. We
will keep enriching implementations of these classes, such as via @add_to_class , in the
rest of the book. Moreover, these fully implemented classes are saved in the D2L library72
, alightweighttoolkit that makes structured modeling for deep learning easy. In particular,
it facilitates reusing many components between projects without changing much at all. For
instance, we can replace just the optimizer, just the model, just the dataset, etc.; this degree
of modularity pays dividends throughout the book in terms of conciseness and simplicity
(this is why we added it) and it can do the same for your own projects.
3.2.6Exercises
1.Locate full implementations of the above classes that are saved in the D2L library73
. We strongly recommend that you look at the implementation in detail once you have
gained some more familiarity with deep learning modeling.
99 Synthetic Regression Data
742.Removethe save_hyperparameters statementinthe Bclass. Canyoustillprint self.a
andself.b? Optional: if you have dived into the full implementation of the HyperPa-
rameters class, can you explain why?
Discussions74.
3.3SyntheticRegressionData
Machine learning is all about extracting information from data. So you might wonder,
what could we possibly learn from synthetic data? While we might not care intrinsically
about the patterns that we ourselves baked into an artificial data generating model, such
datasets are nevertheless useful for didactic purposes, helping us to evaluate the properties
of our learning algorithms and to confirm that our implementations work as expected. For
example, if we create data for which the correct parameters are known a priori, then we
can check that our model can in fact recover them.
%matplotlib inline
import random
import torch
from d2l import torch asd2l
3.3.1Generating the Dataset
For this example, we will work in low dimension for succinctness. The following code
snippet generates 1000 examples with 2-dimensional features drawn from a standard nor-
maldistribution. Theresultingdesignmatrix Xbelongsto R10002. Wegenerateeachlabel
by applying a groundtruth linear function, corrupting them via additive noise ùùê, drawn in-
dependently and identically for each example:
y=Xw¬∏ùëè¬∏ùùê. (3.3.1)
For convenience we assume that ùùêis drawn from a normal distribution with mean ùúá=0
and standard deviation ùúé=0.01. Note that for object-oriented design we add the code to
the__init__ methodofasubclassof d2l.DataModule (introducedin Section3.2.3 ). Itis
good practice to allow the setting of any additional hyperparameters. We accomplish this
with save_hyperparameters() . The batch_size will be determined later.
class SyntheticRegressionData (d2l .DataModule): #@save
"""Synthetic data for linear regression."""
def __init__ (self , w, b, noise =0.01 , num_train =1000 , num_val =1000 ,
batch_size =32):
super ().__init__ ()
self .save_hyperparameters()
n=num_train +num_val
(continues on next page)
100 Linear Neural Networks for Regression
(continued from previous page)
self .X=torch .randn(n, len(w))
noise =torch .randn(n, 1)*noise
self .y=torch .matmul( self .X, w .reshape(( -1,1))) +b+noise
Below, we set the true parameters to w=¬ª2, 3.4¬º>andùëè=4.2. Later, we can check our
estimated parameters against these groundtruth values.
data =SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)
Each row in features consists of a vector in R2and each row in labelsis a scalar. Let‚Äôs
have a look at the first entry.
print ('features: ', data .X[0],'\nlabel: ', data .y[0])
features: tensor([ 0.9026 ,1.0264 ])
label: tensor([ 2.5148 ])
3.3.2Readingthe Dataset
Training machine learning models often requires multiple passes over a dataset, grabbing
one minibatch of examples at a time. This data is then used to update the model. To
illustrate how this works, we implement the get_dataloader method, registering it in
theSyntheticRegressionData class via add_to_class (introduced in Section 3.2.1 ). It
takesa batchsize, a matrix offeatures, anda vectorof labels, and generatesminibatchesof
sizebatch_size . As such, each minibatch consists of a tuple of features and labels. Note
that we need to be mindful of whether we‚Äôre in training or validation mode: in the former,
wewillwanttoreadthedatainrandomorder,whereasforthelatter,beingabletoreaddata
in a pre-defined order may be important for debugging purposes.
@d2l .add_to_class(SyntheticRegressionData)
def get_dataloader (self , train):
iftrain:
indices =list (range (0,self .num_train))
# The examples are read in random order
random .shuffle(indices)
else :
indices =list (range (self .num_train, self .num_train +self .num_val))
for iinrange (0,len(indices), self .batch_size):
batch_indices =torch .tensor(indices[i: i +self .batch_size])
yield self .X[batch_indices], self .y[batch_indices]
To build some intuition, let‚Äôs inspect the first minibatch of data. Each minibatch of fea-
tures provides us with both its size and the dimensionality of input features. Likewise, our
minibatch of labels will have a matching shape given by batch_size .
101 Synthetic Regression Data
X, y =next (iter (data .train_dataloader()))
print ('X shape: ', X.shape, '\ny shape: ', y.shape)
X shape: torch .Size([ 32,2])
y shape: torch .Size([ 32,1])
While seemingly innocuous, the invocation of iter(data.train_dataloader()) illus-
trates the power of Python‚Äôs object-oriented design. Note that we added a method to the
SyntheticRegressionData classaftercreating the dataobject. Nonetheless, the object
benefits from the ex postfacto addition of functionality to the class.
Throughout the iteration we obtain distinct minibatches until the entire dataset has been
exhausted (try this). While the iteration implemented above is good for didactic purposes,
it is inefficient in ways that might get us into trouble with real problems. For example, it
requires that we load all the data in memory and that we perform lots of random memory
access. The built-in iterators implemented in a deep learning framework are considerably
more efficient and they can deal with sources such as data stored in files, data received via
a stream, and data generated or processed on the fly. Next let‚Äôs try to implement the same
method using built-in iterators.
3.3.3Concise Implementation of the Data Loader
Rather than writing our own iterator, we can call the existing API in a framework to load
data. As before, we need a dataset with features Xand labels y. Beyond that, we set
batch_size in the built-in data loader and let it take care of shuffling examples effi-
ciently.
@d2l .add_to_class(d2l .DataModule) #@save
def get_tensorloader (self , tensors, train, indices =slice (0,None )):
tensors =tuple (a[indices] for aintensors)
dataset =torch .utils .data .TensorDataset( *tensors)
return torch .utils .data .DataLoader(dataset, self .batch_size,
shuffle =train)
@d2l .add_to_class(SyntheticRegressionData) #@save
def get_dataloader (self , train):
i=slice (0,self .num_train) iftrain else slice (self .num_train, None )
return self .get_tensorloader(( self .X,self .y), train, i)
The new data loader behaves just like the previous one, except that it is more efficient and
has some added functionality.
X, y =next (iter (data .train_dataloader()))
print ('X shape: ', X.shape, '\ny shape: ', y.shape)
102 Linear Neural Networks for Regression
75
76X shape: torch .Size([ 32,2])
y shape: torch .Size([ 32,1])
For instance, the data loader provided by the framework API supports the built-in __len__
method, so we can query its length, i.e., the number of batches.
len(data .train_dataloader())
32
3.3.4Summary
Data loaders are a convenient way of abstracting out the process of loading and manipu-
lating data. This way the same machine learning algorithm is capable of processing many
differenttypesandsourcesofdatawithouttheneedformodification. Oneofthenicethings
about data loaders is that they can be composed. For instance, we might be loading images
and then have a postprocessing filter that crops them or modifies them in other ways. As
such, data loaders can be used to describe an entire data processing pipeline.
As for the model itself, the two-dimensional linear model is about the simplest we might
encounter. It lets us test out the accuracy of regression models without worrying about
having insufficient amounts of data or an underdetermined system of equations. We will
put this to good use in the next section.
3.3.5Exercises
1.What will happen if the number of examples cannot be divided by the batch size. How
would you change this behavior by specifying a different argument by using the frame-
work‚Äôs API?
2.Suppose that we want to generate a huge dataset, where both the size of the parameter
vector wand the number of examples num_examples are large.
1.What happens if we cannot hold all data in memory?
2.Howwouldyoushufflethedataifitisheldondisk? Yourtaskistodesignan eÔ¨Äicient
algorithm that does not require too many random reads or writes. Hint: pseudoran-
dom permutation generators75allow you to design a reshuffle without the need to
store the permutation table explicitly ( Naor and Reingold, 1999 ).
3.Implement a data generator that produces new data on the fly, every time the iterator is
called.
4.How would you design a random data generator that generates thesame data each time
it is called?
Discussions76.
103 Linear Regression Implementation from Scratch
3.4LinearRegressionImplementationfromScratch
Wearenowreadytoworkthroughafullyfunctioningimplementationoflinearregression.
In this section, we will implement the entire method from scratch, including (i) the model;
(ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the
trainingfunctionthatstitchesallofthesepiecestogether. Finally, wewillrunoursynthetic
datageneratorfrom Section3.3 andapplyourmodelontheresultingdataset. Whilemodern
deep learning frameworks can automate nearly all of this work, implementing things from
scratch is the only way to make sure that you really know what you are doing. Moreover,
whenitistimetocustomizemodels, definingourownlayersorlossfunctions, understand-
ing how things work under the hood will prove handy. In this section, we will rely only
on tensors and automatic differentiation. Later, we will introduce a more concise imple-
mentation, taking advantage of the bells and whistles of deep learning frameworks while
retaining the structure of what follows below.
%matplotlib inline
import torch
from d2l import torch asd2l
3.4.1Definingthe Model
Before we can begin optimizing our model‚Äôs parameters by minibatch SGD, we need to
have some parameters in the first place. In the following we initialize weights by drawing
random numbers from a normal distribution with mean 0 and a standard deviation of 0.01.
The magic number 0.01 often works well in practice, but you can specify a different value
through the argument sigma. Moreover we set the bias to 0. Note that for object-oriented
design we add the code to the __init__ method of a subclass of d2l.Module (introduced
inSection 3.2.2 ).
class LinearRegressionScratch (d2l .Module): #@save
"""The linear regression model implemented from scratch."""
def __init__ (self , num_inputs, lr, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .w=torch .normal( 0, sigma, (num_inputs, 1), requires_grad =True )
self .b=torch .zeros( 1, requires_grad =True )
Next we must define our model, relating its input and parameters to its output. Using the
same notation as (3.1.4 )for our linear model we simply take the matrix‚Äìvector product of
the input features Xand the model weights w, and add the offset ùëèto each example. The
product Xwis a vector and ùëèis a scalar. Because of the broadcasting mechanism (see
Section2.1.4 ), whenweaddavectorandascalar, thescalarisaddedtoeachcomponentof
the vector. The resulting forward method is registered in the LinearRegressionScratch
class via add_to_class (introduced in Section 3.2.1 ).
104 Linear Neural Networks for Regression
@d2l .add_to_class(LinearRegressionScratch) #@save
def forward (self , X):
return torch .matmul(X, self .w)+self .b
3.4.2Defining the Loss Function
Since updating our model requires taking the gradient of our loss function, we ought to
define the loss function first. Here we use the squared loss function in (3.1.5 ). In the
implementation, we need to transform the true value yinto the predicted value‚Äôs shape
y_hat. Theresultreturnedbythefollowingmethodwillalsohavethesameshapeas y_hat.
We also return the averaged loss value among all examples in the minibatch.
@d2l .add_to_class(LinearRegressionScratch) #@save
def loss (self , y_hat, y):
l=(y_hat -y)**2/2
return l.mean()
3.4.3Definingthe OptimizationAlgorithm
As discussed in Section 3.1 , linear regression has a closed-form solution. However, our
goal here is to illustrate how to train more general neural networks, and that requires that
we teach you how to use minibatch SGD. Hence we will take this opportunity to introduce
your first working example of SGD. At each step, using a minibatch randomly drawn from
our dataset, we estimate the gradient of the loss with respect to the parameters. Next, we
update the parameters in the direction that may reduce the loss.
The following code applies the update, given a set of parameters, a learning rate lr. Since
ourlossiscomputedasanaverageovertheminibatch,wedonotneedtoadjustthelearning
rate against the batch size. In later chapters we will investigate how learning rates should
be adjusted for very large minibatches as they arise in distributed large-scale learning. For
now, we can ignore this dependency.
Wedefineour SGDclass,asubclassof d2l.HyperParameters (introducedin Section3.2.1 ),
to have a similar API as the built-in SGD optimizer. We update the parameters in the step
method. The zero_grad method sets all gradients to 0, which must be run before a back-
propagation step.
class SGD(d2l .HyperParameters): #@save
"""Minibatch stochastic gradient descent."""
def __init__ (self , params, lr):
self .save_hyperparameters()
def step (self ):
for param inself .params:
param -=self .lr*param .grad
def zero_grad (self ):
(continues on next page)
105 Linear Regression Implementation from Scratch
(continued from previous page)
for param inself .params:
ifparam .grad isnot None :
param .grad .zero_()
We next define the configure_optimizers method, which returns an instance of the SGD
class.
@d2l .add_to_class(LinearRegressionScratch) #@save
def configure_optimizers (self ):
return SGD([ self .w,self .b], self .lr)
3.4.4Training
Nowthatwehaveallofthepartsinplace(parameters,lossfunction,model,andoptimizer),
we are ready to implement the main training loop. It is crucial that you understand this
codefullysinceyouwillemploysimilartrainingloopsforeveryotherdeeplearningmodel
covered in this book. In each epoch, we iterate through the entire training dataset, passing
once through every example (assuming that the number of examples is divisible by the
batch size). In each iteration , we grab a minibatch of training examples, and compute its
loss through the model‚Äôs training_step method. Then we compute the gradients with
respect to each parameter. Finally, we will call the optimization algorithm to update the
model parameters. In summary, we will execute the following loop:
Initialize parameters ¬πw,ùëè¬∫
Repeat until done
‚ÄìCompute gradient g ùúï¬πw,ùëè¬∫1
jBj√ç
ùëñ2Bùëô¬πx¬πùëñ¬∫,ùë¶¬πùëñ¬∫,w,ùëè¬∫
‚ÄìUpdate parameters ¬πw,ùëè¬∫ ¬πw,ùëè¬∫ ùúÇg
Recallthatthesyntheticregressiondatasetthatwegeneratedin Section3.3 doesnotprovide
a validation dataset. In most cases, however, we will want a validation dataset to measure
our model quality. Here we pass the validation dataloader once in each epoch to mea-
sure the model performance. Following our object-oriented design, the prepare_batch
andfit_epoch methods are registered in the d2l.Trainer class (introduced in Section
3.2.4).
@d2l .add_to_class(d2l .Trainer) #@save
def prepare_batch (self , batch):
return batch
@d2l .add_to_class(d2l .Trainer) #@save
def fit_epoch (self ):
self .model .train()
for batch inself .train_dataloader:
loss =self .model .training_step( self .prepare_batch(batch))
(continues on next page)
106 Linear Neural Networks for Regression
(continued from previous page)
self .optim .zero_grad()
with torch .no_grad():
loss .backward()
ifself .gradient_clip_val >0:# To be discussed later
self .clip_gradients( self .gradient_clip_val, self .model)
self .optim .step()
self .train_batch_idx +=1
ifself .val_dataloader isNone :
return
self .model .eval()
for batch inself .val_dataloader:
with torch .no_grad():
self .model .validation_step( self .prepare_batch(batch))
self .val_batch_idx +=1
We are almost ready to train the model, but first we need some training data. Here we use
theSyntheticRegressionData class and pass in some ground truth parameters. Then
we train our model with the learning rate lr=0.03 and set max_epochs=3 . Note that in
general, both the number of epochs and the learning rate are hyperparameters. In general,
setting hyperparameters is tricky and we will usually want to use a three-way split, one
set for training, a second for hyperparameter selection, and the third reserved for the final
evaluation. We elide these details for now but will revise them later.
model =LinearRegressionScratch( 2, lr =0.03 )
data =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)
trainer =d2l.Trainer(max_epochs =3)
trainer .fit(model, data)
Because we synthesized the dataset ourselves, we know precisely what the true parameters
are. Thus, we can evaluate our success in training by comparing the true parameters with
those that we learned through our training loop. Indeed they turn out to be very close to
each other.
with torch .no_grad():
print (f'error in estimating w: {data .w-model .w.reshape(data .w.shape) }')
print (f'error in estimating b: {data .b-model .b}')
107 Linear Regression Implementation from Scratch
77
78error inestimating w: tensor([ 0.1408 ,-0.1493 ])
error inestimating b: tensor([ 0.2130 ])
We should not take the ability to exactly recover the ground truth parameters for granted.
In general, for deep models unique solutions for the parameters do not exist, and even
for linear models, exactly recovering the parameters is only possible when no feature is
linearlydependentontheothers. However,inmachinelearning,weareoftenlessconcerned
with recovering true underlying parameters, but rather with parameters that lead to highly
accurate prediction ( Vapnik, 1992 ). Fortunately, even on difficult optimization problems,
stochastic gradient descent can often find remarkably good solutions, owing partly to the
fact that, for deep networks, there exist many configurations of the parameters that lead to
highly accurate prediction.
3.4.5Summary
In this section, we took a significant step towards designing deep learning systems by im-
plementing a fully functional neural network model and training loop. In this process, we
built a data loader, a model, a loss function, an optimization procedure, and a visualization
and monitoring tool. We did this by composing a Python object that contains all relevant
componentsfortrainingamodel. Whilethisisnotyetaprofessional-gradeimplementation
it is perfectly functional and code like this could already help you to solve small problems
quickly. In the coming sections, we will see how to do this both more concisely (avoiding
boilerplate code) and moreeÔ¨Äiciently (using our GPUs to their full potential).
3.4.6Exercises
1.What would happen if we were to initialize the weights to zero. Would the algorithm
still work? What if we initialized the parameters with variance 1000rather than 0.01?
2.Assume that you are Georg Simon Ohm77trying to come up with a model for resis-
tancethatrelatesvoltageandcurrent. Canyouuseautomaticdifferentiationtolearnthe
parameters of your model?
3.Can you use Planck‚Äôs Law78to determine the temperature of an object using spectral
energy density? For reference, the spectral density ùêµof radiation emanating from a
black body is ùêµ¬πùúÜ,ùëá¬∫=2‚Ñéùëê2
ùúÜ5
exp‚Ñéùëê
ùúÜùëòùëá 1 1
. HereùúÜis the wavelength, ùëáis the
temperature, ùëêis the speed of light, ‚Ñéis Planck‚Äôs constant, and ùëòis the Boltzmann
constant. You measure the energy for different wavelengths ùúÜand you now need to fit
the spectral density curve to Planck‚Äôs law.
4.Whataretheproblemsyoumightencounterifyouwantedtocomputethesecondderiva-
tives of the loss? How would you fix them?
5.Why is the reshape method needed in the lossfunction?
6.Experimentusingdifferentlearningratestofindouthowquicklythelossfunctionvalue
drops. Can you reduce the error by increasing the number of epochs of training?
108 Linear Neural Networks for Regression
797.Ifthenumberofexamplescannotbedividedbythebatchsize,whathappensto data_iter
at the end of an epoch?
8.Try implementing a different loss function, such as the absolute value loss (y_hat -
d2l.reshape(y, y_hat.shape)).abs().sum() .
1.Check what happens for regular data.
2.Check whether there is a difference in behavior if you actively perturb some entries,
such asùë¶5=10000, ofy.
3.Can you think of a cheap solution for combining the best aspects of squared loss and
absolute value loss? Hint: how can you avoid really large gradient values?
9.Why do we need to reshuffle the dataset? Can you design a case where a maliciously
constructed dataset would break the optimization algorithm otherwise?
Discussions79.
3.5ConciseImplementation of Linear Regression
Deep learning has witnessed a sort of Cambrian explosion over the past decade. The sheer
number of techniques, applications and algorithms by far surpasses the progress of pre-
vious decades. This is due to a fortuitous combination of multiple factors, one of which
is the powerful free tools offered by a number of open-source deep learning frameworks.
Theano ( Bergstraet al., 2010), DistBelief ( Deanet al., 2012), and Caffe ( Jiaet al., 2014)
arguably represent the first generation of such models that found widespread adoption.
In contrast to earlier (seminal) works like SN2 (Simulateur Neuristique) ( Bottou and Le
Cun, 1988 ), which provided a Lisp-like programming experience, modern frameworks of-
fer automatic differentiation and the convenience of Python. These frameworks allow us
to automate and modularize the repetitive work of implementing gradient-based learning
algorithms.
InSection 3.4 , we relied only on (i) tensors for data storage and linear algebra; and (ii)
automatic differentiation for calculating gradients. In practice, because data iterators, loss
functions, optimizers, and neural network layers are so common, modern libraries imple-
ment these components for us as well. In this section, we will show you how to implement
the linear regression model from Section 3.4 concisely by using high-level APIs of deep
learning frameworks.
import numpy asnp
import torch
from torch import nn
from d2l import torch asd2l
109 Concise Implementation of Linear Regression
3.5.1Definingthe Model
Whenweimplementedlinearregressionfromscratchin Section3.4 , wedefinedourmodel
parameters explicitly and coded up the calculations to produce output using basic linear
algebra operations. You shouldknow how to do this. But once your models get more
complex, and once you have to do this nearly every day, you will be glad of the assistance.
The situation is similar to coding up your own blog from scratch. Doing it once or twice
is rewarding and instructive, but you would be a lousy web developer if you spent a month
reinventing the wheel.
For standard operations, we can use a framework‚Äôs predefined layers, which allow us to
focus on the layersused to construct the model rather than worrying about their implemen-
tation. Recall the architecture of a single-layer network as described in Fig. 3.1.2 . The
layer is called fully connected , since each of its inputs is connected to each of its outputs
by means of a matrix‚Äìvector multiplication.
In PyTorch, the fully connected layer is defined in LinearandLazyLinear classes (avail-
able since version 1.8.0). The latter allows users to specify merelythe output dimension,
while the former additionally asks for how many inputs go into this layer. Specifying input
shapes is inconvenient and may require nontrivial calculations (such as in convolutional
layers). Thus, for simplicity, we will use such ‚Äúlazy‚Äù layers whenever we can.
class LinearRegression (d2l .Module): #@save
"""The linear regression model implemented with high-level APIs."""
def __init__ (self , lr):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.LazyLinear( 1)
self .net.weight .data .normal_( 0,0.01 )
self .net.bias .data .fill_( 0)
In the forward method we just invoke the built-in __call__ method of the predefined
layers to compute the outputs.
@d2l .add_to_class(LinearRegression) #@save
def forward (self , X):
return self .net(X)
3.5.2Defining the Loss Function
TheMSELoss class computes the mean squared error (without the 1¬ù2factor in (3.1.5 )).
By default, MSELoss returns the average loss over examples. It is faster (and easier to use)
than implementing our own.
@d2l .add_to_class(LinearRegression) #@save
def loss (self , y_hat, y):
fn=nn.MSELoss()
return fn(y_hat, y)
110 Linear Neural Networks for Regression
3.5.3Definingthe OptimizationAlgorithm
Minibatch SGD is a standard tool for optimizing neural networks and thus PyTorch sup-
ports it alongside a number of variations on this algorithm in the optimmodule. When we
instantiate an SGDinstance, we specify the parameters to optimize over, obtainable from
our model via self.parameters() , and the learning rate ( self.lr ) required by our opti-
mization algorithm.
@d2l .add_to_class(LinearRegression) #@save
def configure_optimizers (self ):
return torch .optim .SGD( self .parameters(), self .lr)
3.5.4Training
Youmighthavenoticedthatexpressingourmodelthroughhigh-levelAPIsofadeeplearn-
ing framework requires fewer lines of code. We did not have to allocate parameters indi-
vidually, define our loss function, or implement minibatch SGD. Once we start working
with much more complex models, the advantages of the high-level API will grow consid-
erably.
Now that we have all the basic pieces in place, the training loop itself is the same as the
one we implemented from scratch. So we just call the fitmethod (introduced in Section
3.2.4),whichreliesontheimplementationofthe fit_epoch methodin Section3.4 ,totrain
our model.
model =LinearRegression(lr =0.03 )
data =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)
trainer =d2l.Trainer(max_epochs =3)
trainer .fit(model, data)
Below, we compare the model parameters learned by training on finite data and the actual
parametersthatgeneratedourdataset. Toaccessparameters,weaccesstheweightsandbias
of the layer that we need. As in our implementation from scratch, note that our estimated
parameters are close to their true counterparts.
@d2l .add_to_class(LinearRegression) #@save
(continues on next page)
111 Concise Implementation of Linear Regression
(continued from previous page)
def get_w_b (self ):
return (self .net.weight .data, self .net.bias .data)
w, b =model .get_w_b()
print (f'error in estimating w: {data .w-w.reshape(data .w.shape) }')
print (f'error in estimating b: {data .b-b}')
error inestimating w: tensor([ 0.0094 ,-0.0030 ])
error inestimating b: tensor([ 0.0137 ])
3.5.5Summary
This section contains the first implementation of a deep network (in this book) to tap into
the conveniences afforded by modern deep learning frameworks, such as MXNet ( Chen
et al., 2015), JAX ( Frostiget al., 2018), PyTorch ( Paszkeet al., 2019), and Tensorflow
(Abadiet al., 2016). We used framework defaults for loading data, defining a layer, a loss
function,anoptimizerandatrainingloop. Whenevertheframeworkprovidesallnecessary
features, it is generally a good idea to use them, since the library implementations of these
componentstendtobeheavilyoptimizedforperformanceandproperlytestedforreliability.
At the same time, try not to forget that these modules canbe implemented directly. This is
especiallyimportantforaspiringresearcherswhowishtoliveontheleadingedgeofmodel
development, where you will be inventing new components that cannot possibly exist in
any current library.
In PyTorch, the datamodule provides tools for data processing, the nnmodule defines a
largenumberofneuralnetworklayersandcommonlossfunctions. Wecaninitializethepa-
rametersbyreplacingtheirvalueswithmethodsendingwith _. Notethatweneedtospecify
the input dimensions of the network. While this is trivial for now, it can have significant
knock-on effects when we want to design complex networks with many layers. Careful
considerations of how to parametrize these networks is needed to allow portability.
3.5.6Exercises
1.How would you need to change the learning rate if you replace the aggregate loss over
the minibatch with an average over the loss on the minibatch?
2.Review the framework documentation to see which loss functions are provided. In par-
ticular, replace the squared loss with Huber‚Äôs robust loss function. That is, use the loss
function
ùëô¬πùë¶,ùë¶0¬∫=(
jùë¶ ùë¶0j ùúé
2ifjùë¶ ùë¶0j>ùúé
1
2ùúé¬πùë¶ ùë¶0¬∫2otherwise(3.5.1)
3.How do you access the gradient of the weights of the model?
112 Linear Neural Networks for Regression
804.What is the effect on the solution if you change the learning rate and the number of
epochs? Does it keep on improving?
5.How does the solution change as you vary the amount of data generated?
1.Plot the estimation error for ÀÜw wand ÀÜùëè ùëèas a function of the amount of data.
Hint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20,
50, ‚Ä¶, 10,000 rather than 1000, 2000, ‚Ä¶, 10,000.
2.Why is the suggestion in the hint appropriate?
Discussions80.
3.6Generalization
Consider two college students diligently preparing for their final exam. Commonly, this
preparation will consist of practicing and testing their abilities by taking exams adminis-
teredinpreviousyears. Nonetheless,doingwellonpastexamsisnoguaranteethattheywill
excelwhenitmatters. Forinstance,imagineastudent,ExtraordinaryEllie,whoseprepara-
tion consisted entirely of memorizing the answers to previous years‚Äô exam questions. Even
ifElliewereendowedwithanextraordinarymemory,andthuscouldperfectlyrecallthean-
swer to any previouslyseen question, she might nevertheless freeze when faced with a new
(previously unseen ) question. By comparison, imagine another student, Inductive Irene,
with comparably poor memorization skills, but a knack for picking up patterns. Note that
if the exam truly consisted of recycled questions from a previous year, Ellie would handily
outperform Irene. Even if Irene‚Äôs inferred patterns yielded 90% accurate predictions, they
couldnevercompetewithEllie‚Äôs100%recall. However,eveniftheexamconsistedentirely
of fresh questions, Irene might maintain her 90% average.
Asmachinelearningscientists,ourgoalistodiscover patterns. Buthowcanwebesurethat
wehavetrulydiscovereda generalpatternandnotsimplymemorizedourdata? Mostofthe
time, our predictions are only useful if our model discovers such a pattern. We do not want
to predict yesterday‚Äôs stock prices, but tomorrow‚Äôs. We do not need to recognize already
diagnoseddiseasesforpreviouslyseenpatients,butratherpreviouslyundiagnosedailments
in previously unseen patients. This problem‚Äîhow to discover patterns that generalize ‚Äîis
the fundamental problem of machine learning, and arguably of all of statistics. We might
cast this problem as just one slice of a far grander question that engulfs all of science:
when are we ever justified in making the leap from particular observations to more general
statements?
In real life, we must fit our models using a finite collection of data. The typical scales
of that data vary wildly across domains. For many important medical problems, we can
only access a few thousand data points. When studying rare diseases, we might be lucky to
accesshundreds. Bycontrast,thelargestpublicdatasetsconsistingoflabeledphotographs,
e.g., ImageNet( Dengetal., 2009), containmillionsofimages. Andsomeunlabeledimage
113 Generalization
collections such as the Flickr YFC100M dataset can be even larger, containing over 100
million images ( Thomeeet al., 2016). However, even at this extreme scale, the number of
available data points remains infinitesimally small compared to the space of all possible
images at a megapixel resolution. Whenever we work with finite samples, we must keep in
mind the risk that we might fit our training data, only to discover that we failed to discover
a generalizable pattern.
The phenomenon of fitting closer to our training data than to the underlying distribution is
calledoverfitting , and techniques for combatting overfitting are often called regularization
methods. While it is no substitute for a proper introduction to statistical learning theory
(see Boucheron etal.(2005), Vapnik ( 1998)), we will give you just enough intuition to get
going. Wewillrevisitgeneralizationinmanychaptersthroughoutthebook,exploringboth
what is known about the principles underlying generalization in various models, and also
heuristictechniquesthathavebeenfound(empirically)toyieldimprovedgeneralizationon
tasks of practical interest.
3.6.1Training Errorand Generalization Error
In the standard supervised learning setting, we assume that the training data and the test
data are drawn independently fromidentical distributions. This is commonly called the
IID assumption . While this assumption is strong, it is worth noting that, absent any such
assumption, we would be dead in the water. Why should we believe that training data
sampled from distribution ùëÉ¬πùëã,ùëå¬∫should tell us how to make predictions on test data
generatedbya differentdistribution ùëÑ¬πùëã,ùëå¬∫? Makingsuchleapsturnsouttorequirestrong
assumptions about how ùëÉandùëÑare related. Later on we will discuss some assumptions
that allow for shifts in distribution but first we need to understand the IID case, where
ùëÉ¬π¬∫=ùëÑ¬π¬∫.
Tobeginwith, weneedtodifferentiatebetweenthe trainingerror ùëÖemp, whichisa statistic
calculated on the training dataset, and the generalization error ùëÖ, which is an expectation
taken with respect to the underlying distribution. You can think of the generalization error
as what you would see if you applied your model to an infinite stream of additional data
examples drawn from the same underlying data distribution. Formally the training error is
expressed as a sum(with the same notation as Section 3.1 ):
ùëÖemp¬ªX,y, ùëì¬º=1
ùëõùëõ√ï
ùëñ=1ùëô¬πx¬πùëñ¬∫,ùë¶¬πùëñ¬∫, ùëì¬πx¬πùëñ¬∫¬∫¬∫, (3.6.1)
while the generalization error is expressed as an integral:
ùëÖ¬ªùëù, ùëì¬º=ùê∏¬πx,ùë¶¬∫ùëÉ¬ªùëô¬πx,ùë¶, ùëì¬πx¬∫¬∫¬º=¬π ¬π
ùëô¬πx,ùë¶, ùëì¬πx¬∫¬∫ùëù¬πx,ùë¶¬∫ùëëxùëëùë¶. (3.6.2)
Problematically, we can never calculate the generalization error ùëÖexactly. Nobody ever
tells us the precise form of the density function ùëù¬πx,ùë¶¬∫. Moreover, we cannot sample
an infinite stream of data points. Thus, in practice, we must estimate the generalization
error by applying our model to an independent test set constituted of a random selection
of examples X0and labels y0that were withheld from our training set. This consists of
114 Linear Neural Networks for Regression
applying the same formula that was used for calculating the empirical training error but to
a test set X0,y0.
Crucially,whenweevaluateourclassifieronthetestset,weareworkingwitha fixedclassi-
fier(itdoesnotdependonthesampleofthetestset), andthusestimatingitserrorissimply
theproblemofmeanestimation. Howeverthesamecannotbesaidforthetrainingset. Note
that the model we wind up with depends explicitly on the selection of the training set and
thus the training error will in general be a biased estimate of the true error on the underly-
ing population. The central question of generalization is then when should we expect our
training error to be close to the population error (and thus the generalization error).
ModelComplexity
In classical theory, when we have simple models and abundant data, the training and gen-
eralization errors tend to be close. However, when we work with more complex models
and/or fewer examples, we expect the training error to go down but the generalization gap
to grow. This should not be surprising. Imagine a model class so expressive that for any
dataset ofùëõexamples, we can find a set of parameters that can perfectly fit arbitrary labels,
even if randomly assigned. In this case, even if we fit our training data perfectly, how can
we conclude anything about the generalization error? For all we know, our generalization
error might be no better than random guessing.
In general, absent any restriction on our model class, we cannot conclude, based on fitting
the training data alone, that our model has discovered any generalizable pattern ( Vapniket
al., 1994). On the other hand, if our model class was not capable of fitting arbitrary labels,
then it must have discovered a pattern. Learning-theoretic ideas about model complexity
derived some inspiration from the ideas of Karl Popper, an influential philosopher of sci-
ence, who formalized the criterion of falsifiability. According to Popper, a theory that can
explainanyandallobservationsisnotascientifictheoryatall! Afterall,whathasittoldus
abouttheworldifithasnotruledoutanypossibility? Inshort,whatwewantisahypothesis
thatcould not explain any observations we might conceivably make and yet nevertheless
happens to be compatible with those observations that we in factmake.
Now what precisely constitutes an appropriate notion of model complexity is a complex
matter. Often, models with more parameters are able to fit a greater number of arbitrarily
assignedlabels. However,thisisnotnecessarilytrue. Forinstance,kernelmethodsoperate
in spaces with infinite numbers of parameters, yet their complexity is controlled by other
means (Sch√∂lkopf and Smola, 2002 ). One notion of complexity that often proves useful
is the range of values that the parameters can take. Here, a model whose parameters are
permitted to take arbitrary values would be more complex. We will revisit this idea in the
nextsection,whenweintroduce weightdecay ,yourfirstpracticalregularizationtechnique.
Notably,itcanbedifficulttocomparecomplexityamongmembersofsubstantiallydifferent
model classes (say, decision trees vs. neural networks).
At this point, we must stress another important point that we will revisit when introducing
deep neural networks. When a model is capable of fitting arbitrary labels, low training
error does not necessarily imply low generalization error. However,itdoesnotnecessarily
115 Generalization
implyhighgeneralizationerroreither! All we can say with confidence is that low training
error alone is not enough to certify low generalization error. Deep neural networks turn
out to be just such models: while they generalize well in practice, they are too powerful
to allow us to conclude much on the basis of training error alone. In these cases we must
rely more heavily on our holdout data to certify generalization after the fact. Error on the
holdout data, i.e., validation set, is called the validationerror .
3.6.2Underfittingor Overfitting?
When we compare the training and validation errors, we want to be mindful of two com-
monsituations. First,wewanttowatchoutforcaseswhenourtrainingerrorandvalidation
error are both substantial but there is a little gap between them. If the model is unable to
reduce the training error, that could mean that our model is too simple (i.e., insufficiently
expressive) to capture the pattern that we are trying to model. Moreover, since the gener-
alization gap (ùëÖemp ùëÖ) between our training and generalization errors is small, we have
reason to believe that we could get away with a more complex model. This phenomenon is
known as underfitting .
On the other hand, as we discussed above, we want to watch out for the cases when our
training error is significantly lower than our validation error, indicating severe overfitting .
Note that overfitting is not always a bad thing. In deep learning especially, the best pre-
dictive models often perform far better on training data than on holdout data. Ultimately,
we usually care about driving the generalization error lower, and only care about the gap
insofar as it becomes an obstacle to that end. Note that if the training error is zero, then the
generalization gap is precisely equal to the generalization error and we can make progress
only by reducing the gap.
PolynomialCurveFitting
To illustrate some classical intuition about overfitting and model complexity, consider the
following: given training data consisting of a single feature ùë•and a corresponding real-
valued label ùë¶, we try to find the polynomial of degree ùëë
ÀÜùë¶=ùëë√ï
ùëñ=0ùë•ùëñùë§ùëñ (3.6.3)
for estimating the label ùë¶. This is just a linear regression problem where our features are
given by the powers of ùë•, the model‚Äôs weights are given by ùë§ùëñ, and the bias is given by ùë§0
sinceùë•0=1for allùë•. Since this is just a linear regression problem, we can use the squared
error as our loss function.
A higher-order polynomial function is more complex than a lower-order polynomial func-
tion,sincethehigher-orderpolynomialhasmoreparametersandthemodelfunction‚Äôsselec-
tion range is wider. Fixing the training dataset, higher-order polynomial functions should
always achieve lower (at worst, equal) training error relative to lower-degree polynomials.
In fact, whenever each data example has a distinct value of ùë•, a polynomial function with
degree equal to the number of data examples can fit the training set perfectly. We compare
116 Linear Neural Networks for Regression
the relationship between polynomial degree (model complexity) and both underfitting and
overfitting in Fig. 3.6.1 .
tFig. 3.6.1 InÔ¨Çuence of model complexity on underÔ¨Åtting and overÔ¨Åtting.
DatasetSize
As the above bound already indicates, another big consideration to bear in mind is dataset
size. Fixing our model, the fewer samples we have in the training dataset, the more likely
(and more severely) we are to encounter overfitting. As we increase the amount of training
data, the generalization error typically decreases. Moreover, in general, more data never
hurts. For a fixed task and data distribution, model complexity should not increase more
rapidly than the amount of data. Given more data, we might attempt to fit a more complex
model. Absent sufficient data, simpler models may be more difficult to beat. For many
tasks, deep learning only outperforms linear models when many thousands of training ex-
amplesareavailable. In part, thecurrent successof deeplearning owesconsiderablytothe
abundance of massive datasets arising from Internet companies, cheap storage, connected
devices, and the broad digitization of the economy.
3.6.3Model Selection
Typically,weselectourfinalmodelonlyafterevaluatingmultiplemodelsthatdifferinvari-
ous ways (different architectures, training objectives, selected features, data preprocessing,
learning rates, etc.). Choosing among manymodels is aptlycalled modelselection .
In principle, we should not touch our test set until after we have chosen all our hyperpa-
rameters. Werewetousethetestdatainthemodelselectionprocess, thereisariskthatwe
might overfit the test data. Then we would be in serious trouble. If we overfit our training
data, there is always the evaluation on test data to keep us honest. But if we overfit the test
data, how would we ever know? See Ong etal.(2005) for an example of how this can lead
to absurd results even for models where the complexity can be tightly controlled.
Thus, we should never rely on the test data for model selection. And yet we cannot rely
solely on the training data for model selection either because we cannot estimate the gen-
eralization error on the very data that we use to train the model.
In practical applications, the picture gets muddier. While ideally we would only touch the
117 Generalization
81
82test data once, to assess the very best model or to compare a small number of models with
each other, real-world test data is seldom discarded after just one use. We can seldom
afford a new test set for each round of experiments. In fact, recycling benchmark data for
decades can have a significant impact on the development of algorithms, e.g., for image
classification81andoptical character recognition82.
The common practice for addressing the problem of training on the test set is to split our
data three ways, incorporating a validation set in addition to the training and test datasets.
The result is a murky business where the boundaries between validation and test data are
worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book
we are really working with what should rightly be called training data and validation data,
with no true test sets. Therefore, the accuracy reported in each experiment of the book is
really the validation accuracy and not a true test set accuracy.
Cross-Validation
Whentrainingdataisscarce,wemightnotevenbeabletoaffordtoholdoutenoughdatato
constituteapropervalidationset. Onepopularsolutiontothisproblemistoemploy ùêæ-fold
cross-validation . Here, the original training data is split into ùêænon-overlapping subsets.
Then model training and validation are executed ùêætimes, each time training on ùêæ 1
subsets and validating on a different subset (the one not used for training in that round).
Finally, the training and validation errors are estimated by averaging over the results from
theùêæexperiments.
3.6.4Summary
This section explored some of the underpinnings of generalization in machine learning.
Someoftheseideasbecomecomplicatedandcounterintuitivewhenwegettodeepermod-
els; here, modelsarecapableofoverfittingdatabadly, andtherelevantnotionsofcomplex-
itycanbebothimplicitandcounterintuitive(e.g.,largerarchitectureswithmoreparameters
generalizing better). We leave you with a few rules of thumb:
1.Use validation sets (or ùêæ-foldcross-validation ) for model selection;
2.More complex models often require more data;
3.Relevant notions of complexity include both the number of parameters and the range of
values that they are allowed to take;
4.Keeping all else equal, more data almost always leads to better generalization;
5.This entire talk of generalization is all predicated on the IID assumption. If we relax
this assumption, allowing for distributions to shift between the train and testing peri-
ods, then we cannot say anything about generalization absent a further (perhaps milder)
assumption.
3.6.5Exercises
1.When can you solve the problem of polynomial regression exactly?
118 Linear Neural Networks for Regression
832.Giveatleastfiveexampleswheredependentrandomvariablesmaketreatingtheproblem
as IID data inadvisable.
3.Can you ever expect to see zero training error? Under which circumstances would you
see zero generalization error?
4.Why isùêæ-fold cross-validation very expensive to compute?
5.Why is theùêæ-fold cross-validation error estimate biased?
6.The VC dimension is defined as the maximum number of points that can be classified
with arbitrary labels f1gby a function of a class of functions. Why might this not be
a good idea for measuring how complex the class of functions is? Hint: consider the
magnitude of the functions.
7.Your manager gives you a difficult dataset on which your current algorithm does not
perform so well. How would you justify to him that you need more data? Hint: you
cannot increase the data but you can decrease it.
Discussions83.
3.7WeightDecay
Now that we have characterized the problem of overfitting, we can introduce our first reg-
ularization technique. Recall that we can always mitigate overfitting by collecting more
training data. However, that can be costly, time consuming, or entirely out of our control,
making it impossible in the short run. For now, we can assume that we already have as
muchhigh-qualitydataasourresourcespermitandfocusthetoolsatourdisposalwhenthe
dataset is taken as a given.
Recallthatinourpolynomialregressionexample( Section3.6.2 )wecouldlimitourmodel‚Äôs
capacity by tweaking the degree of the fitted polynomial. Indeed, limiting the number of
features is a popular technique for mitigating overfitting. However, simply tossing aside
features can be too blunt an instrument. Sticking with the polynomial regression example,
consider what might happen with high-dimensional input. The natural extensions of poly-
nomials to multivariate data are called monomials , which are simply products of powers
of variables. The degree of a monomial is the sum of the powers. For example, ùë•2
1ùë•2, and
ùë•3ùë•2
5are both monomials of degree 3.
Note that the number of terms with degree ùëëblows up rapidly as ùëëgrows larger. Given ùëò
variables, the number of monomials of degree ùëëis ùëò 1¬∏ùëë
ùëò 1. Even small changes in degree,
say from 2to3, dramatically increase the complexity of our model. Thus we often need a
more fine-grained tool for adjusting function complexity.
119 Weight Decay
%matplotlib inline
import torch
from torch import nn
from d2l import torch asd2l
3.7.1Normsand WeightDecay
Rather than directly manipulating the number of parameters, weight decay , operates by
restrictingthevaluesthattheparameterscantake. Morecommonlycalled ‚Ñì2regularization
outside of deep learning circles when optimized by minibatch stochastic gradient descent,
weightdecaymightbethemostwidelyusedtechniqueforregularizingparametricmachine
learningmodels. Thetechniqueismotivatedbythebasicintuitionthatamongallfunctions
ùëì, the function ùëì=0(assigningthevalue 0toallinputs)isin somesensethe simplest, and
that we can measure the complexity of a function by the distance of its parameters from
zero. But how precisely should we measure the distance between a function and zero?
There is no single right answer. In fact, entire branches of mathematics, including parts
of functional analysis and the theory of Banach spaces, are devoted to addressing such
issues.
One simple interpretation might be to measure the complexity of a linear function ùëì¬πx¬∫=
w>xby some norm of its weight vector, e.g., kwk2. Recall that we introduced the ‚Ñì2norm
and‚Ñì1norm, which are special cases of the more general ‚Ñìùëùnorm, in Section 2.3.11 . The
most common method for ensuring a small weight vector is to add its norm as a penalty
term to the problem of minimizing the loss. Thus we replace our original objective, min-
imizing the prediction loss on the training labels , with new objective, minimizing the sum
of the prediction loss and the penalty term . Now, if our weight vector grows too large, our
learningalgorithmmightfocusonminimizingtheweightnorm kwk2ratherthanminimiz-
ing the training error. That is exactly what we want. To illustrate things in code, we revive
our previous example from Section 3.1 for linear regression. There, our loss was given
by
ùêø¬πw,ùëè¬∫=1
ùëõùëõ√ï
ùëñ=11
2
w>x¬πùëñ¬∫¬∏ùëè ùë¶¬πùëñ¬∫2
. (3.7.1)
Recall that x¬πùëñ¬∫are the features, ùë¶¬πùëñ¬∫is the label for any data example ùëñ, and¬πw,ùëè¬∫are
the weight and bias parameters, respectively. To penalize the size of the weight vector,
we must somehow add kwk2to the loss function, but how should the model trade off the
standard loss for this new additive penalty? In practice, we characterize this trade-off via
theregularization constant ùúÜ, a nonnegative hyperparameter that we fit using validation
data:
ùêø¬πw,ùëè¬∫¬∏ùúÜ
2kwk2. (3.7.2)
ForùúÜ=0, we recover our original loss function. For ùúÜ > 0, we restrict the size of kwk.
We divide by 2by convention: when we take the derivative of a quadratic function, the
2and 1¬ù2cancel out, ensuring that the expression for the update looks nice and simple.
The astute reader might wonder why we work with the squared norm and not the standard
120 Linear Neural Networks for Regression
norm(i.e.,theEuclideandistance). Wedothisforcomputationalconvenience. Bysquaring
the‚Ñì2norm, we remove the square root, leaving the sum of squares of each component of
the weight vector. This makes the derivative of the penalty easy to compute: the sum of
derivatives equals the derivative of the sum.
Moreover, you might ask why we work with the ‚Ñì2norm in the first place and not, say,
the‚Ñì1norm. In fact, other choices are valid and popular throughout statistics. While ‚Ñì2-
regularized linear models constitute the classic ridge regression algorithm,‚Ñì1-regularized
linear regression is a similarly fundamental method in statistics, popularly known as lasso
regression . Onereasontoworkwiththe ‚Ñì2normisthatitplacesanoutsizepenaltyonlarge
components of the weight vector. This biases our learning algorithm towards models that
distribute weight evenly across a larger number of features. In practice, this might make
them more robust to measurement error in a single variable. By contrast, ‚Ñì1penalties lead
to models that concentrate weights on a small set of features by clearing the other weights
to zero. This gives us an effective method for featureselection , which may be desirable for
other reasons. For example, if our model only relies on a few features, then we may not
need to collect, store, or transmit data for the other (dropped) features.
Using the same notation in (3.1.11 ), minibatch stochastic gradient descent updates for ‚Ñì2-
regularized regression as follows:
w ¬π1 ùúÇùúÜ¬∫w ùúÇ
jBj√ï
ùëñ2Bx¬πùëñ¬∫
w>x¬πùëñ¬∫¬∏ùëè ùë¶¬πùëñ¬∫
.(3.7.3)
As before, we update wbased on the amount by which our estimate differs from the ob-
servation. However, we also shrink the size of wtowards zero. That is why the method is
sometimescalled‚Äúweightdecay‚Äù: giventhepenaltytermalone,ouroptimizationalgorithm
decaysthe weight at each step of training. In contrast to feature selection, weight decay
offers us a mechanism for continuously adjusting the complexity of a function. Smaller
values ofùúÜcorrespond to less constrained w, whereas larger values of ùúÜconstrain wmore
considerably. Whether we include a corresponding bias penalty ùëè2can vary across imple-
mentations, and may vary across layers of a neural network. Often, we do not regularize
the bias term. Besides, although ‚Ñì2regularization may not be equivalent to weight decay
for other optimization algorithms, the idea of regularization through shrinking the size of
weights still holds true.
3.7.2High-Dimensional Linear Regression
We can illustrate the benefits of weight decay through a simple synthetic example.
First, we generate some data as before:
ùë¶=0.05¬∏ùëë√ï
ùëñ=10.01ùë•ùëñ¬∏ùúñwhereùúñN¬π 0,0.012¬∫. (3.7.4)
In this synthetic dataset, our label is given by an underlying linear function of our inputs,
corrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative
purposes, we can make the effects of overfitting pronounced, by increasing the dimen-
121 Weight Decay
sionality of our problem to ùëë=200and working with a small training set with only 20
examples.
class Data (d2l .DataModule):
def __init__ (self , num_train, num_val, num_inputs, batch_size):
self .save_hyperparameters()
n=num_train +num_val
self .X=torch .randn(n, num_inputs)
noise =torch .randn(n, 1)*0.01
w, b =torch .ones((num_inputs, 1))*0.01 ,0.05
self .y=torch .matmul( self .X, w) +b+noise
def get_dataloader (self , train):
i=slice (0,self .num_train) iftrain else slice (self .num_train, None )
return self .get_tensorloader([ self .X,self .y], train, i)
3.7.3Implementation fromScratch
Now,let‚Äôstryimplementingweightdecayfromscratch. Sinceminibatchstochasticgradient
descent is our optimizer, we just need to add the squared ‚Ñì2penalty to the original loss
function.
Defining‚Ñì2NormPenalty
Perhaps the most convenient way of implementing this penalty is to square all terms in
place and sum them.
def l2_penalty (w):
return (w**2).sum() /2
Definingthe Model
Inthefinalmodel,thelinearregressionandthesquaredlosshavenotchangedsince Section
3.4,sowewilljustdefineasubclassof d2l.LinearRegressionScratch . Theonlychange
here is that our loss now includes the penalty term.
class WeightDecayScratch (d2l .LinearRegressionScratch):
def __init__ (self , num_inputs, lambd, lr, sigma =0.01 ):
super ().__init__ (num_inputs, lr, sigma)
self .save_hyperparameters()
def loss (self , y_hat, y):
return (super ().loss(y_hat, y) +
self .lambd *l2_penalty( self .w))
The following code fits our model on the training set with 20 examples and evaluates it on
the validation set with 100 examples.
122 Linear Neural Networks for Regression
data =Data(num_train =20, num_val =100, num_inputs =200, batch_size =5)
trainer =d2l.Trainer(max_epochs =10)
def train_scratch (lambd):
model =WeightDecayScratch(num_inputs =200, lambd =lambd, lr =0.01 )
model .board .yscale ='log'
trainer .fit(model, data)
print ('L2 norm of w: ',float (l2_penalty(model .w)))
Trainingwithout Regularization
We now run this code with lambd = 0 , disabling weight decay. Note that we overfit
badly, decreasing the training error but not the validation error‚Äîa textbook case of over-
fitting.
train_scratch( 0)
L2 norm of w: 0.009948714636266232
Using WeightDecay
Below, we run with substantial weight decay. Note that the training error increases but
the validation error decreases. This is precisely the effect we expect from regulariza-
tion.
train_scratch( 3)
L2 norm of w: 0.0017270983662456274
3.7.4Concise Implementation
Because weight decay is ubiquitous in neural network optimization, the deep learning
framework makes it especially convenient, integrating weight decay into the optimization
123 Weight Decay
84algorithm itself for easy use in combination with anyloss function. Moreover, this integra-
tion serves a computational benefit, allowing implementation tricks to add weight decay
to the algorithm, without any additional computational overhead. Since the weight decay
portion of the update depends only on the current value of each parameter, the optimizer
must touch each parameter once anyway.
Below, we specify the weight decay hyperparameter directly through weight_decay when
instantiating our optimizer. By default, PyTorch decays both weights and biases simulta-
neously, but we can configure the optimizer to handle different parameters according to
different policies. Here, we only set weight_decay for the weights (the net.weight pa-
rameters), hence the bias (the net.bias parameter) will not decay.
class WeightDecay (d2l .LinearRegression):
def __init__ (self , wd, lr):
super ().__init__ (lr)
self .save_hyperparameters()
self .wd=wd
def configure_optimizers (self ):
return torch .optim .SGD([
{'params ':self .net.weight, 'weight_decay ':self .wd},
{'params ':self .net.bias}], lr =self .lr)
The plot looks similar to that when we implemented weight decay from scratch. How-
ever, this version runs faster and is easier to implement, benefits that will become more
pronounced as you address larger problems and this work becomes more routine.
model =WeightDecay(wd =3, lr =0.01 )
model .board .yscale ='log'
trainer .fit(model, data)
print ('L2 norm of w: ',float (l2_penalty(model .get_w_b()[ 0])))
L2 norm of w: 0.013779522851109505
Sofar,wehavetouchedupononenotionofwhatconstitutesasimplelinearfunction. How-
ever, even for simple nonlinear functions, the situation can be much more complex. To see
this, the concept of reproducing kernel Hilbert space (RKHS)84allows one to apply tools
124 Linear Neural Networks for Regression
85introduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algo-
rithms tend to scale poorly to large, high-dimensional data. In this book we will often
adopt the common heuristic whereby weight decay is applied to all layers of a deep net-
work.
3.7.5Summary
Regularization is a common method for dealing with overfitting. Classical regularization
techniquesaddapenaltytermtothelossfunction(whentraining)toreducethecomplexity
of the learned model. One particular choice for keeping the model simple is using an ‚Ñì2
penalty. This leads to weight decay in the update steps of the minibatch stochastic gradient
descent algorithm. In practice, the weight decay functionality is provided in optimizers
from deep learning frameworks. Different sets of parameters can have different update
behaviors within the same training loop.
3.7.6Exercises
1.Experiment with the value of ùúÜin the estimation problem in this section. Plot training
and validation accuracy as a function of ùúÜ. What do you observe?
2.Use a validation set to find the optimal value of ùúÜ. Is it really the optimal value? Does
this matter?
3.What would the update equations look like if instead of kwk2we used√ç
ùëñjùë§ùëñjas our
penalty of choice ( ‚Ñì1regularization)?
4.We know thatkwk2=w>w. Can you find a similar equation for matrices (see the
Frobenius norm in Section 2.3.11 )?
5.Review the relationship between training error and generalization error. In addition to
weight decay, increased training, and the use of a model of suitable complexity, what
other ways might help us deal with overfitting?
6.In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior
viaùëÉ¬πùë§jùë•¬∫/ùëÉ¬πùë•jùë§¬∫ùëÉ¬πùë§¬∫. How can you identify ùëÉ¬πùë§¬∫with regularization?
Discussions85.
4 Linear Neural Networks for ClassiÔ¨Åcation
Now that you have worked through all of the mechanics you are ready to apply the skills
you have learned to broader kinds of tasks. Even as we pivot towards classification, most
of the plumbing remains the same: loading the data, passing it through the model, generat-
ing output, calculating the loss, taking gradients with respect to weights, and updating the
model. However, the precise form of the targets, the parametrization of the output layer,
and the choice of loss function will adapt to suit the classification setting.
4.1SoftmaxRegression
InSection 3.1 , we introduced linear regression, working through implementations from
scratch in Section 3.4 and again using high-level APIs of a deep learning framework in
Section 3.5 to do the heavy lifting.
Regression is the hammer wereachforwhen wewantto answer howmuch? orhowmany?
questions. Ifyouwanttopredictthenumberofdollars(price)atwhichahousewillbesold,
or the number of wins a baseball team might have, or the number of days that a patient will
remainhospitalizedbeforebeingdischarged,thenyouareprobablylookingforaregression
model. However, even within regression models, there are important distinctions. For
instance, the price of a house will never be negative and changes might often be relative
to its baseline price. As such, it might be more effective to regress on the logarithm of the
price. Likewise, the number of days a patient spends in hospital is a discrete nonnegative
random variable. As such, least mean squares might not be an ideal approach either. This
sortoftime-to-eventmodelingcomeswithahostofothercomplicationsthataredealtwith
in a specialized subfield called survivalmodeling .
The point here is not to overwhelm you but just to let you know that there is a lot more
to estimation than simply minimizing squared errors. And more broadly, there is a lot
more to supervised learning than regression. In this section, we focus on classification
problems where we put aside how much? questions and instead focus on which category?
questions.
Does this email belong in the spam folder or the inbox?
Is this customer more likely to sign up or not to sign up for a subscription service?
125
126 Linear Neural Networks for ClassiÔ¨Åcation
86
87Does this image depict a donkey, a dog, a cat, or a rooster?
Which movie is Aston most likely to watch next?
Which section of the book are you going to read next?
Colloquially, machine learning practitioners overload the word classification to describe
two subtly different problems: (i) those where we are interested only in hard assignments
ofexamplestocategories(classes); and(ii)thosewherewewishtomakesoftassignments,
i.e.,toassesstheprobabilitythateachcategoryapplies. Thedistinctiontendstogetblurred,
inpart, becauseoften, evenwhenweonlycareabouthardassignments, westillusemodels
that make soft assignments.
Even more, there are cases where more than one label might be true. For instance, a news
article might simultaneously cover the topics of entertainment, business, and space flight,
but not the topics of medicine or sports. Thus, categorizing it into one of the above cate-
gories on their own would not be very useful. This problem is commonly known as multi-
label classification86. See Tsoumakas and Katakis ( 2007) for an overview and Huang et
al.(2015) for an effective algorithm when tagging images.
4.1.1Classification
To get our feet wet, let‚Äôs start with a simple image classification problem. Here, each input
consists of a 22grayscale image. We can represent each pixel value with a single scalar,
giving us four features ùë•1,ùë•2,ùë•3,ùë•4. Further, let‚Äôs assume that each image belongs to one
among the categories ‚Äúcat‚Äù, ‚Äúchicken‚Äù, and ‚Äúdog‚Äù.
Next, we have to choose how to represent the labels. We have two obvious choices. Per-
hapsthemostnaturalimpulsewouldbetochoose ùë¶2f1,2,3g,wheretheintegersrepresent
fdog,cat,chickengrespectively. This is a great way of storingsuch information on a com-
puter. If the categories had some natural ordering among them, say if we were trying to
predictfbaby,toddler,adolescent,young adult,adult,geriatricg, then it might even make
sense to cast this as an ordinal regression87problem and keep the labels in this format.
See Moon et al.(2010) for an overview of different types of ranking loss functions and
Beutelet al.(2014) for a Bayesian approach that addresses responses with more than one
mode.
In general, classification problems do not come with natural orderings among the classes.
Fortunately, statisticians long ago invented a simple way to represent categorical data: the
one-hot encoding . A one-hot encoding is a vector with as many components as we have
categories. Thecomponentcorrespondingtoaparticularinstance‚Äôscategoryissetto1and
allothercomponentsaresetto0. Inourcase,alabel ùë¶wouldbeathree-dimensionalvector,
with¬π1,0,0¬∫corresponding to ‚Äúcat‚Äù, ¬π0,1,0¬∫to ‚Äúchicken‚Äù, and¬π0,0,1¬∫to ‚Äúdog‚Äù:
ùë¶2f¬π1,0,0¬∫,¬π0,1,0¬∫,¬π0,0,1¬∫g. (4.1.1)
127 Softmax Regression
LinearModel
In order to estimate the conditional probabilities associated with all the possible classes,
we need a model with multiple outputs, one per class. To address classification with lin-
ear models, we will need as many affine functions as we have outputs. Strictly speaking,
we only need one fewer, since the final category has to be the difference between 1and
the sum of the other categories, but for reasons of symmetry we use a slightly redundant
parametrization. Each output corresponds to its own affine function. In our case, since
we have 4 features and 3 possible output categories, we need 12 scalars to represent the
weights (ùë§with subscripts), and 3 scalars to represent the biases ( ùëèwith subscripts). This
yields:
ùëú1=ùë•1ùë§11¬∏ùë•2ùë§12¬∏ùë•3ùë§13¬∏ùë•4ùë§14¬∏ùëè1,
ùëú2=ùë•1ùë§21¬∏ùë•2ùë§22¬∏ùë•3ùë§23¬∏ùë•4ùë§24¬∏ùëè2,
ùëú3=ùë•1ùë§31¬∏ùë•2ùë§32¬∏ùë•3ùë§33¬∏ùë•4ùë§34¬∏ùëè3.(4.1.2)
The corresponding neural network diagram is shown in Fig. 4.1.1 . Just as in linear regres-
sion,weuseasingle-layerneuralnetwork. Andsincethecalculationofeachoutput, ùëú1,ùëú2,
andùëú3, depends on every input, ùë•1,ùë•2,ùë•3, andùë•4, the output layer can also be described as
afullyconnectedlayer .
tFig. 4.1.1 Softmax regression is a single-layer neural network.
Foramoreconcisenotationweusevectorsandmatrices: o=Wx¬∏bismuchbettersuited
formathematicsandcode. Notethatwehavegatheredallofourweightsintoa 34matrix
and all biases b2R3in a vector.
TheSoftmax
Assuming a suitable loss function, we could try, directly, to minimize the difference be-
tweenoand the labels y. While it turns out that treating classification as a vector-valued
regressionproblemworkssurprisinglywell,itisnonethelessunsatisfactoryinthefollowing
ways:
There is no guarantee that the outputs ùëúùëñsum up to 1in the way we expect probabilities
to behave.
There is no guarantee that the outputs ùëúùëñare even nonnegative, even if their outputs sum
up to 1, or that they do not exceed 1.
Both aspects render the estimation problem difficult to solve and the solution very brittle
to outliers. For instance, if we assume that there is a positive linear dependency between
the number of bedrooms and the likelihood that someone will buy a house, the probability
128 Linear Neural Networks for ClassiÔ¨Åcation
88might exceed 1when it comes to buying a mansion! As such, we need a mechanism to
‚Äúsquish‚Äù the outputs.
There are many ways we might accomplish this goal. For instance, we could assume that
the outputs oare corrupted versions of y, where the corruption occurs by means of adding
noise ùùêdrawnfrom a normal distribution. In other words, y=o¬∏ùùê, whereùúñùëñN¬π 0,ùúé2¬∫.
This is the so-called probit model88, first introduced by Fechner ( 1860). While appealing,
it does not work quite as well nor lead to a particularly nice optimization problem, when
compared to the softmax.
Another way to accomplish this goal (and to ensure nonnegativity) is to use an exponential
functionùëÉ¬πùë¶=ùëñ¬∫/expùëúùëñ. This does indeed satisfy the requirement that the conditional
class probability increases with increasing ùëúùëñ, it is monotonic, and all probabilities are
nonnegative. We can then transform these values so that they add up to 1by dividing each
by their sum. This process is called normalization . Putting these two pieces together gives
us thesoftmax function:
ÀÜy=softmax¬πo¬∫where ÀÜùë¶ùëñ=exp¬πùëúùëñ¬∫√ç
ùëóexp¬πùëúùëó¬∫. (4.1.3)
Note that the largest coordinate of ocorresponds to the most likely class according to ÀÜy.
Moreover, because the softmax operation preserves the ordering among its arguments, we
donotneedtocomputethesoftmaxtodeterminewhichclasshasbeenassignedthehighest
probability. Thus,
argmax
ùëóÀÜùë¶ùëó=argmax
ùëóùëúùëó.(4.1.4)
Theideaofa softmaxdatesbacktoGibbs( 1902), whoadaptedideasfrom physics. Dating
even further back, Boltzmann, the father of modern statistical physics, used this trick to
model a distribution over energy states in gas molecules. In particular, he discovered that
theprevalenceofastateofenergyinathermodynamicensemble,suchasthemoleculesina
gas, is proportional to exp¬π ùê∏¬ùùëòùëá¬∫. Here,ùê∏is the energy of a state, ùëáis the temperature,
andùëòis the Boltzmann constant. When statisticians talk about increasing or decreasing
the ‚Äútemperature‚Äù of a statistical system, they refer to changing ùëáin order to favor lower
or higher energy states. Following Gibbs‚Äô idea, energy equates to error. Energy-based
models ( Ranzatoet al., 2007) use this point of view when describing problems in deep
learning.
Vectorization
Toimprovecomputationalefficiency,wevectorizecalculationsinminibatchesofdata. As-
sume that we are given a minibatch X2Rùëõùëëofùëõexamples with dimensionality (number
of inputs)ùëë. Moreover, assume that we have ùëûcategories in the output. Then the weights
satisfy W2Rùëëùëûand the bias satisfies b2R1ùëû.
O=XW¬∏b,
ÀÜY=softmax¬πO¬∫.(4.1.5)
129 Softmax Regression
This accelerates the dominant operation into a matrix‚Äìmatrix product XW. Moreover,
sinceeachrowin Xrepresentsadataexample,thesoftmaxoperationitselfcanbecomputed
rowwise: for each row of O, exponentiate all entries and then normalize them by the sum.
Note,though,thatcaremustbetakentoavoidexponentiatingandtakinglogarithmsoflarge
numbers,sincethiscancausenumericaloverfloworunderflow. Deeplearningframeworks
take care of this automatically.
4.1.2LossFunction
Now that we have a mapping from features xto probabilities ÀÜ y, we need a way to optimize
the accuracy of this mapping. We will rely on maximum likelihood estimation, the very
samemethodthatweencounteredwhenprovidingaprobabilisticjustificationforthemean
squared error loss in Section 3.1.3 .
Log-Likelihood
The softmax function gives us a vector ÀÜy, which we can interpret as the (estimated) con-
ditional probabilities of each class, given any input x, such as ÀÜùë¶1=ùëÉ¬πùë¶=catjx¬∫. In the
following we assume that for a dataset with features Xthe labels Yare represented using
a one-hot encoding label vector. We can compare the estimates with reality by checking
how probable the actual classes are according to our model, given the features:
ùëÉ¬πYjX¬∫=ùëõ√ñ
ùëñ=1ùëÉ¬πy¬πùëñ¬∫jx¬πùëñ¬∫¬∫. (4.1.6)
We are allowed to use the factorization since we assume that each label is drawn indepen-
dently from its respective distribution ùëÉ¬πyjx¬πùëñ¬∫¬∫. Since maximizing the product of terms
isawkward,wetakethenegativelogarithmtoobtaintheequivalentproblemofminimizing
the negative log-likelihood:
 logùëÉ¬πYjX¬∫=ùëõ√ï
ùëñ=1 logùëÉ¬πy¬πùëñ¬∫jx¬πùëñ¬∫¬∫=ùëõ√ï
ùëñ=1ùëô¬πy¬πùëñ¬∫,ÀÜy¬πùëñ¬∫¬∫, (4.1.7)
where for any pair of label yand model prediction ÀÜyoverùëûclasses, the loss function ùëô
is
ùëô¬πy,ÀÜy¬∫= ùëû√ï
ùëó=1ùë¶ùëólog ÀÜùë¶ùëó. (4.1.8)
For reasons explained later on, the loss function in (4.1.8 )is commonly called the cross-
entropyloss . Since yisaone-hotvectoroflength ùëû, thesumoverallitscoordinates ùëóvan-
ishesforallbutoneterm. Notethattheloss ùëô¬πy,ÀÜy¬∫isboundedfrombelowby 0whenever ÀÜy
is a probability vector: no single entry is larger than 1, hence their negative logarithm can-
not be lower than 0;ùëô¬πy,ÀÜy¬∫=0only if we predict the actual label with certainty . This can
never happen for any finite setting of the weights because taking a softmax output towards
1requires taking the corresponding input ùëúùëñto infinity (or all other outputs ùëúùëóforùëó‚â†ùëñ
to negative infinity). Even if our model could assign an output probability of 0, any error
made when assigning such high confidence would incur infinite loss (  log 0=1).
130 Linear Neural Networks for ClassiÔ¨Åcation
Softmax and Cross-EntropyLoss
Since the softmax function and the corresponding cross-entropy loss are so common, it is
worth understanding a bit better how they are computed. Plugging (4.1.3 )into the defini-
tion of the loss in (4.1.8 )and using the definition of the softmax we obtain
ùëô¬πy,ÀÜy¬∫= ùëû√ï
ùëó=1ùë¶ùëólogexp¬πùëúùëó¬∫
√çùëû
ùëò=1exp¬πùëúùëò¬∫
=ùëû√ï
ùëó=1ùë¶ùëólogùëû√ï
ùëò=1exp¬πùëúùëò¬∫ ùëû√ï
ùëó=1ùë¶ùëóùëúùëó
=logùëû√ï
ùëò=1exp¬πùëúùëò¬∫ ùëû√ï
ùëó=1ùë¶ùëóùëúùëó.(4.1.9)
Tounderstandabitbetterwhatisgoingon,considerthederivativewithrespecttoanylogit
ùëúùëó. We get
ùúïùëúùëóùëô¬πy,ÀÜy¬∫=exp¬πùëúùëó¬∫
√çùëû
ùëò=1exp¬πùëúùëò¬∫ ùë¶ùëó=softmax¬πo¬∫ùëó ùë¶ùëó. (4.1.10)
In other words, the derivative is the difference between the probability assigned by our
model, as expressed by the softmax operation, and what actually happened, as expressed
by elements in the one-hot label vector. In this sense, it is very similar to what we saw in
regression, where the gradient was the difference between the observation ùë¶and estimate
ÀÜùë¶. This is not a coincidence. In any exponential family model, the gradients of the log-
likelihood are given by precisely this term. This fact makes computing gradients easy in
practice.
Nowconsiderthecasewhereweobservenotjustasingleoutcomebutanentiredistribution
over outcomes. We can use the same representation as before for the label y. The only dif-
ferenceisthatratherthanavectorcontainingonlybinaryentries,say ¬π0,0,1¬∫,wenowhave
agenericprobabilityvector, say ¬π0.1,0.2,0.7¬∫. Themaththatweusedpreviouslytodefine
the lossùëôin(4.1.8 )still works well, just that the interpretation is slightly more general. It
is the expected value of the loss for a distribution over labels. This loss is called the cross-
entropyloss anditisoneofthemostcommonlyusedlossesforclassificationproblems. We
can demystify the name by introducing just the basics of information theory. In a nutshell,
itmeasuresthenumberofbitsneededtoencodewhatwesee, y,relativetowhatwepredict
that should happen, ÀÜy. We provide a very basic explanation in the following. For further
details on information theory see Cover and Thomas ( 1999) or MacKay ( 2003).
4.1.3InformationTheory Basics
Manydeeplearningpapersuseintuitionandtermsfrominformationtheory. Tomakesense
of them, we need some common language. This is a survival guide. Information theory
deals with the problem of encoding, decoding, transmitting, and manipulating information
(also known as data).
131 Softmax Regression
Entropy
The central idea in information theory is to quantify the amount of information contained
indata. Thisplacesalimitonourabilitytocompressdata. Foradistribution ùëÉitsentropy,
ùêª¬ªùëÉ¬º, is defined as:
ùêª¬ªùëÉ¬º=√ï
ùëó ùëÉ¬πùëó¬∫logùëÉ¬πùëó¬∫.(4.1.11)
One of the fundamental theorems of information theory states that in order to encode data
drawn randomly from the distribution ùëÉ, we need at least ùêª¬ªùëÉ¬º‚Äúnats‚Äù to encode it ( Shan-
non, 1948 ). If you wonder what a ‚Äúnat‚Äù is, it is the equivalent of bit but when using a code
with baseùëírather than one with base 2. Thus, one nat is1
log¬π2¬∫1.44bit.
Surprisal
Youmightbewonderingwhatcompressionhastodowithprediction. Imaginethatwehave
a stream of data that we want to compress. If it is always easy for us to predict the next
token, then this data is easy to compress. Take the extreme example where every token in
the stream always takes the same value. That is a very boring data stream! And not only
it is boring, but it is also easy to predict. Because the tokens are always the same, we do
not have to transmit any information to communicate the contents of the stream. Easy to
predict, easy to compress.
Howeverifwecannotperfectlypredicteveryevent,thenwemightsometimesbesurprised.
Oursurpriseisgreaterwhenaneventisassignedlowerprobability. ClaudeShannonsettled
onlog1
ùëÉ¬πùëó¬∫= logùëÉ¬πùëó¬∫toquantifyone‚Äôs surprisal atobservinganevent ùëóhavingassigned
it a (subjective) probability ùëÉ¬πùëó¬∫. The entropy defined in (4.1.11 )is then the expected
surprisal when one assigned the correct probabilities that truly match the data-generating
process.
Cross-EntropyRevisited
So if entropy is the level of surprise experienced by someone who knows the true proba-
bility, then you might be wondering, what is cross-entropy? The cross-entropy fromùëÉto
ùëÑ, denotedùêª¬πùëÉ,ùëÑ¬∫, is the expected surprisal of an observer with subjective probabilities
ùëÑupon seeing data that was actually generated according to probabilities ùëÉ. This is given
byùêª¬πùëÉ,ùëÑ¬∫def=√ç
ùëó ùëÉ¬πùëó¬∫logùëÑ¬πùëó¬∫. The lowest possible cross-entropy is achieved when
ùëÉ=ùëÑ. In this case, the cross-entropy from ùëÉtoùëÑisùêª¬πùëÉ,ùëÉ¬∫=ùêª¬πùëÉ¬∫.
In short, we can think of the cross-entropy classification objective in two ways: (i) as max-
imizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus
the number of bits) required to communicate the labels.
4.1.4Summaryand Discussion
Inthissection,weencounteredthefirstnontriviallossfunction,allowingustooptimizeover
discreteoutputspaces. Keyinitsdesignwasthatwetookaprobabilisticapproach,treating
132 Linear Neural Networks for ClassiÔ¨Åcation
89
90discrete categories as instances of draws from a probability distribution. As a side effect,
we encountered the softmax, a convenient activation function that transforms outputs of an
ordinary neural network layer into valid discrete probability distributions. We saw that the
derivative of the cross-entropy loss when combined with softmax behaves very similarly
to the derivative of squared error; namely by taking the difference between the expected
behavior and its prediction. And, while we were only able to scratch the very surface of it,
we encountered exciting connections to statistical physics and information theory.
While this is enough to get you on your way, and hopefully enough to whet your appetite,
we hardly dived deep here. Among other things, we skipped over computational con-
siderations. Specifically, for any fully connected layer with ùëëinputs andùëûoutputs, the
parametrization and computational cost is O¬πùëëùëû¬∫, which can be prohibitively high in prac-
tice. Fortunately, this cost of transforming ùëëinputs intoùëûoutputs can be reduced through
approximation and compression. For instance Deep Fried Convnets ( Yanget al., 2015)
uses a combination of permutations, Fourier transforms, and scaling to reduce the cost
from quadratic to log-linear. Similar techniques work for more advanced structural matrix
approximations ( Sindhwani et al., 2015). Lastly, we can use quaternion-like decomposi-
tions to reduce the cost to O¬πùëëùëû
ùëõ¬∫, again if we are willing to trade off a small amount of
accuracy for computational and storage cost ( Zhanget al., 2021) based on a compression
factorùëõ. This is an active area of research. What makes it challenging is that we do not
necessarily strive for the most compact representation or the smallest number of floating
point operations but rather for the solution that can be executed most efficiently on modern
GPUs.
4.1.5Exercises
1.Wecanexploretheconnectionbetweenexponentialfamiliesandsoftmaxinsomemore
depth.
1.Compute the second derivative of the cross-entropy loss ùëô¬πy,ÀÜy¬∫for softmax.
2.Computethevarianceofthedistributiongivenby softmax¬πo¬∫andshowthatitmatches
the second derivative computed above.
2.Assume that we have three classes which occur with equal probability, i.e., the proba-
bility vector is¬π1
3,1
3,1
3¬∫.
1.What is the problem if we try to design a binary code for it?
2.Can you design a better code? Hint: what happens if we try to encode two indepen-
dent observations? What if we encode ùëõobservations jointly?
3.When encoding signals transmitted over a physical wire, engineers do not always use
binary codes. For instance, PAM-389uses three signal levels f 1,0,1gas opposed to
two levelsf0,1g. How many ternary units do you need to transmit an integer in the
rangef0,..., 7g? Why might this be a better idea in terms of electronics?
4.TheBradley‚ÄìTerry model90uses a logistic model to capture preferences. For a user to
133 Softmax Regression
91
92choose between apples and oranges one assumes scores ùëúappleandùëúorange. Our require-
mentsarethatlargerscoresshouldleadtoahigherlikelihoodinchoosingtheassociated
itemandthattheitemwiththelargestscoreisthemostlikelyonetobechosen( Bradley
and Terry, 1952 ).
1.Prove that softmax satisfies this requirement.
2.What happens if you want to allow for a default option of choosing neither apples
nor oranges? Hint: now the user has three choices.
5.Softmaxgetsitsnamefromthefollowingmapping: RealSoftMax ¬πùëé,ùëè¬∫=log¬πexp¬πùëé¬∫¬∏
exp¬πùëè¬∫¬∫.
1.Prove that RealSoftMax ¬πùëé,ùëè¬∫>max¬πùëé,ùëè¬∫.
2.How small can you make the difference between both functions? Hint: without loss
of generality you can set ùëè=0andùëéùëè.
3.Prove that this holds for ùúÜ 1RealSoftMax¬πùúÜùëé,ùúÜùëè¬∫, provided that ùúÜ> 0.
4.Show that for ùúÜ!1we haveùúÜ 1RealSoftMax¬πùúÜùëé,ùúÜùëè¬∫! max¬πùëé,ùëè¬∫.
5.Construct an analogous softmin function.
6.Extend this to more than two numbers.
6.The function ùëî¬πx¬∫def=log√ç
ùëñexpùë•ùëñis sometimes also referred to as the log-partition
function91.
1.Prove that the function is convex. Hint: to do so, use the fact that the first derivative
amounts to the probabilities from the softmax function and show that the second
derivative is the variance.
2.Show thatùëîis translation invariant, i.e., ùëî¬πx¬∏ùëè¬∫=ùëî¬πx¬∫.
3.What happens if some of the coordinates ùë•ùëñare very large? What happens if they‚Äôre
all very small?
4.Show that if we choose ùëè=maxùëñùë•ùëñwe end up with a numerically stable implemen-
tation.
7.Assume that we have some probability distribution ùëÉ. Suppose we pick another distri-
butionùëÑwithùëÑ¬πùëñ¬∫/ùëÉ¬πùëñ¬∫ùõºforùõº> 0.
1.Which choice of ùõºcorresponds to doubling the temperature? Which choice corre-
sponds to halving it?
2.What happens if we let the temperature approach 0?
3.What happens if we let the temperature approach 1?
Discussions92.
134 Linear Neural Networks for ClassiÔ¨Åcation
934.2The ImageClassification Dataset
One widely used dataset for image classification is the MNIST dataset93(LeCunet al.,
1998) of handwritten digits. At the time of its release in the 1990s it posed a formidable
challenge to most machine learning algorithms, consisting of 60,000 images of 2828
pixelsresolution(plusatestdatasetof10,000images). Toputthingsintoperspective,back
in1995,aSunSPARCStation5withawhopping64MBofRAMandablistering5MFLOPs
wasconsideredstateoftheartequipmentformachinelearningatAT&TBellLaboratories.
Achieving high accuracy on digit recognition was a key component in automating letter
sorting for the USPS in the 1990s. Deep networks such as LeNet-5 ( LeCunet al., 1995),
support vector machines with invariances ( Sch√∂lkopf et al., 1996), and tangent distance
classifiers ( Simardetal., 1998) all could reach error rates below 1%.
For over a decade, MNIST served as thepoint of reference for comparing machine learn-
ing algorithms. While it had a good run as a benchmark dataset, even simple models by
today‚Äôsstandardsachieveclassificationaccuracyover95%, makingitunsuitablefordistin-
guishing between strong models and weaker ones. Even more, the dataset allows for very
high levels of accuracy, not typically seen in many classification problems. This skewed
algorithmic development towards specific families of algorithms that can take advantage
of clean datasets, such as active set methods and boundary-seeking active set algorithms.
Today, MNIST serves as more of a sanity check than as a benchmark. ImageNet ( Denget
al., 2009) poses a much more relevant challenge. Unfortunately, ImageNet is too large for
many of the examples and illustrations in this book, as it would take too long to train to
make the examples interactive. As a substitute we will focus our discussion in the coming
sectionsonthequalitativelysimilar,butmuchsmallerFashion-MNISTdataset( Xiaoetal.,
2017)whichwasreleasedin2017. Itcontainsimagesof10categoriesofclothingat 2828
pixels resolution.
%matplotlib inline
import time
import torch
import torchvision
from torchvision import transforms
from d2l import torch asd2l
d2l.use_svg_display()
4.2.1Loadingthe Dataset
Since the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed
versions of it. We can download and read it into memory using built-in framework utili-
ties.
135 The Image ClassiÔ¨Åcation Dataset
class FashionMNIST (d2l .DataModule): #@save
"""The Fashion-MNIST dataset."""
def __init__ (self , batch_size =64, resize =(28,28)):
super ().__init__ ()
self .save_hyperparameters()
trans =transforms .Compose([transforms .Resize(resize),
transforms .ToTensor()])
self .train =torchvision .datasets .FashionMNIST(
root =self .root, train =True , transform =trans, download =True )
self .val =torchvision .datasets .FashionMNIST(
root =self .root, train =False , transform =trans, download =True )
Fashion-MNIST consists of images from 10 categories, each represented by 6000 images
in the training dataset and by 1000 in the test dataset. A test dataset is used for evaluating
modelperformance(itmustnotbeusedfortraining). Consequentlythetrainingsetandthe
test set contain 60,000 and 10,000 images, respectively.
data =FashionMNIST(resize =(32,32))
len(data .train), len(data .val)
(60000 ,10000 )
Theimagesaregrayscaleandupscaledto 3232pixelsinresolutionabove. Thisissimilar
to the original MNIST dataset which consisted of (binary) black and white images. Note,
though, that most modern image data has three channels (red, green, blue) and that hyper-
spectral images can have in excess of 100 channels (the HyMap sensor has 126 channels).
By convention we store an image as a ùëê‚Ñéùë§tensor, where ùëêis the number of color
channels,‚Ñéis the height and ùë§is the width.
data .train[ 0][0].shape
torch .Size([ 1,32,32])
ThecategoriesofFashion-MNISThavehuman-understandablenames. Thefollowingcon-
venience method converts between numeric labels and their names.
@d2l .add_to_class(FashionMNIST) #@save
def text_labels (self , indices):
"""Return text labels."""
labels =['t-shirt ','trouser ','pullover ','dress ','coat ',
'sandal ','shirt ','sneaker ','bag','ankle boot ']
return [labels[ int(i)] for iinindices]
4.2.2Readinga Minibatch
Tomakeourlifeeasierwhenreadingfromthetrainingandtestsets,weusethebuilt-indata
iterator rather than creating one from scratch. Recall that at each iteration, a data iterator
136 Linear Neural Networks for ClassiÔ¨Åcation
reads a minibatch of data with size batch_size . We also randomly shuffle the examples
for the training data iterator.
@d2l .add_to_class(FashionMNIST) #@save
def get_dataloader (self , train):
data =self .train iftrain else self .val
return torch .utils .data .DataLoader(data, self .batch_size, shuffle =train,
num_workers =self .num_workers)
Toseehowthisworks,let‚Äôsloadaminibatchofimagesbyinvokingthe train_dataloader
method. It contains 64 images.
X, y =next (iter (data .train_dataloader()))
print (X.shape, X .dtype, y .shape, y .dtype)
torch .Size([ 64,1,32,32]) torch .float32 torch .Size([ 64]) torch .int64
Let‚Äôslookatthetimeittakestoreadtheimages. Eventhoughitisabuilt-inloader,itisnot
blazingly fast. Nonetheless, this is sufficient since processing images with a deep network
takes quite a bit longer. Hence it is good enough that training a network will not be I/O
constrained.
tic =time .time()
for X, y indata .train_dataloader():
continue
f'{time .time() -tic:.2f}sec'
'4.69 sec '
4.2.3Visualization
We will often be using the Fashion-MNIST dataset. A convenience function show_images
can be used to visualize the images and the associated labels. Skipping implementation
details, we just show the interface below: we only need to know how to invoke d2l.
show_images rather than how it works for such utility functions.
def show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save
"""Plot a list of images."""
raise NotImplementedError
Let‚Äôsputittogooduse. Ingeneral,itisagoodideatovisualizeandinspectdatathatyouare
training on. Humans are very good at spotting oddities and because of that, visualization
serves as an additional safeguard against mistakes and errors in the design of experiments.
Herearetheimagesandtheircorrespondinglabels(intext)forthefirstfewexamplesinthe
training dataset.
137 The Image ClassiÔ¨Åcation Dataset
94@d2l .add_to_class(FashionMNIST) #@save
def visualize (self , batch, nrows =1, ncols =8, labels =[]):
X, y =batch
ifnot labels:
labels =self .text_labels(y)
d2l.show_images(X .squeeze( 1), nrows, ncols, titles =labels)
batch =next (iter (data .val_dataloader()))
data .visualize(batch)
WearenowreadytoworkwiththeFashion-MNISTdatasetinthesectionsthatfollow.
4.2.4Summary
Wenowhaveaslightlymorerealisticdatasettouseforclassification. Fashion-MNISTisan
apparel classification dataset consisting of images representing 10 categories. We will use
this dataset in subsequent sections and chapters to evaluate various network designs, from
a simple linear model to advanced residual networks. As we commonly do with images,
wereadthemasatensorofshape(batchsize,numberofchannels,height,width). Fornow,
we only have one channel as the images are grayscale (the visualization above uses a false
color palette for improved visibility).
Lastly,dataiteratorsareakeycomponentforefficientperformance. Forinstance,wemight
use GPUs for efficient image decompression, video transcoding, or other preprocessing.
Whenever possible, you should rely on well-implemented data iterators that exploit high-
performance computing to avoid slowing down your training loop.
4.2.5Exercises
1.Does reducing the batch_size (for instance, to 1) affect the reading performance?
2.The data iterator performance is important. Do you think the current implementation
is fast enough? Explore various options to improve it. Use a system profiler to find out
where the bottlenecks are.
3.Check out the framework‚Äôs online API documentation. Which other datasets are avail-
able?
Discussions94.
138 Linear Neural Networks for ClassiÔ¨Åcation
4.3TheBase Classification Model
You may have noticed that the implementations from scratch and the concise implementa-
tion using framework functionality were quite similar in the case of regression. The same
istrueforclassification. Sincemanymodelsinthisbookdealwithclassification,itisworth
addingfunctionalitiestosupportthissettingspecifically. Thissectionprovidesabaseclass
for classification models to simplify future code.
import torch
from d2l import torch asd2l
4.3.1The Classifier Class
We define the Classifier class below. In the validation_step we report both the loss
value and the classification accuracy on a validation batch. We draw an update for every
num_val_batches batches. This has the benefit of generating the averaged loss and ac-
curacy on the whole validation data. These average numbers are not exactly correct if the
final batch contains fewer examples, but we ignore this minor difference to keep the code
simple.
class Classifier (d2l .Module): #@save
"""The base class of classification models."""
def validation_step (self , batch):
Y_hat =self (*batch[: -1])
self .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )
self .plot( 'acc',self .accuracy(Y_hat, batch[ -1]), train =False )
By default we use a stochastic gradient descent optimizer, operating on minibatches, just
as we did in the context of linear regression.
@d2l .add_to_class(d2l .Module) #@save
def configure_optimizers (self ):
return torch .optim .SGD( self .parameters(), lr =self .lr)
4.3.2Accuracy
Given the predicted probability distribution y_hat, we typically choose the class with the
highest predicted probability whenever we must output a hard prediction. Indeed, many
applications require that we make a choice. For instance, Gmail must categorize an email
into‚ÄúPrimary‚Äù,‚ÄúSocial‚Äù,‚ÄúUpdates‚Äù,‚ÄúForums‚Äù,or‚ÄúSpam‚Äù. Itmightestimateprobabilities
internally, but at the end of the day it has to choose one among the classes.
When predictions are consistent with the label class y, they are correct. The classification
accuracy is the fraction of all predictions that are correct. Although it can be difficult to
optimizeaccuracydirectly(itisnotdifferentiable),itisoftentheperformancemeasurethat
139 The Base ClassiÔ¨Åcation Model
95we care about the most. It is often therelevant quantity in benchmarks. As such, we will
nearly always report it when training classifiers.
Accuracyiscomputedasfollows. First, if y_hatisamatrix, weassumethattheseconddi-
mensionstorespredictionscoresforeachclass. Weuse argmaxtoobtainthepredictedclass
by the index for the largest entry in each row. Then we compare the predicted class with
the ground truth yelementwise. Since the equality operator ==is sensitive to data types,
we convert y_hat‚Äôs data type to match that of y. The result is a tensor containing entries
of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions.
@d2l .add_to_class(Classifier) #@save
def accuracy (self , Y_hat, Y, averaged =True ):
"""Compute the number of correct predictions."""
Y_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))
preds =Y_hat .argmax(axis =1).type(Y .dtype)
compare =(preds ==Y.reshape( -1)).type(torch .float32)
return compare .mean() ifaveraged else compare
4.3.3Summary
Classification is a sufficiently common problem that it warrants its own convenience func-
tions. Of central importance in classification is the accuracy of the classifier. Note that
while we often care primarily about accuracy, we train classifiers to optimize a variety of
other objectives for statistical and computational reasons. However, regardless of which
loss function was minimized during training, it is useful to have a convenience method for
assessing the accuracy of our classifier empirically.
4.3.4Exercises
1.Denote byùêøvthe validation loss, and let ùêøq
vbe its quick and dirty estimate computed
by the loss function averaging in this section. Lastly, denote by ùëôb
vthe loss on the last
minibatch. Express ùêøvin terms ofùêøq
v,ùëôb
v, and the sample and minibatch sizes.
2.Show that the quick and dirty estimate ùêøq
vis unbiased. That is, show that ùê∏¬ªùêøv¬º=
ùê∏¬ªùêøq
v¬º. Why would you still want to use ùêøvinstead?
3.Given a multiclass classification loss, denoting by ùëô¬πùë¶,ùë¶0¬∫the penalty of estimating
ùë¶0when we see ùë¶and given a probabilty ùëù¬πùë¶jùë•¬∫, formulate the rule for an optimal
selection ofùë¶0. Hint: express the expected loss, using ùëôandùëù¬πùë¶jùë•¬∫.
Discussions95.
140 Linear Neural Networks for ClassiÔ¨Åcation
964.4Softmax RegressionImplementation from
Scratch
Because softmax regression is so fundamental, we believe that you ought to know how to
implement it yourself. Here, we limit ourselves to defining the softmax-specific aspects of
themodelandreusetheothercomponentsfromourlinearregressionsection,includingthe
training loop.
import torch
from d2l import torch asd2l
4.4.1The Softmax
Let‚Äôs begin with the most important part: the mapping from scalars to probabilities. For a
refresher, recall the operation of the sum operator along specific dimensions in a tensor, as
discussedin Section2.3.6 andSection2.3.7 . Givenamatrix Xwecansumoverallelements
(by default) or only over elements in the same axis. The axisvariable lets us compute row
and column sums:
X=torch .tensor([[ 1.0,2.0,3.0], [ 4.0,5.0,6.0]])
X.sum( 0, keepdims =True ), X .sum( 1, keepdims =True )
(tensor([[ 5.,7.,9.]]),
tensor([[ 6.],
[15.]]))
Computing the softmax requires three steps: (i) exponentiation of each term; (ii) a sum
over each row to compute the normalization constant for each example; (iii) division of
each row by its normalization constant, ensuring that the result sums to 1:
softmax¬πX¬∫ùëñùëó=exp¬πXùëñùëó¬∫√ç
ùëòexp¬πXùëñùëò¬∫. (4.4.1)
The (logarithm of the) denominator is called the (log) partitionfunction . It was introduced
instatistical physics96to sum over all possible states in a thermodynamic ensemble. The
implementation is straightforward:
def softmax (X):
X_exp =torch .exp(X)
partition =X_exp .sum( 1, keepdims =True )
return X_exp /partition # The broadcasting mechanism is applied here
For any input X, we turn each element into a nonnegative number. Each row sums up to
1, as is required for a probability. Caution: the code above is notrobust against very large
or very small arguments. While it is sufficient to illustrate what is happening, you should
141 Softmax Regression Implementation from Scratch
notuse this code verbatim for any serious purpose. Deep learning frameworks have such
protections built in and we will be using the built-in softmax going forward.
X=torch .rand(( 2,5))
X_prob =softmax(X)
X_prob, X_prob .sum( 1)
(tensor([[ 0.2511 ,0.1417 ,0.1158 ,0.2529 ,0.2385 ],
[0.2004 ,0.1419 ,0.1957 ,0.2504 ,0.2117 ]]),
tensor([ 1.,1.]))
4.4.2TheModel
We now have everything that we need to implement the softmax regression model. As in
our linear regression example, each instance will be represented by a fixed-length vector.
Since the raw data here consists of 2828pixel images, we flatten each image, treating
them as vectors of length 784. In later chapters, we will introduce convolutional neural
networks, which exploit the spatial structure in a more satisfying way.
In softmax regression, the number of outputs from our network should be equal to the
number of classes. Since our dataset has 10 classes, our network has an output dimension
of 10. Consequently, our weights constitute a 78410matrix plus a 110row vector for
the biases. As with linear regression, we initialize the weights Wwith Gaussian noise. The
biases are initialized as zeros.
class SoftmaxRegressionScratch (d2l .Classifier):
def __init__ (self , num_inputs, num_outputs, lr, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .W=torch .normal( 0, sigma, size =(num_inputs, num_outputs),
requires_grad =True )
self .b=torch .zeros(num_outputs, requires_grad =True )
def parameters (self ):
return [self .W,self .b]
Thecodebelowdefineshowthenetworkmapseachinputtoanoutput. Notethatweflatten
each 2828pixel image in the batch into a vector using reshape before passing the data
through our model.
@d2l .add_to_class(SoftmaxRegressionScratch)
def forward (self , X):
X=X.reshape(( -1,self .W.shape[ 0]))
return softmax(torch .matmul(X, self .W)+self .b)
4.4.3The Cross-EntropyLoss
Next we need to implement the cross-entropy loss function (introduced in Section 4.1.2 ).
This may be the most common loss function in all of deep learning. At the moment, appli-
142 Linear Neural Networks for ClassiÔ¨Åcation
cations of deep learning easily cast as classification problems far outnumber those better
treated as regression problems.
Recall that cross-entropy takes the negative log-likelihood of the predicted probability as-
signed to the true label. For efficiency we avoid Python for-loops and use indexing instead.
In particular, the one-hot encoding in yallows us to select the matching terms in ÀÜy.
Toseethisinactionwecreatesampledata y_hatwith2examplesofpredictedprobabilities
over 3 classes and their corresponding labels y. The correct labels are 0and2respectively
(i.e., the first and third class). Using yas the indices of the probabilities in y_hat, we can
pick out terms efficiently.
y=torch .tensor([ 0,2])
y_hat =torch .tensor([[ 0.1,0.3,0.6], [ 0.3,0.2,0.5]])
y_hat[[ 0,1], y]
tensor([ 0.1000 ,0.5000 ])
Nowwecanimplementthecross-entropylossfunctionbyaveragingoverthelogarithmsof
the selected probabilities.
def cross_entropy (y_hat, y):
return -torch .log(y_hat[ list (range (len(y_hat))), y]) .mean()
cross_entropy(y_hat, y)
tensor( 1.4979 )
@d2l .add_to_class(SoftmaxRegressionScratch)
def loss (self , y_hat, y):
return cross_entropy(y_hat, y)
4.4.4Training
Wereusethe fitmethoddefinedin Section3.4 totrainthemodelwith10epochs. Notethat
the number of epochs ( max_epochs ), the minibatch size ( batch_size ), and learning rate
(lr) are adjustable hyperparameters. That means that while these values are not learned
during our primary training loop, they still influence the performance of our model, both
vis-√†-vistrainingandgeneralizationperformance. Inpracticeyouwillwanttochoosethese
values based on the validation split of the data and then, ultimately, to evaluate your final
modelonthe testsplit. Asdiscussedin Section3.6.3 ,wewillregardthetestdataofFashion-
MNIST as the validation set, thus reporting validation loss and validation accuracy on this
split.
143 Softmax Regression Implementation from Scratch
data =d2l.FashionMNIST(batch_size =256)
model =SoftmaxRegressionScratch(num_inputs =784, num_outputs =10, lr =0.1)
trainer =d2l.Trainer(max_epochs =10)
trainer .fit(model, data)
4.4.5Prediction
Now that training is complete, our model is ready to classify some images.
X, y =next (iter (data .val_dataloader()))
preds =model(X) .argmax(axis =1)
preds .shape
torch .Size([ 256])
Wearemoreinterestedintheimageswelabel incorrectly . Wevisualizethembycomparing
theiractuallabels(firstlineoftextoutput)withthepredictionsfromthemodel(secondline
of text output).
wrong =preds .type(y .dtype) !=y
X, y, preds =X[wrong], y[wrong], preds[wrong]
labels =[a+'\n'+bfor a, b inzip(
data .text_labels(y), data .text_labels(preds))]
data .visualize([X, y], labels =labels)
4.4.6Summary
By now we are starting to get some experience with solving linear regression and classifi-
cation problems. With it, we have reached what would arguably be the state of the art of
144 Linear Neural Networks for ClassiÔ¨Åcation
971960‚Äì1970s of statistical modeling. In the next section, we will show you how to leverage
deep learning frameworks to implement this model much more efficiently.
4.4.7Exercises
1.Inthissection,wedirectlyimplementedthesoftmaxfunctionbasedonthemathematical
definitionofthesoftmaxoperation. Asdiscussedin Section4.1 thiscancausenumerical
instabilities.
1.Test whether softmax still works correctly if an input has a value of 100.
2.Test whether softmax still works correctly if the largest of all inputs is smaller than
 100?
3.Implement a fix by looking at the value relative to the largest entry in the argument.
2.Implement a cross_entropy function that follows the definition of the cross-entropy
loss function√ç
ùëñùë¶ùëñlog ÀÜùë¶ùëñ.
1.Try it out in the code example of this section.
2.Why do you think it runs more slowly?
3.Should you use it? When would it make sense to?
4.What do you need to be careful of? Hint: consider the domain of the logarithm.
3.Is it always a good idea to return the most likely label? For example, would you do this
for medical diagnosis? How would you try to address this?
4.Assume that we want to use softmax regression to predict the next word based on some
features. What are some problems that might arise from a large vocabulary?
5.Experiment with the hyperparameters of the code in this section. In particular:
1.Plot how the validation loss changes as you change the learning rate.
2.Do the validation and training loss change as you change the minibatch size? How
large or small do you need to go before you see an effect?
Discussions97.
4.5ConciseImplementation of Softmax Regression
Just as high-level deep learning frameworks made it easier to implement linear regression
(seeSection 3.5 ), they are similarly convenient here.
145 Concise Implementation of Softmax Regression
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
4.5.1Definingthe Model
AsinSection3.5 ,weconstructourfullyconnectedlayerusingthebuilt-inlayer. Thebuilt-
in__call__ methodtheninvokes forward wheneverweneedtoapplythenetworktosome
input.
We use a Flatten layer to convert the fourth-order tensor Xto second order by keeping the
dimensionality along the first axis unchanged.
class SoftmaxRegression (d2l .Classifier): #@save
"""The softmax regression model."""
def __init__ (self , num_outputs, lr):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(nn .Flatten(),
nn.LazyLinear(num_outputs))
def forward (self , X):
return self .net(X)
4.5.2Softmax Revisited
InSection 4.4 we calculated our model‚Äôs output and applied the cross-entropy loss. While
this is perfectly reasonable mathematically, it is risky computationally, because of numer-
ical underflow and overflow in the exponentiation.
Recall that the softmax function computes probabilities via ÀÜùë¶ùëó=exp¬πùëúùëó¬∫√ç
ùëòexp¬πùëúùëò¬∫. If some of the
ùëúùëòare very large, i.e., very positive, then exp¬πùëúùëò¬∫might be larger than the largest number
we can have for certain data types. This is called overflow . Likewise, if every argument is
a very large negative number, we will get underflow . For instance, single precision floating
point numbers approximately cover the range of 10 38to1038. As such, if the largest term
inolies outside the interval ¬ª 90,90¬º, the result will not be stable. A way round this
problem is to subtract ¬Øùëúdef=maxùëòùëúùëòfrom all entries:
ÀÜùë¶ùëó=expùëúùëó√ç
ùëòexpùëúùëò=exp¬πùëúùëó ¬Øùëú¬∫exp ¬Øùëú√ç
ùëòexp¬πùëúùëò ¬Øùëú¬∫exp ¬Øùëú=exp¬πùëúùëó ¬Øùëú¬∫√ç
ùëòexp¬πùëúùëò ¬Øùëú¬∫. (4.5.1)
By construction we know that ùëúùëó ¬Øùëú0for allùëó. As such, for a ùëû-class classification
problem, the denominator is contained in the interval ¬ª1,ùëû¬º. Moreover, the numerator
never exceeds 1, thus preventing numerical overflow. Numerical underflow only occurs
when exp¬πùëúùëó ¬Øùëú¬∫numerically evaluates as 0. Nonetheless, a few steps down the road we
might find ourselves in trouble when we want to compute log ÀÜùë¶ùëóaslog 0. In particular, in
backpropagation, we might find ourselves faced with a screenful of the dreaded NaN(Not a
Number) results.
146 Linear Neural Networks for ClassiÔ¨Åcation
98Fortunately, we are saved by the fact that even though we are computing exponential func-
tions, we ultimately intend to take their log (when calculating the cross-entropy loss). By
combining softmax and cross-entropy, we can escape the numerical stability issues alto-
gether. We have:
log ÀÜùë¶ùëó=logexp¬πùëúùëó ¬Øùëú¬∫√ç
ùëòexp¬πùëúùëò ¬Øùëú¬∫=ùëúùëó ¬Øùëú log√ï
ùëòexp¬πùëúùëò ¬Øùëú¬∫. (4.5.2)
This avoids both overflow and underflow. We will want to keep the conventional softmax
function handy in case we ever want to evaluate the output probabilities by our model. But
instead of passing softmax probabilities into our new loss function, we just pass the logits
andcomputethesoftmaxanditslogallatonceinsidethecross-entropylossfunction,which
does smart things like the ‚ÄúLogSumExp trick‚Äù98.
@d2l .add_to_class(d2l .Classifier) #@save
def loss (self , Y_hat, Y, averaged =True ):
Y_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))
Y=Y.reshape(( -1,))
return F.cross_entropy(
Y_hat, Y, reduction ='mean 'ifaveraged else 'none ')
4.5.3Training
Next we train our model. We use Fashion-MNIST images, flattened to 784-dimensional
feature vectors.
data =d2l.FashionMNIST(batch_size =256)
model =SoftmaxRegression(num_outputs =10, lr =0.1)
trainer =d2l.Trainer(max_epochs =10)
trainer .fit(model, data)
Asbefore,thisalgorithmconvergestoasolutionthatisreasonablyaccurate,albeitthistime
with fewer lines of code than before.
4.5.4Summary
High-levelAPIsareveryconvenientathidingfromtheiruserpotentiallydangerousaspects,
such as numerical stability. Moreover, they allow users to design models concisely with
147 Generalization in ClassiÔ¨Åcation
99very few lines of code. This is both a blessing and a curse. The obvious benefit is that it
makesthingshighlyaccessible,eventoengineerswhonevertookasingleclassofstatistics
in their life (in fact, they are part of the target audience of the book). But hiding the sharp
edgesalsocomeswith aprice: a disincentivetoadd newanddifferent componentsonyour
own, since there is little muscle memory for doing it. Moreover, it makes it more difficult
tofixthings whenever the protective padding of a framework fails to cover all the corner
cases entirely. Again, this is due to lack of familiarity.
As such, we strongly urge you to review boththe bare bones and the elegant versions of
many of the implementations that follow. While we emphasize ease of understanding, the
implementations are nonetheless usually quite performant (convolutions are the big excep-
tionhere). Itisourintentiontoallowyoutobuildonthesewhenyouinventsomethingnew
that no framework can give you.
4.5.5Exercises
1.Deep learning uses many different number formats, including FP64 double precision
(used extremely rarely), FP32 single precision, BFLOAT16 (good for compressed rep-
resentations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8.
Compute the smallest and largest argument of the exponential function for which the
result does not lead to numerical underflow or overflow.
2.INT8 is a very limited format consisting of nonzero numbers from 1to255. How could
you extend its dynamic range without using more bits? Do standard multiplication and
addition still work?
3.Increasethenumberofepochsfortraining. Whymightthevalidationaccuracydecrease
after a while? How could we fix this?
4.What happens as you increase the learning rate? Compare the loss curves for several
learning rates. Which one works better? When?
Discussions99.
4.6Generalizationin Classification
So far, we have focused on how to tackle multiclass classification problems by training
(linear) neural networks with multiple outputs and softmax functions. Interpreting our
model‚Äôs outputs as probabilistic predictions, we motivated and derived the cross-entropy
loss function, which calculates the negative log likelihood that our model (for a fixed set
of parameters) assigns to the actual labels. And finally, we put these tools into practice
by fitting our model to the training set. However, as always, our goal is to learn general
patterns, as assessed empirically on previously unseen data (the test set). High accuracy
on the training set means nothing. Whenever each of our inputs is unique (and indeed this
is true for most high-dimensional datasets), we can attain perfect accuracy on the training
148 Linear Neural Networks for ClassiÔ¨Åcation
set by just memorizing the dataset on the first training epoch, and subsequently looking up
the label whenever we see a new image. And yet, memorizing the exact labels associated
with the exact training examples does not tell us how to classify new examples. Absent
further guidance, we might have to fall back on random guessing whenever we encounter
new examples.
A number of burning questions demand immediate attention:
1.How many test examples do we need to give a good estimate of the accuracy of our
classifiers on the underlying population?
2.What happens if we keep evaluating models on the same test repeatedly?
3.Why should we expect that fitting our linear models to the training set should fare any
better than our naive memorization scheme?
Whereas Section3.6 introducedthebasicsofoverfittingandgeneralizationinthecontextof
linear regression, this chapter will go a little deeper, introducing some of the foundational
ideas of statistical learning theory. It turns out that we often can guarantee generalization
a priori: for many models, and for any desired upper bound on the generalization gap ùúñ,
we can often determine some required number of samples ùëõsuch that if our training set
contains at least ùëõsamples, our empirical error will lie within ùúñof the true error, for any
datageneratingdistribution . Unfortunately, it also turns out that while these sorts of guar-
antees provide a profound set of intellectual building blocks, they are of limited practical
utility to the deep learning practitioner. In short, these guarantees suggest that ensuring
generalization of deep neural networks a priori requires an absurd number of examples
(perhapstrillionsormore), evenwhenwefindthat, onthetaskswecareabout, deepneural
networks typically generalize remarkably well with far fewer examples (thousands). Thus
deep learning practitioners often forgo a priori guarantees altogether, instead employing
methods that have generalized well on similar problems in the past, and certifying gen-
eralization post hoc through empirical evaluations. When we get to Chapter 5 , we will
revisit generalization and provide a light introduction to the vast scientific literature that
has sprung in attempts to explain why deep neural networks generalize in practice.
4.6.1The TestSet
Since we have already begun to rely on test sets as the gold standard method for assessing
generalization error, let‚Äôs get started by discussing the properties of such error estimates.
Let‚Äôs focus on a fixed classifier ùëì, without worrying about how it was obtained. Moreover
suppose that we possess a freshdataset of examples D=¬πx¬πùëñ¬∫,ùë¶¬πùëñ¬∫¬∫ùëõ
ùëñ=1that were not used
to train the classifier ùëì. Theempiricalerror of our classifier ùëìonDis simply the fraction
ofinstancesforwhichtheprediction ùëì¬πx¬πùëñ¬∫¬∫disagreeswiththetruelabel ùë¶¬πùëñ¬∫, andisgiven
by the following expression:
ùúñD¬πùëì¬∫=1
ùëõùëõ√ï
ùëñ=11¬πùëì¬πx¬πùëñ¬∫¬∫‚â†ùë¶¬πùëñ¬∫¬∫. (4.6.1)
Bycontrast,the populationerror istheexpected fractionofexamplesintheunderlyingpop-
ulation (some distribution ùëÉ¬πùëã,ùëå¬∫characterized by probability density function ùëù¬πx,ùë¶¬∫)
149 Generalization in ClassiÔ¨Åcation
for which our classifier disagrees with the true label:
ùúñ¬πùëì¬∫=ùê∏¬πx,ùë¶¬∫ùëÉ1¬πùëì¬πx¬∫‚â†ùë¶¬∫=¬π ¬π
1¬πùëì¬πx¬∫‚â†ùë¶¬∫ùëù¬πx,ùë¶¬∫ùëëxùëëùë¶. (4.6.2)
Whileùúñ¬πùëì¬∫is the quantity that we actually care about, we cannot observe it directly, just
as we cannot directly observe the average height in a large population without measuring
every single person. We can only estimate this quantity based on samples. Because our
test setDis statistically representative of the underlying population, we can view ùúñD¬πùëì¬∫
as a statistical estimator of the population error ùúñ¬πùëì¬∫. Moreover, because our quantity of
interestùúñ¬πùëì¬∫isanexpectation(oftherandomvariable 1¬πùëì¬πùëã¬∫‚â†ùëå¬∫)andthecorresponding
estimatorùúñD¬πùëì¬∫isthesampleaverage,estimatingthepopulationerrorissimplytheclassic
problem of mean estimation, which you may recall from Section 2.6 .
An important classical result from probability theory called the centrallimittheorem guar-
antees that whenever we possess ùëõrandom samples ùëé1,...,ùëéùëõdrawn from any distribution
with meanùúáand standard deviation ùúé, then, as the number of samples ùëõapproaches infin-
ity,thesampleaverage ÀÜùúáapproximatelytendstowardsanormaldistributioncenteredatthe
true mean and with standard deviation ùúé¬ùpùëõ. Already, this tells us something important:
asthenumberofexamplesgrowslarge,ourtesterror ùúñD¬πùëì¬∫shouldapproachthetrueerror
ùúñ¬πùëì¬∫at a rate ofO¬π1¬ùpùëõ¬∫. Thus, to estimate our test error twice as precisely, we must
collect four times as large a test set. To reduce our test error by a factor of one hundred, we
must collect ten thousand times as large a test set. In general, such a rate of O¬π1¬ùpùëõ¬∫is
often the best we can hope for in statistics.
Now that we know something about the asymptotic rate at which our test error ùúñD¬πùëì¬∫
converges to the true error ùúñ¬πùëì¬∫, we can zoom in on some important details. Recall that
the random variable of interest 1¬πùëì¬πùëã¬∫‚â†ùëå¬∫can only take values 0and 1and thus is
a Bernoulli random variable, characterized by a parameter indicating the probability that
it takes value 1. Here, 1means that our classifier made an error, so the parameter of our
randomvariableisactuallythetrueerrorrate ùúñ¬πùëì¬∫. Thevariance ùúé2ofaBernoullidepends
on its parameter (here, ùúñ¬πùëì¬∫) according to the expression ùúñ¬πùëì¬∫¬π1 ùúñ¬πùëì¬∫¬∫. Whileùúñ¬πùëì¬∫is
initially unknown, we know that it cannot be greater than 1. A little investigation of this
function reveals that our variance is highest when the true error rate is close to 0.5and can
be far lower when it is close to 0or close to 1. This tells us that the asymptotic standard
deviation of our estimate ùúñD¬πùëì¬∫of the errorùúñ¬πùëì¬∫(over the choice of the ùëõtest samples)
cannot be any greater thanp
0.25¬ùùëõ.
If we ignore the fact that this rate characterizes behavior as the test set size approaches
infinity rather than when we possess finite samples, this tells us that if we want our test
errorùúñD¬πùëì¬∫to approximate the population error ùúñ¬πùëì¬∫such that one standard deviation
corresponds to an interval of 0.01, then we should collect roughly 2500 samples. If we
want to fit two standard deviations in that range and thus be 95% confident that ùúñD¬πùëì¬∫2
ùúñ¬πùëì¬∫0.01, then we will need 10,000 samples!
Thisturnsouttobethesizeofthetestsetsformanypopularbenchmarksinmachinelearn-
ing. You might be surprised to find out that thousands of applied deep learning papers get
published every year making a big deal out of error rate improvements of 0.01or less. Of
150 Linear Neural Networks for ClassiÔ¨Åcation
course, when the error rates are much closer to 0, then an improvement of 0.01can indeed
be a big deal.
One pesky feature of our analysis thus far is that it really only tells us about asymptotics,
i.e., how the relationship between ùúñDandùúñevolves as our sample size goes to infinity.
Fortunately, because our random variable is bounded, we can obtain valid finite sample
bounds by applying an inequality due to Hoeffding (1963):
ùëÉ¬πùúñD¬πùëì¬∫ ùúñ¬πùëì¬∫ùë°¬∫<exp
 2ùëõùë°2
. (4.6.3)
Solving for the smallest dataset size that would allow us to conclude with 95% confidence
that the distance ùë°betweenour estimate ùúñD¬πùëì¬∫and the true error rate ùúñ¬πùëì¬∫does not exceed
0.01, you will find that roughly 15,000 examples are required as compared to the 10,000
examples suggested by the asymptotic analysis above. If you go deeper into statistics you
will find that this trend holds generally. Guarantees that hold even in finite samples are
typically slightly more conservative. Note that in the scheme of things, these numbers
are not so far apart, reflecting the general usefulness of asymptotic analysis for giving us
ballpark figures even if they are not guarantees we can take to court.
4.6.2TestSetReuse
In some sense, you are now set up to succeed at conducting empirical machine learning
research. Nearly all practical models are developed and validated based on test set perfor-
mance and you are now a master of the test set. For any fixed classifier ùëì, you know how
to evaluate its test error ùúñD¬πùëì¬∫, and know precisely what can (and cannot) be said about its
population error ùúñ¬πùëì¬∫.
So let‚Äôs say that you take this knowledge and prepare to train your first model ùëì1. Knowing
justhowconfidentyouneedtobeintheperformanceofyourclassifier‚Äôserrorrateyouapply
our analysis above to determine an appropriate number of examples to set aside for the test
set. Moreover, let‚Äôs assume that you took the lessons from Section 3.6 to heart and made
sure to preserve the sanctity of the test set by conducting all of your preliminary analysis,
hyperparameter tuning, and even selection among multiple competing model architectures
onavalidationset. Finallyyouevaluateyourmodel ùëì1onthetestsetandreportanunbiased
estimate of the population error with an associated confidence interval.
So far everything seems to be going well. However, that night you wake up at 3am with
a brilliant idea for a new modeling approach. The next day, you code up your new model,
tune its hyperparameters on the validation set and not only are you getting your new model
ùëì2to work but its error rate appears to be much lower than ùëì1‚Äôs. However, the thrill of
discovery suddenly fades as you prepare for the final evaluation. You do not have a test
set!
Eventhoughtheoriginaltestset Disstillsittingonyourserver,younowfacetwoformidable
problems. First, when you collected your test set, you determined the required level of pre-
cision under the assumption that you were evaluating a single classifier ùëì. However, if
you get into the business of evaluating multiple classifiers ùëì1,..., ùëìùëòon the same test set,
you must consider the problem of false discovery. Before, you might have been 95% sure
151 Generalization in ClassiÔ¨Åcation
thatùúñD¬πùëì¬∫2ùúñ¬πùëì¬∫0.01for a single classifier ùëìand thus the probability of a misleading
result was a mere 5%. With ùëòclassifiers in the mix, it can be hard to guarantee that there
is not even one among them whose test set performance is misleading. With 20 classifiers
under consideration, you might have no power at all to rule out the possibility that at least
one among them received a misleading score. This problem relates to multiple hypothesis
testing, which despite a vast literature in statistics, remains a persistent problem plaguing
scientific research.
If that is not enough to worry you, there is a special reason to distrust the results that you
get on subsequent evaluations. Recall that our analysis of test set performance rested on
the assumption that the classifier was chosen absent any contact with the test set and thus
we could view the test set as drawn randomly from the underlying population. Here, not
only are you testing multiple functions, the subsequent function ùëì2was chosen after you
observed the test set performance of ùëì1. Once information from the test set has leaked
to the modeler, it can never be a true test set again in the strictest sense. This problem is
calledadaptiveoverfitting andhasrecentlyemergedasatopicofintenseinteresttolearning
theorists and statisticians ( Dworket al., 2015). Fortunately, while it is possible to leak all
information out of a holdout set, and the theoretical worst case scenarios are bleak, these
analyses may be too conservative. In practice, take care to create real test sets, to consult
them as infrequently as possible, to account for multiple hypothesis testing when reporting
confidence intervals, and to dial up your vigilance more aggressively when the stakes are
high and your dataset size is small. When running a series of benchmark challenges, it is
oftengoodpracticetomaintainseveraltestsetssothataftereachround, theoldtestsetcan
be demoted to a validation set.
4.6.3Statistical Learning Theory
Put simply, testsetsareallthatwereallyhave , and yet this fact seems strangely unsatisfy-
ing. First, we seldom possess a true test set ‚Äîunless we are the ones creating the dataset,
someone else has probably already evaluated their own classifier on our ostensible ‚Äútest
set‚Äù. And even when we have first dibs, we soon find ourselves frustrated, wishing we
could evaluate our subsequentmodeling attempts without the gnawing feeling that wecan-
not trust our numbers. Moreover, even a true test set can only tell us post hoc whether a
classifierhasinfactgeneralizedtothepopulation,notwhetherwehaveanyreasontoexpect
apriori that it should generalize.
With these misgivings in mind, you might now be sufficiently primed to see the appeal of
statistical learning theory , the mathematical subfield of machine learning whose practi-
tioners aim to elucidate the fundamental principles that explain why/when models trained
on empirical data can/will generalize to unseen data. One of the primary aims of statistical
learningresearchershasbeentoboundthegeneralizationgap,relatingthepropertiesofthe
model class to the number of samples in the dataset.
Learning theorists aim to bound the difference between the empirical error ùúñS¬πùëìS¬∫of a
learned classifier ùëìS, both trained and evaluated on the training set S, and the true error
ùúñ¬πùëìS¬∫of that same classifier on the underlying population. This might look similar to
the evaluation problem that we just addressed but there is a major difference. Earlier, the
152 Linear Neural Networks for ClassiÔ¨Åcation
classifierùëìwas fixed and we only needed a dataset for evaluative purposes. And indeed,
anyfixedclassifierdoesgeneralize: itserrorona(previouslyunseen)datasetisanunbiased
estimate of the population error. But what can we say when a classifier is trained and
evaluated on the same dataset? Can we ever be confident that the training error will be
close to the testing error?
Suppose that our learned classifier ùëìSmust be chosen from some pre-specified set of func-
tionsF. Recallfromourdiscussionoftestsetsthatwhileitiseasytoestimatetheerrorofa
singleclassifier, thingsgethairywhenwebegintoconsidercollectionsofclassifiers. Even
if the empirical error of any one (fixed) classifier will be close to its true error with high
probability, once we consider a collection of classifiers, we need to worry about the possi-
bility that justone of them will receive a badly estimated error. The worry is that we might
pick such a classifier and thereby grossly underestimate the population error. Moreover,
even for linear models, because their parameters are continuously valued, we are typically
choosing from an infinite class of functions ( jFj=1).
One ambitious solution to the problem is to develop analytic tools for proving uniform
convergence, i.e., that with high probability, the empirical error rate for every classifier
in the class ùëì2 Fwillsimultaneously converge to its true error rate. In other words,
we seek a theoretical principle that would allow us to state that with probability at least
1 ùõø(for some small ùõø) no classifier‚Äôs error rate ùúñ¬πùëì¬∫(among all classifiers in the class
F) will be misestimated by more than some small amount ùõº. Clearly, we cannot make
such statements for all model classes F. Recall the class of memorization machines that
always achieve empirical error 0but never outperform random guessing on the underlying
population.
In a sense the class of memorizers is too flexible. No such a uniform convergence result
couldpossiblyhold. Ontheotherhand,afixedclassifierisuseless‚Äîitgeneralizesperfectly,
but fits neither the training data nor the test data. The central question of learning has
thus historically been framed as a trade-off between more flexible (higher variance) model
classes that better fit the training data but risk overfitting, versus more rigid (higher bias)
model classes that generalize well but risk underfitting. A central question in learning
theoryhasbeentodeveloptheappropriatemathematicalanalysistoquantifywhereamodel
sits along this spectrum, and to provide the associated guarantees.
In a series of seminal papers, Vapnik and Chervonenkis extended the theory on the con-
vergence of relative frequencies to more general classes of functions ( Vapnik and Cher-
vonenkis, 1964 ,Vapnik and Chervonenkis, 1968 ,Vapnik and Chervonenkis, 1971 ,Vap-
nik and Chervonenkis, 1981 ,Vapnik and Chervonenkis, 1991 ,Vapnik and Chervonenkis,
1974). One of the key contributions of this line of work is the Vapnik‚ÄìChervonenkis (VC)
dimension, which measures (one notion of) the complexity (flexibility) of a model class.
Moreover, one of their key results bounds the difference between the empirical error and
the population error as a function of the VC dimension and the number of samples:
ùëÉ ùëÖ¬ªùëù, ùëì¬º ùëÖemp¬ªX,Y, ùëì¬º<ùõº1 ùõøforùõºùëêp
¬πVC logùõø¬∫¬ùùëõ. (4.6.4)
Hereùõø > 0is the probability that the bound is violated, ùõºis the upper bound on the
generalization gap, and ùëõis the dataset size. Lastly, ùëê > 0is a constant that depends only
153 Generalization in ClassiÔ¨Åcation
on the scale of the loss that can be incurred. One use of the bound might be to plug in
desired values of ùõøandùõºto determine how many samples to collect. The VC dimension
quantifies the largest number of data points for which we can assign any arbitrary (binary)
labeling and for each find some model ùëìin the class that agrees with that labeling. For
example, linear models on ùëë-dimensional inputs have VC dimension ùëë¬∏1. It is easy to
see that a line can assign any possible labeling to three points in two dimensions, but not
to four. Unfortunately, the theory tends to be overly pessimistic for more complex models
and obtaining this guarantee typically requires far more examples than are actually needed
to achieve the desired error rate. Note also that fixing the model class and ùõø, our error rate
again decays with the usual O¬π1¬ùpùëõ¬∫rate. It seems unlikely that we could do better in
terms ofùëõ. However, as we vary the model class, VC dimension can present a pessimistic
picture of the generalization gap.
4.6.4Summary
The most straightforward way to evaluate a model is to consult a test set comprised of pre-
viously unseen data. Test set evaluations provide an unbiased estimate of the true error
and converge at the desired O¬π1¬ùpùëõ¬∫rate as the test set grows. We can provide approx-
imate confidence intervals based on exact asymptotic distributions or valid finite sample
confidence intervals based on (more conservative) finite sample guarantees. Indeed test
set evaluation is the bedrock of modern machine learning research. However, test sets are
seldomtruetestsets(used bymultipleresearchersagainand again). Once thesametestset
is used to evaluate multiple models, controlling for false discovery can be difficult. This
cancause hugeproblemsin theory. In practice, the significanceof the problem dependson
the size of the holdout sets in question and whether they are merely being used to choose
hyperparameters or if they are leaking information more directly. Nevertheless, it is good
practicetocuraterealtestsets(ormultiple)andtobeasconservativeaspossibleabouthow
often they are used.
Hoping to provide a more satisfying solution, statistical learning theorists have developed
methodsforguaranteeinguniformconvergenceoveramodelclass. Ifindeedeverymodel‚Äôs
empirical error simultaneously converges to its true error, then we are free to choose the
model that performs best, minimizing the training error, knowing that it too will perform
similarly well on the holdout data. Crucially, any one of such results must depend on some
propertyofthemodelclass. VladimirVapnikandAlexeyChernovenkisintroducedtheVC
dimension, presenting uniform convergence results that hold for all models in a VC class.
The training errors for all models in the class are (simultaneously) guaranteed to be close
to their true errors, and guaranteed to grow even closer at O¬π1¬ùpùëõ¬∫rates. Following the
revolutionarydiscoveryofVCdimension, numerousalternativecomplexitymeasureshave
been proposed, each facilitating an analogous generalization guarantee. See Boucheron
et al.(2005) for a detailed discussion of several advanced ways of measuring function
complexity. Unfortunately, while these complexity measures have become broadly useful
tools in statistical theory, they turn out to be powerless (as straightforwardly applied) for
explainingwhydeepneuralnetworksgeneralize. Deepneuralnetworksoftenhavemillions
of parameters (or more), and can easily assign random labels to large collections of points.
Nevertheless, they generalize well on practical problems and, surprisingly, they often gen-
154 Linear Neural Networks for ClassiÔ¨Åcation
100eralizebetter, when theyare largerand deeper, despite incurring higherVCdimensions. In
the next chapter, we will revisit generalization in the context of deep learning.
4.6.5Exercises
1.If we wish to estimate the error of a fixed model ùëìto within 0.0001with probability
greater than 99.9%, how many samples do we need?
2.Supposethatsomebodyelsepossessesalabeledtestset Dandonlymakesavailablethe
unlabeled inputs (features). Now suppose that you can only access the test set labels by
running a model ùëì(with no restrictions placed on the model class) on each of the un-
labeled inputs and receiving the corresponding error ùúñD¬πùëì¬∫. How many models would
you need to evaluate before you leak the entire test set and thus could appear to have
error 0, regardless of your true error?
3.What is the VC dimension of the class of fifth-order polynomials?
4.What is the VC dimension of axis-aligned rectangles on two-dimensional data?
Discussions100.
4.7Environmentand Distribution Shift
Intheprevioussections,weworkedthroughanumberofhands-onapplicationsofmachine
learning, fitting models to a variety of datasets. And yet, we never stopped to contemplate
either where data came from in the first place or what we ultimately plan to do with the
outputs from our models. Too often, machine learning developers in possession of data
rush to develop models without pausing to consider these fundamental issues.
Many failed machine learning deployments can be traced back to this failure. Sometimes
modelsappeartoperformmarvelouslyasmeasuredbytestsetaccuracybutfailcatastroph-
icallyindeploymentwhenthedistributionofdatasuddenlyshifts. Moreinsidiously,some-
timestheverydeploymentofamodelcanbethecatalystthatperturbsthedatadistribution.
Say, for example, that we trained a model to predict who will repay rather than default on a
loan, finding that an applicant‚Äôs choice of footwear was associated with the risk of default
(Oxfords indicate repayment, sneakers indicate default). We might be inclined thereafter
to grant a loan to any applicant wearing Oxfords and to deny all applicants wearing sneak-
ers.
In this case, our ill-considered leap from pattern recognition to decision-making and our
failure to critically consider the environment might have disastrous consequences. For
starters, as soon as we began making decisions based on footwear, customers would catch
on and change their behavior. Before long, all applicants would be wearing Oxfords, with-
out any coincident improvement in credit-worthiness. Take a minute to digest this because
155 Environment and Distribution Shift
similarissuesaboundinmanyapplicationsofmachinelearning: byintroducingourmodel-
based decisions to the environment, we might break the model.
Whilewecannotpossiblygivethesetopicsacompletetreatmentinonesection,weaimhere
to expose some common concerns, and to stimulate the critical thinking required to detect
such situations early, mitigate damage, and use machine learning responsibly. Some of the
solutions are simple (ask for the ‚Äúright‚Äù data), some are technically difficult (implement a
reinforcement learning system), and others require that we step outside the realm of sta-
tistical prediction altogether and grapple with difficult philosophical questions concerning
the ethical application of algorithms.
4.7.1Typesof Distribution Shift
Tobegin,westickwiththepassivepredictionsettingconsideringthevariouswaysthatdata
distributionsmightshiftandwhatmightbedonetosalvagemodelperformance. Inoneclas-
sic setup, we assume that our training data was sampled from some distribution ùëùùëÜ¬πx,ùë¶¬∫
but that our test data will consist of unlabeled examples drawn from some different distri-
butionùëùùëá¬πx,ùë¶¬∫. Already, we must confront a sobering reality. Absent any assumptions on
howùëùùëÜandùëùùëárelate to each other, learning a robust classifier is impossible.
Consider a binary classification problem, where we wish to distinguish between dogs and
cats. If the distribution can shift in arbitrary ways, then our setup permits the pathological
case in which the distribution over inputs remains constant: ùëùùëÜ¬πx¬∫=ùëùùëá¬πx¬∫, but the labels
are all flipped: ùëùùëÜ¬πùë¶jx¬∫=1 ùëùùëá¬πùë¶jx¬∫. In other words, if God can suddenly decide that
in the future all ‚Äúcats‚Äù are now dogs and what we previously called ‚Äúdogs‚Äù are now cats‚Äî
without any change in the distribution of inputs ùëù¬πx¬∫, then we cannot possibly distinguish
this setting from one in which the distribution did not change at all.
Fortunately,undersomerestrictedassumptionsonthewaysourdatamightchangeinthefu-
ture, principledalgorithmscandetectshiftandsometimesevenadaptonthefly, improving
on the accuracy of the original classifier.
CovariateShift
Among categories of distribution shift, covariate shift may be the most widely studied.
Here, we assume that while the distribution of inputs may change over time, the labeling
function, i.e., the conditional distribution ùëÉ¬πùë¶jx¬∫does not change. Statisticians call this
covariate shift because the problem arises due to a shift in the distribution of the covari-
ates (features). While we can sometimes reason about distribution shift without invoking
causality, we note that covariate shift is the natural assumption to invoke in settings where
we believe that xcausesùë¶.
Consider the challenge of distinguishing cats and dogs. Our training data might consist of
images of the kind in Fig. 4.7.1 .
At test time we are asked to classify the images in Fig. 4.7.2 .
The training set consists of photos, while the test set contains only cartoons. Training on a
156 Linear Neural Networks for ClassiÔ¨Åcation
tFig. 4.7.1 Training data for distinguishing cats and dogs (illustrations: Lafeez Hossain / 500px /
Getty Images; ilkermetinkursova / iStock / Getty Images Plus; GlobalP / iStock / Getty
Images Plus; Musthafa Aboobakuru / 500px / Getty Images).
tFig. 4.7.2 Test data for distinguishing cats and dogs (illustrations: SIBAS_minich / iStock / Getty
Images Plus; Ghrzuzudu / iStock / Getty Images Plus; id-work / DigitalVision Vectors /
Getty Images; Yime / iStock / Getty Images Plus).
dataset with substantially different characteristics from the test set can spell trouble absent
a coherent plan for how to adapt to the new domain.
Label Shift
Label shift describes the converse problem. Here, we assume that the label marginal ùëÉ¬πùë¶¬∫
can change but the class-conditional distribution ùëÉ¬πxjùë¶¬∫remains fixed across domains.
Label shift is a reasonable assumption to make when we believe that ùë¶causes x. For ex-
ample, we may want to predict diagnoses given their symptoms (or other manifestations),
even as the relative prevalence of diagnoses are changing over time. Label shift is the ap-
propriateassumptionherebecausediseasescausesymptoms. Insomedegeneratecasesthe
labelshiftandcovariateshiftassumptionscanholdsimultaneously. Forexample,whenthe
label is deterministic, the covariate shift assumption will be satisfied, even when ùë¶causes
x. Interestingly, in these cases, it is often advantageous to work with methods that flow
from the label shift assumption. That is because these methods tend to involve manipulat-
ing objects that look like labels (often low-dimensional), as opposed to objects that look
like inputs, which tend to be high-dimensional in deep learning.
157 Environment and Distribution Shift
ConceptShift
We may also encounter the related problem of concept shift , which arises when the very
definitions of labels can change. This sounds weird‚Äîa catis acat, no? However, other
categories are subject to changes in usage over time. Diagnostic criteria for mental illness,
what passes for fashionable, and job titles, are all subject to considerable amounts of con-
cept shift. It turns out that if we navigate around the United States, shifting the source of
our data by geography, wewill find considerable concept shift regardingthe distribution of
names for soft drinks as shown in Fig. 4.7.3 .
tFig. 4.7.3 Concept shift for soft drink names in the United States (CC-BY: Alan McConchie,
PopVsSoda.com).
If we were to build a machine translation system, the distribution ùëÉ¬πùë¶jx¬∫might be dif-
ferent depending on our location. This problem can be tricky to spot. We might hope to
exploit knowledge that shift only takes place gradually either in a temporal or geographic
sense.
4.7.2Examplesof Distribution Shift
Before delving into formalism and algorithms, we can discuss some concrete situations
where covariate or concept shift might not be obvious.
Medical Diagnostics
Imaginethatyouwanttodesignanalgorithmtodetectcancer. Youcollectdatafromhealthy
and sick people and you train your algorithm. It works fine, giving you high accuracy and
you conclude that you are ready for a successful career in medical diagnostics. Not so
fast.
Thedistributionsthatgaverisetothetrainingdataandthoseyouwillencounterinthewild
158 Linear Neural Networks for ClassiÔ¨Åcation
mightdifferconsiderably. Thishappenedtoanunfortunatestartupthatsomeofweauthors
worked with years ago. They were developing a blood test for a disease that predominantly
affects older men and hoped to study it using blood samples that they had collected from
patients. However, it is considerably more difficult to obtain blood samples from healthy
men than from sick patients already in the system. To compensate, the startup solicited
blood donations from students on a university campus to serve as healthy controls in de-
veloping their test. Then they asked whether we could help them to build a classifier for
detecting the disease.
As we explained to them, it would indeed be easy to distinguish between the healthy and
sick cohorts with near-perfect accuracy. However, that is because the test subjects differed
in age, hormone levels, physical activity, diet, alcohol consumption, and many more fac-
tors unrelated to the disease. This was unlikely to be the case with real patients. Due to
their sampling procedure, we could expect to encounter extreme covariate shift. Moreover,
this case was unlikely to be correctable via conventional methods. In short, they wasted a
significant sum of money.
Self-Driving Cars
Say a company wanted to leverage machine learning for developing self-driving cars. One
key component here is a roadside detector. Since real annotated data is expensive to get,
they had the (smart and questionable) idea to use synthetic data from a game rendering
engine as additional training data. This worked really well on ‚Äútest data‚Äù drawn from the
renderingengine. Alas, insidearealcaritwasadisaster. Asitturnedout, theroadsidehad
been rendered with a very simplistic texture. More importantly, allthe roadside had been
rendered with the sametexture and the roadside detector learned about this ‚Äúfeature‚Äù very
quickly.
A similar thing happened to the US Army when they first tried to detect tanks in the forest.
Theytookaerialphotographsoftheforestwithouttanks,thendrovethetanksintotheforest
andtookanothersetofpictures. Theclassifierappearedtowork perfectly . Unfortunately,it
hadmerelylearnedhowtodistinguishtreeswithshadowsfromtreeswithoutshadows‚Äîthe
first set of pictures was taken in the early morning, the second set at noon.
Nonstationary Distributions
A much more subtle situation arises when the distribution changes slowly (also known
asnonstationary distribution ) and the model is not updated adequately. Below are some
typical cases.
Wetrainacomputationaladvertisingmodelandthenfailtoupdateitfrequently(e.g.,we
forget to incorporate that an obscure new device called an iPad was just launched).
We build a spam filter. It works well at detecting all spam that we have seen so far. But
then the spammers wise up and craft new messages that look unlike anything we have
seen before.
159 Environment and Distribution Shift
We build a product recommendation system. It works throughout the winter but then
continues to recommend Santa hats long after Christmas.
MoreAnecdotes
We build a face detector. It works well on all benchmarks. Unfortunately it fails on test
data‚Äîthe offending examples are close-ups where the face fills the entire image (no
such data was in the training set).
We build a web search engine for the US market and want to deploy it in the UK.
We train an image classifier by compiling a large dataset where each among a large set
of classes is equally represented in the dataset, say 1000 categories, represented by
1000 images each. Then we deploy the system in the real world, where the actual
label distribution of photographs is decidedly non-uniform.
4.7.3Correctionof Distribution Shift
As we have discussed, there are many cases where training and test distributions ùëÉ¬πx,ùë¶¬∫
are different. In some cases, we get lucky and the models work despite covariate, label,
or concept shift. In other cases, we can do better by employing principled strategies to
cope with the shift. The remainder of this section grows considerably more technical. The
impatient reader could continue on to the next section as this material is not prerequisite to
subsequent concepts.
EmpiricalRisk and Risk
Let‚Äôs first reflect on what exactly is happening during model training: we iterate over fea-
tures and associated labels of training data f¬πx1,ùë¶1¬∫,...,¬πxùëõ,ùë¶ùëõ¬∫gand update the param-
eters of a model ùëìafter every minibatch. For simplicity we do not consider regularization,
so we largely minimize the loss on the training:
minimize
ùëì1
ùëõùëõ√ï
ùëñ=1ùëô¬πùëì¬πxùëñ¬∫,ùë¶ùëñ¬∫, (4.7.1)
whereùëôis the loss function measuring ‚Äúhow bad‚Äù the prediction ùëì¬πxùëñ¬∫is given the associ-
ated labelùë¶ùëñ. Statisticians call the term in (4.7.1 )empirical risk . Theempirical risk is an
average loss over the training data for approximating the risk, which is the expectation of
thelossovertheentirepopulationofdatadrawnfromtheirtruedistribution ùëù¬πx,ùë¶¬∫:
ùê∏ùëù¬πx,ùë¶¬∫¬ªùëô¬πùëì¬πx¬∫,ùë¶¬∫¬º=¬π ¬π
ùëô¬πùëì¬πx¬∫,ùë¶¬∫ùëù¬πx,ùë¶¬∫ùëëxùëëùë¶. (4.7.2)
However, in practice we typically cannot obtain the entire population of data. Thus, em-
pirical risk minimization , which is minimizing the empirical risk in (4.7.1 ), is a practical
strategy for machine learning, with the hope of approximately minimizing the risk.
160 Linear Neural Networks for ClassiÔ¨Åcation
CovariateShift Correction
Assumethatwewanttoestimatesomedependency ùëÉ¬πùë¶jx¬∫forwhichwehavelabeleddata
¬πxùëñ,ùë¶ùëñ¬∫. Unfortunately, the observations xùëñare drawn from some sourcedistribution ùëû¬πx¬∫
rather than the target distribution ùëù¬πx¬∫. Fortunately, the dependency assumption means
that the conditional distribution does not change: ùëù¬πùë¶jx¬∫=ùëû¬πùë¶jx¬∫. If the source
distributionùëû¬πx¬∫is‚Äúwrong‚Äù, wecan correct forthatbyusing thefollowingsimple identity
in the risk:
¬π ¬π
ùëô¬πùëì¬πx¬∫,ùë¶¬∫ùëù¬πùë¶jx¬∫ùëù¬πx¬∫ùëëxùëëùë¶=¬π ¬π
ùëô¬πùëì¬πx¬∫,ùë¶¬∫ùëû¬πùë¶jx¬∫ùëû¬πx¬∫ùëù¬πx¬∫
ùëû¬πx¬∫ùëëxùëëùë¶.
(4.7.3)
In other words, we need to reweigh each data example by the ratio of the probability that it
would have been drawn from the correct distribution to that from the wrong one:
ùõΩùëñdef=ùëù¬πxùëñ¬∫
ùëû¬πxùëñ¬∫. (4.7.4)
Plugging in the weight ùõΩùëñfor each data example ¬πxùëñ,ùë¶ùëñ¬∫we can train our model using
weightedempirical risk minimization :
minimize
ùëì1
ùëõùëõ√ï
ùëñ=1ùõΩùëñùëô¬πùëì¬πxùëñ¬∫,ùë¶ùëñ¬∫. (4.7.5)
Alas, we do not know that ratio, so before we can do anything useful we need to estimate
it. Many methods are available, including some fancy operator-theoretic approaches that
attempt to recalibrate the expectation operator directly using a minimum-norm or a maxi-
mumentropyprinciple. Notethatforanysuchapproach,weneedsamplesdrawnfromboth
distributions‚Äîthe ‚Äútrue‚Äù ùëù, e.g., by access to test data, and the one used for generating the
training setùëû(the latter is trivially available). Note however, that we only need features
xùëù¬πx¬∫; we do not need to access labels ùë¶ùëù¬πùë¶¬∫.
In this case, there exists a very effective approach that will give almost as good results
as the original: namely, logistic regression, which is a special case of softmax regression
(seeSection 4.1 ) for binary classification. This is all that is needed to compute estimated
probability ratios. We learn a classifier to distinguish between data drawn from ùëù¬πx¬∫and
data drawn from ùëû¬πx¬∫. If it is impossible to distinguish between the two distributions then
it means that the associated instances are equally likely to come from either one of those
two distributions. On the other hand, any instances that can be well discriminated should
be significantly overweighted or underweighted accordingly.
Forsimplicity‚Äôssakeassumethatwehaveanequalnumberofinstancesfrombothdistribu-
tionsùëù¬πx¬∫andùëû¬πx¬∫, respectively. Now denote by ùëßlabels that are 1for data drawn from ùëù
and 1for data drawn from ùëû. Then the probability in a mixed dataset is given by
ùëÉ¬πùëß=1jx¬∫=ùëù¬πx¬∫
ùëù¬πx¬∫¬∏ùëû¬πx¬∫and henceùëÉ¬πùëß=1jx¬∫
ùëÉ¬πùëß= 1jx¬∫=ùëù¬πx¬∫
ùëû¬πx¬∫. (4.7.6)
Thus, if we use a logistic regression approach, where ùëÉ¬πùëß=1jx¬∫=1
1¬∏exp¬π ‚Ñé¬πx¬∫¬∫(‚Ñéis a
161 Environment and Distribution Shift
parametrized function), it follows that
ùõΩùëñ=1¬ù¬π1¬∏exp¬π ‚Ñé¬πxùëñ¬∫¬∫¬∫
exp¬π ‚Ñé¬πxùëñ¬∫¬∫¬ù¬π1¬∏exp¬π ‚Ñé¬πxùëñ¬∫¬∫¬∫=exp¬π‚Ñé¬πxùëñ¬∫¬∫. (4.7.7)
As a result, we need to solve two problems: the first, to distinguish between data drawn
frombothdistributions,andthenaweightedempiricalriskminimizationproblemin (4.7.5 )
where we weigh terms by ùõΩùëñ.
Now we are ready to describe a correction algorithm. Suppose that we have a training set
f¬πx1,ùë¶1¬∫,...,¬πxùëõ,ùë¶ùëõ¬∫gand an unlabeled test set fu1,...,uùëög. For covariate shift, we
assume that xùëñfor all 1ùëñùëõare drawn from some source distribution and uùëñfor all
1ùëñùëöare drawn from the target distribution. Here is a prototypical algorithm for
correcting covariate shift:
1.Createabinary-classificationtrainingset: f¬πx1, 1¬∫,...,¬πxùëõ, 1¬∫,¬πu1,1¬∫,...,¬πuùëö,1¬∫g.
2.Train a binary classifier using logistic regression to get the function ‚Ñé.
3.Weigh training data using ùõΩùëñ=exp¬π‚Ñé¬πxùëñ¬∫¬∫or betterùõΩùëñ=min¬πexp¬π‚Ñé¬πxùëñ¬∫¬∫,ùëê¬∫for some
constantùëê.
4.Use weights ùõΩùëñfor training onf¬πx1,ùë¶1¬∫,...,¬πxùëõ,ùë¶ùëõ¬∫gin(4.7.5 ).
Note that the above algorithm relies on a crucial assumption. For this scheme to work, we
need that each data example in the target (e.g., test time) distribution had nonzero proba-
bility of occurring at training time. If we find a point where ùëù¬πx¬∫>0butùëû¬πx¬∫=0, then
the corresponding importance weight should be infinity.
LabelShift Correction
Assume that we are dealing with a classification task with ùëòcategories. Using the same
notation in Section 4.7.3 ,ùëûandùëùare the source distribution (e.g., training time) and target
distribution (e.g., test time), respectively. Assume that the distribution of labels shifts over
time:ùëû¬πùë¶¬∫‚â†ùëù¬πùë¶¬∫, but the class-conditional distribution stays the same: ùëû¬πxjùë¶¬∫=ùëù¬πxj
ùë¶¬∫. If the source distribution ùëû¬πùë¶¬∫is ‚Äúwrong‚Äù, we can correct for that according to the
following identity in the risk as defined in (4.7.2 ):
¬π ¬π
ùëô¬πùëì¬πx¬∫,ùë¶¬∫ùëù¬πxjùë¶¬∫ùëù¬πùë¶¬∫ùëëxùëëùë¶=¬π ¬π
ùëô¬πùëì¬πx¬∫,ùë¶¬∫ùëû¬πxjùë¶¬∫ùëû¬πùë¶¬∫ùëù¬πùë¶¬∫
ùëû¬πùë¶¬∫ùëëxùëëùë¶.
(4.7.8)
Here, our importance weights will correspond to the label likelihood ratios:
ùõΩùëñdef=ùëù¬πùë¶ùëñ¬∫
ùëû¬πùë¶ùëñ¬∫. (4.7.9)
One nice thing about label shift is that if we have a reasonably good model on the source
distribution, then we can get consistent estimates of these weights without ever having to
deal with the ambient dimension. In deep learning, the inputs tend to be high-dimensional
objects like images, while the labels are often simpler objects like categories.
To estimate the target label distribution, we first take our reasonably good off-the-shelf
162 Linear Neural Networks for ClassiÔ¨Åcation
classifier (typically trained on the training data) and compute its ‚Äúconfusion‚Äù matrix using
thevalidationset(alsofromthetrainingdistribution). The confusionmatrix ,C,issimplya
ùëòùëòmatrix, whereeachcolumncorrespondstothelabelcategory(groundtruth)andeach
row corresponds to our model‚Äôs predicted category. Each cell‚Äôs value ùëêùëñùëóis the fraction of
total predictions on the validation set where the true label was ùëóand our model predicted
ùëñ.
Now, we cannot calculate the confusion matrix on the target data directly because we do
not get to see the labels for the examples that we see in the wild, unless we invest in a
complex real-time annotation pipeline. What we can do, however, is average all of our
model‚Äôs predictions at test time together, yielding the mean model outputs ùúá¬πÀÜy¬∫ 2Rùëò,
where theùëñthelementùúá¬πÀÜùë¶ùëñ¬∫is the fraction of the total predictions on the test set where our
model predicted ùëñ.
It turns out that under some mild conditions‚Äîif our classifier was reasonably accurate in
the first place, and if the target data contains only categories that we have seen before, and
if the label shift assumption holds in the first place (the strongest assumption here)‚Äîwe
can estimate the test set label distribution by solving a simple linear system
Cùëù¬πy¬∫=ùúá¬πÀÜy¬∫, (4.7.10)
because as an estimate√çùëò
ùëó=1ùëêùëñùëóùëù¬πùë¶ùëó¬∫=ùúá¬πÀÜùë¶ùëñ¬∫holds for all 1ùëñùëò, whereùëù¬πùë¶ùëó¬∫is
theùëóthelement of the ùëò-dimensional label distribution vector ùëù¬πy¬∫. If our classifier is
sufficiently accurate to begin with, then the confusion matrix Cwill be invertible, and we
get a solution ùëù¬πy¬∫=C 1ùúá¬πÀÜy¬∫.
Because we observe the labels on the source data, it is easy to estimate the distribution
ùëû¬πùë¶¬∫. Then, for any training example ùëñwith labelùë¶ùëñ, we can take the ratio of our esti-
matedùëù¬πùë¶ùëñ¬∫¬ùùëû¬πùë¶ùëñ¬∫to calculate the weight ùõΩùëñ, and plug this into weighted empirical risk
minimization in (4.7.5 ).
ConceptShift Correction
Conceptshiftismuchhardertofixinaprincipledmanner. Forinstance,inasituationwhere
suddenly the problem changes from distinguishing cats from dogs to one of distinguishing
white from black animals, it will be unreasonable to assume that we can do much better
than just collecting new labels and training from scratch. Fortunately, in practice, such
extreme shifts are rare. Instead, what usually happens is that the task keeps on changing
slowly. To make things more concrete, here are some examples:
Incomputationaladvertising,newproductsarelaunched,oldproductsbecomelesspop-
ular. This means that the distribution over ads and their popularity changes gradually
and any click-through rate predictor needs to change gradually with it.
Traffic camera lenses degrade gradually due to environmental wear, affecting image
quality progressively.
News content changes gradually (i.e., most of the news remains unchanged but new sto-
ries appear).
163 Environment and Distribution Shift
In such cases, we can use the same approach that we used for training networks to make
them adapt to the change in the data. In other words, we use the existing network weights
andsimplyperformafewupdatestepswiththenewdataratherthantrainingfromscratch.
4.7.4A Taxonomyof Learning Problems
Armed with knowledge about how to deal with changes in distributions, we can now con-
sider some other aspects of machine learning problem formulation.
BatchLearning
Inbatch learning , we have access to training features and labels f¬πx1,ùë¶1¬∫,...,¬πxùëõ,ùë¶ùëõ¬∫g,
whichweusetotrainamodel ùëì¬πx¬∫. Lateron,wedeploythismodeltoscorenewdata ¬πx,ùë¶¬∫
drawn from the same distribution. This is the default assumption for any of the problems
that we discuss here. For instance, we might train a cat detector based on lots of pictures
of cats and dogs. Once we have trained it, we ship it as part of a smart catdoor computer
vision system that lets only cats in. This is then installed in a customer‚Äôs home and is never
updated again (barring extreme circumstances).
Online Learning
Now imagine that the data ¬πxùëñ,ùë¶ùëñ¬∫arrives one sample at a time. More specifically, assume
that we first observe xùëñ, then we need to come up with an estimate ùëì¬πxùëñ¬∫. Only once
we have done this do we observe ùë¶ùëñand so receive a reward or incur a loss, given our
decision. Many real problems fall into this category. For example, we need to predict
tomorrow‚Äôs stock price, which allows us to trade based on that estimate and at the end
of the day we find out whether our estimate made us a profit. In other words, in online
learning , we have the following cycle where we are continuously improving our model
given new observations:
modelùëìùë° !dataxùë° !estimateùëìùë°¬πxùë°¬∫ !
observationùë¶ùë° !lossùëô¬πùë¶ùë°, ùëìùë°¬πxùë°¬∫¬∫ !modelùëìùë°¬∏1(4.7.11)
Bandits
Banditsare a special case of the problem above. While in most learning problems we have
a continuously parametrized function ùëìwhere we want to learn its parameters (e.g., a deep
network), in a banditproblem we only have a finite number of arms that we can pull, i.e.,
a finite number of actions that we can take. It is not very surprising that for this simpler
problem stronger theoretical guarantees in terms of optimality can be obtained. We list
it mainly since this problem is often (confusingly) treated as if it were a distinct learning
setting.
164 Linear Neural Networks for ClassiÔ¨Åcation
Control
In many cases the environment remembers what we did. Not necessarily in an adversarial
manner but it will just remember and the response will depend on what happened before.
For instance, a coffee boiler controller will observe different temperatures depending on
whether it was heating the boiler previously. PID (proportional-integral-derivative) con-
troller algorithms are a popular choice there. Likewise, a user‚Äôs behavior on a news site
willdependonwhatweshowedthempreviously(e.g.,theywillreadmostnewsonlyonce).
Many such algorithms form a model of the environment in which they act so as to make
their decisions appear less random. Recently, control theory (e.g., PID variants) has also
beenusedtoautomaticallytunehyperparameterstoachievebetterdisentanglingandrecon-
struction quality, and improve the diversity of generated text and the reconstruction quality
of generated images ( Shaoetal., 2020).
ReinforcementLearning
In the more general case of an environment with memory, we may encounter situations
where the environment is trying to cooperate with us (cooperative games, in particular
for non-zero-sum games), or others where the environment will try to win. Chess, Go,
Backgammon, or StarCraft are some of the cases in reinforcement learning . Likewise, we
mightwanttobuildagoodcontrollerforautonomouscars. Othercarsarelikelytorespond
to the autonomous car‚Äôs driving style in nontrivial ways, e.g., trying to avoid it, trying to
cause an accident, or trying to cooperate with it.
Considering the Environment
One key distinction between the different situations above is that a strategy that might have
worked throughout in the case of a stationary environment, might not work throughout in
anenvironmentthatcanadapt. Forinstance,anarbitrageopportunitydiscoveredbyatrader
is likely to disappear once it is exploited. The speed and manner at which the environment
changes determines to a large extent the type of algorithms that we can bring to bear. For
instance, if we know that things may only change slowly, we can force any estimate to
change only slowly, too. If we know that the environment might change instantaneously,
but only very infrequently, we can make allowances for that. These types of knowledge are
crucial for the aspiring data scientist in dealing with concept shift, i.e., when the problem
that is being solved can change over time.
4.7.5Fairness,Accountability,and Transparencyin Machine
Learning
Finally, it is important to remember that when you deploy machine learning systems you
are not merely optimizing a predictive model‚Äîyou are typically providing a tool that will
be used to (partially or fully) automate decisions. These technical systems can impact the
lives of individuals who are subject to the resulting decisions. The leap from considering
predictions to making decisions raises not only new technical questions, but also a slew of
165 Environment and Distribution Shift
ethicalquestionsthatmustbecarefullyconsidered. Ifwearedeployingamedicaldiagnos-
tic system, we need to know for which populations it may work and for which it may not.
Overlooking foreseeable risks to the welfare of a subpopulation could cause us to adminis-
ter inferior care. Moreover, once we contemplate decision-making systems, we must step
back and reconsider how we evaluate our technology. Among other consequences of this
changeofscope,wewillfindthat accuracy isseldomtherightmeasure. Forinstance,when
translatingpredictionsintoactions,wewilloftenwanttotakeintoaccountthepotentialcost
sensitivity of erring in various ways. If one way of misclassifying an image could be per-
ceived as a racial sleight of hand, while misclassification to a different category would be
harmless, then we might want to adjust our thresholds accordingly, accounting for societal
values in designing the decision-making protocol. We also want to be careful about how
prediction systems can lead to feedback loops. For example, consider predictive policing
systems, which allocate patrol officers to areas with high forecasted crime. It is easy to see
how a worrying pattern can emerge:
1.Neighborhoods with more crime get more patrols.
2.Consequently,morecrimesarediscoveredintheseneighborhoods,enteringthetraining
data available for future iterations.
3.Exposed to more positives, the model predicts yet more crime in these neighborhoods.
4.In the next iteration, the updated model targets the same neighborhood even more heav-
ily leading to yet more crimes discovered, etc.
Often,thevariousmechanismsbywhichamodel‚Äôspredictionsbecomecoupledtoitstrain-
ing data are unaccounted for in the modeling process. This can lead to what researchers
callrunaway feedback loops . Additionally, we want to be careful about whether we are
addressing the right problem in the first place. Predictive algorithms now play an outsize
role in mediating the dissemination of information. Should the news that an individual en-
counters be determined by the set of Facebook pages they have Liked? These are just a few
amongthemanypressingethicaldilemmasthatyoumightencounterinacareerinmachine
learning.
4.7.6Summary
In many cases training and test sets do not come from the same distribution. This is called
distribution shift. The risk is the expectation of the loss over the entire population of data
drawn from their true distribution. However, this entire population is usually unavailable.
Empirical risk is an average loss over the training data to approximate the risk. In practice,
we perform empirical risk minimization.
Under the corresponding assumptions, covariate and label shift can be detected and cor-
rected for at test time. Failure to account for this bias can become problematic at test time.
Insomecases,theenvironmentmayrememberautomatedactionsandrespondinsurprising
ways. We must account for this possibility when building models and continue to moni-
tor live systems, open to the possibility that our models and the environment will become
entangled in unanticipated ways.
166 Linear Neural Networks for ClassiÔ¨Åcation
1014.7.7Exercises
1.What could happen when we change the behavior of a search engine? What might the
users do? What about the advertisers?
2.Implement a covariate shift detector. Hint: build a classifier.
3.Implement a covariate shift corrector.
4.Besides distribution shift, what else could affect how the empirical risk approximates
the risk?
Discussions101.
5 Multilayer Perceptrons
Inthischapter,wewillintroduceyourfirsttruly deepnetwork. Thesimplestdeepnetworks
are called multilayerperceptrons , and they consist of multiple layers of neurons each fully
connected to those in the layer below (from which they receive input) and those above
(which they, in turn, influence). Although automatic differentiation significantly simplifies
the implementation of deep learning algorithms, we will dive deep into how these gradi-
ents are calculated in deep networks. Then we will be ready to discuss issues relating to
numerical stability and parameter initialization that are key to successfully training deep
networks. When we train such high-capacity models we run the risk of overfitting. Thus,
we will revisit regularization and generalization for deep networks. Throughout, we aim to
giveyouafirmgraspnotjustoftheconceptsbutalsoofthepracticeofusingdeepnetworks.
At the end of this chapter, we apply what we have introduced so far to a real case: house
price prediction. We punt matters relating to the computational performance, scalability,
and efficiency of our models to subsequent chapters.
5.1MultilayerPerceptrons
InSection4.1 ,weintroducedsoftmaxregression,implementingthealgorithmfromscratch
(Section4.4 )andusinghigh-levelAPIs( Section4.5 ). Thisallowedustotrainclassifiersca-
pableofrecognizing10categoriesofclothingfromlow-resolutionimages. Alongtheway,
we learned how to wrangle data, coerce our outputs into a valid probability distribution,
applyanappropriatelossfunction,andminimizeitwithrespecttoourmodel‚Äôsparameters.
Now that we have mastered these mechanics in the context of simple linear models, we
canlaunchourexplorationofdeepneuralnetworks,thecomparativelyrichclassofmodels
with which this book is primarily concerned.
%matplotlib inline
import torch
from d2l import torch asd2l
5.1.1Hidden Layers
We described affine transformations in Section 3.1.1 as linear transformations with added
bias. To begin, recall the model architecture corresponding to our softmax regression ex-
167
168 Multilayer Perceptrons
ample, illustrated in Fig. 4.1.1 . This model maps inputs directly to outputs via a single
affine transformation, followed by a softmax operation. If our labels truly were related to
the input data by a simple affine transformation, then this approach would be sufficient.
However, linearity (in affine transformations) is a strongassumption.
Limitations of Linear Models
For example, linearity implies the weakerassumption of monotonicity , i.e., that any in-
crease in our feature must either always cause an increase in our model‚Äôs output (if the
corresponding weight is positive), or always cause a decrease in our model‚Äôs output (if
the corresponding weight is negative). Sometimes that makes sense. For example, if we
weretryingtopredictwhetheranindividualwillrepayaloan,wemightreasonablyassume
that all other things being equal, an applicant with a higher income would always be more
likely to repay than one with a lower income. While monotonic, this relationship likely
is not linearly associated with the probability of repayment. An increase in income from
$0 to $50,000 likely corresponds to a bigger increase in likelihood of repayment than an
increase from $1 million to $1.05 million. One way to handle this might be to postprocess
our outcome such that linearity becomes more plausible, by using the logistic map (and
thus the logarithm of the probability of outcome).
Note that we can easily come up with examples that violate monotonicity. Say for example
that we want to predict health as a function of body temperature. For individuals with a
normal body temperature above 37¬∞C (98.6¬∞F), higher temperatures indicate greater risk.
However, if the body temperatures drops below 37¬∞C, lower temperatures indicate greater
risk! Again, we might resolve the problem with some clever preprocessing, such as using
the distance from 37¬∞C as a feature.
But what about classifying images of cats and dogs? Should increasing the intensity of the
pixelatlocation(13, 17)alwaysincrease(oralwaysdecrease)thelikelihoodthattheimage
depicts a dog? Reliance on a linear model corresponds to the implicit assumption that the
only requirement for differentiating cats and dogs is to assess the brightness of individual
pixels. This approach is doomed to fail in a world where inverting an image preserves the
category.
And yet despite the apparent absurdity of linearity here, as compared with our previous
examples, it is less obvious that we could address the problem with a simple preprocessing
fix. That is, because the significance of any pixel depends in complex ways on its context
(the values of the surrounding pixels). While there might exist a representation of our data
that would take into account the relevant interactions among our features, on top of which
a linear model wouldbe suitable, we simplydo not knowhow to calculate it byhand. With
deep neural networks, we used observational data to jointly learn both a representation via
hidden layers and a linear predictor that acts upon that representation.
This problem of nonlinearity has been studied for at least a century ( Fisher, 1925 ). For
instance, decision trees in their most basic form use a sequence of binary decisions to de-
cide upon class membership ( Quinlan, 1993 ). Likewise, kernel methods have been used
for many decades to model nonlinear dependencies ( Aronszajn, 1950 ). This has found its
169 Multilayer Perceptrons
way into nonparametric spline models ( Wahba, 1990 ) and kernel methods ( Sch√∂lkopf and
Smola, 2002 ). It is also something that the brain solves quite naturally. After all, neu-
rons feed into other neurons which, in turn, feed into other neurons again ( Ram√≥n y Cajal
and Azoulay, 1894 ). Consequently we have a sequence of relatively simple transforma-
tions.
IncorporatingHidden Layers
We can overcome the limitations of linear models by incorporating one or more hidden
layers. The easiest way to do this is to stack many fully connected layers on top of one
another. Eachlayerfeedsinto thelayeraboveit, untilwegenerateoutputs. Wecanthinkof
the firstùêø 1layers as our representation and the final layer as our linear predictor. This
architecture is commonly called a multilayer perceptron , often abbreviated as MLP(Fig.
5.1.1).
tFig. 5.1.1 An MLP with a hidden layer of Ô¨Åve hidden units.
This MLP has four inputs, three outputs, and its hidden layer contains five hidden units.
Sincetheinputlayerdoesnotinvolveanycalculations,producingoutputswiththisnetwork
requires implementing the computations for both the hidden and output layers; thus, the
number of layers in this MLP is two. Note that both layers are fully connected. Every
inputinfluenceseveryneuroninthehiddenlayer,andeachoftheseinturninfluencesevery
neuron in the output layer. Alas, we are not quite done yet.
FromLinear to Nonlinear
Asbefore,wedenotebythematrix X2Rùëõùëëaminibatchof ùëõexampleswhereeachexam-
ple hasùëëinputs (features). For a one-hidden-layer MLP whose hidden layer has ‚Ñéhidden
units, we denote by H2Rùëõ‚Ñéthe outputs of the hidden layer, which are hidden represen-
tations. Since the hidden and output layers are both fully connected, we have hidden-layer
weights W¬π1¬∫2Rùëë‚Ñéand biases b¬π1¬∫2R1‚Ñéand output-layer weights W¬π2¬∫2R‚Ñéùëûand
biases b¬π2¬∫2R1ùëû. This allows us to calculate the outputs O2Rùëõùëûof the one-hidden-
layer MLP as follows:
H=XW¬π1¬∫¬∏b¬π1¬∫,
O=HW¬π2¬∫¬∏b¬π2¬∫.(5.1.1)
Note that after adding the hidden layer, our model now requires us to track and update
additionalsetsofparameters. Sowhathavewegainedinexchange? Youmightbesurprised
170 Multilayer Perceptrons
tofindoutthat‚Äîinthemodeldefinedabove‚Äî wegainnothingforourtroubles ! Thereason
isplain. Thehiddenunitsabovearegivenbyanaffinefunctionoftheinputs,andtheoutputs
(pre-softmax) are just an affine function of the hidden units. An affine function of an affine
function is itself an affine function. Moreover, our linear model was already capable of
representing any affine function.
Toseethisformallywecanjustcollapseoutthehiddenlayerintheabovedefinition,yielding
an equivalent single-layer model with parameters W=W¬π1¬∫W¬π2¬∫andb=b¬π1¬∫W¬π2¬∫¬∏
b¬π2¬∫:
O=¬πXW¬π1¬∫¬∏b¬π1¬∫¬∫W¬π2¬∫¬∏b¬π2¬∫=XW¬π1¬∫W¬π2¬∫¬∏b¬π1¬∫W¬π2¬∫¬∏b¬π2¬∫=XW¬∏b.
(5.1.2)
In order to realize the potential of multilayer architectures, we need one more key ingre-
dient: a nonlinear activation function ùúéto be applied to each hidden unit following the
affine transformation. For instance, a popular choice is the ReLU (rectified linear unit) ac-
tivation function ( Nair and Hinton, 2010 )ùúé¬πùë•¬∫=max¬π0,ùë•¬∫operating on its arguments
elementwise. The outputs of activation functions ùúé¬π¬∫are called activations . In general,
with activation functions in place, it is no longer possible to collapse our MLP into a linear
model:
H=ùúé¬πXW¬π1¬∫¬∏b¬π1¬∫¬∫,
O=HW¬π2¬∫¬∏b¬π2¬∫.(5.1.3)
Since each row in Xcorresponds to an example in the minibatch, with some abuse of
notation, we define the nonlinearity ùúéto apply to its inputs in a rowwise fashion, i.e., one
example at a time. Note that we used the same notation for softmax when we denoted a
rowwise operation in Section 4.1.1 . Quite frequently the activation functions we use apply
notmerelyrowwisebutelementwise. Thatmeansthataftercomputingthelinearportionof
the layer, we can calculate each activation without looking at the values taken by the other
hidden units.
To build more general MLPs, we can continue stacking such hidden layers, e.g., H¬π1¬∫=
ùúé1¬πXW¬π1¬∫¬∏b¬π1¬∫¬∫andH¬π2¬∫=ùúé2¬πH¬π1¬∫W¬π2¬∫¬∏b¬π2¬∫¬∫,oneatopanother,yieldingevermore
expressive models.
UniversalApproximators
We know that the brain is capable of very sophisticated statistical analysis. As such, it is
worthasking,just howpowerful adeepnetworkcouldbe. Thisquestionhasbeenanswered
multiple times, e.g., in Cybenko ( 1989) in the context of MLPs, and in Micchelli ( 1984) in
the context of reproducing kernel Hilbert spaces in a way that could be seen as radial basis
function(RBF)networkswithasinglehiddenlayer. These(andrelatedresults)suggestthat
even with a single-hidden-layer network, given enough nodes (possibly absurdly many),
and the right set of weights, we can model any function. Actually learning that function
is the hard part, though. You might think of your neural network as being a bit like the
C programming language. The language, like any other modern language, is capable of
171 Multilayer Perceptrons
expressing any computable program. But actually coming up with a program that meets
your specifications is the hard part.
Moreover,justbecauseasingle-hidden-layernetwork canlearnanyfunctiondoesnotmean
that you should try to solve all of your problems with one. In fact, in this case kernel
methodsarewaymoreeffective,sincetheyarecapableofsolvingtheproblem exactlyeven
in infinite dimensional spaces ( Kimeldorf and Wahba, 1971 ,Sch√∂lkopf et al., 2001). In
fact, we can approximate many functions much more compactly by using deeper (rather
thanwider)networks( SimonyanandZisserman,2014 ). Wewilltouchuponmorerigorous
arguments in subsequent chapters.
5.1.2ActivationFunctions
Activation functions decide whether a neuron should be activated or not by calculating the
weighted sum and further adding bias to it. They are differentiable operators for trans-
forming input signals to outputs, while most of them add nonlinearity. Because activation
functionsarefundamentaltodeeplearning,let‚Äôsbrieflysurveysomecommonones.
ReLUFunction
The most popular choice, due to both simplicity of implementation and its good perfor-
mance on a variety of predictive tasks, is the rectifiedlinearunit (ReLU) (Nairand Hinton,
2010). ReLU provides a very simple nonlinear transformation. Given an element ùë•, the
function is defined as the maximum of that element and 0:
ReLU¬πùë•¬∫=max¬πùë•,0¬∫. (5.1.4)
Informally, the ReLU function retains only positive elements and discards all negative el-
ements by setting the corresponding activations to 0. To gain some intuition, we can plot
the function. As you can see, the activation function is piecewise linear.
x=torch .arange( -8.0,8.0,0.1, requires_grad =True )
y=torch .relu(x)
d2l.plot(x .detach(), y .detach(), 'x','relu(x) ', figsize =(5,2.5))
When the input is negative, the derivative of the ReLU function is 0, and when the input
is positive, the derivative of the ReLU function is 1. Note that the ReLU function is not
172 Multilayer Perceptrons
differentiable when the input takes value precisely equal to 0. In these cases, we default to
the left-hand-side derivative and say that the derivative is 0 when the input is 0. We can
getawaywiththisbecausetheinputmayneveractuallybezero(mathematicianswouldsay
that it is nondifferentiable on a set of measure zero). There is an old adage that if subtle
boundary conditions matter, we are probably doing ( real) mathematics, not engineering.
That conventional wisdom may apply here, or at least, the fact that we are not performing
constrained optimization ( Mangasarian, 1965 ,Rockafellar, 1970 ). We plot the derivative
of the ReLU function below.
y.backward(torch .ones_like(x), retain_graph =True )
d2l.plot(x .detach(), x .grad, 'x','grad of relu ', figsize =(5,2.5))
The reason for using ReLU is that its derivatives are particularly well behaved: either they
vanish or they just let the argument through. This makes optimization better behaved and
it mitigated the well-documented problem of vanishing gradients that plagued previous
versions of neural networks (more on this later).
Note that there are many variants to the ReLU function, including the parametrized ReLU
(pReLU) function ( Heet al., 2015). This variation adds a linear term to ReLU, so some
information still gets through, even when the argument is negative:
pReLU¬πùë•¬∫=max¬π0,ùë•¬∫¬∏ùõºmin¬π0,ùë•¬∫. (5.1.5)
Sigmoid Function
Thesigmoidfunction transforms those inputs whose values lie in the domain R, to outputs
that lie on the interval (0, 1). For that reason, the sigmoid is often called a squashingfunc-
tion: it squashes any input in the range (-inf, inf) to some value in the range (0, 1):
sigmoid¬πùë•¬∫=1
1¬∏exp¬π ùë•¬∫. (5.1.6)
In the earliest neural networks, scientists were interested in modeling biological neurons
that either fireordo not fire . Thus the pioneers of this field, going all the way back to
McCulloch and Pitts, the inventors of the artificial neuron, focused on thresholding units
(McCulloch and Pitts, 1943 ). A thresholding activation takes value 0 when its input is
below some threshold and value 1 when the input exceeds the threshold.
173 Multilayer Perceptrons
Whenattentionshiftedtogradient-basedlearning,thesigmoidfunctionwasanaturalchoice
because it is a smooth, differentiable approximation to a thresholding unit. Sigmoids are
still widely used as activation functions on the output units when we want to interpret the
outputsasprobabilitiesforbinaryclassificationproblems: youcanthinkofthesigmoidasa
specialcaseofthesoftmax. However, thesigmoidhaslargelybeenreplacedbythesimpler
and more easily trainable ReLU for most use in hidden layers. Much of this has to do with
the fact that the sigmoid poses challenges for optimization ( LeCunet al., 1998) since its
gradient vanishes for large positive andnegative arguments. This can lead to plateaus that
are difficult to escape from. Nonetheless sigmoids are important. In later chapters (e.g.,
Section 10.1 ) on recurrent neural networks, we will describe architectures that leverage
sigmoid units to control the flow of information across time.
Below, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid
function approaches a linear transformation.
y=torch .sigmoid(x)
d2l.plot(x .detach(), y .detach(), 'x','sigmoid(x) ', figsize =(5,2.5))
The derivative of the sigmoid function is given by the following equation:
ùëë
ùëëùë•sigmoid¬πùë•¬∫=exp¬π ùë•¬∫
¬π1¬∏exp¬π ùë•¬∫¬∫2=sigmoid¬πùë•¬∫¬π1 sigmoid¬πùë•¬∫¬∫. (5.1.7)
The derivative of the sigmoid function is plotted below. Note that when the input is 0, the
derivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from
0 in either direction, the derivative approaches 0.
# Clear out previous gradients
x.grad .data .zero_()
y.backward(torch .ones_like(x),retain_graph =True )
d2l.plot(x .detach(), x .grad, 'x','grad of sigmoid ', figsize =(5,2.5))
TanhFunction
Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs,
transforming them into elements on the interval between  1and1:
tanh¬πùë•¬∫=1 exp¬π 2ùë•¬∫
1¬∏exp¬π 2ùë•¬∫. (5.1.8)
174 Multilayer Perceptrons
We plot the tanh function below. Note that as input nears 0, the tanh function approaches a
linear transformation. Although the shape of the function is similar to that of the sigmoid
function, the tanh function exhibits point symmetry about the origin of the coordinate sys-
tem (Kalman and Kwasny, 1992 ).
y=torch .tanh(x)
d2l.plot(x .detach(), y .detach(), 'x','tanh(x) ', figsize =(5,2.5))
The derivative of the tanh function is:
ùëë
ùëëùë•tanh¬πùë•¬∫=1 tanh2¬πùë•¬∫. (5.1.9)
It is plotted below. As the input nears 0, the derivative of the tanh function approaches a
maximum of 1. And as we saw with the sigmoid function, as input moves away from 0 in
either direction, the derivative of the tanh function approaches 0.
# Clear out previous gradients
x.grad .data .zero_()
y.backward(torch .ones_like(x),retain_graph =True )
d2l.plot(x .detach(), x .grad, 'x','grad of tanh ', figsize =(5,2.5))
5.1.3Summaryand Discussion
We now know how to incorporate nonlinearities to build expressive multilayer neural net-
work architectures. As a side note, your knowledge already puts you in command of a sim-
ilar toolkit to a practitioner circa 1990. In some ways, you have an advantage over anyone
175 Multilayer Perceptrons
102working back then, because you can leverage powerful open-source deep learning frame-
works to build models rapidly, using only a few lines of code. Previously, training these
networks required researchers to code up layers and derivatives explicitly in C, Fortran, or
even Lisp (in the case of LeNet).
A secondary benefit is that ReLU is significantly more amenable to optimization than the
sigmoid or the tanh function. One could argue that this was one of the key innovations that
helped the resurgence of deep learning over the past decade. Note, though, that research in
activation functions has not stopped. For instance, the GELU (Gaussian error linear unit)
activationfunction ùë•Œ¶¬πùë•¬∫byHendrycksandGimpel( 2016)(Œ¶¬πùë•¬∫isthestandardGaussian
cumulative distribution function) and the Swish activation function ùúé¬πùë•¬∫=ùë•sigmoid¬πùõΩùë•¬∫
asproposedinRamachandran etal.(2017)canyieldbetteraccuracyinmanycases.
5.1.4Exercises
1.Show that adding layers to a lineardeep network, i.e., a network without nonlinearity
ùúécan never increase the expressive power of the network. Give an example where it
actively reduces it.
2.Compute the derivative of the pReLU activation function.
3.Compute the derivative of the Swish activation function ùë•sigmoid¬πùõΩùë•¬∫.
4.Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise
linear function.
5.Sigmoid and tanh are very similar.
1.Show that tanh¬πùë•¬∫¬∏1=2 sigmoid¬π2ùë•¬∫.
2.Prove that the function classes parametrized by both nonlinearities are identical.
Hint: affine layers have bias terms, too.
6.Assume that we have a nonlinearity that applies to one minibatch at a time, such as the
batch normalization ( Ioffe and Szegedy, 2015 ). What kinds of problems do you expect
this to cause?
7.Provide an example where the gradients vanish for the sigmoid activation function.
Discussions102.
176 Multilayer Perceptrons
5.2Implementation of MultilayerPerceptrons
Multilayerperceptrons(MLPs)arenotmuchmorecomplextoimplementthansimplelinear
models. The key conceptual difference is that we now concatenate multiple layers.
import torch
from torch import nn
from d2l import torch asd2l
5.2.1Implementation fromScratch
Let‚Äôs begin again by implementing such a network from scratch.
InitializingModel Parameters
Recall that Fashion-MNIST contains 10 classes, and that each image consists of a 28
28=784grid of grayscale pixel values. As before we will disregard the spatial structure
among the pixels for now, so we can think of this as a classification dataset with 784 input
featuresand10classes. Tobegin,wewillimplementanMLPwithonehiddenlayerand256
hidden units. Both the number of layersand their width are adjustable(theyare considered
hyperparameters). Typically, wechoosethelayerwidthstobedivisiblebylargerpowersof
2. This is computationally efficient due to the way memory is allocated and addressed in
hardware.
Again, we will represent our parameters with several tensors. Note that foreverylayer , we
must keep track of one weight matrix and one bias vector. As always, we allocate memory
for the gradients of the loss with respect to these parameters.
In the code below we use nn.Parameter to automatically register a class attribute as a
parameter to be tracked by autograd (Section 2.5 ).
class MLPScratch (d2l .Classifier):
def __init__ (self , num_inputs, num_outputs, num_hiddens, lr, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .W1=nn.Parameter(torch .randn(num_inputs, num_hiddens) *sigma)
self .b1=nn.Parameter(torch .zeros(num_hiddens))
self .W2=nn.Parameter(torch .randn(num_hiddens, num_outputs) *sigma)
self .b2=nn.Parameter(torch .zeros(num_outputs))
Model
To make sure we know how everything works, we will implement the ReLU activation
ourselves rather than invoking the built-in relufunction directly.
177 Implementation of Multilayer Perceptrons
def relu (X):
a=torch .zeros_like(X)
return torch .max(X, a)
Since we are disregarding spatial structure, we reshape each two-dimensional image into
a flat vector of length num_inputs . Finally, we implement our model with just a few lines
of code. Since we use the framework built-in autograd this is all that it takes.
@d2l .add_to_class(MLPScratch)
def forward (self , X):
X=X.reshape(( -1,self .num_inputs))
H=relu(torch .matmul(X, self .W1) +self .b1)
return torch .matmul(H, self .W2) +self .b2
Training
Fortunately, the training loop for MLPs is exactly the same as for softmax regression.
We define the model, data, and trainer, then finally invoke the fitmethod on model and
data.
model =MLPScratch(num_inputs =784, num_outputs =10, num_hiddens =256, lr =0.1)
data =d2l.FashionMNIST(batch_size =256)
trainer =d2l.Trainer(max_epochs =10)
trainer .fit(model, data)
5.2.2Concise Implementation
As you might expect, by relying on the high-level APIs, we can implement MLPs even
more concisely.
Model
Comparedwithourconciseimplementationofsoftmaxregressionimplementation( Section
4.5),theonlydifferenceisthatweadd twofullyconnectedlayerswherewepreviouslyadded
onlyone. The first is the hidden layer, the second is the output layer.
178 Multilayer Perceptrons
class MLP(d2l .Classifier):
def __init__ (self , num_outputs, num_hiddens, lr):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(nn .Flatten(), nn .LazyLinear(num_hiddens),
nn.ReLU(), nn .LazyLinear(num_outputs))
Previously, we defined forward methods for models to transform input using the model
parameters. These operations are essentially a pipeline: you take an input and apply a
transformation (e.g., matrix multiplication with weights followed by bias addition), then
repetitively use the output of the current transformation as input to the next transforma-
tion. However, you may have noticed that no forward method is defined here. In fact,
MLPinherits the forward method from the Moduleclass (Section 3.2.2 ) to simply invoke
self.net(X) (Xis input), which is now defined as a sequence of transformations via the
Sequential class. The Sequential class abstracts the forward process enabling us to fo-
cus on the transformations. We will further discuss how the Sequential class works in
Section 6.1.2 .
Training
The training loop is exactly the same as when we implemented softmax regression. This
modularity enables us to separate matters concerning the model architecture from orthog-
onal considerations.
model =MLP(num_outputs =10, num_hiddens =256, lr =0.1)
trainer .fit(model, data)
5.2.3Summary
Now that we have more practice in designing deep networks, the step from a single to mul-
tiple layers of deep networks does not pose such a significant challenge any longer. In
particular, we can reuse the training algorithm and data loader. Note, though, that imple-
menting MLPs from scratch is nonetheless messy: naming and keeping track of the model
parameters makes it difficult to extend models. For instance, imagine wanting to insert
anotherlayerbetweenlayers42and43. Thismightnowbelayer42b,unlesswearewilling
179 Implementation of Multilayer Perceptrons
103to perform sequential renaming. Moreover, if we implement the network from scratch, it
is much more difficult for the framework to perform meaningful performance optimiza-
tions.
Nonetheless, you have now reached the state of the art of the late 1980s when fully con-
nected deep networks were the method of choice for neural network modeling. Our next
conceptual step will be to consider images. Before we do so, we need to review a number
of statistical basics and details on how to compute models efficiently.
5.2.4Exercises
1.Change the number of hidden units num_hiddens and plot how its number affects the
accuracy of the model. What is the best value of this hyperparameter?
2.Try adding a hidden layer to see how it affects the results.
3.Whyisitabadideatoinsertahiddenlayerwithasingleneuron? Whatcouldgowrong?
4.Howdoeschangingthelearningratealteryourresults? Withallotherparametersfixed,
which learning rate gives you the best results? How does this relate to the number of
epochs?
5.Let‚Äôs optimize over all hyperparameters jointly, i.e., learning rate, number of epochs,
number of hidden layers, and number of hidden units per layer.
1.What is the best result you can get by optimizing over all of them?
2.Why it is much more challenging to deal with multiple hyperparameters?
3.Describe an efficient strategy for optimizing over multiple parameters jointly.
6.Compare the speed of the framework and the from-scratch implementation for a chal-
lenging problem. How does it change with the complexity of the network?
7.Measure the speed of tensor‚Äìmatrix multiplications for well-aligned and misaligned
matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and
1032.
1.How does this change between GPUs and CPUs?
2.Determine the memory bus width of your CPU and GPU.
8.Try out different activation functions. Which one works best?
9.Is there a difference between weight initializations of the network? Does it matter?
Discussions103.
180 Multilayer Perceptrons
5.3ForwardPropagation,BackwardPropagation,
and Computational Graphs
So far, we have trained our models with minibatch stochastic gradient descent. However,
when we implemented the algorithm, we only worried about the calculations involved in
forwardpropagation through the model. When it came time to calculate the gradients, we
justinvokedthebackpropagationfunctionprovidedbythedeeplearningframework.
The automatic calculation of gradients profoundly simplifies the implementation of deep
learning algorithms. Before automatic differentiation, even small changes to complicated
models required recalculating complicated derivatives by hand. Surprisingly often, aca-
demic papers had to allocate numerous pages to deriving update rules. While we must
continue to rely on automatic differentiation so we can focus on the interesting parts, you
ought to know how these gradients are calculated under the hood if you want to go beyond
a shallow understanding of deep learning.
In this section, we take a deep dive into the details of backward propagation (more com-
monly called backpropagation ). To convey some insight for both the techniques and their
implementations, we rely on some basic mathematics and computational graphs. To start,
we focus our exposition on a one-hidden-layer MLP with weight decay ( ‚Ñì2regularization,
to be described in subsequent chapters).
5.3.1ForwardPropagation
Forwardpropagation (orforwardpass )referstothecalculationandstorageofintermediate
variables(includingoutputs)foraneuralnetworkinorderfromtheinputlayertotheoutput
layer. We now work step-by-step through the mechanics of a neural network with one
hiddenlayer. ThismayseemtediousbutintheeternalwordsoffunkvirtuosoJamesBrown,
you must ‚Äúpay the cost to be the boss‚Äù.
Forthesakeofsimplicity,let‚Äôsassumethattheinputexampleis x2Rùëëandthatourhidden
layer does not include a bias term. Here the intermediate variable is:
z=W¬π1¬∫x, (5.3.1)
where W¬π1¬∫2R‚Ñéùëëis the weight parameter of the hidden layer. After running the inter-
mediate variable z2R‚Ñéthrough the activation function ùúôwe obtain our hidden activation
vector of length ‚Ñé:
h=ùúô¬πz¬∫. (5.3.2)
The hidden layer output his also an intermediate variable. Assuming that the parameters
of the output layer possess only a weight of W¬π2¬∫2Rùëû‚Ñé, we can obtain an output layer
variable with a vector of length ùëû:
o=W¬π2¬∫h. (5.3.3)
181 Forward Propagation, Backward Propagation, and Computational Graphs
Assuming that the loss function is ùëôand the example label is ùë¶, we can then calculate the
loss term for a single data example,
ùêø=ùëô¬πo,ùë¶¬∫. (5.3.4)
As we will see the definition of ‚Ñì2regularization to be introduced later, given the hyperpa-
rameterùúÜ, the regularization term is
ùë†=ùúÜ
2
kW¬π1¬∫k2
F¬∏kW¬π2¬∫k2
F
, (5.3.5)
where the Frobenius norm of the matrix is simply the ‚Ñì2norm applied after flattening the
matrixintoavector. Finally,themodel‚Äôsregularizedlossonagivendataexampleis:
ùêΩ=ùêø¬∏ùë†. (5.3.6)
We refer toùêΩas theobjectivefunction in the following discussion.
5.3.2Computational Graph of ForwardPropagation
Plottingcomputational graphs helps us visualize the dependencies of operators and vari-
ables within the calculation. Fig. 5.3.1 contains the graph associated with the simple net-
work described above, where squares denote variables and circles denote operators. The
lower-left corner signifies the input and the upper-right corner is the output. Notice that
the directions of the arrows (which illustrate data flow) are primarily rightward and up-
ward.
tFig. 5.3.1 Computational graph of forward propagation.
5.3.3Backpropagation
Backpropagation refers to the method of calculating the gradient of neural network param-
eters. In short, the method traverses the network in reverse order, from the output to the
input layer, according to the chain rule from calculus. The algorithm stores any interme-
diate variables (partial derivatives) required while calculating the gradient with respect to
some parameters. Assume that we have functions Y=ùëì¬πX¬∫andZ=ùëî¬πY¬∫, in which the
inputandtheoutput X,Y,Zaretensorsofarbitraryshapes. Byusingthechainrule,wecan
compute the derivative of Zwith respect to Xvia
ùúïZ
ùúïX=prodùúïZ
ùúïY,ùúïY
ùúïX
. (5.3.7)
Here we use the prod operator to multiply its arguments after the necessary operations,
such as transposition and swapping input positions, have been carried out. For vectors,
this is straightforward: it is simply matrix‚Äìmatrix multiplication. For higher dimensional
182 Multilayer Perceptrons
tensors, we use the appropriate counterpart. The operator prod hides all the notational
overhead.
Recall that the parameters of the simple network with one hidden layer, whose computa-
tional graph is in Fig. 5.3.1 , areW¬π1¬∫andW¬π2¬∫. The objective of backpropagation is to
calculate the gradients ùúïùêΩ¬ùùúïW¬π1¬∫andùúïùêΩ¬ùùúïW¬π2¬∫. To accomplish this, we apply the chain
rule and calculate, in turn, the gradient of each intermediate variable and parameter. The
orderofcalculationsarereversedrelativetothoseperformedinforwardpropagation, since
we need to start with the outcome of the computational graph and work our way towards
theparameters. Thefirststepistocalculatethegradientsoftheobjectivefunction ùêΩ=ùêø¬∏ùë†
with respect to the loss term ùêøand the regularization term ùë†:
ùúïùêΩ
ùúïùêø=1andùúïùêΩ
ùúïùë†=1. (5.3.8)
Next, we compute the gradient of the objective function with respect to variable of the
output layer oaccording to the chain rule:
ùúïùêΩ
ùúïo=prodùúïùêΩ
ùúïùêø,ùúïùêø
ùúïo
=ùúïùêø
ùúïo2Rùëû. (5.3.9)
Next, we calculate the gradients of the regularization term with respect to both parame-
ters:
ùúïùë†
ùúïW¬π1¬∫=ùúÜW¬π1¬∫andùúïùë†
ùúïW¬π2¬∫=ùúÜW¬π2¬∫. (5.3.10)
Now we are able to calculate the gradient ùúïùêΩ¬ùùúïW¬π2¬∫2Rùëû‚Ñéof the model parameters
closest to the output layer. Using the chain rule yields:
ùúïùêΩ
ùúïW¬π2¬∫=prodùúïùêΩ
ùúïo,ùúïo
ùúïW¬π2¬∫
¬∏prodùúïùêΩ
ùúïùë†,ùúïùë†
ùúïW¬π2¬∫
=ùúïùêΩ
ùúïoh>¬∏ùúÜW¬π2¬∫. (5.3.11)
To obtain the gradient with respect to W¬π1¬∫we need to continue backpropagation along
the output layer to the hidden layer. The gradient with respect to the hidden layer output
ùúïùêΩ¬ùùúïh2R‚Ñéis given by
ùúïùêΩ
ùúïh=prodùúïùêΩ
ùúïo,ùúïo
ùúïh
=W¬π2¬∫>ùúïùêΩ
ùúïo. (5.3.12)
Since the activation function ùúôapplies elementwise, calculating the gradient ùúïùêΩ¬ùùúïz2R‚Ñé
oftheintermediatevariable zrequiresthatweusetheelementwisemultiplicationoperator,
which we denote by :
ùúïùêΩ
ùúïz=prodùúïùêΩ
ùúïh,ùúïh
ùúïz
=ùúïùêΩ
ùúïhùúô0¬πz¬∫. (5.3.13)
Finally, we can obtain the gradient ùúïùêΩ¬ùùúïW¬π1¬∫2R‚Ñéùëëof the model parameters closest to
the input layer. According to the chain rule, we get
ùúïùêΩ
ùúïW¬π1¬∫=prodùúïùêΩ
ùúïz,ùúïz
ùúïW¬π1¬∫
¬∏prodùúïùêΩ
ùúïùë†,ùúïùë†
ùúïW¬π1¬∫
=ùúïùêΩ
ùúïzx>¬∏ùúÜW¬π1¬∫. (5.3.14)
183 Forward Propagation, Backward Propagation, and Computational Graphs
5.3.4TrainingNeuralNetworks
When training neural networks, forward and backward propagation depend on each other.
In particular, for forward propagation, we traverse the computational graph in the direc-
tion of dependencies and compute all the variables on its path. These are then used for
backpropagation where the compute order on the graph is reversed.
Taketheaforementionedsimplenetworkasanillustrativeexample. Ontheonehand,com-
puting the regularization term (5.3.5 )during forward propagation depends on the current
valuesofmodelparameters W¬π1¬∫andW¬π2¬∫. Theyaregivenbytheoptimizationalgorithm
according to backpropagation in the most recent iteration. On the other hand, the gradient
calculationfortheparameter (5.3.11 )duringbackpropagationdependsonthecurrentvalue
of the hidden layer output h, which is given by forward propagation.
Therefore when training neural networks, once model parameters are initialized, we alter-
nate forward propagation with backpropagation, updating model parameters using gradi-
ents given by backpropagation. Note that backpropagation reuses the stored intermediate
values from forward propagation to avoid duplicate calculations. One of the consequences
is that we need to retain the intermediate values until backpropagation is complete. This is
also one of the reasons whytraining requires significantly more memory than plain predic-
tion. Besides, thesizeofsuchintermediatevaluesisroughlyproportionaltothenumberof
network layers and the batch size. Thus, training deeper networks using larger batch sizes
more easily leads to out-of-memory errors.
5.3.5Summary
Forward propagation sequentially calculates and stores intermediate variables within the
computational graph defined by the neural network. It proceeds from the input to the out-
put layer. Backpropagation sequentially calculates and stores the gradients of intermediate
variables and parameters within the neural network in the reversed order. When training
deep learning models, forward propagation and backpropagation are interdependent, and
training requires significantly more memory than prediction.
5.3.6Exercises
1.Assume that the inputs Xto some scalar function ùëìareùëõùëömatrices. What is the
dimensionality of the gradient of ùëìwith respect to X?
2.Add a bias to the hidden layer of the model described in this section (you do not need
to include bias in the regularization term).
1.Draw the corresponding computational graph.
2.Derive the forward and backward propagation equations.
3.Computethememoryfootprintfortrainingandpredictioninthemodeldescribedinthis
section.
4.Assume that you want to compute second derivatives. What happens to the computa-
tional graph? How long do you expect the calculation to take?
184 Multilayer Perceptrons
1045.Assume that the computational graph is too large for your GPU.
1.Can you partition it over more than one GPU?
2.What are the advantages and disadvantages over training on a smaller minibatch?
Discussions104.
5.4NumericalStability and Initialization
Thus far, every model that we have implemented required that we initialize its parameters
according to some pre-specified distribution. Until now, we took the initialization scheme
for granted, glossing over the details of how these choices are made. You might have even
gotten the impression that these choices are not especially important. On the contrary, the
choice of initialization scheme plays a significant role in neural network learning, and it
can be crucial for maintaining numerical stability. Moreover, these choices can be tied up
in interesting ways with the choice of the nonlinear activation function. Which function
we choose and how we initialize parameters can determine how quickly our optimization
algorithm converges. Poor choices here can cause us to encounter exploding or vanishing
gradients while training. In this section, we delve into these topics in greater detail and
discuss some useful heuristics that you will find useful throughout your career in deep
learning.
%matplotlib inline
import torch
from d2l import torch asd2l
5.4.1Vanishingand Exploding Gradients
Consider a deep network with ùêølayers, input xand output o. With each layer ùëôdefined by
a transformation ùëìùëôparametrized by weights W¬πùëô¬∫, whose hidden layer output is h¬πùëô¬∫(let
h¬π0¬∫=x), our network can be expressed as:
h¬πùëô¬∫=ùëìùëô¬πh¬πùëô 1¬∫¬∫and thus o=ùëìùêøùëì1¬πx¬∫. (5.4.1)
If all the hidden layer output and the input are vectors, we can write the gradient of owith
respect to any set of parameters W¬πùëô¬∫as follows:
ùúïW¬πùëô¬∫o=ùúïh¬πùêø 1¬∫h¬πùêø¬∫
|        {z        }
M¬πùêø¬∫def=ùúïh¬πùëô¬∫h¬πùëô¬∏1¬∫
|      {z      }
M¬πùëô¬∏1¬∫def=ùúïW¬πùëô¬∫h¬πùëô¬∫
|     {z     }
v¬πùëô¬∫def=.
(5.4.2)
In other words, this gradient is the product of ùêø ùëômatrices M¬πùêø¬∫M¬πùëô¬∏1¬∫and the
gradientvector v¬πùëô¬∫. Thuswearesusceptibletothesameproblemsofnumericalunderflow
that often crop up when multiplying together too many probabilities. When dealing with
probabilities, a common trick is to switch into log-space, i.e., shifting pressure from the
185 Numerical Stability and Initialization
mantissatotheexponentofthenumericalrepresentation. Unfortunately,ourproblemabove
is more serious: initially the matrices M¬πùëô¬∫may have a wide variety of eigenvalues. They
might be small or large, and their product might be very large orvery small .
The risks posed by unstable gradients go beyond numerical representation. Gradients of
unpredictablemagnitudealsothreatenthestabilityofouroptimizationalgorithms. Wemay
be facing parameter updates that are either (i) excessively large, destroying our model (the
exploding gradient problem); or (ii) excessively small (the vanishing gradient problem),
rendering learning impossible as parameters hardly move on each update.
VanishingGradients
One frequent culprit causing the vanishing gradient problem is the choice of the activation
functionùúéthat is appended following each layer‚Äôs linear operations. Historically, the sig-
moidfunction 1¬ù¬π1¬∏exp¬π ùë•¬∫¬∫(introducedin Section5.1 )waspopularbecauseitresembles
a thresholding function. Since early artificial neural networks were inspired by biological
neural networks, the idea of neurons that fire either fullyornot at all (like biological neu-
rons) seemed appealing. Let‚Äôs take a closer look at the sigmoid to see why it can cause
vanishing gradients.
x=torch .arange( -8.0,8.0,0.1, requires_grad =True )
y=torch .sigmoid(x)
y.backward(torch .ones_like(x))
d2l.plot(x .detach() .numpy(), [y .detach() .numpy(), x .grad .numpy()],
legend =['sigmoid ','gradient '], figsize =(4.5,2.5))
As you can see, the sigmoid‚Äôs gradient vanishes both when its inputs are large and when
theyaresmall. Moreover, whenbackpropagatingthroughmanylayers, unlessweareinthe
Goldilocks zone, where the inputs to many of the sigmoids are close to zero, the gradients
of the overall product may vanish. When our network boasts many layers, unless we are
careful,thegradientwilllikelybecutoffatsomelayer. Indeed,thisproblemusedtoplague
deep network training. Consequently, ReLUs, which are more stable (but less neurally
plausible), have emerged as the default choice for practitioners.
186 Multilayer Perceptrons
ExplodingGradients
The opposite problem, when gradients explode, can be similarly vexing. To illustrate this
a bit better, we draw 100 Gaussian random matrices and multiply them with some initial
matrix. Forthescalethatwepicked(thechoiceofthevariance ùúé2=1),thematrixproduct
explodes. When this happens because of the initialization of a deep network, we have no
chance of getting a gradient descent optimizer to converge.
M=torch .normal( 0,1, size =(4,4))
print ('a single matrix \n',M)
for iinrange (100):
M=M@torch .normal( 0,1, size =(4,4))
print ('after multiplying 100 matrices \n', M)
a single matrix
tensor([[ -0.8755 ,-1.2171 ,1.3316 ,0.1357 ],
[0.4399 ,1.4073 ,-1.9131 ,-0.4608 ],
[-2.1420 ,0.3643 ,-0.5267 ,1.0277 ],
[-0.1734 ,-0.7549 ,2.3024 ,1.3085 ]])
after multiplying 100 matrices
tensor([[ -2.9185e+23 ,1.3915e+25 ,-1.1865e+25 ,1.4354e+24 ],
[4.9142e+23 ,-2.3430e+25 ,1.9979e+25 ,-2.4169e+24 ],
[2.6578e+23 ,-1.2672e+25 ,1.0805e+25 ,-1.3072e+24 ],
[-5.2223e+23 ,2.4899e+25 ,-2.1231e+25 ,2.5684e+24 ]])
Breakingthe Symmetry
Another problem in neural network design is the symmetry inherent in their parametriza-
tion. Assume that we have a simple MLP with one hidden layer and two units. In this case,
we could permute the weights W¬π1¬∫of the first layer and likewise permute the weights of
the output layer to obtain the same function. There is nothing special differentiating the
first and second hidden units. In other words, we have permutation symmetry among the
hidden units of each layer.
This is more than just a theoretical nuisance. Consider the aforementioned one-hidden-
layer MLP with two hidden units. For illustration, suppose that the output layer transforms
thetwohiddenunitsintoonlyoneoutputunit. Imaginewhatwouldhappenifweinitialized
alltheparameters ofthe hiddenlayeras W¬π1¬∫=ùëêforsomeconstant ùëê. In thiscase, during
forwardpropagationeitherhiddenunittakesthesameinputsandparametersproducingthe
sameactivationwhichisfedtotheoutputunit. Duringbackpropagation,differentiatingthe
output unit with respect to parameters W¬π1¬∫gives a gradient all of whose elements take
thesamevalue. Thus,aftergradient-basediteration(e.g.,minibatchstochasticgradientde-
scent),alltheelementsof W¬π1¬∫stilltakethesamevalue. Suchiterationswouldnever break
the symmetry on their own and we might never be able to realize the network‚Äôs expressive
power. The hidden layer would behave as if it had only a single unit. Note that while mini-
batch stochastic gradient descent would not break this symmetry, dropout regularization
(to be introduced later) would!
187 Numerical Stability and Initialization
5.4.2ParameterInitialization
One way of addressing‚Äîor at least mitigating‚Äîthe issues raised above is through care-
ful initialization. As we will see later, additional care during optimization and suitable
regularization can further enhance stability.
DefaultInitialization
In the previous sections, e.g., in Section 3.5 , we used a normal distribution to initialize the
values of our weights. If we do not specify the initialization method, the framework will
useadefaultrandominitializationmethod,whichoftenworkswellinpracticeformoderate
problem sizes.
XavierInitialization
Let‚Äôs look at the scale distribution of an output ùëúùëñfor some fully connected layer without
nonlinearities . Withùëõininputsùë•ùëóand their associated weights ùë§ùëñùëófor this layer, an output
is given by
ùëúùëñ=ùëõin√ï
ùëó=1ùë§ùëñùëóùë•ùëó. (5.4.3)
Theweights ùë§ùëñùëóarealldrawnindependentlyfromthesamedistribution. Furthermore,let‚Äôs
assume that this distribution has zero mean and variance ùúé2. Note that this does not mean
that the distribution has to be Gaussian, just that the mean and variance need to exist. For
now,let‚Äôsassumethattheinputstothelayer ùë•ùëóalsohavezeromeanandvariance ùõæ2andthat
they are independent of ùë§ùëñùëóand independent of each other. In this case, we can compute
the mean ofùëúùëñ:
ùê∏¬ªùëúùëñ¬º=ùëõin√ï
ùëó=1ùê∏¬ªùë§ùëñùëóùë•ùëó¬º
=ùëõin√ï
ùëó=1ùê∏¬ªùë§ùëñùëó¬ºùê∏¬ªùë•ùëó¬º
=0,(5.4.4)
and the variance:
Var¬ªùëúùëñ¬º=ùê∏¬ªùëú2
ùëñ¬º ¬πùê∏¬ªùëúùëñ¬º¬∫2
=ùëõin√ï
ùëó=1ùê∏¬ªùë§2
ùëñùëóùë•2
ùëó¬º 0
=ùëõin√ï
ùëó=1ùê∏¬ªùë§2
ùëñùëó¬ºùê∏¬ªùë•2
ùëó¬º
=ùëõinùúé2ùõæ2.(5.4.5)
One way to keep the variance fixed is to set ùëõinùúé2=1. Now consider backpropagation.
There we face a similar problem, albeit with gradients being propagated from the layers
188 Multilayer Perceptrons
closer to the output. Using the same reasoning as for forward propagation, we see that the
gradients‚Äô variance can blow up unless ùëõoutùúé2=1, whereùëõoutis the number of outputs
of this layer. This leaves us in a dilemma: we cannot possibly satisfy both conditions
simultaneously. Instead, we simply try to satisfy:
1
2¬πùëõin¬∏ùëõout¬∫ùúé2=1or equivalently ùúé=r
2
ùëõin¬∏ùëõout. (5.4.6)
This is the reasoning underlying the now-standard and practically beneficial Xavierinitial-
ization,namedafterthefirstauthorofitscreators( GlorotandBengio,2010 ). Typically,the
XavierinitializationsamplesweightsfromaGaussiandistributionwithzeromeanandvari-
anceùúé2=2
ùëõin¬∏ùëõout. We can also adapt this to choose the variance when sampling weights
from a uniform distribution. Note that the uniform distribution ùëà¬π ùëé,ùëé¬∫has varianceùëé2
3.
Pluggingùëé2
3into our condition on ùúé2prompts us to initialize according to
ùëà¬©¬≠
¬´ s
6
ùëõin¬∏ùëõout,s
6
ùëõin¬∏ùëõout¬™¬Æ
¬¨. (5.4.7)
Though the assumption for nonexistence of nonlinearities in the above mathematical rea-
soning can be easily violated in neural networks, the Xavier initialization method turns out
to work well in practice.
Beyond
The reasoning above barely scratches the surface of modern approaches to parameter ini-
tialization. A deep learning framework often implements over a dozen different heuristics.
Moreover, parameter initialization continues to be a hot area of fundamental research in
deep learning. Among these are heuristics specialized for tied (shared) parameters, super-
resolution, sequence models, and other situations. For instance, Xiao etal.(2018) demon-
strated the possibility of training 10,000-layer neural networks without architectural tricks
by using a carefully-designed initialization method.
If the topic interests you we suggest a deep dive into this module‚Äôs offerings, reading the
papersthatproposedandanalyzedeachheuristic,andthenexploringthelatestpublications
on the topic. Perhaps you will stumble across or even invent a clever idea and contribute
an implementation to deep learning frameworks.
5.4.3Summary
Vanishing and exploding gradients are common issues in deep networks. Great care in
parameter initialization is required to ensure that gradients and parameters remain well
controlled. Initializationheuristicsareneededtoensurethattheinitialgradientsareneither
too large nor too small. Random initialization is key to ensuring that symmetry is broken
before optimization. Xavier initialization suggests that, for each layer, variance of any
outputisnotaffectedbythenumberofinputs,andvarianceofanygradientisnotaffectedby
thenumberofoutputs. ReLUactivationfunctionsmitigatethevanishinggradientproblem.
This can accelerate convergence.
189 Generalization in Deep Learning
1055.4.4Exercises
1.Can you design other cases where a neural network might exhibit symmetry that needs
breaking, besides the permutation symmetry in an MLP‚Äôs layers?
2.Can we initialize all weight parameters in linear regression or in softmax regression to
the same value?
3.Look up analytic bounds on the eigenvalues of the product of two matrices. What does
this tell you about ensuring that gradients are well conditioned?
4.If we know that some terms diverge, can we fix this after the fact? Look at the paper on
layerwise adaptive rate scaling for inspiration ( Youetal., 2017).
Discussions105.
5.5Generalizationin Deep Learning
InChapter 3 andChapter 4 , we tackled regression and classification problems by fitting
linear models to training data. In both cases, we provided practical algorithms for finding
the parameters that maximized the likelihood of the observed training labels. And then,
towards the end of each chapter, we recalled that fitting the training data was only an in-
termediate goal. Our real quest all along was to discover general patterns on the basis
of which we can make accurate predictions even on new examples drawn from the same
underlying population. Machine learning researchers are consumers of optimization algo-
rithms. Sometimes, we must even develop new optimization algorithms. But at the end
of the day, optimization is merely a means to an end. At its core, machine learning is a
statistical discipline and we wish to optimize training loss only insofar as some statistical
principle (known or unknown) leads the resulting models to generalize beyond the training
set.
On the bright side, it turns out that deep neural networks trained by stochastic gradient de-
scent generalize remarkably well across myriad prediction problems, spanning computer
vision; natural language processing; time series data; recommender systems; electronic
health records; protein folding; value function approximation in video games and board
games; and numerous other domains. On the downside, if you were looking for a straight-
forward account of either the optimization story (why we can fit them to training data) or
thegeneralizationstory(whytheresultingmodelsgeneralizetounseenexamples),thenyou
might want to pour yourself a drink. While our procedures for optimizing linear models
and the statistical properties of the solutions are both described well by a comprehensive
body of theory, our understanding of deep learning still resembles the wild west on both
fronts.
Both the theory and practice of deep learning are rapidly evolving, with theorists adopting
new strategies to explain what‚Äôs going on, even as practitioners continue to innovate at
190 Multilayer Perceptrons
a blistering pace, building arsenals of heuristics for training deep networks and a body of
intuitionsandfolkknowledgethatprovideguidancefordecidingwhichtechniquestoapply
in which situations.
The summary of the present moment is that the theory of deep learning has produced
promising lines of attack and scattered fascinating results, but still appears far from a com-
prehensive account of both (i) why we are able to optimize neural networks and (ii) how
modelslearnedbygradientdescentmanagetogeneralizesowell,evenonhigh-dimensional
tasks. However,inpractice,(i)isseldomaproblem(wecanalwaysfindparametersthatwill
fit all of our training data) and thus understanding generalization is far the bigger problem.
On the other hand, even absent the comfort of a coherent scientific theory, practitioners
have developed a large collection of techniques that may help you to produce models that
generalize well in practice. While no pithy summary can possibly do justice to the vast
topic of generalization in deep learning, and while the overall state of research is far from
resolved, we hope, in this section, to present a broad overview of the state of research and
practice.
5.5.1RevisitingOverfittingand Regularization
According to the ‚Äúno free lunch‚Äù theorem of Wolpert and Macready ( 1995), any learn-
ing algorithm generalizes better on data with certain distributions, and worse with other
distributions. Thus, given a finite training set, a model relies on certain assumptions: to
achieve human-level performance it may be useful to identify inductive biases that reflect
how humans think about the world. Such inductive biases show preferences for solutions
with certain properties. For example, a deep MLP has an inductive bias towards building
up a complicated function by the composition of simpler functions.
With machine learning models encoding inductive biases, our approach to training them
typicallyconsistsoftwophases: (i)fitthetrainingdata; and(ii)estimatethe generalization
error(thetrueerrorontheunderlyingpopulation)byevaluatingthemodelonholdoutdata.
The difference between our fit on the training data and our fit on the test data is called the
generalizationgap andwhenthisislarge,wesaythatourmodels overfittothetrainingdata.
In extreme cases of overfitting, we might exactly fit the training data, even when the test
errorremainssignificant. Andintheclassicalview,theinterpretationisthatourmodelsare
toocomplex, requiringthatweeithershrinkthenumberoffeatures, thenumberofnonzero
parameters learned, or the size of the parameters as quantified. Recall the plot of model
complexity compared with loss ( Fig. 3.6.1 ) fromSection 3.6 .
However deep learning complicates this picture in counterintuitive ways. First, for classifi-
cation problems, our models are typically expressive enough to perfectly fit every training
example, even in datasets consisting of millions ( Zhanget al., 2021). In the classical pic-
ture, we might think that this setting lies on the far right extreme of the model complexity
axis, and that any improvements in generalization error must come by way of regulariza-
tion,eitherbyreducingthecomplexityofthemodelclass,orbyapplyingapenalty,severely
constraining the set of values that our parameters might take. But that is where things start
to get weird.
191 Generalization in Deep Learning
Strangely, for many deep learning tasks (e.g., image recognition and text classification) we
are typically choosing among model architectures, all of which can achieve arbitrarily low
training loss (and zero training error). Because all models under consideration achieve
zero training error, theonlyavenueforfurthergainsistoreduceoverfitting . Even stranger,
it is often the case that despite fitting the training data perfectly, we can actually reduce
the generalization error further by making the model even more expressive , e.g., adding
layers, nodes, or training for a larger number of epochs. Stranger yet, the pattern relating
thegeneralizationgaptothe complexity ofthemodel(ascaptured,forexample,inthedepth
or width of the networks) can be non-monotonic, with greater complexity hurting at first
but subsequently helping in a so-called ‚Äúdouble-descent‚Äù pattern ( Nakkiran et al., 2021).
Thus the deep learning practitioner possesses a bag of tricks, some of which seemingly
restrict the model in some fashion and others that seemingly make it even more expressive,
and all of which, in some sense, are applied to mitigate overfitting.
Complicating things even further, while the guarantees provided by classical learning the-
ory can be conservative even for classical models, they appear powerless to explain why
it is that deep neural networks generalize in the first place. Because deep neural networks
are capable of fitting arbitrary labels even for large datasets, and despite the use of famil-
iar methods such as ‚Ñì2regularization, traditional complexity-based generalization bounds,
e.g., those based on the VC dimension or Rademacher complexity of a hypothesis class
cannot explain why neural networks generalize.
5.5.2InspirationfromNonparametrics
Approaching deep learning for the first time, it is tempting to think of them as parametric
models. Afterall,themodels dohavemillionsofparameters. Whenweupdatethemodels,
we update their parameters. When we save the models, we write their parameters to disk.
However, mathematics and computer science are riddled with counterintuitive changes of
perspective, and surprising isomorphisms between seemingly different problems. While
neural networks clearly haveparameters, in some ways it can be more fruitful to think of
them as behaving like nonparametric models. So what precisely makes a model nonpara-
metric? While the name covers a diverse set of approaches, one common theme is that
nonparametric methods tend to have a level of complexity that grows as the amount of
available data grows.
Perhapsthesimplestexampleofanonparametricmodelisthe ùëò-nearestneighboralgorithm
(we will cover more nonparametric models later, for example in Section 11.2 ). Here, at
training time, the learner simply memorizes the dataset. Then, at prediction time, when
confronted with a new point x, the learner looks up the ùëònearest neighbors (the ùëòpoints
x0
ùëñthat minimize some distance ùëë¬πx,x0
ùëñ¬∫). Whenùëò=1, this algorithm is called 1-nearest
neighbors, and the algorithm will always achieve a training error of zero. That however,
does not mean that the algorithm will not generalize. In fact, it turns out that under some
mild conditions, the 1-nearest neighbor algorithm is consistent (eventually converging to
the optimal predictor).
Note that 1-nearest neighbor requires that we specify some distance function ùëë, or equiva-
lently, that we specify some vector-valued basis function ùúô¬πx¬∫for featurizing our data. For
192 Multilayer Perceptrons
any choice of the distance metric, we will achieve zero training error and eventually reach
an optimal predictor, but different distance metrics ùëëencode different inductive biases and
with a finite amount of available data will yield different predictors. Different choices of
the distance metric ùëërepresent different assumptions about the underlying patterns and the
performanceofthedifferentpredictorswilldependonhowcompatibletheassumptionsare
with the observed data.
Inasense, becauseneuralnetworksareover-parametrized, possessingmanymoreparame-
tersthanareneededtofitthetrainingdata, theytendto interpolate thetrainingdata(fitting
it perfectly) and thus behave, in some ways, more like nonparametric models. More re-
cent theoretical research has established deep connection between large neural networks
and nonparametric methods, notably kernel methods. In particular, Jacot et al.(2018)
demonstrated that in the limit, as multilayer perceptrons with randomly initialized weights
grow infinitely wide, they become equivalent to (nonparametric) kernel methods for a spe-
cific choice of the kernel function (essentially, a distance function), which they call the
neural tangent kernel. While current neural tangent kernel models may not fully explain
the behavior of modern deep networks, their success as an analytical tool underscores the
usefulnessofnonparametricmodelingforunderstandingthebehaviorofover-parametrized
deep networks.
5.5.3EarlyStopping
While deep neural networks are capable of fitting arbitrary labels, even when labels are
assigned incorrectly or randomly ( Zhanget al., 2021), this capability only emerges over
many iterations of training. A new line of work ( Rolnicket al., 2017) has revealed that
in the setting of label noise, neural networks tend to fit cleanly labeled data first and only
subsequently to interpolate the mislabeled data. Moreover, it has been established that this
phenomenon translates directly into a guarantee on generalization: whenever a model has
fitted the cleanly labeled data but not randomly labeled examples included in the training
set, it has in fact generalized ( Gargetal., 2021).
Togetherthesefindingshelptomotivate earlystopping ,aclassictechniqueforregularizing
deepneuralnetworks. Here,ratherthandirectlyconstrainingthevaluesoftheweights,one
constrains the number of epochs of training. The most common way to determine the
stopping criterion is to monitor validation error throughout training (typically by checking
once after each epoch) and to cut off training when the validation error has not decreased
by more than some small amount ùúñfor some number of epochs. This is sometimes called a
patiencecriterion . As well as the potential to lead to better generalization in the setting of
noisylabels,anotherbenefitofearlystoppingisthetimesaved. Oncethepatiencecriterion
is met, one can terminate training. For large models that might require days of training
simultaneously across eight or more GPUs, well-tuned early stopping can save researchers
days of time and can save their employers many thousands of dollars.
Notably, when there is no label noise and datasets are realizable (the classes are truly sep-
arable, e.g., distinguishing cats from dogs), early stopping tends not to lead to significant
improvements in generalization. On the other hand, when there is label noise, or intrinsic
193 Generalization in Deep Learning
variabilityinthelabel(e.g., predictingmortalityamongpatients), earlystoppingiscrucial.
Training models until they interpolate noisy data is typically a bad idea.
5.5.4Classical RegularizationMethodsforDeep Networks
InChapter 3 , we described several classical regularization techniques for constraining the
complexity of our models. In particular, Section 3.7 introduced a method called weight
decay,whichconsistsofaddingaregularizationtermtothelossfunctioninordertopenalize
large values of the weights. Depending on which weight norm is penalized this technique
is known either as ridge regularization (for ‚Ñì2penalty) or lasso regularization (for an ‚Ñì1
penalty). In the classical analysis of these regularizers, they are considered as sufficiently
restrictiveonthevaluesthattheweightscantaketopreventthemodelfromfittingarbitrary
labels.
In deep learning implementations, weight decay remains a popular tool. However, re-
searchershavenotedthattypicalstrengthsof ‚Ñì2regularizationareinsufficienttopreventthe
networksfrominterpolatingthedata( Zhangetal.,2021)andthusthebenefitsifinterpreted
as regularization might only make sense in combination with the early stopping criterion.
Absent early stopping, it is possible that just like the number of layers or number of nodes
(indeeplearning)orthedistancemetric(in1-nearestneighbor),thesemethodsmayleadto
better generalization not because they meaningfully constrain the power of the neural net-
work but rather because they somehow encode inductive biases that are better compatible
with the patterns found in datasets of interests. Thus, classical regularizers remain popular
in deep learning implementations, even if the theoretical rationale for their efficacy may be
radically different.
Notably, deep learning researchers have also built on techniques first popularized in classi-
calregularizationcontexts,suchasaddingnoisetomodelinputs. Inthenextsectionwewill
introduce the famous dropout technique (invented by Srivastava et al.(2014)), which has
become a mainstay of deep learning, even as the theoretical basis for its efficacy remains
similarly mysterious.
5.5.5Summary
Unlike classical linear models, which tend to have fewer parameters than examples, deep
networks tend to be over-parametrized, and for most tasks are capable of perfectly fitting
thetrainingset. This interpolationregime challengesmanyhardfast-heldintuitions. Func-
tionally, neural networks look like parametric models. But thinking of them as nonpara-
metric models can sometimes be a more reliable source of intuition. Because it is often
the case that all deep networks under consideration are capable of fitting all of the train-
ing labels, nearly all gains must come by mitigating overfitting (closing the generalization
gap). Paradoxically, the interventions that reduce the generalization gap sometimes appear
to increase model complexity and at other times appear to decrease complexity. However,
these methods seldom decrease complexity sufficiently for classical theory to explain the
generalization of deep networks, and why certain choices lead to improved generalization
remains for the most part a massive open question despite the concerted efforts of many
brilliant researchers.
194 Multilayer Perceptrons
1065.5.6Exercises
1.Inwhatsensedotraditionalcomplexity-basedmeasuresfailtoaccountforgeneralization
of deep neural networks?
2.Why might earlystopping be considered a regularization technique?
3.How do researchers typically determine the stopping criterion?
4.What important factor seems to differentiate cases when early stopping leads to big
improvements in generalization?
5.Beyond generalization, describe another benefit of early stopping.
Discussions106.
5.6Dropout
Let‚Äôs think briefly about what we expect from a good predictive model. We want it to pe-
form well on unseen data. Classical generalization theory suggests that to close the gap
between train and test performance, we should aim for a simple model. Simplicity can
come in the form of a small number of dimensions. We explored this when discussing the
monomial basis functions of linear models in Section 3.6 . Additionally, as we saw when
discussingweightdecay( ‚Ñì2regularization)in Section3.7 ,the(inverse)normoftheparam-
eters also represents a useful measure of simplicity. Another useful notion of simplicity
is smoothness, i.e., that the function should not be sensitive to small changes to its inputs.
For instance, when we classify images, we would expect that adding some random noise to
the pixels should be mostly harmless.
Bishop( 1995)formalizedthisideawhenheprovedthattrainingwithinputnoiseisequiva-
lent to Tikhonov regularization. This work drew a clear mathematical connection between
the requirement that a function be smooth (and thus simple), and the requirement that it be
resilient to perturbations in the input.
Then, Srivastava etal.(2014)developedacleverideaforhowtoapplyBishop‚Äôsideatothe
internal layers of a network, too. Their idea, called dropout, involves injecting noise while
computing each internal layer during forward propagation, and it has become a standard
technique for training neural networks. The method is called dropout because we literally
drop out some neurons during training. Throughout training, on each iteration, standard
dropout consists of zeroing out some fraction of the nodes in each layer before calculating
the subsequent layer.
To be clear, we are imposing our own narrative with the link to Bishop. The original pa-
per on dropout offers intuition through a surprising analogy to sexual reproduction. The
authors argue that neural network overfitting is characterized by a state in which each layer
195 Dropout
relies on a specific pattern of activations in the previous layer, calling this condition co-
adaptation . Dropout, they claim, breaks up co-adaptation just as sexual reproduction is
argued to break up co-adapted genes. While such an justification of this theory is cer-
tainlyupfordebate, thedropouttechniqueitselfhasprovedenduring, andvariousformsof
dropout are implemented in most deep learning libraries.
Thekeychallengeishowto injectthisnoise. Oneidea istoinjectitin an unbiased manner
sothattheexpectedvalueofeachlayer‚Äîwhilefixingtheothers‚Äîequalsthevalueitwould
havetakenabsentnoise. InBishop‚Äôswork,headdedGaussiannoisetotheinputstoalinear
model. At each training iteration, he added noise sampled from a distribution with mean
zeroùúñN¬π 0,ùúé2¬∫to the input x, yielding a perturbed point x0=x¬∏ùúñ. In expectation,
ùê∏¬ªx0¬º=x.
In standard dropout regularization, one zeros out some fraction of the nodes in each layer
andthendebiases eachlayerbynormalizingbythefractionofnodesthatwereretained(not
dropped out). In other words, with dropoutprobability ùëù, each intermediate activation ‚Ñéis
replaced by a random variable ‚Ñé0as follows:
‚Ñé0=(
0with probability ùëù
‚Ñé
1 ùëùotherwise(5.6.1)
By design, the expectation remains unchanged, i.e., ùê∏¬ª‚Ñé0¬º=‚Ñé.
import torch
from torch import nn
from d2l import torch asd2l
5.6.1Dropoutin Practice
Recall the MLP with a hidden layer and five hidden units from Fig. 5.1.1 . When we apply
dropout to a hidden layer, zeroing out each hidden unit with probability ùëù, the result can
be viewed as a network containing only a subset of the original neurons. In Fig. 5.6.1 ,‚Ñé2
and‚Ñé5are removed. Consequently, the calculation of the outputs no longer depends on ‚Ñé2
or‚Ñé5andtheirrespectivegradientalsovanisheswhenperformingbackpropagation. Inthis
way, the calculation of the output layer cannot be overly dependent on any one element of
‚Ñé1,...,‚Ñé 5.
tFig. 5.6.1 MLP before and after dropout.
196 Multilayer Perceptrons
Typically, we disable dropout at test time. Given a trained model and a new example,
we do not drop out any nodes and thus do not need to normalize. However, there are
someexceptions: someresearchersusedropoutattesttimeasaheuristicforestimatingthe
uncertainty of neural network predictions: if the predictions agree across many different
dropout outputs, then we might say that the network is more confident.
5.6.2Implementation fromScratch
Toimplementthedropoutfunctionforasinglelayer,wemustdrawasmanysamplesfroma
Bernoulli(binary)randomvariableasourlayerhasdimensions,wheretherandomvariable
takes value 1(keep) with probability 1 ùëùand0(drop) with probability ùëù. One easy way
to implement this is to first draw samples from the uniform distribution ùëà¬ª0,1¬º. Then we
can keep those nodes for which the corresponding sample is greater than ùëù, dropping the
rest.
Inthefollowingcode,weimplementa dropout_layer functionthatdropsouttheelements
inthetensorinput Xwithprobability dropout , rescalingtheremainderasdescribedabove:
dividing the survivors by 1.0-dropout .
def dropout_layer (X, dropout):
assert 0<=dropout <=1
ifdropout ==1:return torch .zeros_like(X)
mask =(torch .rand(X .shape) >dropout) .float()
return mask *X/(1.0 -dropout)
We can test out the dropout_layer function on a few examples. In the following lines of
code, we pass our input Xthrough the dropout operation, with probabilities 0, 0.5, and 1,
respectively.
X=torch .arange( 16, dtype =torch .float32) .reshape(( 2,8))
print ('dropout_p = 0: ', dropout_layer(X, 0))
print ('dropout_p = 0.5: ', dropout_layer(X, 0.5))
print ('dropout_p = 1: ', dropout_layer(X, 1))
dropout_p =0: tensor([[ 0.,1.,2.,3.,4.,5.,6.,7.],
[8.,9.,10.,11.,12.,13.,14.,15.]])
dropout_p =0.5: tensor([[ 0.,2.,0.,6.,8.,0.,0.,0.],
[16.,18.,20.,22.,24.,26.,28.,30.]])
dropout_p =1: tensor([[ 0.,0.,0.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,0.,0.,0.]])
Definingthe Model
Themodelbelowappliesdropouttotheoutputofeachhiddenlayer(followingtheactivation
function). We can set dropout probabilities for each layer separately. A common choice is
to set a lower dropout probability closer to the input layer. We ensure that dropout is only
active during training.
197 Dropout
class DropoutMLPScratch (d2l .Classifier):
def __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,
dropout_1, dropout_2, lr):
super ().__init__ ()
self .save_hyperparameters()
self .lin1 =nn.LazyLinear(num_hiddens_1)
self .lin2 =nn.LazyLinear(num_hiddens_2)
self .lin3 =nn.LazyLinear(num_outputs)
self .relu =nn.ReLU()
def forward (self , X):
H1=self .relu( self .lin1(X .reshape((X .shape[ 0],-1))))
ifself .training:
H1=dropout_layer(H1, self .dropout_1)
H2=self .relu( self .lin2(H1))
ifself .training:
H2=dropout_layer(H2, self .dropout_2)
return self .lin3(H2)
Training
The following is similar to the training of MLPs described previously.
hparams ={'num_outputs ':10,'num_hiddens_1 ':256,'num_hiddens_2 ':256,
'dropout_1 ':0.5,'dropout_2 ':0.5,'lr':0.1}
model =DropoutMLPScratch( **hparams)
data =d2l.FashionMNIST(batch_size =256)
trainer =d2l.Trainer(max_epochs =10)
trainer .fit(model, data)
5.6.3Concise Implementation
With high-level APIs, all we need to do is add a Dropout layer after each fully connected
layer, passing in the dropout probability as the only argument to its constructor. During
training, the Dropout layer will randomly drop out outputs of the previous layer (or equiv-
alently, the inputs to the subsequent layer) according to the specified dropout probability.
When not in training mode, the Dropout layer simply passes the data through during test-
ing.
198 Multilayer Perceptrons
class DropoutMLP (d2l .Classifier):
def __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,
dropout_1, dropout_2, lr):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nn.Flatten(), nn .LazyLinear(num_hiddens_1), nn .ReLU(),
nn.Dropout(dropout_1), nn .LazyLinear(num_hiddens_2), nn .ReLU(),
nn.Dropout(dropout_2), nn .LazyLinear(num_outputs))
Next, we train the model.
model =DropoutMLP( **hparams)
trainer .fit(model, data)
5.6.4Summary
Beyond controlling the number of dimensions and the size of the weight vector, dropout is
yet another tool for avoiding overfitting. Often tools are used jointly. Note that dropout is
used only during training: it replaces an activation ‚Ñéwith a random variable with expected
value‚Ñé.
5.6.5Exercises
1.What happens if you change the dropout probabilities for the first and second layers?
In particular, what happens if you switch the ones for both layers? Design an experi-
ment to answer these questions, describe your results quantitatively, and summarize the
qualitative takeaways.
2.Increase the number of epochs and compare the results obtained when using dropout
with those when not using it.
3.What is the variance of the activations in each hidden layer when dropout is and is not
applied? Draw a plot to show how this quantity evolves over time for both models.
4.Why is dropout not typically used at test time?
5.Usingthemodelinthissectionasanexample, comparetheeffectsofusingdropoutand
199 Predicting House Prices on Kaggle
107
108weightdecay. Whathappenswhendropoutandweightdecayareusedatthesametime?
Are the results additive? Are there diminished returns (or worse)? Do they cancel each
other out?
6.What happens if we apply dropout to the individual weights of the weight matrix rather
than the activations?
7.Invent another technique for injecting random noise at each layer that is different from
thestandarddropouttechnique. Canyoudevelopamethodthatoutperformsdropouton
the Fashion-MNIST dataset (for a fixed architecture)?
Discussions107.
5.7PredictingHouse Prices on Kaggle
Now that we have introduced some basic tools for building and training deep networks and
regularizing them with techniques including weight decay and dropout, we are ready to
put all this knowledge into practice by participating in a Kaggle competition. The house
price prediction competition is a great place to start. The data is fairly generic and do not
exhibit exotic structure that might require specialized models (as audio or video might).
This dataset, collected by De Cock ( 2011), covers house prices in Ames, Iowa from the
period 2006‚Äì2010. It is considerably larger than the famous Boston housing dataset108of
Harrison and Rubinfeld (1978), boasting both more examples and more features.
In this section, we will walk you through details of data preprocessing, model design, and
hyperparameter selection. We hope that through a hands-on approach, you will gain some
intuitions that will guide you in your career as a data scientist.
%matplotlib inline
import pandas aspd
import torch
from torch import nn
from d2l import torch asd2l
5.7.1DownloadingData
Throughout the book, we will train and test models on various downloaded datasets. Here,
we implement two utility functions for downloading and extracting zip or tar files. Again,
we skip implementation details of such utility functions.
def download (url, folder, sha1_hash =None ):
"""Download a file to folder and return the local filepath."""
def extract (filename, folder):
"""Extract a zip/tar file into folder."""
200 Multilayer Perceptrons
1095.7.2Kaggle
Kaggle109is a popular platform that hosts machine learning competitions. Each com-
petition centers on a dataset and many are sponsored by stakeholders who offer prizes to
the winning solutions. The platform helps users to interact via forums and shared code,
fostering both collaboration and competition. While leaderboard chasing often spirals out
of control, with researchers focusing myopically on preprocessing steps rather than asking
fundamental questions, there is also tremendous value in the objectivity of a platform that
facilitates direct quantitative comparisons among competing approaches as well as code
sharing so that everyone can learn what did and did not work. If you want to participate in
a Kaggle competition, you will first need to register for an account (see Fig. 5.7.1 ).
tFig. 5.7.1 The Kaggle website.
On the house price prediction competition page, as illustrated in Fig. 5.7.2 , you can find
the dataset (under the ‚ÄúData‚Äù tab), submit predictions, and see your ranking, The URL is
right here:
https://www.kaggle.com/c/house-prices-advanced-regression-techniques
tFig. 5.7.2 The house price prediction competition page.
5.7.3Accessingand Readingthe Dataset
201 Predicting House Prices on Kaggle
Note that the competition data is separated into training and test sets. Each record includes
the property value of the house and attributes such as street type, year of construction, roof
type, basement condition, etc. The features consist of various data types. For example,
the year of construction is represented by an integer, the roof type by discrete categorical
assignments, and other features by floating point numbers. And here is where reality com-
plicates things: for some examples, some data is altogether missing with the missing value
marked simply as ‚Äúna‚Äù. The price of each house is included for the training set only (it is
a competition after all). We will want to partition the training set to create a validation set,
but we only get to evaluate our models on the official test set after uploading predictions to
Kaggle. The ‚ÄúData‚Äù tab on the competition tab in Fig. 5.7.2 has links for downloading the
data.
To get started, we will read in and process the data using pandas, which we introduced
inSection 2.2 . For convenience, we can download and cache the Kaggle housing dataset.
If a file corresponding to this dataset already exists in the cache directory and its SHA-1
matches sha1_hash , our code will use the cached file to avoid clogging up your Internet
with redundant downloads.
class KaggleHouse (d2l .DataModule):
def __init__ (self , batch_size, train =None , val =None ):
super ().__init__ ()
self .save_hyperparameters()
ifself .train isNone :
self .raw_train =pd.read_csv(d2l .download(
d2l.DATA_URL +'kaggle_house_pred_train.csv ',self .root,
sha1_hash ='585e9cc93e70b39160e7921475f9bcd7d31219ce '))
self .raw_val =pd.read_csv(d2l .download(
d2l.DATA_URL +'kaggle_house_pred_test.csv ',self .root,
sha1_hash ='fa19780a7b011d9b009e8bff8e99922a8ee2eb90 '))
Thetrainingdatasetincludes1460examples,80features,andonelabel,whilethevalidation
data contains 1459 examples and 80 features.
data =KaggleHouse(batch_size =64)
print (data .raw_train .shape)
print (data .raw_val .shape)
Downloading ../data /kaggle_house_pred_train .csv from http ://d2l-data .s3-
‚Ü©!accelerate .amazonaws .com/kaggle_house_pred_train .csv...
Downloading ../data /kaggle_house_pred_test .csv from http ://d2l-data .s3-
‚Ü©!accelerate .amazonaws .com/kaggle_house_pred_test .csv...
(1460 ,81)
(1459 ,80)
5.7.4Data Preprocessing
Let‚Äôs take a look at the first four and final two features as well as the label (SalePrice) from
the first four examples.
202 Multilayer Perceptrons
print (data .raw_train .iloc[: 4, [0,1,2,3,-3,-2,-1]])
Id MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice
0 1 60 RL 65.0 WD Normal 208500
1 2 20 RL 80.0 WD Normal 181500
2 3 60 RL 68.0 WD Normal 223500
3 4 70 RL 60.0 WD Abnorml 140000
We can see that in each example, the first feature is the identifier. This helps the model
determineeachtrainingexample. Whilethisisconvenient,itdoesnotcarryanyinformation
for prediction purposes. Hence, we will remove it from the dataset before feeding the data
intothemodel. Furthermore,givenawidevarietyofdatatypes,wewillneedtopreprocess
the data before we can start modeling.
Let‚Äôs start with the numerical features. First, we apply a heuristic, replacing all missing
values by the corresponding feature‚Äôs mean. Then, to put all features on a common scale,
westandardize the data by rescaling features to zero mean and unit variance:
ùë• ùë• ùúá
ùúé, (5.7.1)
whereùúáandùúédenotemeanandstandarddeviation, respectively. Toverifythatthisindeed
transforms our feature (variable) such that it has zero mean and unit variance, note that
ùê∏¬ªùë• ùúá
ùúé¬º=ùúá ùúá
ùúé=0and thatùê∏¬ª¬πùë• ùúá¬∫2¬º=¬πùúé2¬∏ùúá2¬∫ 2ùúá2¬∏ùúá2=ùúé2. Intuitively, we
standardize the data for two reasons. First, it proves convenient for optimization. Second,
becausewedonotknow aprioriwhichfeatureswillberelevant,wedonotwanttopenalize
coefficients assigned to one feature more than any other.
Nextwedealwithdiscretevalues. Theseincludefeaturessuchas‚ÄúMSZoning‚Äù. Wereplace
them by a one-hot encoding in the same way that we earlier transformed multiclass labels
into vectors (see Section 4.1.1 ). For instance, ‚ÄúMSZoning‚Äù assumes the values ‚ÄúRL‚Äù and
‚ÄúRM‚Äù. Dropping the ‚ÄúMSZoning‚Äù feature, two new indicator features ‚ÄúMSZoning_RL‚Äù
and ‚ÄúMSZoning_RM‚Äù are created with values being either 0 or 1. According to one-hot
encoding, if the original value of ‚ÄúMSZoning‚Äù is ‚ÄúRL‚Äù, then ‚ÄúMSZoning_RL‚Äù is 1 and
‚ÄúMSZoning_RM‚Äù is 0. The pandaspackage does this automatically for us.
@d2l .add_to_class(KaggleHouse)
def preprocess (self ):
# Remove the ID and label columns
label ='SalePrice '
features =pd.concat(
(self .raw_train .drop(columns =['Id', label]),
self .raw_val .drop(columns =['Id'])))
# Standardize numerical columns
numeric_features =features .dtypes[features .dtypes !='object '].index
features[numeric_features] =features[numeric_features] .apply(
lambda x: (x -x.mean()) /(x.std()))
# Replace NAN numerical features by 0
features[numeric_features] =features[numeric_features] .fillna( 0)
(continues on next page)
203 Predicting House Prices on Kaggle
(continued from previous page)
# Replace discrete features by one-hot encoding
features =pd.get_dummies(features, dummy_na =True )
# Save preprocessed features
self .train =features[: self .raw_train .shape[ 0]].copy()
self .train[label] =self .raw_train[label]
self .val =features[ self .raw_train .shape[ 0]:].copy()
Youcanseethatthisconversionincreasesthenumberoffeaturesfrom79to331(excluding
ID and label columns).
data .preprocess()
data .train .shape
(1460 ,331)
5.7.5ErrorMeasure
To get started we will train a linear model with squared loss. Not surprisingly, our linear
modelwillnotleadtoacompetition-winningsubmissionbutitdoesprovideasanitycheck
to see whether there is meaningful information in the data. If we cannot do better than
random guessing here, then there might be a good chance that we have a data processing
bug. And if things work, the linear model will serve as a baseline giving us some intuition
about how close the simple model gets to the best reported models, giving us a sense of
how much gain we should expect from fancier models.
With house prices, as with stock prices, we care about relative quantities more than ab-
solute quantities. Thus we tend to care more about the relative errorùë¶ ÀÜùë¶
ùë¶than about the
absolute error ùë¶ ÀÜùë¶. For instance, if our prediction is off by $100,000 when estimating the
price of a house in rural Ohio, where the value of a typical house is $125,000, then we are
probably doing a horrible job. On the other hand, if we err by this amount in Los Altos
Hills, California, this might represent a stunningly accurate prediction (there, the median
house price exceeds $4 million).
Onewaytoaddressthisproblemistomeasurethediscrepancyinthelogarithmoftheprice
estimates. In fact, this is also the official error measure used by the competition to evaluate
the quality of submissions. After all, a small value ùõøforjlogùë¶ log ÀÜùë¶jùõøtranslates into
ùëí ùõøÀÜùë¶
ùë¶ùëíùõø. Thisleadstothefollowingroot-mean-squared-errorbetweenthelogarithm
of the predicted price and the logarithm of the label price:
vt
1
ùëõùëõ√ï
ùëñ=1¬πlogùë¶ùëñ log ÀÜùë¶ùëñ¬∫2. (5.7.2)
@d2l .add_to_class(KaggleHouse)
def get_dataloader (self , train):
(continues on next page)
204 Multilayer Perceptrons
(continued from previous page)
label ='SalePrice '
data =self .train iftrain else self .val
iflabel not indata: return
get_tensor =lambda x: torch .tensor(x .values .astype( float ),
dtype =torch .float32)
# Logarithm of prices
tensors =(get_tensor(data .drop(columns =[label])), # X
torch .log(get_tensor(data[label])) .reshape(( -1,1))) # Y
return self .get_tensorloader(tensors, train)
5.7.6ùêæ-FoldCross-Validation
You might recall that we introduced cross-validation in Section 3.6.3 , where we discussed
how to deal with model selection. We will put this to good use to select the model design
and to adjust the hyperparameters. We first need a function that returns the ùëñthfold of the
data in aùêæ-fold cross-validation procedure. It proceeds by slicing out the ùëñthsegment as
validationdataandreturningtherestastrainingdata. Notethatthisisnotthemostefficient
wayofhandlingdataandwewoulddefinitelydosomethingmuchsmarterifourdatasetwas
considerably larger. But this added complexity might obfuscate our code unnecessarily so
we can safely omit it here owing to the simplicity of our problem.
def k_fold_data (data, k):
rets =[]
fold_size =data .train .shape[ 0]//k
for jinrange (k):
idx =range (j*fold_size, (j +1)*fold_size)
rets .append(KaggleHouse(data .batch_size, data .train .drop(index =idx),
data .train .loc[idx]))
return rets
Theaveragevalidationerrorisreturnedwhenwetrain ùêætimesintheùêæ-foldcross-validation.
def k_fold (trainer, data, k, lr):
val_loss, models =[], []
for i, data_fold inenumerate (k_fold_data(data, k)):
model =d2l.LinearRegression(lr)
model .board .yscale ='log'
ifi!=0: model .board .display =False
trainer .fit(model, data_fold)
val_loss .append( float (model .board .data[ 'val_loss '][-1].y))
models .append(model)
print (f'average validation log mse = {sum(val_loss) /len(val_loss) }')
return models
5.7.7Model Selection
In this example, we pick an untuned set of hyperparameters and leave it up to the reader to
improvethemodel. Findingagoodchoicecantaketime,dependingonhowmanyvariables
one optimizes over. With a large enough dataset, and the normal sorts of hyperparameters,
205 Predicting House Prices on Kaggle
ùêæ-fold cross-validation tends to be reasonably resilient against multiple testing. However,
if we try an unreasonably large number of options we might find that our validation perfor-
mance is no longer representative of the true error.
trainer =d2l.Trainer(max_epochs =10)
models =k_fold(trainer, data, k =5, lr =0.01 )
average validation log mse =0.17325432986021042
Noticethatsometimesthenumberoftrainingerrorsforasetofhyperparameterscanbevery
low, even as the number of errors on ùêæ-fold cross-validation grows considerably higher.
This indicates that we are overfitting. Throughout training you will want to monitor both
numbers. Less overfitting might indicate that our data can support a more powerful model.
Massive overfitting might suggest that we can gain by incorporating regularization tech-
niques.
5.7.8SubmittingPredictionson Kaggle
Now that we know what a good choice of hyperparameters should be, we might calculate
the average predictions on the test set by all the ùêæmodels. Saving the predictions in a csv
file will simplify uploading the results to Kaggle. The following code will generate a file
called submission.csv .
preds =[model(torch .tensor(data .val.values .astype( float ), dtype =torch .
‚Ü©!float32))
for model inmodels]
# Taking exponentiation of predictions in the logarithm scale
ensemble_preds =torch .exp(torch .cat(preds, 1)).mean( 1)
submission =pd.DataFrame({ 'Id':data .raw_val .Id,
'SalePrice ':ensemble_preds .detach() .numpy()})
submission .to_csv( 'submission.csv ', index =False )
Next, as demonstrated in Fig. 5.7.3 , we can submit our predictions on Kaggle and see how
they compare with the actual house prices (labels) on the test set. The steps are quite
simple:
Log in to the Kaggle website and visit the house price prediction competition page.
206 Multilayer Perceptrons
110Click the ‚ÄúSubmit Predictions‚Äù or ‚ÄúLate Submission‚Äù button.
Click the ‚ÄúUpload Submission File‚Äù button in the dashed box at the bottom of the page
and select the prediction file you wish to upload.
Click the ‚ÄúMake Submission‚Äù button at the bottom of the page to view your results.
tFig. 5.7.3 Submitting data to Kaggle
5.7.9Summaryand Discussion
Realdataoftencontainsamixofdifferentdatatypesandneedstobepreprocessed. Rescal-
ing real-valued data to zero mean and unit variance is a good default. So is replacing miss-
ing values with their mean. Furthermore, transforming categorical features into indicator
features allows us to treat them like one-hot vectors. When we tend to care more about the
relative error than about the absolute error, we can measure the discrepancy in the loga-
rithm of the prediction. To select the model and adjust the hyperparameters, we can use
ùêæ-fold cross-validation .
5.7.10Exercises
1.Submit your predictions for this section to Kaggle. How good are they?
2.Is it always a good idea to replace missing values by a mean? Hint: can you construct a
situation where the values are not missing at random?
3.Improve the score by tuning the hyperparameters through ùêæ-fold cross-validation.
4.Improve the score by improving the model (e.g., layers, weight decay, and dropout).
5.What happens if we do not standardize the continuous numerical features as we have
done in this section?
Discussions110.
6 Builders‚Äô Guide
Alongside giant datasets and powerful hardware, great software tools have played an in-
dispensable role in the rapid progress of deep learning. Starting with the pathbreaking
Theano library released in 2007, flexible open-source tools have enabled researchers to
rapidly prototype models, avoiding repetitive work when recycling standard components
while still maintaining the ability to make low-level modifications. Over time, deep learn-
ing‚Äôslibrarieshaveevolvedtoofferincreasinglycoarseabstractions. Justassemiconductor
designers went from specifying transistors to logical circuits to writing code, neural net-
worksresearchershavemovedfromthinkingaboutthebehaviorofindividualartificialneu-
ronstoconceivingofnetworksintermsofwholelayers,andnowoftendesignarchitectures
with far coarser blocksin mind.
So far, we have introduced some basic machine learning concepts, ramping up to fully-
functional deep learning models. In the last chapter, we implemented each component of
an MLP from scratch and even showed how to leverage high-level APIs to roll out the
same models effortlessly. To get you that far that fast, we called upon the libraries, but
skipped over more advanced details about how they work . In this chapter, we will peel
back the curtain, digging deeper into the key components of deep learning computation,
namely model construction, parameter access and initialization, designing custom layers
and blocks, reading and writing models to disk, and leveraging GPUs to achieve dramatic
speedups. These insights will move you from enduser topoweruser , giving you the tools
neededtoreapthebenefitsofamaturedeeplearninglibrarywhileretainingtheflexibilityto
implement more complex models, including those you invent yourself! While this chapter
doesnotintroduceanynewmodelsordatasets,theadvancedmodelingchaptersthatfollow
rely heavily on these techniques.
6.1Layersand Modules
When we first introduced neural networks, we focused on linear models with a single out-
put. Here, the entire model consists of just a single neuron. Note that a single neuron (i)
takes some set of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of
associated parameters that can be updated to optimize some objective function of interest.
Then, once we started thinking about networks with multiple outputs, we leveraged vec-
torized arithmetic to characterize an entire layer of neurons. Just like individual neurons,
207
208 Builders‚Äô Guide
layers (i) take a set of inputs, (ii) generate corresponding outputs, and (iii) are described by
a set of tunable parameters. When we worked through softmax regression, a single layer
was itself the model. However, even when we subsequently introduced MLPs, we could
still think of the model as retaining this same basic structure.
Interestingly,forMLPs,boththeentiremodelanditsconstituentlayerssharethisstructure.
Theentiremodeltakesinrawinputs(thefeatures), generatesoutputs(thepredictions), and
possesses parameters (the combined parameters from all constituent layers). Likewise,
each individual layer ingests inputs (supplied by the previous layer) generates outputs (the
inputs to the subsequent layer), and possesses a set of tunable parameters that are updated
according to the signal that flows backwards from the subsequent layer.
While you might think that neurons, layers, and models give us enough abstractions to go
about our business, it turns out that we often find it convenient to speak about components
that are larger than an individual layer but smaller than the entire model. For example, the
ResNet-152architecture,whichiswildlypopularincomputervision,possesseshundredsof
layers. These layers consist of repeating patterns of groups of layers . Implementing such
a network one layer at a time can grow tedious. This concern is not just hypothetical‚Äî
such design patterns are common in practice. The ResNet architecture mentioned above
wonthe2015ImageNetandCOCOcomputervisioncompetitionsforbothrecognitionand
detection ( Heetal., 2016) and remains a go-to architecture for many vision tasks. Similar
architectures in which layers are arranged in various repeating patterns are now ubiquitous
in other domains, including natural language processing and speech.
To implement these complex networks, we introduce the concept of a neural network mod-
ule. A module could describe a single layer, a component consisting of multiple layers,
or the entire model itself! One benefit of working with the module abstraction is that they
can be combined into larger artifacts, often recursively. This is illustrated in Fig. 6.1.1 .
By defining code to generate modules of arbitrary complexity on demand, we can write
surprisingly compact code and still implement complex neural networks.
tFig. 6.1.1 Multiple layers are combined into modules, forming repeating patterns of larger models.
From a programming standpoint, a module is represented by a class. Any subclass of it
must define a forward propagation method that transforms its input into output and must
store any necessary parameters. Note that some modules do not require any parameters at
209 Layers and Modules
all. Finally a module must possess a backpropagation method, for purposes of calculating
gradients. Fortunately,duetosomebehind-the-scenesmagicsuppliedbytheautodifferen-
tiation (introduced in Section 2.5 ) when defining our own module, we only need to worry
about parameters and the forward propagation method.
import torch
from torch import nn
from torch .nnimport functional asF
To begin, we revisit the code that we used to implement MLPs ( Section 5.1 ). The follow-
ing code generates a network with one fully connected hidden layer with 256 units and
ReLU activation, followed by a fully connected output layer with ten units (no activation
function).
net =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))
X=torch .rand( 2,20)
net(X) .shape
torch .Size([ 2,10])
In this example, we constructed our model by instantiating an nn.Sequential , with layers
in the order that they should be executed passed as arguments. In short, nn.Sequential
defines a special kind of Module, the class that presents a module in PyTorch. It maintains
anorderedlistofconstituent Modules. Notethateachofthetwofullyconnectedlayersisan
instanceofthe Linearclasswhichis itselfa subclassof Module. The forwardpropagation
(forward ) method is also remarkably simple: it chains each module in the list together,
passing the output of each as input to the next. Note that until now, we have been invok-
ing our models via the construction net(X)to obtain their outputs. This is actually just
shorthand for net.__call__(X) .
6.1.1A Custom Module
Perhaps the easiest way to develop intuition about how a module works is to implement
one ourselves. Before we do that, we briefly summarize the basic functionality that each
module must provide:
1.Ingest input data as arguments to its forward propagation method.
2.Generate an output by having the forward propagation method return a value. Note
that the output may have a different shape from the input. For example, the first fully
connected layer in our model above ingests an input of arbitrary dimension but returns
an output of dimension 256.
3.Calculate the gradient of its output with respect to its input, which can be accessed via
its backpropagation method. Typically this happens automatically.
210 Builders‚Äô Guide
4.Store and provide access to those parameters necessary for executing the forward prop-
agation computation.
5.Initialize model parameters as needed.
In the following snippet, we code up a module from scratch corresponding to an MLP
with one hidden layer with 256 hidden units, and a 10-dimensional output layer. Note that
theMLPclass below inherits the class that represents a module. We will heavily rely on
the parent class‚Äôs methods, supplying only our own constructor (the __init__ method in
Python) and the forward propagation method.
class MLP(nn.Module):
def __init__ (self ):
# Call the constructor of the parent class nn.Module to perform
# the necessary initialization
super ().__init__ ()
self .hidden =nn.LazyLinear( 256)
self .out =nn.LazyLinear( 10)
# Define the forward propagation of the model, that is, how to return the
# required model output based on the input X
def forward (self , X):
return self .out(F .relu( self .hidden(X)))
Let‚Äôs first focus on the forward propagation method. Note that it takes Xas input, calcu-
lates the hidden representation with the activation function applied, and outputs its logits.
In this MLPimplementation, both layers are instance variables. To see why this is reason-
able, imagine instantiating two MLPs, net1andnet2, and training them on different data.
Naturally, we would expect them to represent two different learned models.
We instantiate the MLP‚Äôs layers in the constructor and subsequently invoke these layers on
each call to the forward propagation method. Note a few key details. First, our customized
__init__ method invokes the parent class‚Äôs __init__ method via super().__init__()
sparing us the pain of restating boilerplate code applicable to most modules. We then
instantiate our two fully connected layers, assigning them to self.hidden andself.out .
Note that unless we implement a new layer, we need not worry about the backpropagation
method or parameter initialization. The system will generate these methods automatically.
Let‚Äôs try this out.
net =MLP()
net(X) .shape
torch .Size([ 2,10])
Akeyvirtueofthemoduleabstractionisitsversatility. Wecansubclassamoduletocreate
layers(suchasthefullyconnectedlayerclass), entiremodels(suchasthe MLPclassabove),
or various components of intermediate complexity. We exploit this versatility throughout
the coming chapters, such as when addressing convolutional neural networks.
211 Layers and Modules
6.1.2The SequentialModule
We can now take a closer look at how the Sequential class works. Recall that Sequen-
tialwas designed to daisy-chain other modules together. To build our own simplified
MySequential , we just need to define two key methods:
1.A method for appending modules one by one to a list.
2.Aforwardpropagationmethodforpassinganinputthroughthechainofmodules,inthe
same order as they were appended.
The following MySequential class delivers the same functionality of the default Sequen-
tialclass.
class MySequential (nn.Module):
def __init__ (self ,*args):
super ().__init__ ()
for idx, module inenumerate (args):
self .add_module( str(idx), module)
def forward (self , X):
for module inself .children():
X=module(X)
return X
Inthe __init__ method,weaddeverymodulebycallingthe add_modules method. These
modules can be accessed by the children method at a later date. In this way the system
knows the added modules, and it will properly initialize each module‚Äôs parameters.
When our MySequential ‚Äôs forward propagation method is invoked, each added module is
executed in the order in which they were added. We can now reimplement an MLP using
ourMySequential class.
net =MySequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))
net(X) .shape
torch .Size([ 2,10])
Note that this use of MySequential is identical to the code we previously wrote for the
Sequential class (as described in Section 5.1 ).
6.1.3ExecutingCode in the ForwardPropagationMethod
TheSequential class makes model construction easy, allowing us to assemble new archi-
tectures without having to define our own class. However, not all architectures are simple
daisy chains. When greater flexibility is required, we will want to define our own blocks.
For example, we might want to execute Python‚Äôs control flow within the forward propaga-
tion method. Moreover, we might want to perform arbitrary mathematical operations, not
simply relying on predefined neural network layers.
212 Builders‚Äô Guide
You may have noticed that until now, all of the operations in our networks have acted upon
our network‚Äôs activations and its parameters. Sometimes, however, we might want to in-
corporate terms that are neither the result of previous layers nor updatable parameters. We
call these constant parameters . Say for example that we want a layer that calculates the
functionùëì¬πx,w¬∫=ùëêw>x, where xis the input, wis our parameter, and ùëêis some speci-
fiedconstantthatisnotupdatedduringoptimization. Soweimplementa FixedHiddenMLP
class as follows.
class FixedHiddenMLP (nn.Module):
def __init__ (self ):
super ().__init__ ()
# Random weight parameters that will not compute gradients and
# therefore keep constant during training
self .rand_weight =torch .rand(( 20,20))
self .linear =nn.LazyLinear( 20)
def forward (self , X):
X=self .linear(X)
X=F.relu(X @self .rand_weight +1)
# Reuse the fully connected layer. This is equivalent to sharing
# parameters with two fully connected layers
X=self .linear(X)
# Control flow
while X.abs() .sum() >1:
X/=2
return X.sum()
In this model, we implement a hidden layer whose weights ( self.rand_weight ) are ini-
tialized randomly at instantiation and are thereafter constant. This weight is not a model
parameter and thus it is never updated by backpropagation. The network then passes the
output of this ‚Äúfixed‚Äù layer through a fully connected layer.
Note that before returning the output, our model did something unusual. We ran a while-
loop, testing on the condition its ‚Ñì1norm is larger than 1, and dividing our output vector
by2until it satisfied the condition. Finally, we returned the sum of the entries in X. To our
knowledge, no standard neural network performs this operation. Note that this particular
operation may not be useful in any real-world task. Our point is only to show you how to
integrate arbitrary code into the flow of your neural network computations.
net =FixedHiddenMLP()
net(X)
tensor( -0.3836 , grad_fn =<SumBackward0 >)
We can mix and match various ways of assembling modules together. In the following
example, we nest modules in some creative ways.
class NestMLP (nn.Module):
(continues on next page)
213 Parameter Management
111(continued from previous page)
def __init__ (self ):
super ().__init__ ()
self .net =nn.Sequential(nn .LazyLinear( 64), nn .ReLU(),
nn.LazyLinear( 32), nn .ReLU())
self .linear =nn.LazyLinear( 16)
def forward (self , X):
return self .linear( self .net(X))
chimera =nn.Sequential(NestMLP(), nn .LazyLinear( 20), FixedHiddenMLP())
chimera(X)
tensor( 0.0679 , grad_fn =<SumBackward0 >)
6.1.4Summary
Individuallayerscanbemodules. Manylayerscancompriseamodule. Manymodulescan
comprise a module.
A module can contain code. Modules take care of lots of housekeeping, including param-
eter initialization and backpropagation. Sequential concatenations of layers and modules
are handled by the Sequential module.
6.1.5Exercises
1.What kinds of problems will occur if you change MySequential to store modules in a
Python list?
2.Implement a module that takes two modules as an argument, say net1andnet2and
returns the concatenated output of both networks in the forward propagation. This is
also called a parallelmodule .
3.Assume that you want to concatenate multiple instances of the same network. Imple-
ment a factory function that generates multiple instances of the same module and build
a larger network from it.
Discussions111.
6.2ParameterManagement
Once we have chosen an architecture and set our hyperparameters, we proceed to the train-
ing loop, where our goal is to find parameter values that minimize our loss function. After
training, we will need these parameters in order to make future predictions. Additionally,
we will sometimes wish to extract the parameters perhaps to reuse them in some other
214 Builders‚Äô Guide
context, to save our model to disk so that it may be executed in other software, or for ex-
amination in the hope of gaining scientific understanding.
Most of the time, we will be able to ignore the nitty-gritty details of how parameters are
declared and manipulated, relying on deep learning frameworks to do the heavy lifting.
However, when we move away from stacked architectures with standard layers, we will
sometimes need to get into the weeds of declaring and manipulating parameters. In this
section, we cover the following:
Accessing parameters for debugging, diagnostics, and visualizations.
Sharing parameters across different model components.
import torch
from torch import nn
We start by focusing on an MLP with one hidden layer.
net =nn.Sequential(nn .LazyLinear( 8),
nn.ReLU(),
nn.LazyLinear( 1))
X=torch .rand(size =(2,4))
net(X) .shape
torch .Size([ 2,1])
6.2.1ParameterAccess
Let‚Äôs start with how to access parameters from the models that you already know.
Whenamodelisdefinedviathe Sequential class,wecanfirstaccessanylayerbyindexing
into the model as though it were a list. Each layer‚Äôs parameters are conveniently located in
its attribute.
We can inspect the parameters of the second fully connected layer as follows.
net[ 2].state_dict()
OrderedDict([( 'weight ',
tensor([[ -0.1649 ,0.0605 ,0.1694 ,-0.2524 ,0.3526 ,-0.3414 ,-
‚Ü©!0.2322 ,0.0822 ]])),
('bias ', tensor([ 0.0709 ]))])
We can see that this fully connected layer contains two parameters, corresponding to that
layer‚Äôs weights and biases, respectively.
215 Parameter Management
TargetedParameters
Note that each parameter is represented as an instance of the parameter class. To do any-
thing useful with the parameters, we first need to access the underlying numerical values.
There are several ways to do this. Some are simpler while others are more general. The
following code extracts the bias from the second neural network layer, which returns a
parameter class instance, and further accesses that parameter‚Äôs value.
type (net[ 2].bias), net[ 2].bias .data
(torch .nn.parameter .Parameter, tensor([ 0.0709 ]))
Parameters are complex objects, containing values, gradients, and additional information.
That is why we need to request the value explicitly.
In addition to the value, each parameter also allows us to access the gradient. Because we
have not invoked backpropagation for this network yet, it is in its initial state.
net[ 2].weight .grad ==None
True
AllParametersat Once
When we need to perform operations on all parameters, accessing them one-by-one can
grow tedious. The situation can grow especially unwieldy when we work with more com-
plex,e.g.,nested,modules,sincewewouldneedtorecursethroughtheentiretreetoextract
each sub-module‚Äôs parameters. Below we demonstrate accessing the parameters of all lay-
ers.
[(name, param .shape) for name, param innet.named_parameters()]
[('0.weight ', torch .Size([ 8,4])),
('0.bias ', torch .Size([ 8])),
('2.weight ', torch .Size([ 1,8])),
('2.bias ', torch .Size([ 1]))]
6.2.2TiedParameters
Often,wewanttoshareparametersacrossmultiplelayers. Let‚Äôsseehowtodothiselegantly.
In the following we allocate a fully connected layer and then use its parameters specifically
to set those of another layer. Here we need to run the forward propagation net(X)before
accessing the parameters.
216 Builders‚Äô Guide
112# We need to give the shared layer a name so that we can refer to its
# parameters
shared =nn.LazyLinear( 8)
net =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(),
shared, nn .ReLU(),
shared, nn .ReLU(),
nn.LazyLinear( 1))
net(X)
# Check whether the parameters are the same
print (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])
net[ 2].weight .data[ 0,0]=100
# Make sure that they are actually the same object rather than just having the
# same value
print (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])
tensor([ True ,True ,True ,True ,True ,True ,True ,True ])
tensor([ True ,True ,True ,True ,True ,True ,True ,True ])
This example shows that the parameters of the second and third layer are tied. They are
not just equal, they are represented by the same exact tensor. Thus, if we change one of the
parameters, the other one changes, too.
You might wonder, when parameters are tied what happens to the gradients? Since the
model parameters contain gradients, the gradients of the second hidden layer and the third
hidden layer are added together during backpropagation.
6.2.3Summary
We have several ways of accessing and tying model parameters.
6.2.4Exercises
1.Use the NestMLP model defined in Section 6.1 and access the parameters of the various
layers.
2.Construct an MLP containing a shared parameter layer and train it. During the training
process, observe the model parameters and gradients of each layer.
3.Why is sharing parameters a good idea?
Discussions112.
6.3ParameterInitialization
Now that we know how to access the parameters, let‚Äôs look at how to initialize them prop-
erly. We discussed the need for proper initialization in Section 5.4 . The deep learning
217 Parameter Initialization
framework provides default random initializations to its layers. However, we often want to
initialize our weights according to various other protocols. The framework provides most
commonly used protocols, and also allows to create a custom initializer.
import torch
from torch import nn
Bydefault,PyTorchinitializesweightandbiasmatricesuniformlybydrawingfromarange
that is computed according to the input and output dimension. PyTorch‚Äôs nn.init module
provides a variety of preset initialization methods.
net =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(), nn .LazyLinear( 1))
X=torch .rand(size =(2,4))
net(X) .shape
torch .Size([ 2,1])
6.3.1Built-in Initialization
Let‚Äôsbeginbycallingonbuilt-ininitializers. Thecodebelowinitializesallweightparame-
ters as Gaussian random variables with standard deviation 0.01, while bias parameters are
cleared to zero.
def init_normal (module):
iftype (module) ==nn.Linear:
nn.init .normal_(module .weight, mean =0, std =0.01 )
nn.init .zeros_(module .bias)
net.apply(init_normal)
net[ 0].weight .data[ 0], net[ 0].bias .data[ 0]
(tensor([ -0.0129 ,-0.0007 ,-0.0033 ,0.0276 ]), tensor( 0.))
We can also initialize all the parameters to a given constant value (say, 1).
def init_constant (module):
iftype (module) ==nn.Linear:
nn.init .constant_(module .weight, 1)
nn.init .zeros_(module .bias)
net.apply(init_constant)
net[ 0].weight .data[ 0], net[ 0].bias .data[ 0]
(tensor([ 1.,1.,1.,1.]), tensor( 0.))
We can also apply different initializers for certain blocks. For example, below we initialize
218 Builders‚Äô Guide
the first layer with the Xavier initializer and initialize the second layer to a constant value
of 42.
def init_xavier (module):
iftype (module) ==nn.Linear:
nn.init .xavier_uniform_(module .weight)
def init_42 (module):
iftype (module) ==nn.Linear:
nn.init .constant_(module .weight, 42)
net[ 0].apply(init_xavier)
net[ 2].apply(init_42)
print (net[ 0].weight .data[ 0])
print (net[ 2].weight .data)
tensor([ -0.0974 ,0.1707 ,0.5840 ,-0.5032 ])
tensor([[ 42.,42.,42.,42.,42.,42.,42.,42.]])
CustomInitialization
Sometimes,theinitializationmethodsweneedarenotprovidedbythedeeplearningframe-
work. In the example below, we define an initializer for any weight parameter ùë§using the
following strange distribution:
ùë§8>>> <
>>>:ùëà¬π5,10¬∫with probability1
4
0 with probability1
2
ùëà¬π 10, 5¬∫with probability1
4(6.3.1)
Again, we implement a my_init function to apply to net.
def my_init (module):
iftype (module) ==nn.Linear:
print ("Init ",*[(name, param .shape)
for name, param inmodule .named_parameters()][ 0])
nn.init .uniform_(module .weight, -10,10)
module .weight .data *=module .weight .data .abs() >=5
net.apply(my_init)
net[ 0].weight[: 2]
Init weight torch .Size([ 8,4])
Init weight torch .Size([ 1,8])
tensor([[ 0.0000 ,-7.6364 ,-0.0000 ,-6.1206 ],
[9.3516 ,-0.0000 ,5.1208 ,-8.4003 ]], grad_fn =<SliceBackward0 >)
Note that we always have the option of setting parameters directly.
219 Lazy Initialization
113net[ 0].weight .data[:] +=1
net[ 0].weight .data[ 0,0]=42
net[ 0].weight .data[ 0]
tensor([ 42.0000 ,-6.6364 ,1.0000 ,-5.1206 ])
6.3.2Summary
We can initialize parameters using built-in and custom initializers.
6.3.3Exercises
Look up the online documentation for more built-in initializers.
Discussions113.
6.4Lazy Initialization
Sofar,itmightseemthatwegotawaywithbeingsloppyinsettingupournetworks. Specif-
ically, we did the following unintuitive things, which might not seem like they should
work:
We defined the network architectures without specifying the input dimensionality.
We added layers without specifying the output dimension of the previous layer.
We even ‚Äúinitialized‚Äù these parameters before providing enough information to deter-
mine how many parameters our models should contain.
Youmightbesurprisedthatourcoderunsatall. Afterall,thereisnowaythedeeplearning
framework could tell what the input dimensionality of a network would be. The trick here
is that the framework defersinitialization , waiting until the first time we pass data through
the model, to infer the sizes of each layer on the fly.
Later on, when working with convolutional neural networks, this technique will become
even more convenient since the input dimensionality (e.g., the resolution of an image) will
affect the dimensionality of each subsequent layer. Hence the ability to set parameters
without the need to know, at the time of writing the code, the value of the dimension can
greatly simplify the task of specifying and subsequently modifying our models. Next, we
go deeper into the mechanics of initialization.
import torch
from torch import nn
from d2l import torch asd2l
220 Builders‚Äô Guide
To begin, let‚Äôs instantiate an MLP.
net =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))
Atthispoint,thenetworkcannotpossiblyknowthedimensionsoftheinputlayer‚Äôsweights
because the input dimension remains unknown.
Consequently the framework has not yet initialized any parameters. We confirm by at-
tempting to access the parameters below.
net[ 0].weight
<UninitializedParameter >
Next let‚Äôs pass data through the network to make the framework finally initialize parame-
ters.
X=torch .rand( 2,20)
net(X)
net[ 0].weight .shape
torch .Size([ 256,20])
As soon as we know the input dimensionality, 20, the framework can identify the shape of
the first layer‚Äôs weight matrix by plugging in the value of 20. Having recognized the first
layer‚Äôs shape, the framework proceeds to the second layer, and so on through the computa-
tional graph until all shapes are known. Note that in this case, only the first layer requires
lazy initialization, but the framework initializes sequentially. Once all parameter shapes
are known, the framework can finally initialize the parameters.
The following method passes in dummy inputs through the network for a dry run to infer
allparametershapesandsubsequentlyinitializestheparameters. Itwillbeusedlaterwhen
default random initializations are not desired.
@d2l .add_to_class(d2l .Module) #@save
def apply_init (self , inputs, init =None ):
self .forward( *inputs)
ifinit isnot None :
self .net.apply(init)
6.4.1Summary
Lazy initialization can be convenient, allowing the framework to infer parameter shapes
automatically, making it easy to modify architectures and eliminating one common source
of errors. We can pass data through the model to make the framework finally initialize
parameters.
221 Custom Layers
1146.4.2Exercises
1.Whathappensifyouspecifytheinputdimensionstothefirstlayerbutnottosubsequent
layers? Do you get immediate initialization?
2.What happens if you specify mismatching dimensions?
3.What would you need to do if you have input of varying dimensionality? Hint: look at
the parameter tying.
Discussions114.
6.5Custom Layers
One factor behind deep learning‚Äôs success is the availability of a wide range of layers that
can be composed in creative ways to design architectures suitable for a wide variety of
tasks. For instance, researchers have invented layers specifically for handling images, text,
looping over sequential data, and performing dynamic programming. Sooner or later, you
will need a layer that does not exist yet in the deep learning framework. In these cases, you
must build a custom layer. In this section, we show you how.
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
6.5.1Layerswithout Parameters
To start, we construct a custom layer that does not have any parameters of its own. This
shouldlookfamiliarifyourecallourintroductiontomodulesin Section6.1 . Thefollowing
CenteredLayer classsimplysubtractsthemeanfromitsinput. Tobuildit,wesimplyneed
toinheritfromthebaselayerclassandimplementtheforwardpropagationfunction.
class CenteredLayer (nn.Module):
def __init__ (self ):
super ().__init__ ()
def forward (self , X):
return X-X.mean()
Let‚Äôs verify that our layer works as intended by feeding some data through it.
layer =CenteredLayer()
layer(torch .tensor([ 1.0,2,3,4,5]))
222 Builders‚Äô Guide
tensor([ -2.,-1.,0.,1.,2.])
We can now incorporate our layer as a component in constructing more complex mod-
els.
net =nn.Sequential(nn .LazyLinear( 128), CenteredLayer())
As an extra sanity check, we can send random data through the network and check that the
mean is in fact 0. Because we are dealing with floating point numbers, we may still see a
very small nonzero number due to quantization.
Y=net(torch .rand( 4,8))
Y.mean()
tensor( -6.5193e-09 , grad_fn =<MeanBackward0 >)
6.5.2Layerswith Parameters
Now that we know how to define simple layers, let‚Äôs move on to defining layers with pa-
rameters that can be adjusted through training. We can use built-in functions to create
parameters, which provide some basic housekeeping functionality. In particular, they gov-
ern access, initialization, sharing, saving, and loading model parameters. This way, among
other benefits, we will not need to write custom serialization routines for every custom
layer.
Now let‚Äôs implement our own version of the fully connected layer. Recall that this layer
requires two parameters, one to represent the weight and the other for the bias. In this im-
plementation, we bake in the ReLU activation as a default. This layer requires two input
arguments: in_units andunits, which denote the number of inputs and outputs, respec-
tively.
class MyLinear (nn.Module):
def __init__ (self , in_units, units):
super ().__init__ ()
self .weight =nn.Parameter(torch .randn(in_units, units))
self .bias =nn.Parameter(torch .randn(units,))
def forward (self , X):
linear =torch .matmul(X, self .weight .data) +self .bias .data
return F.relu(linear)
Next, we instantiate the MyLinear class and access its model parameters.
linear =MyLinear( 5,3)
linear .weight
223 File I/O
115Parameter containing:
tensor([[ 0.4783 ,0.4284 ,-0.0899 ],
[-0.6347 ,0.2913 ,-0.0822 ],
[-0.4325 ,-0.1645 ,-0.3274 ],
[1.1898 ,0.6482 ,-1.2384 ],
[-0.1479 ,0.0264 ,-0.9597 ]], requires_grad =True )
We can directly carry out forward propagation calculations using custom layers.
linear(torch .rand( 2,5))
tensor([[ 0.0000 ,0.9316 ,0.0000 ],
[0.1808 ,1.4208 ,0.0000 ]])
We can also construct models using custom layers. Once we have that we can use it just
like the built-in fully connected layer.
net =nn.Sequential(MyLinear( 64,8), MyLinear( 8,1))
net(torch .rand( 2,64))
tensor([[ 0.0000 ],
[13.0800 ]])
6.5.3Summary
We can design custom layers via the basic layer class. This allows us to define flexible
new layers that behave differently from any existing layers in the library. Once defined,
custom layers can be invoked in arbitrary contexts and architectures. Layers can have local
parameters, which can be created through built-in functions.
6.5.4Exercises
1.Design a layer that takes an input and computes a tensor reduction, i.e., it returns ùë¶ùëò=√ç
ùëñ,ùëóùëäùëñùëóùëòùë•ùëñùë•ùëó.
2.Design a layer that returns the leading half of the Fourier coefficients of the data.
Discussions115.
6.6File I/O
So far we have discussed how to process data and how to build, train, and test deep learn-
ing models. However, at some point we will hopefully be happy enough with the learned
224 Builders‚Äô Guide
models that we will want to save the results for later use in various contexts (perhaps even
to make predictions in deployment). Additionally, when running a long training process,
the best practice is to periodically save intermediate results (checkpointing) to ensure that
we do not lose several days‚Äô worth of computation if we trip over the power cord of our
server. Thus it is time to learn how to load and store both individual weight vectors and
entire models. This section addresses both issues.
import torch
from torch import nn
from torch .nnimport functional asF
6.6.1Loading and SavingTensors
For individual tensors, we can directly invoke the loadandsavefunctions to read and
write them respectively. Both functions require that we supply a name, and saverequires
as input the variable to be saved.
x=torch .arange( 4)
torch .save(x, 'x-file ')
We can now read the data from the stored file back into memory.
x2=torch .load( 'x-file ')
x2
tensor([ 0,1,2,3])
We can store a list of tensors and read them back into memory.
y=torch .zeros( 4)
torch .save([x, y], 'x-files ')
x2, y2 =torch .load( 'x-files ')
(x2, y2)
(tensor([ 0,1,2,3]), tensor([ 0.,0.,0.,0.]))
We can even write and read a dictionary that maps from strings to tensors. This is conve-
nient when we want to read or write all the weights in a model.
mydict ={'x': x, 'y': y}
torch .save(mydict, 'mydict ')
mydict2 =torch .load( 'mydict ')
mydict2
225 File I/O
{'x': tensor([ 0,1,2,3]), 'y': tensor([ 0.,0.,0.,0.])}
6.6.2Loading and SavingModel Parameters
Saving individual weight vectors (or other tensors) is useful, but it gets very tedious if we
want to save (and later load) an entire model. After all, we might have hundreds of param-
eter groups sprinkled throughout. For this reason the deep learning framework provides
built-in functionalities to load and save entire networks. An important detail to note is that
this saves model parameters and not the entire model. For example, if we have a 3-layer
MLP, we need to specify the architecture separately. The reason for this is that the models
themselves can contain arbitrary code, hence they cannot be serialized as naturally. Thus,
in order to reinstate a model, we need to generate the architecture in code and then load the
parameters from disk. Let‚Äôs start with our familiar MLP.
class MLP(nn.Module):
def __init__ (self ):
super ().__init__ ()
self .hidden =nn.LazyLinear( 256)
self .output =nn.LazyLinear( 10)
def forward (self , x):
return self .output(F .relu( self .hidden(x)))
net =MLP()
X=torch .randn(size =(2,20))
Y=net(X)
Next, we store the parameters of the model as a file with the name ‚Äúmlp.params‚Äù.
torch .save(net .state_dict(), 'mlp.params ')
To recover the model, we instantiate a clone of the original MLP model. Instead of ran-
domlyinitializingthemodelparameters,wereadtheparametersstoredinthefiledirectly.
clone =MLP()
clone .load_state_dict(torch .load( 'mlp.params '))
clone .eval()
MLP(
(hidden): LazyLinear(in_features =0, out_features =256, bias =True )
(output): LazyLinear(in_features =0, out_features =10, bias =True )
)
Sincebothinstanceshavethesamemodelparameters,thecomputationalresultofthesame
input Xshould be the same. Let‚Äôs verify this.
226 Builders‚Äô Guide
116
117Y_clone =clone(X)
Y_clone ==Y
tensor([[ True ,True ,True ,True ,True ,True ,True ,True ,True ,True ],
[True ,True ,True ,True ,True ,True ,True ,True ,True ,True ]])
6.6.3Summary
Thesaveandloadfunctions can be used to perform file I/O for tensor objects. We can
saveandloadtheentiresetsofparametersforanetworkviaaparameterdictionary. Saving
the architecture has to be done in code rather than in parameters.
6.6.4Exercises
1.Even if there is no need to deploy trained models to a different device, what are the
practical benefits of storing model parameters?
2.Assume that we want to reuse only parts of a network to be incorporated into a network
having a different architecture. How would you go about using, say the first two layers
from a previous network in a new network?
3.Howwouldyougoaboutsavingthenetworkarchitectureandparameters? Whatrestric-
tions would you impose on the architecture?
Discussions116.
6.7GPUs
Intab_intro_decade , we illustrated the rapid growth of computation over the past two
decades. In a nutshell, GPU performance has increased by a factor of 1000 every decade
since 2000. This offers great opportunities but it also suggests that there was significant
demand for such performance.
Inthissection,webegintodiscusshowtoharnessthiscomputationalperformanceforyour
research. First by using a single GPU and at a later point, how to use multiple GPUs and
multiple servers (with multiple GPUs).
Specifically, we will discuss how to use a single NVIDIA GPU for calculations. First,
make sure you have at least one NVIDIA GPU installed. Then, download the NVIDIA
driverandCUDA117andfollowthepromptstosettheappropriatepath. Oncetheseprepa-
rations are complete, the nvidia-smi command can be used to view the graphics card
information.
In PyTorch, every array has a device; we often refer it as a context. So far, by default, all
227 GPUs
variables and associated computation have been assigned to the CPU. Typically, other con-
textsmightbevariousGPUs. Thingscangetevenhairierwhenwedeployjobsacrossmul-
tiple servers. By assigning arrays to contexts intelligently, we can minimize the time spent
transferring data between devices. For example, when training neural networks on a server
with a GPU, we typically prefer for the model‚Äôs parameters to live on the GPU.
To run the programs in this section, you need at least two GPUs. Note that this might
be extravagant for most desktop computers but it is easily available in the cloud, e.g., by
usingtheAWSEC2multi-GPUinstances. Almostallothersectionsdo notrequiremultiple
GPUs, but here we simply wish to illustrate data flow between different devices.
import torch
from torch import nn
from d2l import torch asd2l
6.7.1ComputingDevices
We can specify devices, such as CPUs and GPUs, for storage and calculation. By default,
tensors are created in the main memory and then the CPU is used for calculations.
In PyTorch, the CPU and GPU can be indicated by torch.device('cpu') andtorch.
device('cuda') . It should be noted that the cpudevice means all physical CPUs and
memory. This means that PyTorch‚Äôs calculations will try to use all CPU cores. However, a
gpudevice only represents one card and the corresponding memory. If there are multiple
GPUs, we use torch.device(f'cuda:{i}') to represent the ùëñthGPU (ùëñstarts at 0). Also,
gpu:0andgpuare equivalent.
def cpu(): #@save
"""Get the CPU device."""
return torch .device( 'cpu')
def gpu(i=0): #@save
"""Get a GPU device."""
return torch .device( f'cuda: {i}')
cpu(), gpu(), gpu( 1)
(device( type ='cpu'),
device( type ='cuda ', index =0),
device( type ='cuda ', index =1))
We can query the number of available GPUs.
def num_gpus (): #@save
"""Get the number of available GPUs."""
return torch .cuda .device_count()
num_gpus()
228 Builders‚Äô Guide
2
Now we define two convenient functions that allow us to run code even if the requested
GPUs do not exist.
def try_gpu (i=0): #@save
"""Return gpu(i) if exists, otherwise return cpu()."""
ifnum_gpus() >=i+1:
return gpu(i)
return cpu()
def try_all_gpus (): #@save
"""Return all available GPUs, or [cpu(),] if no GPU exists."""
return [gpu(i) for iinrange (num_gpus())]
try_gpu(), try_gpu( 10), try_all_gpus()
(device( type ='cuda ', index =0),
device( type ='cpu'),
[device( type ='cuda ', index =0), device( type ='cuda ', index =1)])
6.7.2Tensorsand GPUs
By default, tensors are created on the CPU. We can query the device where the tensor is
located.
x=torch .tensor([ 1,2,3])
x.device
device( type ='cpu')
It is important to note that whenever we want to operate on multiple terms, they need to be
on the same device. For instance, if we sum two tensors, we need to make sure that both
arguments live on the same device‚Äîotherwise the framework would not know where to
store the result or even how to decide where to perform the computation.
Storageon the GPU
There are several ways to store a tensor on the GPU. For example, we can specify a stor-
age device when creating a tensor. Next, we create the tensor variable Xon the first gpu.
The tensor created on a GPU only consumes the memory of this GPU. We can use the
nvidia-smi command to view GPU memory usage. In general, we need to make sure that
we do not create data that exceeds the GPU memory limit.
X=torch .ones( 2,3, device =try_gpu())
X
229 GPUs
tensor([[ 1.,1.,1.],
[1.,1.,1.]], device ='cuda:0 ')
Assuming that you have at least two GPUs, the following code will create a random tensor,
Y, on the second GPU.
Y=torch .rand( 2,3, device =try_gpu( 1))
Y
tensor([[ 0.0022 ,0.5723 ,0.2890 ],
[0.1456 ,0.3537 ,0.7359 ]], device ='cuda:1 ')
Copying
If we want to compute X + Y, we need to decide where to perform this operation. For
instance, as shown in Fig. 6.7.1 , we can transfer Xto the second GPU and perform the
operation there. Do notsimply add XandY, since this will result in an exception. The
runtime engine would not know what to do: it cannot find data on the same device and it
fails. Since Ylives on the second GPU, we need to move Xthere before we can add the
two.
tFig. 6.7.1 Copy data to perform an operation on the same device.
Z=X.cuda( 1)
print (X)
print (Z)
tensor([[ 1.,1.,1.],
[1.,1.,1.]], device ='cuda:0 ')
tensor([[ 1.,1.,1.],
[1.,1.,1.]], device ='cuda:1 ')
Now that the data (both ZandY) are on the same GPU), we can add them up.
Y+Z
tensor([[ 1.0022 ,1.5723 ,1.2890 ],
[1.1456 ,1.3537 ,1.7359 ]], device ='cuda:1 ')
230 Builders‚Äô Guide
Butwhatifyourvariable ZalreadylivedonyoursecondGPU?Whathappensifwestillcall
Z.cuda(1) ? It will return Zinstead of making a copy and allocating new memory.
Z.cuda( 1)isZ
True
Side Notes
People use GPUs to do machine learning because they expect them to be fast. But trans-
ferring variables between devices is slow: much slower than computation. So we want you
to be 100% certain that you want to do something slow before we let you do it. If the deep
learning framework just did the copy automatically without crashing then you might not
realize that you had written some slow code.
Transferring data is not only slow, it also makes parallelization a lot more difficult, since
we have to wait for data to be sent (or rather to be received) before we can proceed with
more operations. This is why copy operations should be taken with great care. As a rule of
thumb, many small operations are much worse than one big operation. Moreover, several
operations at a time are much better than many single operations interspersed in the code
unless you know what you are doing. This is the case since such operations can block if
one device has to wait for the other before it can do something else. It is a bit like ordering
your coffee in a queue rather than pre-ordering it by phone and finding out that it is ready
when you are.
Last, whenweprinttensorsorconverttensorstotheNumPyformat, ifthedataisnotinthe
main memory, the framework will copy it to the main memory first, resulting in additional
transmission overhead. Even worse, it is now subject to the dreaded global interpreter lock
that makes everything wait for Python to complete.
6.7.3NeuralNetworksand GPUs
Similarly, a neural network model can specify devices. The following code puts the model
parameters on the GPU.
net =nn.Sequential(nn .LazyLinear( 1))
net =net.to(device =try_gpu())
WewillseemanymoreexamplesofhowtorunmodelsonGPUsinthefollowingchapters,
simplybecausethemodelswillbecomesomewhatmorecomputationallyintensive.
For example, when the input is a tensor on the GPU, the model will calculate the result on
the same GPU.
net(X)
231 GPUs
tensor([[ 0.7802 ],
[0.7802 ]], device ='cuda:0 ', grad_fn =<AddmmBackward0 >)
Let‚Äôs confirm that the model parameters are stored on the same GPU.
net[ 0].weight .data .device
device( type ='cuda ', index =0)
Let the trainer support GPU.
@d2l .add_to_class(d2l .Trainer) #@save
def __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):
self .save_hyperparameters()
self .gpus =[d2l .gpu(i) for iinrange (min(num_gpus, d2l .num_gpus()))]
@d2l .add_to_class(d2l .Trainer) #@save
def prepare_batch (self , batch):
ifself .gpus:
batch =[a.to(self .gpus[ 0])for ainbatch]
return batch
@d2l .add_to_class(d2l .Trainer) #@save
def prepare_model (self , model):
model .trainer =self
model .board .xlim =[0,self .max_epochs]
ifself .gpus:
model .to(self .gpus[ 0])
self .model =model
In short, as long as all data and parameters are on the same device, we can learn models
efficiently. In the following chapters we will see several such examples.
6.7.4Summary
We can specify devices for storage and calculation, such as the CPU or GPU. By default,
data is created in the main memory and then uses the CPU for calculations. The deep
learning framework requires all input data for calculation to be on the same device, be it
CPU or the same GPU. You can lose significant performance by moving data without care.
A typical mistake is as follows: computing the loss for every minibatch on the GPU and
reporting it back to the user on the command line (or logging it in a NumPy ndarray ) will
triggeraglobalinterpreterlockwhichstallsallGPUs. Itismuchbettertoallocatememory
for logging inside the GPU and only move larger logs.
6.7.5Exercises
1.Try a larger computation task, such as the multiplication of large matrices, and see the
difference in speed between the CPU and GPU. What about a task with a small number
of calculations?
232 Builders‚Äô Guide
1182.How should we read and write model parameters on the GPU?
3.Measure the time it takes to compute 1000 matrix‚Äìmatrix multiplications of 100100
matricesandlogtheFrobeniusnormoftheoutputmatrixoneresultatatime. Compare
it with keeping a log on the GPU and transferring only the final result.
4.Measure how much time it takes to perform two matrix‚Äìmatrix multiplications on two
GPUs at the same time. Compare it with computing in in sequence on one GPU. Hint:
you should see almost linear scaling.
Discussions118.
7 Convolutional Neural Networks
Image data is represented as a two-dimensional grid of pixels, be the image monochro-
matic or in color. Accordingly each pixel corresponds to one or multiple numerical values
respectively. So far we have ignored this rich structure and treated images as vectors of
numbersby flattening them, irrespectiveofthe spatial relation betweenpixels. This deeply
unsatisfyingapproachwasnecessaryinordertofeedtheresultingone-dimensionalvectors
through a fully connected MLP.
Because these networks are invariant to the order of the features, we could get similar
results regardless of whether we preserve an order corresponding to the spatial structure
of the pixels or if we permute the columns of our design matrix before fitting the MLP‚Äôs
parameters. Ideally,wewouldleverageourpriorknowledgethatnearbypixelsaretypically
related to each other, to build efficient models for learning from image data.
This chapter introduces convolutional neural networks (CNNs) ( LeCunet al., 1995), a
powerful family of neural networks that are designed for precisely this purpose. CNN-
based architectures are now ubiquitous in the field of computer vision. For instance, on the
Imagnetcollection( Dengetal.,2009)itwasonlytheuseofconvolutionalneuralnetworks,
in short Convnets, that provided significant performance improvements ( Krizhevsky etal.,
2012).
Modern CNNs, as they are called colloquially, owe their design to inspirations from biol-
ogy,grouptheory,andahealthydoseofexperimentaltinkering. Inadditiontotheirsample
efficiency in achieving accurate models, CNNs tend to be computationally efficient, both
because they require fewer parameters than fully connected architectures and because con-
volutions are easy to parallelize across GPU cores ( Chetluret al., 2014). Consequently,
practitioners often apply CNNs whenever possible, and increasingly they have emerged
as credible competitors even on tasks with a one-dimensional sequence structure, such as
audio (Abdel-Hamid et al., 2014), text (Kalchbrenner et al., 2014), and time series analy-
sis (LeCunet al., 1995), where recurrent neural networks are conventionally used. Some
clever adaptations of CNNs have also brought them to bear on graph-structured data ( Kipf
and Welling, 2016 ) and in recommender systems.
First, wewilldivemoredeeplyintothemotivationforconvolutionalneuralnetworks. This
is followed by a walk through the basic operations that comprise the backbone of all con-
volutional networks. These include the convolutional layersthemselves, nitty-gritty details
including padding and stride, the pooling layers used to aggregate information across ad-
jacent spatial regions, the use of multiple channels at each layer, and a careful discussion
233
234 Convolutional Neural Networks
of the structure of modern architectures. We will conclude the chapter with a full working
example of LeNet, the first convolutional network successfully deployed, long before the
riseofmoderndeeplearning. Inthenextchapter, wewilldiveintofullimplementationsof
some popular and comparatively recent CNN architectures whose designs represent most
of the techniques commonly used by modern practitioners.
7.1FromFullyConnected Layersto Convolutions
To this day, the models that we have discussed so far remain appropriate options when we
are dealing with tabular data. By tabular, we mean that the data consist of rows corre-
sponding to examples and columns corresponding to features. With tabular data, we might
anticipate that the patterns we seek could involve interactions among the features, but we
do not assume any structure a priori concerning how the features interact.
Sometimes, we truly lack the knowledge to be able to guide the construction of fancier
architectures. In these cases, an MLP may be the best that we can do. However, for high-
dimensional perceptual data, such structureless networks can grow unwieldy.
For instance, let‚Äôs return to our running example of distinguishing cats from dogs. Say that
we do a thorough job in data collection, collecting an annotated dataset of one-megapixel
photographs. This means that each input to the network has one million dimensions. Even
anaggressivereductiontoonethousandhiddendimensionswouldrequireafullyconnected
layercharacterizedby 106103=109parameters. UnlesswehavelotsofGPUs,atalentfor
distributed optimization, and an extraordinary amount of patience, learning the parameters
of this network may turn out to be infeasible.
A careful reader might object to this argument on the basis that one megapixel resolution
may not be necessary. However, while we might be able to get away with one hundred
thousand pixels, our hidden layer of size 1000 grossly underestimates the number of hid-
den units that it takes to learn good representations of images, so a practical system will
still require billions of parameters. Moreover, learning a classifier by fitting so many pa-
rameters might require collecting an enormous dataset. And yet today both humans and
computers are able to distinguish cats from dogs quite well, seemingly contradicting these
intuitions. That is because images exhibit rich structure that can be exploited by humans
and machine learning models alike. Convolutional neural networks (CNNs) are one cre-
ative way that machine learning has embraced for exploiting some of the known structure
in natural images.
7.1.1Invariance
Imagine that we want to detect an object in an image. It seems reasonable that whatever
methodweusetorecognizeobjectsshouldnotbeoverlyconcernedwiththepreciselocation
of the object in the image. Ideally, our system should exploit this knowledge. Pigs usually
do not fly and planes usually do not swim. Nonetheless, we should still recognize a pig
235 From Fully Connected Layers to Convolutions
were one to appear at the top of the image. We can draw some inspiration here from the
children‚Äôsgame‚ÄúWhere‚ÄôsWaldo‚Äù(whichitselfhasinspiredmanyreal-lifeimitations,such
as that depicted in Fig. 7.1.1 ). The game consists of a number of chaotic scenes bursting
with activities. Waldo shows up somewhere in each, typically lurking in some unlikely
location. The reader‚Äôs goal is to locate him. Despite his characteristic outfit, this can be
surprisingly difficult, due to the large number of distractions. However, what Waldo looks
likedoesnotdependupon whereWaldoislocated . WecouldsweeptheimagewithaWaldo
detector that could assign a score to each patch, indicating the likelihood that the patch
contains Waldo. In fact, many object detection and segmentation algorithms are based
on this approach ( Longet al., 2015). CNNs systematize this idea of spatial invariance ,
exploiting it to learn useful representations with fewer parameters.
tFig. 7.1.1 Can you Ô¨Ånd Waldo (image courtesy of William Murphy (Infomatique))?
We can now make these intuitions more concrete by enumerating a few desiderata to guide
our design of a neural network architecture suitable for computer vision:
1.Intheearliestlayers,ournetworkshouldrespondsimilarlytothesamepatch,regardless
of where it appears in the image. This principle is called translation invariance (or
translationequivariance ).
2.The earliest layers of the network should focus on local regions, without regard for the
contents of the image in distant regions. This is the localityprinciple. Eventually, these
local representations can be aggregated to make predictions at the whole image level.
3.As we proceed, deeper layers should be able to capture longer-range features of the
image, in a way similar to higher level vision in nature.
Let‚Äôs see how this translates into mathematics.
7.1.2Constraining the MLP
Tostartoff,wecanconsideranMLPwithtwo-dimensionalimages Xasinputsandtheirim-
mediatehiddenrepresentations Hsimilarlyrepresentedasmatrices(theyaretwo-dimensional
236 Convolutional Neural Networks
tensors in code), where both XandHhave the same shape. Let that sink in. We now
imagine that not only the inputs but also the hidden representations possess spatial struc-
ture.
Let¬ªX¬ºùëñ,ùëóand¬ªH¬ºùëñ,ùëódenote the pixel at location ¬πùëñ,ùëó¬∫in the input image and hidden rep-
resentation,respectively. Consequently,tohaveeachofthehiddenunitsreceiveinputfrom
eachoftheinputpixels,wewouldswitchfromusingweightmatrices(aswedidpreviously
in MLPs) to representing our parameters as fourth-order weight tensors W. Suppose that
Ucontains biases, we could formally express the fully connected layer as
¬ªH¬ºùëñ,ùëó=¬ªU¬ºùëñ,ùëó¬∏√ï
ùëò√ï
ùëô¬ªW¬ºùëñ,ùëó,ùëò,ùëô¬ªX¬ºùëò,ùëô
=¬ªU¬ºùëñ,ùëó¬∏√ï
ùëé√ï
ùëè¬ªV¬ºùëñ,ùëó,ùëé,ùëè¬ªX¬ºùëñ¬∏ùëé,ùëó¬∏ùëè.(7.1.1)
Theswitchfrom WtoVisentirelycosmeticfornowsincethereisaone-to-onecorrespon-
dence between coefficients in both fourth-order tensors. We simply re-index the subscripts
¬πùëò,ùëô¬∫such thatùëò=ùëñ¬∏ùëéandùëô=ùëó¬∏ùëè. In other words, we set ¬ªV¬ºùëñ,ùëó,ùëé,ùëè =¬ªW¬ºùëñ,ùëó,ùëñ¬∏ùëé,ùëó¬∏ùëè.
The indicesùëéandùëèrun over both positive and negative offsets, covering the entire image.
For any given location ( ùëñ,ùëó) in the hidden representation ¬ªH¬ºùëñ,ùëó, we compute its value by
summing over pixels in ùë•, centered around¬πùëñ,ùëó¬∫and weighted by¬ªV¬ºùëñ,ùëó,ùëé,ùëè. Before we
carry on, let‚Äôs consider the total number of parameters required for a singlelayer in this
parametrization: a 10001000image (1 megapixel) is mapped to a 10001000hidden
representation. This requires 1012parameters, far beyond what computers currently can
handle.
TranslationInvariance
Now let‚Äôs invoke the first principle established above: translation invariance ( Zhanget al.,
1988). This implies that a shift in the input Xshould simply lead to a shift in the hidden
representation H. This is only possible if VandUdo not actually depend on ¬πùëñ,ùëó¬∫. As
such, we have¬ªV¬ºùëñ,ùëó,ùëé,ùëè =¬ªV¬ºùëé,ùëèandUis a constant, say ùë¢. As a result, we can simplify
the definition for H:
¬ªH¬ºùëñ,ùëó=ùë¢¬∏√ï
ùëé√ï
ùëè¬ªV¬ºùëé,ùëè¬ªX¬ºùëñ¬∏ùëé,ùëó¬∏ùëè.(7.1.2)
This is aconvolution ! We are effectively weighting pixels at ¬πùëñ¬∏ùëé,ùëó¬∏ùëè¬∫in the vicinity of
location¬πùëñ,ùëó¬∫with coefficients¬ªV¬ºùëé,ùëèto obtain the value¬ªH¬ºùëñ,ùëó. Note that¬ªV¬ºùëé,ùëèneeds
many fewer coefficients than ¬ªV¬ºùëñ,ùëó,ùëé,ùëèsince it no longer depends on the location within
the image. Consequently, the number of parameters required is no longer 1012but a much
more reasonable 4106: we still have the dependency on ùëé,ùëè2¬π  1000,1000¬∫. In short,
we have made significant progress. Time-delay neural networks (TDNNs) are some of the
first examples to exploit this idea ( Waibeletal., 1989).
Locality
Now let‚Äôs invoke the second principle: locality. As motivated above, we believe that we
should not have to look very far away from location ¬πùëñ,ùëó¬∫in order to glean relevant infor-
237 From Fully Connected Layers to Convolutions
mation to assess what is going on at ¬ªH¬ºùëñ,ùëó. This means that outside some range jùëéj>Œî
orjùëèj>Œî, we should set¬ªV¬ºùëé,ùëè=0. Equivalently, we can rewrite ¬ªH¬ºùëñ,ùëóas
¬ªH¬ºùëñ,ùëó=ùë¢¬∏Œî√ï
ùëé= ŒîŒî√ï
ùëè= Œî¬ªV¬ºùëé,ùëè¬ªX¬ºùëñ¬∏ùëé,ùëó¬∏ùëè. (7.1.3)
Thisreducesthenumberofparametersfrom 4106to4Œî2,where Œîistypicallysmallerthan
10. As such, we reduced the number of parameters by another four orders of magnitude.
Note that (7.1.3 ), is what is called, in a nutshell, a convolutional layer .Convolutional
neuralnetworks (CNNs) are a special family of neural networks that contain convolutional
layers. In the deep learning research community, Vis referred to as a convolution kernel ,
afilter, or simply the layer‚Äôs weightsthat are learnable parameters.
While previously, we might have required billions of parameters to represent just a single
layer in an image-processing network, we now typically need just a few hundred, without
altering the dimensionality of either the inputs or the hidden representations. The price
paidforthisdrasticreductioninparametersisthatourfeaturesarenowtranslationinvariant
and that our layer can only incorporate local information, when determining the value of
each hidden activation. All learning depends on imposing inductive bias. When that bias
agrees with reality, we get sample-efficient models that generalize well to unseen data. But
of course, if those biases do not agree with reality, e.g., if images turned out not to be
translation invariant, our models might struggle even to fit our training data.
Thisdramaticreductioninparametersbringsustoourlastdesideratum,namelythatdeeper
layersshouldrepresentlargerandmorecomplexaspectsofanimage. Thiscanbeachieved
by interleaving nonlinearities and convolutional layers repeatedly.
7.1.3Convolutions
Let‚Äôs briefly review why (7.1.3 )is called a convolution. In mathematics, the convolution
between two functions ( Rudin, 1973 ), sayùëì,ùëî:Rùëë!Ris defined as
¬πùëìùëî¬∫¬πx¬∫=¬π
ùëì¬πz¬∫ùëî¬πx z¬∫ùëëz. (7.1.4)
Thatis,wemeasuretheoverlapbetween ùëìandùëîwhenonefunctionis‚Äúflipped‚Äùandshifted
byx. Whenever we have discrete objects, the integral turns into a sum. For instance, for
vectors from the set of square-summable infinite-dimensional vectors with index running
overZwe obtain the following definition:
¬πùëìùëî¬∫¬πùëñ¬∫=√ï
ùëéùëì¬πùëé¬∫ùëî¬πùëñ ùëé¬∫.(7.1.5)
For two-dimensional tensors, we have a corresponding sum with indices ¬πùëé,ùëè¬∫forùëìand
¬πùëñ ùëé,ùëó ùëè¬∫forùëî, respectively:
¬πùëìùëî¬∫¬πùëñ,ùëó¬∫=√ï
ùëé√ï
ùëèùëì¬πùëé,ùëè¬∫ùëî¬πùëñ ùëé,ùëó ùëè¬∫.(7.1.6)
This looks similar to (7.1.3 ), with one major difference. Rather than using ¬πùëñ¬∏ùëé,ùëó¬∏ùëè¬∫,
we are using the difference instead. Note, though, that this distinction is mostly cosmetic
238 Convolutional Neural Networks
sincewecanalwaysmatchthenotationbetween (7.1.3 )and(7.1.6 ). Ouroriginaldefinition
in(7.1.3 )more properly describes a cross-correlation . We will come back to this in the
following section.
7.1.4Channels
ReturningtoourWaldodetector,let‚Äôsseewhatthislookslike. Theconvolutionallayerpicks
windows of a given size and weighs intensities according to the filter V, as demonstrated
inFig. 7.1.2 . We might aim to learn a model so that wherever the ‚Äúwaldoness‚Äù is highest,
we should find a peak in the hidden layer representations.
tFig. 7.1.2 Detect Waldo (image courtesy of William Murphy (Infomatique)).
There is just one problem with this approach. So far, we blissfully ignored that images
consist of three channels: red, green, and blue. In sum, images are not two-dimensional
objects but rather third-order tensors, characterized by a height, width, and channel, e.g.,
withshape 102410243pixels. Whilethefirsttwooftheseaxesconcernspatialrelation-
ships,thethirdcanberegardedasassigningamultidimensionalrepresentationtoeachpixel
location. We thus index Xas¬ªX¬ºùëñ,ùëó,ùëò. The convolutional filter has to adapt accordingly.
Instead of¬ªV¬ºùëé,ùëè, we now have¬ªV¬ºùëé,ùëè,ùëê.
Moreover, just as our input consists of a third-order tensor, it turns out to be a good idea
to similarly formulate our hidden representations as third-order tensors H. In other words,
ratherthanjusthavingasinglehiddenrepresentationcorrespondingtoeachspatiallocation,
we want an entire vector of hidden representations corresponding to each spatial location.
We could think of the hidden representations as comprising a number of two-dimensional
grids stacked on top of each other. As in the inputs, these are sometimes called channels .
They are also sometimes called feature maps , as each provides a spatialized set of learned
featuresforthesubsequentlayer. Intuitively,youmightimaginethatatlowerlayersthatare
closer to inputs, some channels could become specialized to recognize edges while others
could recognize textures.
Tosupportmultiplechannelsinbothinputs( X)andhiddenrepresentations( H),wecanadd
239 From Fully Connected Layers to Convolutions
a fourth coordinate to V:¬ªV¬ºùëé,ùëè,ùëê,ùëë. Putting everything together we have:
¬ªH¬ºùëñ,ùëó,ùëë=Œî√ï
ùëé= ŒîŒî√ï
ùëè= Œî√ï
ùëê¬ªV¬ºùëé,ùëè,ùëê,ùëë¬ªX¬ºùëñ¬∏ùëé,ùëó¬∏ùëè,ùëê, (7.1.7)
whereùëëindexes the output channels in the hidden representations H. The subsequent con-
volutionallayerwillgoontotakeathird-ordertensor, H,asinput. Wetake (7.1.7 ),because
of its generality, as the definition of a convolutional layer for multiple channels, where V
is a kernel or filter of the layer.
Thereare still manyoperations that weneed to address. Forinstance, weneed to figure out
how to combine all the hidden representations to a single output, e.g., whether there is a
Waldoanywhere in the image. We also need to decide how to compute things efficiently,
howtocombinemultiplelayers, appropriateactivationfunctions, andhowtomakereason-
able design choices to yield networks that are effective in practice. We turn to these issues
in the remainder of the chapter.
7.1.5Summaryand Discussion
In this section we derived the structure of convolutional neural networks from first prin-
ciples. While it is unclear whether this was the route taken to the invention of CNNs, it
is satisfying to know that they are the rightchoice when applying reasonable principles
to how image processing and computer vision algorithms should operate, at least at lower
levels. In particular, translation invariance in images implies that all patches of an image
will be treated in the same manner. Locality means that only a small neighborhood of pix-
els will be used to compute the corresponding hidden representations. Some of the earliest
references to CNNs are in the form of the Neocognitron ( Fukushima, 1982 ).
A second principle that we encountered in our reasoning is how to reduce the number of
parameters in a function class without limiting its expressive power, at least, whenever
certain assumptions on the model hold. We saw a dramatic reduction of complexity as a
result of this restriction, turning computationally and statistically infeasible problems into
tractable models.
Addingchannelsallowedustobringbacksomeofthecomplexitythatwaslostduetothere-
strictions imposed on the convolutional kernel by locality and translation invariance. Note
that it is quite natural to add channels other than just red, green, and blue. Many satellite
images, in particular for agriculture and meteorology, have tens to hundreds of channels,
generating hyperspectral images instead. They report data on many different wavelengths.
In the following we will see how to use convolutions effectively to manipulate the dimen-
sionalityoftheimagestheyoperateon, howtomovefromlocation-basedtochannel-based
representations, and how to deal with large numbers of categories efficiently.
7.1.6Exercises
1.Assume that the size of the convolution kernel is Œî = 0. Show that in this case the
convolution kernel implements an MLP independently for each set of channels. This
leads to the Network in Network architectures ( Linetal., 2013).
240 Convolutional Neural Networks
1192.Audio data is often represented as a one-dimensional sequence.
1.When might you want to impose locality and translation invariance for audio?
2.Derive the convolution operations for audio.
3.Can you treat audio using the same tools as computer vision? Hint: use the spectro-
gram.
3.Why might translation invariance not be a good idea after all? Give an example.
4.Do you think that convolutional layers might also be applicable for text data? Which
problems might you encounter with language?
5.What happens with convolutions when an object is at the boundary of an image?
6.Prove that the convolution is symmetric, i.e., ùëìùëî=ùëîùëì.
Discussions119.
7.2ConvolutionsforImages
Now that we understand how convolutional layers work in theory, we are ready to see how
they work in practice. Building on our motivation of convolutional neural networks as
efficient architectures for exploring structure in image data, we stick with images as our
running example.
import torch
from torch import nn
from d2l import torch asd2l
7.2.1The Cross-CorrelationOperation
Recallthatstrictlyspeaking,convolutionallayersareamisnomer,sincetheoperationsthey
express are more accurately described as cross-correlations. Based on our descriptions of
convolutional layers in Section 7.1 , in such a layer, an input tensor and a kernel tensor are
combined to produce an output tensor through a cross-correlation operation.
Let‚Äôsignorechannelsfornowandseehowthisworkswithtwo-dimensionaldataandhidden
representations. In Fig. 7.2.1 , the input is a two-dimensional tensor with a height of 3 and
width of 3. We mark the shape of the tensor as 33or (3,3). The height and width of the
kernel are both 2. The shape of the kernelwindow (orconvolutionwindow ) is given by the
height and width of the kernel (here it is 22).
Inthetwo-dimensionalcross-correlationoperation,webeginwiththeconvolutionwindow
positioned at the upper-left corner of the input tensor and slide it across the input tensor,
both from left to right and top to bottom. When the convolution window slides to a certain
241 Convolutions for Images
tFig. 7.2.1 Two-dimensional cross-correlation operation. The shaded portions are the Ô¨Årst output
element as well as the input and kernel tensor elements used for the output computation:
00¬∏11¬∏32¬∏43=19.
position, the input subtensor contained in that window and the kernel tensor are multiplied
elementwise and the resulting tensor is summed up yielding a single scalar value. This
result gives the value of the output tensor at the corresponding location. Here, the output
tensor has a height of 2 and width of 2 and the four elements are derived from the two-
dimensional cross-correlation operation:
00¬∏11¬∏32¬∏43=19,
10¬∏21¬∏42¬∏53=25,
30¬∏41¬∏62¬∏73=37,
40¬∏51¬∏72¬∏83=43.(7.2.1)
Note that along each axis, the output size is slightly smaller than the input size. Because
the kernel has width and height greater than 1, we can only properly compute the cross-
correlation for locations where the kernel fits wholly within the image, the output size is
given by the input size ùëõhùëõwminus the size of the convolution kernel ùëòhùëòwvia
¬πùëõh ùëòh¬∏1¬∫¬πùëõw ùëòw¬∏1¬∫. (7.2.2)
This is the case since we need enough space to ‚Äúshift‚Äù the convolution kernel across the
image. Later we will see how to keep the size unchanged by padding the image with zeros
around its boundary so that there is enough space to shift the kernel. Next, we implement
this process in the corr2dfunction, which accepts an input tensor Xand a kernel tensor K
and returns an output tensor Y.
def corr2d (X, K): #@save
"""Compute 2D cross-correlation."""
h, w =K.shape
Y=torch .zeros((X .shape[ 0]-h+1, X.shape[ 1]-w+1))
for iinrange (Y.shape[ 0]):
for jinrange (Y.shape[ 1]):
Y[i, j] =(X[i:i +h, j:j +w]*K).sum()
return Y
We can construct the input tensor Xand the kernel tensor KfromFig. 7.2.1 to validate
the output of the above implementation of the two-dimensional cross-correlation opera-
tion.
X=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])
(continues on next page)
242 Convolutional Neural Networks
(continued from previous page)
K=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])
corr2d(X, K)
tensor([[ 19.,25.],
[37.,43.]])
7.2.2ConvolutionalLayers
Aconvolutionallayercross-correlatestheinputandkernelandaddsascalarbiastoproduce
an output. The two parameters of a convolutional layer are the kernel and the scalar bias.
When training models based on convolutional layers, we typically initialize the kernels
randomly, just as we would with a fully connected layer.
Wearenowreadytoimplementatwo-dimensionalconvolutionallayerbasedonthe corr2d
function defined above. In the __init__ constructor method, wedeclare weightandbias
as the two model parameters. The forward propagation method calls the corr2dfunction
and adds the bias.
class Conv2D (nn.Module):
def __init__ (self , kernel_size):
super ().__init__ ()
self .weight =nn.Parameter(torch .rand(kernel_size))
self .bias =nn.Parameter(torch .zeros( 1))
def forward (self , x):
return corr2d(x, self .weight) +self .bias
In‚Ñéùë§convolutionoran ‚Ñéùë§convolutionkernel,theheightandwidthoftheconvolution
kernel are‚Ñéandùë§, respectively. We also refer to a convolutional layer with an ‚Ñéùë§
convolution kernel simply as an ‚Ñéùë§convolutional layer.
7.2.3Object EdgeDetectionin Images
Let‚Äôs take a moment to parse a simple application of a convolutional layer: detecting the
edgeofanobjectinanimagebyfindingthelocationofthepixelchange. First,weconstruct
an ‚Äúimage‚Äù of 68pixels. The middle four columns are black ( 0) and the rest are white
(1).
X=torch .ones(( 6,8))
X[:, 2:6]=0
X
tensor([[ 1.,1.,0.,0.,0.,0.,1.,1.],
[1.,1.,0.,0.,0.,0.,1.,1.],
[1.,1.,0.,0.,0.,0.,1.,1.],
(continues on next page)
243 Convolutions for Images
(continued from previous page)
[1.,1.,0.,0.,0.,0.,1.,1.],
[1.,1.,0.,0.,0.,0.,1.,1.],
[1.,1.,0.,0.,0.,0.,1.,1.]])
Next, we construct a kernel Kwith a height of 1 and a width of 2. When we perform
the cross-correlation operation with the input, if the horizontally adjacent elements are
the same, the output is 0. Otherwise, the output is nonzero. Note that this kernel is a
special case of a finite difference operator. At location ¬πùëñ,ùëó¬∫it computes ùë•ùëñ,ùëó ùë•¬πùëñ¬∏1¬∫,ùëó,
i.e., it computes the difference between the values of horizontally adjacent pixels. This is
a discrete approximation of the first derivative in the horizontal direction. After all, for
a functionùëì¬πùëñ,ùëó¬∫its derivative ùúïùëñùëì¬πùëñ,ùëó¬∫=limùúñ!0ùëì¬πùëñ,ùëó¬∫ ùëì¬πùëñ¬∏ùúñ,ùëó¬∫
ùúñ. Let‚Äôs see how this
works in practice.
K=torch .tensor([[ 1.0,-1.0]])
We are ready to perform the cross-correlation operation with arguments X(our input) and
K(our kernel). As you can see, we detect 1for the edge from white to black and  1for the
edge from black to white. All other outputs take value 0.
Y=corr2d(X, K)
Y
tensor([[ 0.,1.,0.,0.,0.,-1.,0.],
[0.,1.,0.,0.,0.,-1.,0.],
[0.,1.,0.,0.,0.,-1.,0.],
[0.,1.,0.,0.,0.,-1.,0.],
[0.,1.,0.,0.,0.,-1.,0.],
[0.,1.,0.,0.,0.,-1.,0.]])
Wecannowapplythekerneltothetransposedimage. Asexpected, itvanishes. Thekernel
Konly detects vertical edges.
corr2d(X .t(), K)
tensor([[ 0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.]])
7.2.4Learning a Kernel
244 Convolutional Neural Networks
Designinganedgedetectorbyfinitedifferences [1, -1] isneatifweknowthisisprecisely
what we are looking for. However, as we look at larger kernels, and consider successive
layers of convolutions, it might be impossible to specify precisely what each filter should
be doing manually.
Now let‚Äôs see whether we can learn the kernel that generated Yfrom Xby looking at the
input‚Äìoutput pairs only. We first construct a convolutional layer and initialize its kernel as
arandomtensor. Next,ineachiteration,wewillusethesquarederrortocompare Ywiththe
output of the convolutional layer. We can then calculate the gradient to update the kernel.
For the sake of simplicity, in the following we use the built-in class for two-dimensional
convolutional layers and ignore the bias.
# Construct a two-dimensional convolutional layer with 1 output channel and a
# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here
conv2d =nn.LazyConv2d( 1, kernel_size =(1,2), bias =False )
# The two-dimensional convolutional layer uses four-dimensional input and
# output in the format of (example, channel, height, width), where the batch
# size (number of examples in the batch) and the number of channels are both 1
X=X.reshape(( 1,1,6,8))
Y=Y.reshape(( 1,1,6,7))
lr=3e-2 # Learning rate
for iinrange (10):
Y_hat =conv2d(X)
l=(Y_hat -Y)**2
conv2d .zero_grad()
l.sum() .backward()
# Update the kernel
conv2d .weight .data[:] -=lr*conv2d .weight .grad
if(i+1)%2==0:
print (f'epoch {i+1}, loss {l.sum() :.3f}')
epoch 2, loss 16.481
epoch 4, loss 5.069
epoch 6, loss 1.794
epoch 8, loss 0.688
epoch 10, loss 0.274
Notethattheerrorhasdroppedtoasmallvalueafter10iterations. Nowwewilltakealook
at the kernel tensor we learned.
conv2d .weight .data .reshape(( 1,2))
tensor([[ 1.0398 ,-0.9328 ]])
Indeed, the learned kernel tensor is remarkably close to the kernel tensor Kwe defined
earlier.
245 Convolutions for Images
7.2.5Cross-Correlationand Convolution
Recallourobservationfrom Section7.1 ofthecorrespondencebetweenthecross-correlation
andconvolutionoperations. Herelet‚Äôscontinuetoconsidertwo-dimensionalconvolutional
layers. What if such layers perform strict convolution operations as defined in (7.1.6 )in-
steadofcross-correlations? Inordertoobtaintheoutputofthestrict convolution operation,
weonlyneedtoflipthetwo-dimensionalkerneltensorbothhorizontallyandvertically,and
then perform the cross-correlation operation with the input tensor.
It is noteworthy that since kernels are learned from data in deep learning, the outputs of
convolutional layers remain unaffected no matter such layers perform either the strict con-
volution operations or the cross-correlation operations.
To illustrate this, suppose that a convolutional layer performs cross-correlation and learns
the kernel in Fig. 7.2.1 , which is here denoted as the matrix K. Assuming that other con-
ditions remain unchanged, when this layer instead performs strict convolution , the learned
kernel K0will be the same as KafterK0is flipped both horizontally and vertically. That
is to say, when the convolutional layer performs strict convolution for the input in Fig.
7.2.1andK0, the same output in Fig. 7.2.1 (cross-correlation of the input and K) will be
obtained.
Inkeepingwithstandardterminologyindeeplearningliterature,wewillcontinuetoreferto
thecross-correlationoperationasaconvolutioneventhough,strictly-speaking,itisslightly
different. Furthermore, we use the term element to refer to an entry (or component) of any
tensor representing a layer representation or a convolution kernel.
7.2.6FeatureMap and ReceptiveField
As described in Section 7.1.4 , the convolutional layer output in Fig. 7.2.1 is sometimes
called afeature map , as it can be regarded as the learned representations (features) in the
spatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any el-
ementùë•of some layer, its receptive field refers to all the elements (from all the previous
layers) that may affect the calculation of ùë•during the forward propagation. Note that the
receptive field may be larger than the actual size of the input.
Let‚Äôs continue to use Fig. 7.2.1 to explain the receptive field. Given the 22convolution
kernel, the receptive field of the shaded output element (of value 19) is the four elements
in the shaded portion of the input. Now let‚Äôs denote the 22output as Yand consider a
deeperCNNwithanadditional 22convolutionallayerthattakes Yasitsinput,outputting
a single element ùëß. In this case, the receptive field of ùëßonYincludes all the four elements
ofY,whilethereceptivefieldontheinputincludesallthenineinputelements. Thus,when
any element in a feature map needs a larger receptive field to detect input features over a
broader area, we can build a deeper network.
Receptive fields derive their name from neurophysiology. A series of experiments on a
rangeofanimalsusingdifferentstimuli( HubelandWiesel, 1959 ,HubelandWiesel, 1962 ,
Hubel and Wiesel, 1968 ) explored the response of what is called the visual cortex on said
stimuli. By and large they found that lower levels respond to edges and related shapes.
246 Convolutional Neural Networks
Lateron,Field( 1987)illustratedthiseffectonnaturalimageswith,whatcanonlybecalled,
convolutional kernels. We reprint a key figure in Fig. 7.2.2 to illustrate the striking simi-
larities.
tFig. 7.2.2 Figure and caption taken from Field ( 1987 ): An example of coding with six different
channels. (Left) Examples of the six types of sensor associated with each channel. (Right)
Convolution of the image in (Middle) with the six sensors shown in (Left). The response
of the individual sensors is determined by sampling these Ô¨Åltered images at a distance
proportional to the size of the sensor (shown with dots). This diagram shows the response
of only the even symmetric sensors.
As it turns out, this relation even holds for the features computed by deeper layers of net-
works trained on image classification tasks, as demonstrated in, for example, Kuzovkin et
al.(2018). Suffice it to say, convolutions have proven to be an incredibly powerful tool for
computer vision, both in biology and in code. As such, it is not surprising (in hindsight)
that they heralded the recent success in deep learning.
7.2.7Summary
The core computation required for a convolutional layer is a cross-correlation operation.
We saw that a simple nested for-loop is all that is required to compute its value. If we
have multiple input and multiple output channels, we are performing a matrix‚Äìmatrix op-
eration between channels. As can be seen, the computation is straightforward and, most
importantly, highly local. This affords significant hardware optimization and many recent
results in computer vision are only possible because of that. After all, it means that chip
designers can invest in fast computation rather than memory when it comes to optimizing
for convolutions. While this may not lead to optimal designs for other applications, it does
open the door to ubiquitous and affordable computer vision.
247 Padding and Stride
120In terms of convolutions themselves, they can be used for many purposes, for example
detecting edges and lines, blurring images, or sharpening them. Most importantly, it is
not necessary that the statistician (or engineer) invents suitable filters. Instead, we can
simplylearnthem from data. This replaces feature engineering heuristics by evidence-
based statistics. Lastly, and quite delightfully, these filters are not just advantageous for
building deep networks but they also correspond to receptive fields and feature maps in the
brain. This gives us confidence that we are on the right track.
7.2.8Exercises
1.Construct an image Xwith diagonal edges.
1.What happens if you apply the kernel Kin this section to it?
2.What happens if you transpose X?
3.What happens if you transpose K?
2.Design some kernels manually.
1.Given a directional vector v=¬πùë£1,ùë£2¬∫, derive an edge-detection kernel that detects
edges orthogonal to v, i.e., edges in the direction ¬πùë£2, ùë£1¬∫.
2.Derive a finite difference operator for the second derivative. What is the minimum
size of the convolutional kernel associated with it? Which structures in images re-
spond most strongly to it?
3.How would you design a blur kernel? Why might you want to use such a kernel?
4.What is the minimum size of a kernel to obtain a derivative of order ùëë?
3.When you try to automatically find the gradient for the Conv2Dclass we created, what
kind of error message do you see?
4.Howdoyourepresentacross-correlationoperationasamatrixmultiplicationbychang-
ing the input and kernel tensors?
Discussions120.
7.3Paddingand Stride
Recall the example of a convolution in Fig. 7.2.1 . The input had both a height and width of
3and the convolutionkernel hadboth a height and width of2, yielding anoutput represen-
tation with dimension 22. Assuming that the input shape is ùëõhùëõwand the convolution
kernel shape is ùëòhùëòw, the output shape will be ¬πùëõh ùëòh¬∏1¬∫¬πùëõw ùëòw¬∏1¬∫: we can
only shift the convolution kernel so far until it runs out of pixels to apply the convolution
to.
248 Convolutional Neural Networks
In the following we will explore a number of techniques, including padding and strided
convolutions, that offer more control over the size of the output. As motivation, note that
sincekernelsgenerallyhavewidthandheightgreaterthan 1,afterapplyingmanysuccessive
convolutions, wetendtowindupwithoutputsthatareconsiderablysmallerthanourinput.
If we start with a 240240pixel image, ten layers of 55convolutions reduce the image
to200200pixels, slicing off 30%of the image and with it obliterating any interesting
information on the boundaries of the original image. Padding is the most popular tool for
handling this issue. In other cases, we may want to reduce the dimensionality drastically,
e.g., if we find the original input resolution to be unwieldy. Strided convolutions are a
popular technique that can help in these instances.
import torch
from torch import nn
7.3.1Padding
As described above, one tricky issue when applying convolutional layers is that we tend
to lose pixels on the perimeter of our image. Consider Fig. 7.3.1 that depicts the pixel
utilization as a function of the convolution kernel size and the position within the image.
The pixels in the corners are hardly used at all.
tFig. 7.3.1 Pixel utilization for convolutions of size 1 1, 22, and 33 respectively.
Since we typically use small kernels, for any given convolution we might only lose a few
pixels but this can add up as we apply many successive convolutional layers. One straight-
forward solution to this problem is to add extra pixels of filler around the boundary of our
input image, thus increasing the effective size of the image. Typically, we set the values of
the extra pixels to zero. In Fig. 7.3.2 , we pad a 33input, increasing its size to 55. The
correspondingoutputthenincreasestoa 44matrix. Theshadedportionsarethefirstout-
putelementaswellastheinputandkerneltensorelementsusedfortheoutputcomputation:
00¬∏01¬∏02¬∏03=0.
tFig. 7.3.2 Two-dimensional cross-correlation with padding.
249 Padding and Stride
In general, if we add a total of ùëùhrows of padding (roughly half on top and half on bottom)
and a total of ùëùwcolumns of padding (roughly half on the left and half on the right), the
output shape will be
¬πùëõh ùëòh¬∏ùëùh¬∏1¬∫¬πùëõw ùëòw¬∏ùëùw¬∏1¬∫. (7.3.1)
This means that the height and width of the output will increase by ùëùhandùëùw, respec-
tively.
In many cases, we will want to set ùëùh=ùëòh 1andùëùw=ùëòw 1to give the input and
output the same height and width. This will make it easier to predict the output shape of
each layer when constructing the network. Assuming that ùëòhis odd here, we will pad ùëùh¬ù2
rows on both sides of the height. If ùëòhis even, one possibility is to pad dùëùh¬ù2erows on the
top of the input and bùëùh¬ù2crows on the bottom. We will pad both sides of the width in the
same way.
CNNs commonly use convolution kernels with odd height and width values, such as 1, 3,
5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality
while padding with the same number of rows on top and bottom, and the same number of
columns on left and right.
Moreover, this practice of using odd kernels and padding to precisely preserve dimension-
ality offers a clerical benefit. For any two-dimensional tensor X, when the kernel‚Äôs size is
odd and the number of padding rows and columns on all sides are the same, thereby pro-
ducinganoutputwiththesameheightandwidthastheinput,weknowthattheoutput Y[i,
j]is calculated by cross-correlation of the input and convolution kernel with the window
centered on X[i, j] .
In the following example, we create a two-dimensional convolutional layer with a height
and width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and
width of 8, we find that the height and width of the output is also 8.
# We define a helper function to calculate convolutions. It initializes the
# convolutional layer weights and performs corresponding dimensionality
# elevations and reductions on the input and output
def comp_conv2d (conv2d, X):
# (1, 1) indicates that batch size and the number of channels are both 1
X=X.reshape(( 1,1)+X.shape)
Y=conv2d(X)
# Strip the first two dimensions: examples and channels
return Y.reshape(Y .shape[ 2:])
# 1 row and column is padded on either side, so a total of 2 rows or columns
# are added
conv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1)
X=torch .rand(size =(8,8))
comp_conv2d(conv2d, X) .shape
torch .Size([ 8,8])
250 Convolutional Neural Networks
When the height and width of the convolution kernel are different, we can make the output
and input have the same height and width by setting different padding numbers for height
and width.
# We use a convolution kernel with height 5 and width 3. The padding on either
# side of the height and width are 2 and 1, respectively
conv2d =nn.LazyConv2d( 1, kernel_size =(5,3), padding =(2,1))
comp_conv2d(conv2d, X) .shape
torch .Size([ 8,8])
7.3.2Stride
When computing the cross-correlation, we start with the convolution window at the upper-
left corner of the input tensor, and then slide it over all locations both down and to the
right. In the previous examples, we defaulted to sliding one element at a time. However,
sometimes, either for computational efficiency or because we wish to downsample, we
move our window more than one element at a time, skipping the intermediate locations.
This is particularly useful if the convolution kernel is large since it captures a large area of
the underlying image.
We refer to the number of rows and columns traversed per slide as stride. So far, we have
usedstridesof1,bothforheightandwidth. Sometimes,wemaywanttousealargerstride.
Fig. 7.3.3 shows a two-dimensional cross-correlation operation with a stride of 3 vertically
and 2 horizontally. The shaded portions are the output elements as well as the input and
kernel tensor elements used for the output computation: 00¬∏01¬∏12¬∏23=8,
00¬∏61¬∏02¬∏03=6. Wecanseethatwhenthesecondelementofthefirstcolumnis
generated,theconvolutionwindowslidesdownthreerows. Theconvolutionwindowslides
two columns to the right when the second element of the first row is generated. When the
convolution window continues to slide two columns to the right on the input, there is no
output because the input element cannot fill the window (unless we add another column of
padding).
tFig. 7.3.3 Cross-correlation with strides of 3 and 2 for height and width, respectively.
In general, when the stride for the height is ùë†hand the stride for the width is ùë†w, the output
shape is
b¬πùëõh ùëòh¬∏ùëùh¬∏ùë†h¬∫¬ùùë†hcb¬πùëõw ùëòw¬∏ùëùw¬∏ùë†w¬∫¬ùùë†wc. (7.3.2)
If we setùëùh=ùëòh 1andùëùw=ùëòw 1, then the output shape can be simplified to b¬πùëõh¬∏
251 Padding and Stride
ùë†h 1¬∫¬ùùë†hcb¬πùëõw¬∏ùë†w 1¬∫¬ùùë†wc. Going a step further, if the input height and width are
divisible by the strides on the height and width, then the output shape will be ¬πùëõh¬ùùë†h¬∫
¬πùëõw¬ùùë†w¬∫.
Below, we set the strides on both the height and width to 2, thus halving the input height
and width.
conv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1, stride =2)
comp_conv2d(conv2d, X) .shape
torch .Size([ 4,4])
Let‚Äôs look at a slightly more complicated example.
conv2d =nn.LazyConv2d( 1, kernel_size =(3,5), padding =(0,1), stride =(3,4))
comp_conv2d(conv2d, X) .shape
torch .Size([ 2,2])
7.3.3Summaryand Discussion
Padding can increase the height and width of the output. This is often used to give the
output the same height and width as the input to avoid undesirable shrinkage of the output.
Moreover,itensuresthatallpixelsareusedequallyfrequently. Typicallywepicksymmetric
padding on both sides of the input height and width. In this case we refer to ¬πùëùh,ùëùw¬∫
padding. Most commonly we set ùëùh=ùëùw, in which case we simply state that we choose
paddingùëù.
A similar convention applies to strides. When horizontal stride ùë†hand vertical stride ùë†w
match, we simply talk about stride ùë†. The stride can reduce the resolution of the output, for
example reducing the height and width of the output to only 1¬ùùëõof the height and width of
the input for ùëõ> 1. By default, the padding is 0 and the stride is 1.
So far all padding that we discussed simply extended images with zeros. This has signif-
icant computational benefit since it is trivial to accomplish. Moreover, operators can be
engineered to take advantage of this padding implicitly without the need to allocate addi-
tional memory. At the same time, it allows CNNs to encode implicit position information
withinanimage,simplybylearningwherethe‚Äúwhitespace‚Äùis. Therearemanyalternatives
to zero-padding. Alsallakh et al.(2020) provided an extensive overview of those (albeit
without a clear case for when to use nonzero paddings unless artifacts occur).
7.3.4Exercises
1.Given the final code example in this section with kernel size ¬π3,5¬∫, padding¬π0,1¬∫, and
stride¬π3,4¬∫, calculate the output shape to check if it is consistent with the experimental
result.
252 Convolutional Neural Networks
1212.For audio signals, what does a stride of 2 correspond to?
3.Implement mirror padding, i.e., padding where the border values are simply mirrored
to extend tensors.
4.What are the computational benefits of a stride larger than 1?
5.What might be statistical benefits of a stride larger than 1?
6.Howwouldyouimplementastrideof1
2? Whatdoesitcorrespondto? Whenwouldthis
be useful?
Discussions121.
7.4MultipleInputand Multiple Output Channels
While we described the multiple channels that comprise each image (e.g., color images
have the standard RGB channels to indicate the amount of red, green and blue) and con-
volutional layers for multiple channels in Section 7.1.4 , until now, we simplified all of our
numerical examples by working with just a single input and a single output channel. This
allowedustothinkofourinputs,convolutionkernels,andoutputseachastwo-dimensional
tensors.
When we add channels into the mix, our inputs and hidden representations both become
three-dimensional tensors. For example, each RGB input image has shape 3‚Ñéùë§. We
refer to this axis, with a size of 3, as the channel dimension. The notion of channels is
as old as CNNs themselves: for instance LeNet-5 ( LeCunet al., 1995) uses them. In this
section, we will take a deeper look at convolution kernels with multiple input and multiple
output channels.
import torch
from d2l import torch asd2l
7.4.1Multiple InputChannels
When the input data contains multiple channels, we need to construct a convolution kernel
with the same number of input channels as the input data, so that it can perform cross-
correlation with the input data. Assuming that the number of channels for the input data
isùëêi, the number of input channels of the convolution kernel also needs to be ùëêi. If our
convolution kernel‚Äôs window shape is ùëòhùëòw, then, when ùëêi=1, we can think of our
convolution kernel as just a two-dimensional tensor of shape ùëòhùëòw.
However, when ùëêi>1, we need a kernel that contains a tensor of shape ùëòhùëòwforev-
eryinput channel. Concatenating these ùëêitensors together yields a convolution kernel of
shapeùëêiùëòhùëòw. Since the input and convolution kernel each have ùëêichannels, we can
253 Multiple Input and Multiple Output Channels
perform a cross-correlation operation on the two-dimensional tensor of the input and the
two-dimensional tensor of the convolution kernel for each channel, adding the ùëêiresults
together (summing over the channels) to yield a two-dimensional tensor. This is the result
of a two-dimensional cross-correlation between a multi-channel input and a multi-input-
channel convolution kernel.
Fig.7.4.1 providesanexampleofatwo-dimensionalcross-correlationwithtwoinputchan-
nels. The shaded portions are the first output element as well as the input and kernel tensor
elements used for the output computation: ¬π11¬∏22¬∏43¬∏54¬∫¬∏¬π 00¬∏11¬∏
32¬∏43¬∫=56.
tFig. 7.4.1 Cross-correlation computation with two input channels.
Tomakesurewereallyunderstandwhatisgoingonhere,wecanimplementcross-correlation
operations with multiple input channels ourselves. Notice that all we are doing is perform-
ing a cross-correlation operation per channel and then adding up the results.
def corr2d_multi_in (X, K):
# Iterate through the 0th dimension (channel) of K first, then add them up
return sum(d2l .corr2d(x, k) for x, k inzip(X, K))
We can construct the input tensor Xand the kernel tensor Kcorresponding to the values in
Fig. 7.4.1 to validate the output of the cross-correlation operation.
X=torch .tensor([[[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]],
[[1.0,2.0,3.0], [ 4.0,5.0,6.0], [ 7.0,8.0,9.0]]])
K=torch .tensor([[[ 0.0,1.0], [ 2.0,3.0]], [[ 1.0,2.0], [ 3.0,4.0]]])
corr2d_multi_in(X, K)
tensor([[ 56.,72.],
[104. ,120. ]])
7.4.2MultipleOutput Channels
Regardless of the number of input channels, so far we always ended up with one output
channel. However, as we discussed in Section 7.1.4 , it turns out to be essential to have
multiple channels at each layer. In the most popular neural network architectures, we actu-
allyincreasethechanneldimensionaswegodeeperintheneuralnetwork, typicallydown-
sampling to trade off spatial resolution for greater channel depth . Intuitively, you could
254 Convolutional Neural Networks
think of each channel as responding to a different set of features. The reality is a bit more
complicatedthanthis. Anaiveinterpretationwouldsuggestthatrepresentationsarelearned
independentlyperpixelorperchannel. Instead,channelsareoptimizedtobejointlyuseful.
This means that rather than mapping a single channel to an edge detector, it may simply
mean that some direction in channel space corresponds to detecting edges.
Denotebyùëêiandùëêothenumberofinputandoutputchannels,respectively,andby ùëòhandùëòw
the height and width of the kernel. To get an output with multiple channels, we can create
a kernel tensor of shape ùëêiùëòhùëòwforeveryoutput channel. We concatenate them on the
output channel dimension, so that the shape of the convolution kernel is ùëêoùëêiùëòhùëòw.
In cross-correlation operations, the result on each output channel is calculated from the
convolution kernel corresponding to that output channel and takes input from all channels
in the input tensor.
We implement a cross-correlation function to calculate the output of multiple channels as
shown below.
def corr2d_multi_in_out (X, K):
# Iterate through the 0th dimension of K, and each time, perform
# cross-correlation operations with input X. All of the results are
# stacked together
return torch .stack([corr2d_multi_in(X, k) for kinK], 0)
We construct a trivial convolution kernel with three output channels by concatenating the
kernel tensor for Kwith K+1andK+2.
K=torch .stack((K, K +1, K +2),0)
K.shape
torch .Size([ 3,2,2,2])
Below,weperformcross-correlationoperationsontheinputtensor Xwiththekerneltensor
K. Now the output contains three channels. The result of the first channel is consistent with
the result of the previous input tensor Xand the multi-input channel, single-output channel
kernel.
corr2d_multi_in_out(X, K)
tensor([[[ 56.,72.],
[104. ,120. ]],
[[76.,100. ],
[148. ,172. ]],
[[96.,128. ],
[192. ,224. ]]])
255 Multiple Input and Multiple Output Channels
7.4.3 11ConvolutionalLayer
At first, a 11convolution, i.e., ùëòh=ùëòw=1, does not seem to make much sense.
After all, a convolution correlates adjacent pixels. A 11convolution obviously does
not. Nonetheless, they are popular operations that are sometimes included in the designs
of complex deep networks ( Linetal., 2013,Szegedyetal., 2017). Let‚Äôs see in some detail
what it actually does.
Becausetheminimumwindowisused, the 11convolutionlosestheabilityoflargercon-
volutional layers to recognize patterns consisting of interactions among adjacent elements
in the height and width dimensions. The only computation of the 11convolution occurs
on the channel dimension.
Fig.7.4.2 showsthecross-correlationcomputationusingthe 11convolutionkernelwith3
inputchannelsand2outputchannels. Notethattheinputsandoutputshavethesameheight
and width. Each element in the output is derived from a linear combination of elements at
the same position in the input image. You could think of the 11convolutional layer as
constituting a fully connected layer applied at every single pixel location to transform the
ùëêicorresponding input values into ùëêooutput values. Because this is still a convolutional
layer, the weights are tied across pixel location. Thus the 11convolutional layer requires
ùëêoùëêiweights (plus the bias). Also note that convolutional layers are typically followed
by nonlinearities. This ensures that 11convolutions cannot simply be folded into other
convolutions.
tFig. 7.4.2 The cross-correlation computation uses the 1 1 convolution kernel with three input
channels and two output channels. The input and output have the same height and width.
Let‚Äôs check whether this works in practice: we implement a 11convolution using a fully
connectedlayer. Theonlythingisthatweneedtomakesomeadjustmentstothedatashape
before and after the matrix multiplication.
def corr2d_multi_in_out_1x1 (X, K):
c_i, h, w =X.shape
c_o =K.shape[ 0]
X=X.reshape((c_i, h *w))
K=K.reshape((c_o, c_i))
# Matrix multiplication in the fully connected layer
Y=torch .matmul(K, X)
return Y.reshape((c_o, h, w))
Whenperforming 11convolutions,theabovefunctionisequivalenttothepreviouslyim-
plemented cross-correlation function corr2d_multi_in_out . Let‚Äôs check this with some
sample data.
256 Convolutional Neural Networks
X=torch .normal( 0,1, (3,3,3))
K=torch .normal( 0,1, (2,3,1,1))
Y1=corr2d_multi_in_out_1x1(X, K)
Y2=corr2d_multi_in_out(X, K)
assert float (torch .abs(Y1 -Y2).sum()) <1e-6
7.4.4Discussion
Channels allow us to combine the best of both worlds: MLPs that allow for significant
nonlinearities and convolutions that allow for localized analysis of features. In particular,
channels allow the CNN to reason with multiple features, such as edge and shape detec-
tors at the same time. They also offer a practical trade-off between the drastic parameter
reduction arising from translation invariance and locality, and the need for expressive and
diverse models in computer vision.
Note, though, that thisflexibilitycomes ata price. Givenanimageofsize ¬π‚Ñéùë§¬∫, thecost
for computing a ùëòùëòconvolution isO¬π‚Ñéùë§ùëò2¬∫. Forùëêiandùëêoinput and output channels
respectively this increases to O¬π‚Ñéùë§ùëò2ùëêiùëêo¬∫. For a 256256pixel image with a
55kernel and 128input and output channels respectively this amounts to over 53 billion
operations (we count multiplications and additions separately). Later on we will encounter
effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations
to be block-diagonal, leading to architectures such as ResNeXt ( Xieetal., 2017).
7.4.5Exercises
1.Assume that we have two convolution kernels of size ùëò1andùëò2, respectively (with no
nonlinearity in between).
1.Prove that the result of the operation can be expressed by a single convolution.
2.What is the dimensionality of the equivalent single convolution?
3.Is the converse true, i.e., can you always decompose a convolution into two smaller
ones?
2.Assumeaninputofshape ùëêi‚Ñéùë§andaconvolutionkernelofshape ùëêoùëêiùëòhùëòw,
padding of¬πùëùh,ùëùw¬∫, and stride of¬πùë†h,ùë†w¬∫.
1.What is the computational cost (multiplications and additions) for the forward prop-
agation?
2.What is the memory footprint?
3.What is the memory footprint for the backward computation?
4.What is the computational cost for the backpropagation?
3.By what factor does the number of calculations increase if we double both the number
of input channels ùëêiand the number of output channels ùëêo? What happens if we double
the padding?
257 Pooling
1224.Are the variables Y1andY2in the final example of this section exactly the same? Why?
5.Express convolutions as a matrix multiplication, even when the convolution window is
not11.
6.Your task is to implement fast convolutions with a ùëòùëòkernel. One of the algorithm
candidates is to scan horizontally across the source, reading a ùëò-wide strip and comput-
ing the 1-wide output strip one value at a time. The alternative is to read a ùëò¬∏Œîwide
strip and compute a Œî-wide output strip. Why is the latter preferable? Is there a limit to
how large you should choose Œî?
7.Assume that we have a ùëêùëêmatrix.
1.Howmuchfasterisittomultiplywithablock-diagonalmatrixifthematrixisbroken
up intoùëèblocks?
2.What is the downside of having ùëèblocks? How could you fix it, at least partly?
Discussions122.
7.5Pooling
In many cases our ultimate task asks some global question about the image, e.g., does it
contain a cat? Consequently, the units of our final layer should be sensitive to the entire
input. By gradually aggregating information, yielding coarser and coarser maps, we ac-
complish this goal of ultimately learning a global representation, while keeping all of the
advantages of convolutional layers at the intermediate layers of processing. The deeper
we go in the network, the larger the receptive field (relative to the input) to which each
hidden node is sensitive. Reducing spatial resolution accelerates this process, since the
convolution kernels cover a larger effective area.
Moreover, when detecting lower-level features, such as edges (as discussed in Section 7.2 ),
we often want our representations to be somewhat invariant to translation. For instance,
if we take the image Xwith a sharp delineation between black and white and shift the
whole image by one pixel to the right, i.e., Z[i, j] = X[i, j + 1] , then the output
for the new image Zmight be vastly different. The edge will have shifted by one pixel. In
reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and
a stationary object, vibration of the camera due to the movement of the shutter might shift
everything by a pixel or so (high-end cameras are loaded with special features to address
this problem).
This section introduces pooling layers , which serve the dual purposes of mitigating the
sensitivity of convolutional layers to location and of spatially downsampling representa-
tions.
258 Convolutional Neural Networks
import torch
from torch import nn
from d2l import torch asd2l
7.5.1Maximum Poolingand AveragePooling
Like convolutional layers, pooling operators consist of a fixed-shape window that is slid
over all regions in the input according to its stride, computing a single output for each lo-
cation traversed by the fixed-shape window (sometimes known as the pooling window ).
However, unlike the cross-correlation computation of the inputs and kernels in the con-
volutional layer, the pooling layer contains no parameters (there is no kernel). Instead,
pooling operators are deterministic, typically calculating either the maximum or the aver-
age value of the elements in the pooling window. These operations are called maximum
pooling(max-pooling for short) and averagepooling , respectively.
Averagepooling isessentiallyasoldasCNNs. Theideaisakintodownsamplinganimage.
Rather than just taking the value of every second (or third) pixel for the lower resolution
image, we can average over adjacent pixels to obtain an image with better signal-to-noise
ratio since we are combining the information from multiple adjacent pixels. Max-pooling
was introduced in Riesenhuber and Poggio ( 1999) in the context of cognitive neuroscience
todescribehowinformationaggregationmightbeaggregatedhierarchicallyforthepurpose
ofobjectrecognition;therealreadywasanearlierversioninspeechrecognition( Yamaguchi
et al., 1990). In almost all cases, max-pooling, as it is also referred to, is preferable to
average pooling.
In both cases, as with the cross-correlation operator, we can think of the pooling window
as starting from the upper-left of the input tensor and sliding across it from left to right and
top to bottom. At each location that the pooling window hits, it computes the maximum or
average value of the input subtensor in the window, depending on whether max or average
pooling is employed.
tFig. 7.5.1 Max-pooling with a pooling window shape of 2 2. The shaded portions are the Ô¨Årst
output element as well as the input tensor elements used for the output computation:
max¬π0,1,3,4¬∫=4.
The output tensor in Fig. 7.5.1 has a height of 2 and a width of 2. The four elements are
259 Pooling
derived from the maximum value in each pooling window:
max¬π0,1,3,4¬∫=4,
max¬π1,2,4,5¬∫=5,
max¬π3,4,6,7¬∫=7,
max¬π4,5,7,8¬∫=8.(7.5.1)
More generally, we can define a ùëùùëûpooling layer by aggregating over a region of said
size. Returning to the problem of edge detection, we use the output of the convolutional
layer as input for 22max-pooling. Denote by Xthe input of the convolutional layer input
andYthe pooling layer output. Regardless of whether or not the values of X[i, j] ,X[i,
j + 1],X[i+1, j] andX[i+1, j + 1] are different, the pooling layer always outputs
Y[i, j] = 1 . That is to say, using the 22max-pooling layer, we can still detect if the
pattern recognized by the convolutional layer movesno more than one element in height or
width.
Inthecodebelow,weimplementtheforwardpropagationofthepoolinglayerinthe pool2d
function. Thisfunctionissimilartothe corr2dfunctionin Section7.2 . However,nokernel
isneeded, computingtheoutputaseitherthemaximumortheaverageofeachregioninthe
input.
def pool2d (X, pool_size, mode ='max'):
p_h, p_w =pool_size
Y=torch .zeros((X .shape[ 0]-p_h +1, X.shape[ 1]-p_w +1))
for iinrange (Y.shape[ 0]):
for jinrange (Y.shape[ 1]):
ifmode =='max':
Y[i, j] =X[i: i +p_h, j: j +p_w] .max()
elif mode =='avg':
Y[i, j] =X[i: i +p_h, j: j +p_w] .mean()
return Y
Wecanconstructtheinputtensor XinFig.7.5.1 tovalidatetheoutputofthetwo-dimensional
max-pooling layer.
X=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])
pool2d(X, ( 2,2))
tensor([[ 4.,5.],
[7.,8.]])
Also, we can experiment with the average pooling layer.
pool2d(X, ( 2,2),'avg')
tensor([[ 2.,3.],
[5.,6.]])
260 Convolutional Neural Networks
7.5.2Paddingand Stride
Aswithconvolutionallayers,poolinglayerschangetheoutputshape. Andasbefore,wecan
adjusttheoperationtoachieveadesiredoutputshapebypaddingtheinputandadjustingthe
stride. We can demonstrate the use of padding and strides in pooling layers via the built-in
two-dimensional max-pooling layer from the deep learning framework. We first construct
an input tensor Xwhose shape has four dimensions, where the number of examples (batch
size) and number of channels are both 1.
X=torch .arange( 16, dtype =torch .float32) .reshape(( 1,1,4,4))
X
tensor([[[[ 0.,1.,2.,3.],
[4.,5.,6.,7.],
[8.,9.,10.,11.],
[12.,13.,14.,15.]]]])
Since pooling aggregates information from an area, deep learning frameworks default to
matching pooling window sizes and stride. For instance, if we use a pooling window of
shape (3, 3)we get a stride shape of (3, 3)by default.
pool2d =nn.MaxPool2d( 3)
# Pooling has no model parameters, hence it needs no initialization
pool2d(X)
tensor([[[[ 10.]]]])
Needless to say, the stride and padding can be manually specified to override framework
defaults if required.
pool2d =nn.MaxPool2d( 3, padding =1, stride =2)
pool2d(X)
tensor([[[[ 5.,7.],
[13.,15.]]]])
Of course, we can specify an arbitrary rectangular pooling window with arbitrary height
and width respectively, as the example below shows.
pool2d =nn.MaxPool2d(( 2,3), stride =(2,3), padding =(0,1))
pool2d(X)
tensor([[[[ 5.,7.],
[13.,15.]]]])
261 Pooling
7.5.3Multiple Channels
When processing multi-channel input data, the pooling layer pools each input channel sep-
arately, rather than summing the inputs up over channels as in a convolutional layer. This
meansthatthenumberofoutputchannelsforthepoolinglayeristhesameasthenumberof
input channels. Below, we will concatenate tensors XandX + 1on the channel dimension
to construct an input with two channels.
X=torch .cat((X, X +1),1)
X
tensor([[[[ 0.,1.,2.,3.],
[4.,5.,6.,7.],
[8.,9.,10.,11.],
[12.,13.,14.,15.]],
[[1.,2.,3.,4.],
[5.,6.,7.,8.],
[9.,10.,11.,12.],
[13.,14.,15.,16.]]]])
As we can see, the number of output channels is still two after pooling.
pool2d =nn.MaxPool2d( 3, padding =1, stride =2)
pool2d(X)
tensor([[[[ 5.,7.],
[13.,15.]],
[[6.,8.],
[14.,16.]]]])
7.5.4Summary
Pooling is an exceedingly simple operation. It does exactly what its name indicates, ag-
gregate results over a window of values. All convolution semantics, such as strides and
padding apply in the same way as they did previously. Note that pooling is indifferent to
channels, i.e., it leaves the number of channels unchanged and it applies to each channel
separately. Lastly,ofthetwopopularpoolingchoices,max-poolingispreferabletoaverage
pooling, as it confers some degree of invariance to output. A popular choice is to pick a
pooling window size of 22to quarter the spatial resolution of output.
Notethattherearemanymorewaysofreducingresolutionbeyondpooling. Forinstance,in
stochastic pooling ( Zeiler and Fergus, 2013 ) and fractional max-pooling ( Graham, 2014 )
aggregation is combined with randomization. This can slightly improve the accuracy in
some cases. Lastly, as we will see later with the attention mechanism, there are more
refined ways of aggregating over outputs, e.g., by using the alignment between a query and
representation vectors.
262 Convolutional Neural Networks
1237.5.5Exercises
1.Implement average pooling through a convolution.
2.Prove that max-pooling cannot be implemented through a convolution alone.
3.Max-pooling can be accomplished using ReLU operations, i.e., ReLU ¬πùë•¬∫=max¬π0,ùë•¬∫.
1.Express max¬πùëé,ùëè¬∫by using only ReLU operations.
2.Use this to implement max-pooling by means of convolutions and ReLU layers.
3.How many channels and layers do you need for a 22convolution? How many for
a33convolution?
4.Whatisthecomputationalcostofthepoolinglayer? Assumethattheinputtothepooling
layer is of size ùëê‚Ñéùë§, the pooling window has a shape of ùëùhùëùwwith a padding of
¬πùëùh,ùëùw¬∫and a stride of¬πùë†h,ùë†w¬∫.
5.Why do you expect max-pooling and average pooling to work differently?
6.Do we need a separate minimum pooling layer? Can you replace it with another opera-
tion?
7.We could use the softmax operation for pooling. Why might it not be so popular?
Discussions123.
7.6ConvolutionalNeuralNetworks(LeNet)
Wenowhavealltheingredientsrequiredtoassembleafully-functionalCNN.Inourearlier
encounterwithimagedata,weappliedalinearmodelwithsoftmaxregression( Section4.4 )
and an MLP ( Section 5.2 ) to pictures of clothing in the Fashion-MNIST dataset. To make
such data amenable we first flattened each image from a 2828matrix into a fixed-length
784-dimensional vector, and thereafter processed them in fully connected layers. Now that
we have a handle on convolutional layers, we can retain the spatial structure in our images.
As an additional benefit of replacing fully connected layers with convolutional layers, we
will enjoy more parsimonious models that require far fewer parameters.
In this section, we will introduce LeNet, among the first published CNNs to capture wide
attention for its performance on computer vision tasks. The model was introduced by (and
named for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of rec-
ognizing handwritten digits in images ( LeCunet al., 1998). This work represented the
culmination of a decade of research developing the technology; LeCun‚Äôs team published
thefirststudytosuccessfullytrainCNNsviabackpropagation( LeCunetal.,1989).
AtthetimeLeNetachievedoutstandingresultsmatchingtheperformanceofsupportvector
machines, then a dominant approach in supervised learning, achieving an error rate of less
263 Convolutional Neural Networks (LeNet)
than1%perdigit. LeNetwaseventuallyadaptedtorecognizedigitsforprocessingdeposits
in ATM machines. To this day, some ATMs still run the code that Yann LeCun and his
colleague Leon Bottou wrote in the 1990s!
import torch
from torch import nn
from d2l import torch asd2l
7.6.1LeNet
At a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consist-
ing of two convolutional layers; and (ii) a dense block consisting of three fully connected
layers. The architecture is summarized in Fig. 7.6.1 .
tFig. 7.6.1 Data Ô¨Çow in LeNet. The input is a handwritten digit, the output is a probability over 10
possible outcomes.
The basic units in each convolutional block are a convolutional layer, a sigmoid activation
function, and a subsequent average pooling operation. Note that while ReLUs and max-
poolingworkbetter,theyhadnotyetbeendiscovered. Eachconvolutionallayerusesa 55
kernel and a sigmoid activation function. These layers map spatially arranged inputs to a
numberoftwo-dimensionalfeaturemaps,typicallyincreasingthenumberofchannels. The
firstconvolutionallayerhas6outputchannels,whilethesecondhas16. Each 22pooling
operation (stride 2) reduces dimensionality by a factor of 4via spatial downsampling. The
convolutional block emits an output with shape given by (batch size, number of channel,
height, width).
Inordertopassoutputfromtheconvolutionalblocktothedenseblock,wemustflatteneach
exampleintheminibatch. Inotherwords,wetakethisfour-dimensionalinputandtransform
itintothetwo-dimensionalinputexpectedbyfullyconnectedlayers: asareminder,thetwo-
dimensional representation that we desire uses the first dimension to index examples in the
minibatch and the second to give the flat vector representation of each example. LeNet‚Äôs
dense block has three fully connected layers, with 120, 84, and 10 outputs, respectively.
Becausewearestillperformingclassification,the10-dimensionaloutputlayercorresponds
to the number of possible output classes.
264 Convolutional Neural Networks
While getting to the point where you truly understand what is going on inside LeNet may
have taken a bit of work, we hope that the following code snippet will convince you that
implementing such models with modern deep learning frameworks is remarkably simple.
We need only to instantiate a Sequential block and chain together the appropriate layers,
using Xavier initialization as introduced in Section 5.4.2 .
def init_cnn (module): #@save
"""Initialize weights for CNNs."""
iftype (module) ==nn.Linear ortype (module) ==nn.Conv2d:
nn.init .xavier_uniform_(module .weight)
class LeNet (d2l .Classifier): #@save
"""The LeNet-5 model."""
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nn.LazyConv2d( 6, kernel_size =5, padding =2), nn .Sigmoid(),
nn.AvgPool2d(kernel_size =2, stride =2),
nn.LazyConv2d( 16, kernel_size =5), nn .Sigmoid(),
nn.AvgPool2d(kernel_size =2, stride =2),
nn.Flatten(),
nn.LazyLinear( 120), nn .Sigmoid(),
nn.LazyLinear( 84), nn .Sigmoid(),
nn.LazyLinear(num_classes))
We have taken some liberty in the reproduction of LeNet insofar as we have replaced the
Gaussian activation layer by a softmax layer. This greatly simplifies the implementation,
notleastdue tothefactthatthe Gaussiandecoderis rarelyused nowadays. Other thanthat,
this network matches the original LeNet-5 architecture.
Let‚Äôs see what happens inside the network. By passing a single-channel (black and white)
2828imagethroughthenetworkandprintingtheoutputshapeateachlayer,wecaninspect
the model to ensure that its operations line up with what we expect from Fig. 7.6.2 .
tFig. 7.6.2 Compressed notation for LeNet-5.
265 Convolutional Neural Networks (LeNet)
@d2l .add_to_class(d2l .Classifier) #@save
def layer_summary (self , X_shape):
X=torch .randn( *X_shape)
for layer inself .net:
X=layer(X)
print (layer .__class__ .__name__ ,'output shape: \t', X.shape)
model =LeNet()
model .layer_summary(( 1,1,28,28))
Conv2d output shape: torch .Size([ 1,6,28,28])
Sigmoid output shape: torch .Size([ 1,6,28,28])
AvgPool2d output shape: torch .Size([ 1,6,14,14])
Conv2d output shape: torch .Size([ 1,16,10,10])
Sigmoid output shape: torch .Size([ 1,16,10,10])
AvgPool2d output shape: torch .Size([ 1,16,5,5])
Flatten output shape: torch .Size([ 1,400])
Linear output shape: torch .Size([ 1,120])
Sigmoid output shape: torch .Size([ 1,120])
Linear output shape: torch .Size([ 1,84])
Sigmoid output shape: torch .Size([ 1,84])
Linear output shape: torch .Size([ 1,10])
Note that the height and width of the representation at each layer throughout the convolu-
tional block is reduced (compared with the previous layer). The first convolutional layer
uses two pixels of padding to compensate for the reduction in height and width that would
otherwise result from using a 55kernel. As an aside, the image size of 2828pixels in
theoriginalMNISTOCRdatasetisaresultof trimming twopixelrows(andcolumns)from
the original scans that measured 3232pixels. This was done primarily to save space (a
30% reduction) at a time when megabytes mattered.
In contrast, the second convolutional layer forgoes padding, and thus the height and width
are both reduced by four pixels. As we go up the stack of layers, the number of channels
increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and
16 after the second convolutional layer. However, each pooling layer halves the height and
width. Finally,eachfullyconnectedlayerreducesdimensionality,finallyemittinganoutput
whose dimension matches the number of classes.
7.6.2Training
Now that we have implemented the model, let‚Äôs run an experiment to see how the LeNet-5
model fares on Fashion-MNIST.
While CNNs have fewer parameters, they can still be more expensive to compute than
similarly deep MLPs because each parameter participates in many more multiplications.
If you have access to a GPU, this might be a good time to put it into action to speed up
training. Note that the d2l.Trainer class takes care of all details. By default, it initializes
the model parameters on the available devices. Just as with MLPs, our loss function is
cross-entropy, and we minimize it via minibatch stochastic gradient descent.
266 Convolutional Neural Networks
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128)
model =LeNet(lr =0.1)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], init_cnn)
trainer .fit(model, data)
7.6.3Summary
We have made significant progress in this chapter. We moved from the MLPs of the 1980s
to the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form
of LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on
Fashion-MNISTachievablewithLeNet-5bothtotheverybestpossiblewithMLPs( Section
5.2)andthosewithsignificantlymoreadvancedarchitecturessuchasResNet( Section8.6 ).
LeNetismuchmoresimilartothelatterthantotheformer. Oneoftheprimarydifferences,
asweshallsee, isthatgreateramountsofcomputationenabledsignificantlymorecomplex
architectures.
AseconddifferenceistherelativeeasewithwhichwewereabletoimplementLeNet. What
used to be an engineering challenge worth months of C++ and assembly code, engineering
to improve SN, an early Lisp-based deep learning tool ( Bottou and Le Cun, 1988 ), and fi-
nallyexperimentationwithmodelscannowbeaccomplishedinminutes. Itisthisincredible
productivity boost that has democratized deep learning model development tremendously.
In the next chapter we will journey down this rabbit to hole to see where it takes us.
7.6.4Exercises
1.Let‚Äôs modernize LeNet. Implement and test the following changes:
1.Replace average pooling with max-pooling.
2.Replace the softmax layer with ReLU.
2.Try to change the size of the LeNet style network to improve its accuracy in addition to
max-pooling and ReLU.
1.Adjust the convolution window size.
2.Adjust the number of output channels.
267 Convolutional Neural Networks (LeNet)
1243.Adjust the number of convolution layers.
4.Adjust the number of fully connected layers.
5.Adjustthe learning rates and other training details (e.g., initialization and number of
epochs).
3.Try out the improved network on the original MNIST dataset.
4.Display the activations of the first and second layer of LeNet for different inputs (e.g.,
sweaters and coats).
5.What happens to the activations when you feed significantly different images into the
network (e.g., cats, cars, or even random noise)?
Discussions124.
125
8 Modern Convolutional Neural Networks
Now that we understand the basics of wiring together CNNs, let‚Äôs take a tour of modern
CNN architectures. This tour is, by necessity, incomplete, thanks to the plethora of excit-
ingnewdesignsbeingadded. Theirimportancederivesfromthefactthatnotonlycanthey
be used directly for vision tasks, but they also serve as basic feature generators for more
advancedtaskssuchastracking( Zhangetal., 2021), segmentation( Longetal., 2015), ob-
ject detection ( Redmon and Farhadi, 2018 ), or style transformation ( Gatysetal., 2016). In
this chapter, most sections correspond to a significant CNN architecture that was at some
point (or currently) the base model upon which many research projects and deployed sys-
temswerebuilt. Eachofthesenetworkswasbrieflyadominantarchitectureandmanywere
winners or runners-up in the ImageNet competition125which has served as a barometer
of progress on supervised learning in computer vision since 2010. It is only recently that
Transformers have begun to displace CNNs, starting with Dosovitskiy et al.(2021) and
followed by the Swin Transformer ( Liuet al., 2021). We will cover this development later
inChapter 11 .
While the idea of deepneural networks is quite simple (stack together a bunch of layers),
performance can vary wildly across architectures and hyperparameter choices. The neural
networksdescribedinthischapteraretheproductofintuition,afewmathematicalinsights,
andalotoftrialanderror. Wepresentthesemodelsinchronologicalorder,partlytoconvey
a sense of the history so that you can form your own intuitions about where the field is
headingandperhapsdevelopyourownarchitectures. Forinstance,batchnormalizationand
residual connections described in this chapter have offered two popular ideas for training
and designing deep models, both of which have since also been applied to architectures
beyond computer vision.
We begin our tour of modern CNNs with AlexNet ( Krizhevsky etal., 2012), the firstlarge-
scale network deployed to beat conventional computer vision methods on a large-scale vi-
sion challenge; the VGG network ( Simonyan and Zisserman, 2014 ), which makes use of a
numberofrepeatingblocksofelements;thenetworkinnetwork(NiN)thatconvolveswhole
neural networks patch-wise over inputs ( Linet al., 2013); GoogLeNet that uses networks
with multi-branch convolutions ( Szegedyet al., 2015); the residual network (ResNet) ( He
etal.,2016),whichremainsoneofthemostpopularoff-the-shelfarchitecturesincomputer
vision; ResNeXt blocks ( Xieet al., 2017) for sparser connections; and DenseNet ( Huang
et al., 2017) for a generalization of the residual architecture. Over time many special opti-
mizations for efficient networks have been developed, such as coordinate shifts (ShiftNet)
(Wuet al., 2018). This culminated in the automatic search for efficient architectures such
268
269 Deep Convolutional Neural Networks (AlexNet)
as MobileNet v3 ( Howardet al., 2019). It also includes the semi-automatic design explo-
ration of Radosavovic et al.(2020) that led to the RegNetX/Y which we will discuss later
in this chapter. The work is instructive insofar as it offers a path for marrying brute force
computationwiththeingenuityofanexperimenterinthesearchforefficientdesignspaces.
Of note is also the work of Liu et al.(2022) as it shows that training techniques (e.g., op-
timizers, data augmentation, and regularization) play a pivotal role in improving accuracy.
It also shows that long-held assumptions, such as the size of a convolution window, may
need to be revisited, given the increase in computation and data. We will cover this and
many more questions in due course throughout this chapter.
8.1DeepConvolutionalNeuralNetworks(AlexNet)
Although CNNs were well known in the computer vision and machine learning commu-
nities following the introduction of LeNet ( LeCunet al., 1995), they did not immediately
dominate the field. Although LeNet achieved good results on early small datasets, the per-
formance and feasibility of training CNNs on larger, more realistic datasets had yet to be
established. In fact, for much of the intervening time between the early 1990s and the wa-
tershed results of 2012 ( Krizhevsky et al., 2012), neural networks were often surpassed
by other machine learning methods, such as kernel methods ( Sch√∂lkopf and Smola, 2002 ),
ensemble methods ( Freund and Schapire, 1996 ), and structured estimation ( Taskaret al.,
2004).
For computer vision, this comparison is perhaps not entirely accurate. That is, although
the inputs to convolutional networks consist of raw or lightly-processed (e.g., by center-
ing) pixel values, practitioners would never feed raw pixels into traditional models. In-
stead, typical computer vision pipelines consisted of manually engineering feature extrac-
tion pipelines, such as SIFT ( Lowe, 2004 ), SURF ( Bayet al., 2006), and bags of visual
words (Sivic and Zisserman, 2003 ). Rather than learning the features, the features were
crafted. Most of the progress came from having more clever ideas for feature extraction on
the one hand and deep insight into geometry ( Hartley and Zisserman, 2000 ) on the other.
The learning algorithm was often considered an afterthought.
Although some neural network accelerators were available in the 1990s, they were not yet
sufficiently powerful to make deep multichannel, multilayer CNNs with a large number
of parameters. For instance, NVIDIA‚Äôs GeForce 256 from 1999 was able to process at
most 480 million floating-point operations, such as additions and multiplications, per sec-
ond (MFLOPS), without any meaningful programming framework for operations beyond
games. Today‚Äôs accelerators are able to perform in excess of 1000 TFLOPs per device.
Moreover, datasets were still relatively small: OCR on 60,000 low-resolution 2828pixel
images was considered a highly challenging task. Added to these obstacles, key tricks for
training neural networks including parameter initialization heuristics ( Glorot and Bengio,
2010),clevervariantsofstochasticgradientdescent( KingmaandBa,2014 ),non-squashing
270 Modern Convolutional Neural Networks
126activation functions ( Nair and Hinton, 2010 ), and effective regularization techniques ( Sri-
vastavaetal., 2014) were still missing.
Thus, rather than training end-to-end (pixel to classification) systems, classical pipelines
looked more like this:
1.Obtain an interesting dataset. In the early days, these datasets required expensive sen-
sors. For instance, the Apple QuickTake 100126of 1994 sported a whopping 0.3
megapixel(VGA)resolution,capableofstoringupto8images,allforthepriceof$1000.
2.Preprocess the dataset with hand-crafted features based on some knowledge of optics,
geometry, other analytic tools, and occasionally on the serendipitous discoveries by
lucky graduate students.
3.Feed the data through a standard set of feature extractors such as the SIFT (scale-
invariant feature transform) ( Lowe, 2004 ), the SURF (speeded up robust features) ( Bay
etal., 2006), or anynumber of other hand-tuned pipelines. OpenCV stillprovidesSIFT
extractors to this day!
4.Dump the resulting representations into your favorite classifier, likely a linear model or
kernel method, to train a classifier.
If you spoke to machine learning researchers, they would reply that machine learning was
both important and beautiful. Elegant theories proved the properties of various classifiers
(Boucheron et al., 2005) and convex optimization ( Boyd and Vandenberghe, 2004 ) had
become the mainstay for obtaining them. The field of machine learning was thriving, rig-
orous, and eminently useful. However, if you spoke to a computer vision researcher, you
would hear a very different story. The dirty truth of image recognition, they would tell
you, is that features, geometry ( Hartley and Zisserman, 2000 ,Hartley and Kahl, 2009 ),
and engineering, rather than novel learning algorithms, drove progress. Computer vision
researchers justifiably believed that a slightly bigger or cleaner dataset or a slightly im-
provedfeature-extractionpipelinematteredfarmoretothefinalaccuracythananylearning
algorithm.
import torch
from torch import nn
from d2l import torch asd2l
8.1.1Representation Learning
Anotherwaytocastthestateofaffairsisthatthemostimportantpartofthepipelinewasthe
representation. And up until 2012 the representation was calculated mostly mechanically.
In fact, engineering a new set of feature functions, improving results, and writing up the
method all featured prominently in papers. SIFT ( Lowe, 2004 ), SURF ( Bayet al., 2006),
HOG (histograms of oriented gradient) ( Dalal and Triggs, 2005 ), bags of visual words
(Sivic and Zisserman, 2003 ), and similar feature extractors ruled the roost.
Another group of researchers, including Yann LeCun, Geoff Hinton, Yoshua Bengio, An-
drew Ng, Shun-ichi Amari, and Juergen Schmidhuber, had different plans. They believed
271 Deep Convolutional Neural Networks (AlexNet)
that features themselves ought to be learned. Moreover, they believed that to be reasonably
complex, the features ought to be hierarchically composed with multiple jointly learned
layers, each with learnable parameters. In the case of an image, the lowest layers might
come to detect edges, colors, and textures, by analogy with how the visual system in ani-
malsprocessesitsinput. Inparticular, theautomaticdesignofvisualfeaturessuchasthose
obtained by sparse coding ( Olshausen and Field, 1996 ) remained an open challenge until
the advent of modern CNNs. It was not until Dean etal.(2012), Le (2013) that the idea of
generating features from image data automatically gained significant traction.
The first modern CNN ( Krizhevsky etal., 2012), namedAlexNet after one of its inventors,
Alex Krizhevsky, is largely an evolutionary improvementoverLeNet. It achievedexcellent
performance in the 2012 ImageNet challenge.
tFig. 8.1.1 Image Ô¨Ålters learned by the Ô¨Årst layer of AlexNet. Reproduction courtesy of Krizhevsky
et al. ( 2012 ).
Interestingly, in the lowest layers of the network, the model learned feature extractors that
resembled some traditional filters. Fig. 8.1.1 shows lower-level image descriptors. Higher
layers in the network might build upon these representations to represent larger structures,
like eyes, noses, blades of grass, and so on. Even higher layers might represent whole
objects like people, airplanes, dogs, or frisbees. Ultimately, the final hidden state learns a
compact representation of the image that summarizes its contents such that data belonging
to different categories can be easily separated.
AlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This
begsthequestion: whydidittakesolong? Akeydifferencewasthat,overtheprevioustwo
decades,theamountofdataandthecomputingpoweravailablehadincreasedsignificantly.
As such AlexNet was much larger: it was trained on much more data, and on much faster
GPUs compared to the CPUs available in 1995.
272 Modern Convolutional Neural Networks
Missing Ingredient: Data
Deep models with many layers require large amounts of data in order to enter the regime
where they significantly outperform traditional methods based on convex optimizations
(e.g.,linearandkernelmethods). However,giventhelimitedstoragecapacityofcomputers,
the relative expense of (imaging) sensors, and the comparatively tighter research budgets
in the 1990s, most research relied on tiny datasets. Numerous papers relied on the UCI
collection of datasets, many of which contained only hundreds or (a few) thousands of
images captured in low resolution and often with an artificially clean background.
In 2009, the ImageNet dataset was released ( Denget al., 2009), challenging researchers
to learn models from 1 million examples, 1000 each from 1000 distinct categories of ob-
jects. The categories themselves were based on the most popular noun nodes in WordNet
(Miller, 1995 ). The ImageNet team used Google Image Search to prefilter large candidate
sets foreachcategory and employedthe Amazon MechanicalTurkcrowdsourcingpipeline
toconfirmforeachimagewhetheritbelongedtotheassociatedcategory. Thisscalewasun-
precedented, exceeding others by over an order of magnitude (e.g., CIFAR-100 has 60,000
images). Anotheraspectwasthattheimageswereatrelativelyhighresolutionof 224224
pixels, unlike the 80 million-sized TinyImages dataset ( Torralbaet al., 2008), consisting
of3232pixel thumbnails. This allowed for the formation of higher-level features. The
associated competition, dubbed the ImageNet Large Scale Visual Recognition Challenge
(Russakovsky et al., 2015), pushed computer vision and machine learning research for-
ward, challenging researchers to identify which models performed best at a greater scale
thanacademicshadpreviouslyconsidered. Thelargestvisiondatasets, suchasLAION-5B
(Schuhmann etal., 2022) contain billions of images with additional metadata.
Missing Ingredient: Hardware
Deep learning models are voracious consumers of compute cycles. Training can take hun-
dreds of epochs, and each iteration requires passing data through many layers of compu-
tationally expensive linear algebra operations. This is one of the main reasons why in the
1990s and early 2000s, simple algorithms based on the more-efficiently optimized convex
objectives were preferred.
Graphical processing units (GPUs) proved to be a game changer in making deep learn-
ing feasible. These chips had earlier been developed for accelerating graphics processing
to benefit computer games. In particular, they were optimized for high throughput 44
matrix‚Äìvector products, which are needed for many computer graphics tasks. Fortunately,
the math is strikingly similar to that required for calculating convolutional layers. Around
that time, NVIDIA and ATI had begun optimizing GPUs for general computing opera-
tions (Fernando, 2004 ), going as far as to market them as general-purpose GPUs (GPG-
PUs).
To provide some intuition, consider the cores of a modern microprocessor (CPU). Each
of the cores is fairly powerful running at a high clock frequency and sporting large caches
(up to several megabytes of L3). Each core is well-suited to executing a wide range of in-
structions, with branch predictors, a deep pipeline, specialized execution units, speculative
273 Deep Convolutional Neural Networks (AlexNet)
127execution, and many other bells and whistles that enable it to run a large variety of pro-
grams with sophisticated control flow. This apparent strength, however, is also its Achilles
heel: general-purpose cores are very expensive to build. They excel at general-purpose
code with lots of control flow. This requires lots of chip area, not just for the actual ALU
(arithmetic logical unit) where computation happens, but also for all the aforementioned
bells and whistles, plus memory interfaces, caching logic between cores, high-speed in-
terconnects, and so on. CPUs are comparatively bad at any single task when compared
withdedicatedhardware. Modernlaptopshave4‚Äì8cores,andevenhigh-endserversrarely
exceed 64 cores per socket, simply because it is not cost-effective.
By comparison, GPUs can consist of thousands of small processing elements (NIVIDA‚Äôs
latestAmperechipshaveupto6912CUDAcores),oftengroupedintolargergroups(NVIDIA
calls them warps). The details differ somewhat between NVIDIA, AMD, ARM and other
chip vendors. While each core is relatively weak, running at about 1GHz clock frequency,
it is the total number of such cores that makes GPUs orders of magnitude faster than
CPUs. For instance, NVIDIA‚Äôs recent Ampere A100 GPU offers over 300 TFLOPs per
chip for specialized 16-bit precision (BFLOAT16) matrix-matrix multiplications, and up
to 20 TFLOPs for more general-purpose floating point operations (FP32). At the same
time, floating point performance of CPUs rarely exceeds 1 TFLOPs. For instance, Ama-
zon‚Äôs Graviton 3 reaches 2 TFLOPs peak performance for 16-bit precision operations, a
number similar to the GPU performance of Apple‚Äôs M1 processor.
There are many reasons why GPUs are much faster than CPUs in terms of FLOPs. First,
powerconsumptiontendstogrow quadratically withclockfrequency. Hence,forthepower
budget of a CPU core that runs four times faster (a typical number), you can use 16 GPU
coresat1
4thespeed,whichyields 161
4=4timestheperformance. Second,GPUcoresare
much simpler (in fact, for a long time they were not even ableto execute general-purpose
code), which makes them more energy efficient. For instance, (i) they tend not to support
speculative evaluation, (ii) it typically is not possible to program each processing element
individually, and (iii) the caches per core tend to be much smaller. Last, many operations
in deep learning require high memory bandwidth. Again, GPUs shine here with buses that
are at least 10 times as wide as many CPUs.
Back to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever im-
plemented a deep CNN that could run on GPUs. They realized that the computational bot-
tlenecksinCNNs, convolutionsandmatrixmultiplications, arealloperationsthatcouldbe
parallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, either of
whichwascapableof1.5TFLOPs(stillachallengeformostCPUsadecadelater),theyim-
plemented fast convolutions. The cuda-convnet127code was good enough that for several
yearsitwastheindustrystandardandpoweredthefirstcoupleofyearsofthedeeplearning
boom.
8.1.2AlexNet
AlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recog-
nition Challenge 2012 by a large margin ( Russakovsky etal., 2013). This network showed,
274 Modern Convolutional Neural Networks
for the first time, that the features obtained by learning can transcend manually-designed
features, breaking the previous paradigm in computer vision.
ThearchitecturesofAlexNetandLeNetarestrikinglysimilar,as Fig.8.1.2 illustrates. Note
that we provide a slightly streamlined version of AlexNet removing some of the design
quirks that were needed in 2012 to make the model fit on two small GPUs.
tFig. 8.1.2 From LeNet (left) to AlexNet (right).
There are also significant differences between AlexNet and LeNet. First, AlexNet is much
deeper than the comparatively small LeNet-5. AlexNet consists of eight layers: five con-
volutional layers, two fully connected hidden layers, and one fully connected output layer.
Second, AlexNet used the ReLU instead of the sigmoid as its activation function. Let‚Äôs
delve into the details below.
Architecture
In AlexNet‚Äôs first layer, the convolution window shape is 1111. Since the images in
ImageNet are eight times taller and wider than the MNIST images, objects in ImageNet
datatendtooccupymorepixelswithmorevisualdetail. Consequently,alargerconvolution
window is needed to capture the object. The convolution window shape in the second
layer is reduced to 55, followed by 33. In addition, after the first, second, and fifth
convolutional layers, the network adds max-pooling layers with a window shape of 3
3and a stride of 2. Moreover, AlexNet has ten times more convolution channels than
LeNet.
Afterthefinalconvolutionallayer,therearetwohugefullyconnectedlayerswith4096out-
puts. These layers require nearly 1GB model parameters. Because of the limited memory
275 Deep Convolutional Neural Networks (AlexNet)
in early GPUs, the original AlexNet used a dual data stream design, so that each of their
twoGPUscouldberesponsibleforstoringandcomputingonlyitshalfofthemodel. Fortu-
nately,GPUmemoryiscomparativelyabundantnow,sowerarelyneedtobreakupmodels
acrossGPUsthesedays(ourversionoftheAlexNetmodeldeviatesfromtheoriginalpaper
in this aspect).
ActivationFunctions
Furthermore, AlexNet changed the sigmoid activation function to a simpler ReLU activa-
tionfunction. Ontheonehand,thecomputationoftheReLUactivationfunctionissimpler.
For example, it does not have the exponentiation operation found in the sigmoid activation
function. On the other hand, the ReLU activation function makes model training easier
when using different parameter initialization methods. This is because, when the output
of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is
almost0,sothatbackpropagationcannotcontinuetoupdatesomeofthemodelparameters.
Bycontrast,thegradientoftheReLUactivationfunctioninthepositiveintervalisalways1
(Section5.1.2 ). Therefore,ifthemodelparametersarenotproperlyinitialized,thesigmoid
function may obtain a gradient of almost 0 in the positive interval, meaning that the model
cannot be effectively trained.
Capacity Controland Preprocessing
AlexNet controls the model complexity of the fully connected layer by dropout ( Section
5.6), while LeNet only uses weight decay. To augment the data even further, the training
loop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and
color changes. This makes the model more robust and the larger sample size effectively
reduces overfitting. See Buslaev etal.(2020) for an in-depth review of such preprocessing
steps.
class AlexNet (d2l .Classifier):
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nn.LazyConv2d( 96, kernel_size =11, stride =4, padding =1),
nn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2),
nn.LazyConv2d( 256, kernel_size =5, padding =2), nn .ReLU(),
nn.MaxPool2d(kernel_size =3, stride =2),
nn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),
nn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),
nn.LazyConv2d( 256, kernel_size =3, padding =1), nn .ReLU(),
nn.MaxPool2d(kernel_size =3, stride =2), nn .Flatten(),
nn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout(p =0.5),
nn.LazyLinear( 4096 ), nn .ReLU(),nn .Dropout(p =0.5),
nn.LazyLinear(num_classes))
self .net.apply(d2l .init_cnn)
We construct a single-channel data example with both height and width of 224 to observe
the output shape of each layer. It matches the AlexNet architecture in Fig. 8.1.2 .
276 Modern Convolutional Neural Networks
AlexNet() .layer_summary(( 1,1,224,224))
Conv2d output shape: torch .Size([ 1,96,54,54])
ReLU output shape: torch .Size([ 1,96,54,54])
MaxPool2d output shape: torch .Size([ 1,96,26,26])
Conv2d output shape: torch .Size([ 1,256,26,26])
ReLU output shape: torch .Size([ 1,256,26,26])
MaxPool2d output shape: torch .Size([ 1,256,12,12])
Conv2d output shape: torch .Size([ 1,384,12,12])
ReLU output shape: torch .Size([ 1,384,12,12])
Conv2d output shape: torch .Size([ 1,384,12,12])
ReLU output shape: torch .Size([ 1,384,12,12])
Conv2d output shape: torch .Size([ 1,256,12,12])
ReLU output shape: torch .Size([ 1,256,12,12])
MaxPool2d output shape: torch .Size([ 1,256,5,5])
Flatten output shape: torch .Size([ 1,6400 ])
Linear output shape: torch .Size([ 1,4096 ])
ReLU output shape: torch .Size([ 1,4096 ])
Dropout output shape: torch .Size([ 1,4096 ])
Linear output shape: torch .Size([ 1,4096 ])
ReLU output shape: torch .Size([ 1,4096 ])
Dropout output shape: torch .Size([ 1,4096 ])
Linear output shape: torch .Size([ 1,10])
8.1.3Training
Although AlexNet was trained on ImageNet in Krizhevsky et al.(2012), we use Fashion-
MNIST here since training an ImageNet model to convergence could take hours or days
even on a modern GPU. One of the problems with applying AlexNet directly on Fashion-
MNIST is that its images have lower resolution ( 2828pixels) than ImageNet images. To
makethingswork,weupsamplethemto 224224. Thisisgenerallynotasmartpractice,as
itsimplyincreasesthecomputationalcomplexitywithoutaddinginformation. Nonetheless,
we do it here to be faithful to the AlexNet architecture. We perform this resizing with the
resizeargument in the d2l.FashionMNIST constructor.
Now, we can start training AlexNet. Compared to LeNet in Section 7.6 , the main change
here is the use of a smaller learning rate and much slower training due to the deeper and
wider network, the higher image resolution, and the more costly convolutions.
model =AlexNet(lr =0.01 )
data =d2l.FashionMNIST(batch_size =128, resize =(224,224))
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
trainer .fit(model, data)
8.1.4Discussion
AlexNet‚Äôs structure bears a striking resemblance to LeNet, with a number of critical im-
provements, both for accuracy (dropout) and for ease of training (ReLU). What is equally
striking is the amount of progress that has been made in terms of deep learning tooling.
277 Deep Convolutional Neural Networks (AlexNet)
What was several months of work in 2012 can now be accomplished in a dozen lines of
code using any modern framework.
Reviewingthe architecture, wesee thatAlexNethas anAchillesheel whenit comes toeffi-
ciency: thelasttwohiddenlayersrequirematricesofsize 64004096and40964096,re-
spectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both
ofwhichareanontrivialoutlay,especiallyonsmallerdevices,suchasmobilephones. This
isoneofthereasonswhyAlexNethasbeensurpassedbymuchmoreeffectivearchitectures
that we will cover in the following sections. Nonetheless, it is a key step from shallow to
deep networks that are used nowadays. Note that even though the number of parameters
exceedsbyfartheamountoftrainingdatainourexperiments(thelasttwolayershavemore
than 40 million parameters, trained on a datasets of 60 thousand images), there is hardly
anyoverfitting: trainingandvalidationlossarevirtuallyidenticalthroughouttraining. This
is due to the improved regularization, such as dropout, inherent in modern deep network
designs.
Although it seems that there are only a few more lines in AlexNet‚Äôs implementation than
inLeNet‚Äôs, ittooktheacademiccommunitymanyyearstoembracethisconceptualchange
and take advantage of its excellent experimental results. This was also due to the lack of
efficient computational tools. At the time neither DistBelief ( Deanet al., 2012) nor Caffe
(Jiaetal.,2014)existed,andTheano( Bergstraetal.,2010)stilllackedmanydistinguishing
features. ItwastheavailabilityofTensorFlow( Abadietal.,2016)thatdramaticallychanged
the situation.
8.1.5Exercises
1.Followinguponthediscussionabove,analyzethecomputationalpropertiesofAlexNet.
1.Compute the memory footprint for convolutions and fully connected layers, respec-
tively. Which one dominates?
2.Calculatethecomputationalcostfortheconvolutionsandthefullyconnectedlayers.
3.Howdoesthememory(readandwritebandwidth,latency,size)affectcomputation?
Is there any difference in its effects for training and inference?
2.You are a chip designer and need to trade off computation and memory bandwidth.
For example, a faster chip requires more power and possibly a larger chip area. More
278 Modern Convolutional Neural Networks
128memory bandwidth requires more pins and control logic, thus also more area. How do
you optimize?
3.Why do engineers no longer report performance benchmarks on AlexNet?
4.Try increasing the number of epochs when training AlexNet. Compared with LeNet,
how do the results differ? Why?
5.AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the
low resolution of the initial images.
1.Try simplifying the model to make the training faster, while ensuring that the accu-
racy does not drop significantly.
2.Design a better model that works directly on 2828images.
6.Modify the batch size, and observe the changes in throughput (images/s), accuracy, and
GPU memory.
7.ApplydropoutandReLUtoLeNet-5. Doesitimprove? Canyouimprovethingsfurther
by preprocessing to take advantage of the invariances inherent in the images?
8.CanyoumakeAlexNetoverfit? Whichfeaturedoyouneedtoremoveorchangetobreak
training?
Discussions128.
8.2NetworksUsing Blocks(VGG)
While AlexNet offered empirical evidence that deep CNNs can achieve good results, it did
not provide a general template to guide subsequent researchers in designing new networks.
In the following sections, we will introduce several heuristic concepts commonly used to
design deep networks.
ProgressinthisfieldmirrorsthatofVLSI(verylargescaleintegration)inchipdesignwhere
engineersmovedfromplacingtransistorstologicalelementstologicblocks( Mead,1980 ).
Similarly,thedesignofneuralnetworkarchitectureshasgrownprogressivelymoreabstract,
with researchers moving from thinking in terms of individual neurons to whole layers,
and now to blocks, repeating patterns of layers. A decade later, this has now progressed
to researchers using entire trained models to repurpose them for different, albeit related,
tasks. Such large pretrained models are typically called foundationmodels (Bommasani et
al., 2021).
Back to network design. The idea of using blocks first emerged from the Visual Geometry
Group(VGG)atOxfordUniversity,intheireponymously-named VGGnetwork( Simonyan
and Zisserman, 2014 ). It is easy to implement these repeated structures in code with any
modern deep learning framework by using loops and subroutines.
279 Networks Using Blocks (VGG)
import torch
from torch import nn
from d2l import torch asd2l
8.2.1VGGBlocks
The basic building block of CNNs is a sequence of the following: (i) a convolutional layer
with padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling
layersuchasmax-poolingtoreducetheresolution. Oneoftheproblemswiththisapproach
is that the spatial resolution decreases quite rapidly. In particular, this imposes a hard limit
oflog2ùëëconvolutional layers on the network before all dimensions ( ùëë) are used up. For
instance,inthecaseofImageNet,itwouldbeimpossibletohavemorethan8convolutional
layers in this way.
The key idea of Simonyan and Zisserman ( 2014) was to use multiple convolutions in be-
tween downsampling via max-pooling in the form of a block. They were primarily in-
terested in whether deep or wide networks perform better. For instance, the successive
application of two 33convolutions touches the same pixels as a single 55convolution
does. At the same time, the latter uses approximately as many parameters ( 25ùëê2) as three
33convolutionsdo( 39ùëê2). Inaratherdetailedanalysistheyshowedthatdeepandnar-
row networks significantly outperform their shallow counterparts. This set deep learning
on a quest for ever deeper networks with over 100 layers for typical applications. Stacking
33convolutions has become a gold standard in later deep networks (a design decision
only to be revisited recently by Liu et al.(2022)). Consequently, fast implementations for
small convolutions have become a staple on GPUs ( Lavin and Gray, 2016 ).
Back to VGG: a VGG block consists of a sequence of convolutions with 33kernels with
padding of 1 (keeping height and width) followed by a 22max-pooling layer with stride
of 2 (halving height and width after each block). In the code below, we define a function
called vgg_block to implement one VGG block.
The function below takes two arguments, corresponding to the number of convolutional
layers num_convs and the number of output channels num_channels .
def vgg_block (num_convs, out_channels):
layers =[]
for _inrange (num_convs):
layers .append(nn .LazyConv2d(out_channels, kernel_size =3, padding =1))
layers .append(nn .ReLU())
layers .append(nn .MaxPool2d(kernel_size =2,stride =2))
return nn.Sequential( *layers)
8.2.2VGGNetwork
Like AlexNet and LeNet, the VGG Network can be partitioned into two parts: the first
consisting mostly of convolutional and pooling layers and the second consisting of fully
280 Modern Convolutional Neural Networks
connected layers that are identical to those in AlexNet. The key difference is that the con-
volutional layers are grouped in nonlinear transformations that leave the dimensonality un-
changed, followed by a resolution-reduction step, as depicted in Fig. 8.2.1 .
tFig. 8.2.1 From AlexNet to VGG. The key difference is that VGG consists of blocks of layers,
whereas AlexNet‚Äôs layers are all designed individually.
The convolutional part of the network connects several VGG blocks from Fig. 8.2.1 (also
defined in the vgg_block function) in succession. This grouping of convolutions is a pat-
tern that has remained almost unchanged over the past decade, although the specific choice
of operations has undergone considerable modifications. The variable archconsists of a
list of tuples (one per block), where eachcontains two values: the number of convolutional
layers and the number of output channels, which are precisely the arguments required to
call the vgg_block function. As such, VGG defines a familyof networks rather than just a
specific manifestation. To build a specific network we simply iterate over archto compose
the blocks.
class VGG(d2l .Classifier):
def __init__ (self , arch, lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
conv_blks =[]
for (num_convs, out_channels) inarch:
conv_blks .append(vgg_block(num_convs, out_channels))
self .net =nn.Sequential(
*conv_blks, nn .Flatten(),
(continues on next page)
281 Networks Using Blocks (VGG)
(continued from previous page)
nn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),
nn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),
nn.LazyLinear(num_classes))
self .net.apply(d2l .init_cnn)
The original VGG network had five convolutional blocks, among which the first two have
oneconvolutionallayereachandthelatterthreecontaintwoconvolutionallayerseach. The
first blockhas 64 output channelsand each subsequentblockdoubles the number of output
channels,untilthatnumberreaches512. Sincethisnetworkuseseightconvolutionallayers
and three fully connected layers, it is often called VGG-11.
VGG(arch =((1,64), ( 1,128), ( 2,256), ( 2,512), ( 2,512))).layer_summary(
(1,1,224,224))
Sequential output shape: torch .Size([ 1,64,112,112])
Sequential output shape: torch .Size([ 1,128,56,56])
Sequential output shape: torch .Size([ 1,256,28,28])
Sequential output shape: torch .Size([ 1,512,14,14])
Sequential output shape: torch .Size([ 1,512,7,7])
Flatten output shape: torch .Size([ 1,25088 ])
Linear output shape: torch .Size([ 1,4096 ])
ReLU output shape: torch .Size([ 1,4096 ])
Dropout output shape: torch .Size([ 1,4096 ])
Linear output shape: torch .Size([ 1,4096 ])
ReLU output shape: torch .Size([ 1,4096 ])
Dropout output shape: torch .Size([ 1,4096 ])
Linear output shape: torch .Size([ 1,10])
Asyoucansee,wehalveheightandwidthateachblock,finallyreachingaheightandwidth
of 7 before flattening the representations for processing by the fully connected part of the
network. Simonyan and Zisserman ( 2014) described several other variants of VGG. In
fact,ithasbecomethenormtopropose families ofnetworkswithdifferentspeed‚Äìaccuracy
trade-off when introducing a new architecture.
8.2.3Training
Since VGG-11 is computationally more demanding than AlexNet we construct a network
with a smaller number of channels. This is more than sufficient for training on Fashion-
MNIST. The model training process is similar to that of AlexNet in Section 8.1 . Again ob-
servetheclosematchbetweenvalidationandtrainingloss, suggestingonlyasmallamount
of overfitting.
model =VGG(arch =((1,16), ( 1,32), ( 2,64), ( 2,128), ( 2,128)), lr =0.01 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(224,224))
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
282 Modern Convolutional Neural Networks
8.2.4Summary
One might argue that VGG is the first truly modern convolutional neural network. While
AlexNetintroducedmanyofthecomponentsofwhatmakedeeplearningeffectiveatscale,
it is VGG that arguably introduced key properties such as blocks of multiple convolutions
and a preference for deep and narrow networks. It is also the first network that is actually
an entire family of similarly parametrized models, giving the practitioner ample trade-off
between complexity and speed. This is also the place where modern deep learning frame-
works shine. It is no longer necessary to generate XML configuration files to specify a
network but rather, to assemble said networks through simple Python code.
More recently ParNet ( Goyalet al., 2021) demonstrated that it is possible to achieve com-
petitive performance using a much more shallow architecture through a large number of
parallel computations. This is an exciting development and there is hope that it will influ-
ence architecture designs in the future. For the remainder of the chapter, though, we will
follow the path of scientific progress over the past decade.
8.2.5Exercises
1.ComparedwithAlexNet,VGGismuchslowerintermsofcomputation,anditalsoneeds
more GPU memory.
1.Compare the number of parameters needed for AlexNet and VGG.
2.Compare the number of floating point operations used in the convolutional layers
and in the fully connected layers.
3.How could you reduce the computational cost created by the fully connected layers?
2.When displaying the dimensions associated with the various layers of the network, we
only see the information associated with eight blocks (plus some auxiliary transforms),
even though the network has 11 layers. Where did the remaining three layers go?
3.UseTable1intheVGGpaper( SimonyanandZisserman,2014 )toconstructothercom-
mon models, such as VGG-16 or VGG-19.
4.Upsampling the resolution in Fashion-MNIST eight-fold from 2828to224224
dimensions is very wasteful. Try modifying the network architecture and resolution
conversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without
283 Network in Network (NiN)
129reducing the accuracy of the network? Consult the VGG paper ( Simonyan and Zisser-
man, 2014 ) for ideas on adding more nonlinearities prior to downsampling.
Discussions129.
8.3Networkin Network(NiN)
LeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting
spatialstructure via a sequence of convolutions and pooling layers and post-process the
representations via fully connected layers. The improvements upon LeNet by AlexNet and
VGG mainly lie in how these later networks widen and deepen these two modules.
This design poses two major challenges. First, the fully connected layers at the end of
the architecture consume tremendous numbers of parameters. For instance, even a simple
model such as VGG-11 requires a monstrous matrix, occupying almost 400MB of RAM
in single precision (FP32). This is a significant impediment to computation, in particular
on mobile and embedded devices. After all, even high-end mobile phones sport no more
than 8GB of RAM. At the time VGG was invented, this was an order of magnitude less
(the iPhone 4S had 512MB). As such, it would have been difficult to justify spending the
majority of memory on an image classifier.
Second, it is equally impossible to add fully connected layers earlier in the network to
increasethedegreeofnonlinearity: doingsowoulddestroythespatialstructureandrequire
potentially even more memory.
Thenetwork in network (NiN) blocks ( Linet al., 2013) offer an alternative, capable of
solving both problems in one simple strategy. They were proposed based on a very simple
insight: (i) use 11convolutions to add local nonlinearities across the channel activations
and(ii)useglobalaveragepoolingtointegrateacrossalllocationsinthelastrepresentation
layer. Note that global average pooling would not be effective, were it not for the added
nonlinearities. Let‚Äôs dive into this in detail.
import torch
from torch import nn
from d2l import torch asd2l
8.3.1NiN Blocks
RecallSection7.4.3 . Initwesaidthattheinputsandoutputsofconvolutionallayersconsist
of four-dimensional tensors with axes corresponding to the example, channel, height, and
width. Also recall that the inputs and outputs of fully connected layers are typically two-
dimensional tensors corresponding to the example and feature. The idea behind NiN is
to apply a fully connected layer at each pixel location (for each height and width). The
284 Modern Convolutional Neural Networks
resulting 11convolutioncanbethoughtofasafullyconnectedlayeractingindependently
on each pixel location.
Fig.8.3.1 illustratesthemainstructuraldifferencesbetweenVGGandNiN,andtheirblocks.
NoteboththedifferenceintheNiNblocks(theinitialconvolutionisfollowedby 11con-
volutions, whereas VGG retains 33convolutions) and at the end where we no longer
require a giant fully connected layer.
tFig. 8.3.1 Comparing the architectures of VGG and NiN, and of their blocks.
def nin_block (out_channels, kernel_size, strides, padding):
return nn.Sequential(
nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn .ReLU(),
nn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU(),
nn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU())
8.3.2NiN Model
NiNusesthesameinitialconvolutionsizesasAlexNet(itwasproposedshortlythereafter).
The kernel sizes are 1111,55, and 33, respectively, and the numbers of output
285 Network in Network (NiN)
channelsmatchthoseofAlexNet. EachNiNblockisfollowedbyamax-poolinglayerwith
a stride of 2 and a window shape of 33.
The second significant difference between NiN and both AlexNet and VGG is that NiN
avoids fully connected layers altogether. Instead, NiN uses a NiN block with a number of
output channels equal to the number of label classes, followed by a globalaverage pooling
layer, yielding a vector of logits. This design significantly reduces the number of required
model parameters, albeit at the expense of a potential increase in training time.
class NiN(d2l .Classifier):
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nin_block( 96, kernel_size =11, strides =4, padding =0),
nn.MaxPool2d( 3, stride =2),
nin_block( 256, kernel_size =5, strides =1, padding =2),
nn.MaxPool2d( 3, stride =2),
nin_block( 384, kernel_size =3, strides =1, padding =1),
nn.MaxPool2d( 3, stride =2),
nn.Dropout( 0.5),
nin_block(num_classes, kernel_size =3, strides =1, padding =1),
nn.AdaptiveAvgPool2d(( 1,1)),
nn.Flatten())
self .net.apply(d2l .init_cnn)
We create a data example to see the output shape of each block.
NiN() .layer_summary(( 1,1,224,224))
Sequential output shape: torch .Size([ 1,96,54,54])
MaxPool2d output shape: torch .Size([ 1,96,26,26])
Sequential output shape: torch .Size([ 1,256,26,26])
MaxPool2d output shape: torch .Size([ 1,256,12,12])
Sequential output shape: torch .Size([ 1,384,12,12])
MaxPool2d output shape: torch .Size([ 1,384,5,5])
Dropout output shape: torch .Size([ 1,384,5,5])
Sequential output shape: torch .Size([ 1,10,5,5])
AdaptiveAvgPool2d output shape: torch .Size([ 1,10,1,1])
Flatten output shape: torch .Size([ 1,10])
8.3.3Training
AsbeforeweuseFashion-MNISTtotrainthemodelusingthesameoptimizerthatweused
for AlexNet and VGG.
model =NiN(lr =0.05 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(224,224))
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
286 Modern Convolutional Neural Networks
8.3.4Summary
NiN has dramatically fewer parameters than AlexNet and VGG. This stems primarily from
thefactthatitneedsnogiantfullyconnectedlayers. Instead, itusesglobalaveragepooling
to aggregate across all image locations after the last stage of the network body. This obvi-
ates the need for expensive (learned) reduction operations and replaces them by a simple
average. What surprised researchers at the time was the fact that this averaging operation
did not harm accuracy. Note that averaging across a low-resolution representation (with
many channels) also adds to the amount of translation invariance that the network can han-
dle.
Choosing fewer convolutions with wide kernels and replacing them by 11convolutions
aids the quest for fewer parameters further. It can cater for a significant amount of non-
linearity across channels within any given location. Both 11convolutions and global
average pooling significantly influenced subsequent CNN designs.
8.3.5Exercises
1.Why are there two 11convolutional layers per NiN block? Increase their number to
three. Reduce their number to one. What changes?
2.What changes if you replace the 11convolutions by 33convolutions?
3.What happens if you replace the global average pooling by a fully connected layer
(speed, accuracy, number of parameters)?
4.Calculate the resource usage for NiN.
1.What is the number of parameters?
2.What is the amount of computation?
3.What is the amount of memory needed during training?
4.What is the amount of memory needed during prediction?
5.What are possible problems with reducing the 38455representation to a 1055
representation in one step?
6.Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19
to design a family of NiN-like networks.
287 Multi-Branch Networks (GoogLeNet)
130Discussions130.
8.4Multi-BranchNetworks(GoogLeNet)
In 2014,GoogLeNet won the ImageNet Challenge ( Szegedyetal., 2015), using a structure
that combined the strengths of NiN ( Linetal., 2013), repeated blocks ( Simonyan and Zis-
serman, 2014 ), and a cocktail of convolution kernels. It was arguablyalso the firstnetwork
that exhibited a clear distinction among the stem (data ingest), body (data processing), and
head (prediction) in a CNN. This design pattern has persisted ever since in the design of
deep networks: the stemis given by the first two or three convolutions that operate on the
image. They extract low-level features from the underlying images. This is followed by a
bodyof convolutional blocks. Finally, the headmaps the features obtained so far to the
required classification, segmentation, detection, or tracking problem at hand.
ThekeycontributioninGoogLeNetwasthedesignofthenetworkbody. Itsolvedtheprob-
lem of selecting convolution kernels in an ingenious way. While other works tried to iden-
tifywhichconvolution,rangingfrom 11to1111wouldbebest,itsimply concatenated
multi-branch convolutions. In what follows we introduce a slightly simplified version of
GoogLeNet: theoriginaldesignincludedanumberoftricksforstabilizingtrainingthrough
intermediate loss functions, applied to multiple layers of the network. They are no longer
necessary due to the availability of improved training algorithms.
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
8.4.1InceptionBlocks
The basic convolutional block in GoogLeNet is called an Inception block , stemming from
the meme ‚Äúwe need to go deeper‚Äù from the movie Inception .
tFig. 8.4.1 Structure of the Inception block.
As depicted in Fig. 8.4.1 , the inception block consists of four parallel branches. The first
three branches use convolutional layers with window sizes of 11,33, and 55to
extract information from different spatial sizes. The middle two branches also add a 11
288 Modern Convolutional Neural Networks
convolution of the input to reduce the number of channels, reducing the model‚Äôs complex-
ity. The fourth branch uses a 33max-pooling layer, followed by a 11convolutional
layer to change the number of channels. The four branches all use appropriate padding
to give the input and output the same height and width. Finally, the outputs along each
branchareconcatenatedalongthechanneldimensionandcomprisetheblock‚Äôsoutput. The
commonly-tunedhyperparametersoftheInceptionblockarethenumberofoutputchannels
per layer, i.e., how to allocate capacity among convolutions of different size.
class Inception (nn.Module):
# c1--c4 are the number of output channels for each branch
def __init__ (self , c1, c2, c3, c4, **kwargs):
super (Inception, self ).__init__ (**kwargs)
# Branch 1
self .b1_1 =nn.LazyConv2d(c1, kernel_size =1)
# Branch 2
self .b2_1 =nn.LazyConv2d(c2[ 0], kernel_size =1)
self .b2_2 =nn.LazyConv2d(c2[ 1], kernel_size =3, padding =1)
# Branch 3
self .b3_1 =nn.LazyConv2d(c3[ 0], kernel_size =1)
self .b3_2 =nn.LazyConv2d(c3[ 1], kernel_size =5, padding =2)
# Branch 4
self .b4_1 =nn.MaxPool2d(kernel_size =3, stride =1, padding =1)
self .b4_2 =nn.LazyConv2d(c4, kernel_size =1)
def forward (self , x):
b1=F.relu( self .b1_1(x))
b2=F.relu( self .b2_2(F .relu( self .b2_1(x))))
b3=F.relu( self .b3_2(F .relu( self .b3_1(x))))
b4=F.relu( self .b4_2( self .b4_1(x)))
return torch .cat((b1, b2, b3, b4), dim =1)
To gain some intuition for why this network works so well, consider the combination of
the filters. They explore the image in a variety of filter sizes. This means that details at
differentextentscanberecognizedefficientlybyfiltersofdifferentsizes. Atthesametime,
we can allocate different amounts of parameters for different filters.
8.4.2GoogLeNetModel
As shown in Fig. 8.4.2 , GoogLeNet uses a stack of a total of 9 inception blocks, arranged
into three groups with max-pooling in between, and global average pooling in its head to
generate its estimates. Max-pooling between inception blocks reduces the dimensionality.
At its stem, the first module is similar to AlexNet and LeNet.
tFig. 8.4.2 The GoogLeNet architecture.
289 Multi-Branch Networks (GoogLeNet)
We can now implement GoogLeNet piece by piece. Let‚Äôs begin with the stem. The first
module uses a 64-channel 77convolutional layer.
class GoogleNet (d2l .Classifier):
def b1(self ):
return nn.Sequential(
nn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),
nn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2, padding =1))
The second module uses two convolutional layers: first, a 64-channel 11convolutional
layer, followed by a 33convolutional layer that triples the number of channels. This
corresponds to the second branch in the Inception block and concludes the design of the
body. At this point we have 192 channels.
@d2l .add_to_class(GoogleNet)
def b2(self ):
return nn.Sequential(
nn.LazyConv2d( 64, kernel_size =1), nn .ReLU(),
nn.LazyConv2d( 192, kernel_size =3, padding =1), nn .ReLU(),
nn.MaxPool2d(kernel_size =3, stride =2, padding =1))
The third module connects two complete Inception blocks in series. The number of output
channels of the first Inception block is 64¬∏128¬∏32¬∏32=256. This amounts to a ratio of
the number of output channels among the four branches of 2 : 4 : 1 : 1 . To achieve this, we
firstreducetheinputdimensionsby1
2andby1
12inthesecondandthirdbranchrespectively
to arrive at 96=192¬ù2and16=192¬ù12channels respectively.
The number of output channels of the second Inception block is increased to 128¬∏192¬∏
96¬∏64=480, yielding a ratio of 128 : 192 : 96 : 64 =4 : 6 : 3 : 2 . As before, we need to
reduce the number of intermediate dimensions in the second and third channel. A scale of
1
2and1
8respectively suffices, yielding 128and32channels respectively. This is captured
by the arguments of the following Inception block constructors.
@d2l .add_to_class(GoogleNet)
def b3(self ):
return nn.Sequential(Inception( 64, (96,128), ( 16,32),32),
Inception( 128, (128,192), ( 32,96),64),
nn.MaxPool2d(kernel_size =3, stride =2, padding =1))
The fourth module is more complicated. It connects five Inception blocks in series, and
they have 192¬∏208¬∏48¬∏64=512,160¬∏224¬∏64¬∏64=512,128¬∏256¬∏64¬∏64=512,
112¬∏288¬∏64¬∏64=528, and 256¬∏320¬∏128¬∏128=832output channels, respectively.
The number of channels assigned to these branches is similar to that in the third module:
thesecondbranchwiththe 33convolutionallayeroutputsthelargestnumberofchannels,
followed by the first branch with only the 11convolutional layer, the third branch with
the55convolutional layer, and the fourth branch with the 33max-pooling layer. The
second and third branches will first reduce the number of channels according to the ratio.
These ratios are slightly different in different Inception blocks.
290 Modern Convolutional Neural Networks
@d2l .add_to_class(GoogleNet)
def b4(self ):
return nn.Sequential(Inception( 192, (96,208), ( 16,48),64),
Inception( 160, (112,224), ( 24,64),64),
Inception( 128, (128,256), ( 24,64),64),
Inception( 112, (144,288), ( 32,64),64),
Inception( 256, (160,320), ( 32,128),128),
nn.MaxPool2d(kernel_size =3, stride =2, padding =1))
ThefifthmodulehastwoInceptionblockswith 256¬∏320¬∏128¬∏128=832and384¬∏384¬∏
128¬∏128=1024output channels. The number of channels assigned to each branch is the
same as that in the third and fourth modules, but differs in specific values. It should be
notedthat thefifth blockis followedbytheoutput layer. Thisblockusesthe globalaverage
pooling layer to change the height and width of each channel to 1, just as in NiN. Finally,
we turn the output into a two-dimensional array followed by a fully connected layer whose
number of outputs is the number of label classes.
@d2l .add_to_class(GoogleNet)
def b5(self ):
return nn.Sequential(Inception( 256, (160,320), ( 32,128),128),
Inception( 384, (192,384), ( 48,128),128),
nn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten())
Nowthatwedefinedallblocks b1through b5, itisjustamatterofassemblingthemallinto
a full network.
@d2l .add_to_class(GoogleNet)
def __init__ (self , lr =0.1, num_classes =10):
super (GoogleNet, self ).__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential( self .b1(), self .b2(), self .b3(), self .b4(),
self .b5(), nn .LazyLinear(num_classes))
self .net.apply(d2l .init_cnn)
The GoogLeNet model is computationally complex. Note the large number of relatively
arbitraryhyperparametersintermsofthenumberofchannelschosen,thenumberofblocks
prior to dimensionality reduction, the relative partitioning of capacity across channels, etc.
Much of it is due to the fact that at the time when GoogLeNet was introduced, automatic
tools for network definition or design exploration were not yet available. For instance, by
nowwetakeitforgrantedthatacompetentdeeplearningframeworkiscapableofinferring
dimensionalities of input tensors automatically. At the time, many such configurations had
to be specified explicitly by the experimenter, thus often slowing down active experimen-
tation. Moreover, the tools needed for automatic exploration were still in flux and initial
experiments largely amounted to costly brute-force exploration, genetic algorithms, and
similar strategies.
For now the only modification we will carry out is to reduce the input height and width
from 224 to 96 to have a reasonable training time on Fashion-MNIST. This simplifies the
291 Multi-Branch Networks (GoogLeNet)
computation. Let‚Äôshavealookatthechangesintheshapeoftheoutputbetweenthevarious
modules.
model =GoogleNet() .layer_summary(( 1,1,96,96))
Sequential output shape: torch .Size([ 1,64,24,24])
Sequential output shape: torch .Size([ 1,192,12,12])
Sequential output shape: torch .Size([ 1,480,6,6])
Sequential output shape: torch .Size([ 1,832,3,3])
Sequential output shape: torch .Size([ 1,1024 ])
Linear output shape: torch .Size([ 1,10])
8.4.3Training
Asbefore,wetrainourmodelusingtheFashion-MNISTdataset. Wetransformitto 9696
pixel resolution before invoking the training procedure.
model =GoogleNet(lr =0.01 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(96,96))
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
8.4.4Discussion
A key feature of GoogLeNet is that it is actually cheaper to compute than its predecessors
while simultaneously providing improved accuracy. This marks the beginning of a much
moredeliberatenetworkdesignthattradesoffthecostofevaluatinganetworkwithareduc-
tion in errors. It also marks the beginning of experimentation at a block level with network
designhyperparameters,eventhoughitwasentirelymanualatthetime. Wewillrevisitthis
topic inSection 8.8 when discussing strategies for network structure exploration.
Over the following sections we will encounter a number of design choices (e.g., batch nor-
malization, residual connections, and channel grouping) that allow us to improve networks
significantly. For now, you can be proud to have implemented what is arguably the first
truly modern CNN.
292 Modern Convolutional Neural Networks
1318.4.5Exercises
1.GoogLeNetwasso successfulthat it wentthrough a numberof iterations, progressively
improving speed and accuracy. Try to implement and run some of them. They include
the following:
1.Add a batch normalization layer ( Ioffe and Szegedy, 2015 ), as described later in
Section 8.5 .
2.Make adjustments to the Inception block (width, choice and order of convolutions),
as described in Szegedy etal.(2016).
3.Uselabelsmoothingformodelregularization, asdescribedinSzegedy etal.(2016).
4.MakefurtheradjustmentstotheInceptionblockbyaddingresidualconnection( Szegedy
etal., 2017), as described later in Section 8.6 .
2.What is the minimum image size needed for GoogLeNet to work?
3.Can you design a variant of GoogLeNet that works on Fashion-MNIST‚Äôs native resolu-
tion of 2828pixels? How would you need to change the stem, the body, and the head
of the network, if anything at all?
4.Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do
the latter two network architectures significantly reduce the model parameter size?
5.ComparetheamountofcomputationneededinGoogLeNetandAlexNet. Howdoesthis
affect the design of an accelerator chip, e.g., in terms of memory size, memory band-
width,cachesize,theamountofcomputation,andthebenefitofspecializedoperations?
Discussions131.
8.5BatchNormalization
Trainingdeepneuralnetworksisdifficult. Gettingthemtoconvergeinareasonableamount
of time can be tricky. In this section, we describe batch normalization , a popular and
effective technique that consistently accelerates the convergence of deep networks ( Ioffe
and Szegedy, 2015 ). Together with residual blocks‚Äîcovered later in Section 8.6 ‚Äîbatch
normalization has made it possible for practitioners to routinely train networks with over
100 layers. A secondary (serendipitous) benefit of batch normalization lies in its inherent
regularization.
import torch
from torch import nn
from d2l import torch asd2l
293 Batch Normalization
8.5.1TrainingDeep Networks
When working with data, we often preprocess before training. Choices regarding data pre-
processingoftenmakeanenormousdifferenceinthefinalresults. Recallourapplicationof
MLPs to predicting house prices ( Section 5.7 ). Our first step when working with real data
wastostandardizeourinputfeaturestohavezeromean ùùÅ=0andunitvariance ùö∫=1across
multiple observations ( Friedman, 1987 ), frequently rescaling the latter so that the diagonal
is unity, i.e., Œ£ùëñùëñ=1. Yet another strategy is to rescale vectors to unit length, possibly
zero mean per observation . This can work well, e.g., for spatial sensor data. These pre-
processing techniques and many others, are beneficial for keeping the estimation problem
well controlled. For a review of feature selection and extraction see the article of Guyon et
al.(2008), for example. Standardizing vectors also has the nice side-effect of constraining
the function complexity of functions that act upon it. For instance, the celebrated radius-
margin bound ( Vapnik, 1995 ) in support vector machines and the Perceptron Convergence
Theorem ( Novikoff, 1962 ) rely on inputs of bounded norm.
Intuitively,thisstandardizationplaysnicelywithouroptimizerssinceitputstheparameters
a priori on a similar scale. As such, it is only natural to ask whether a corresponding
normalization step insidea deep network might not be beneficial. While this is not quite
the reasoning that led to the invention of batch normalization ( Ioffe and Szegedy, 2015 ),
it is a useful way of understanding it and its cousin, layer normalization ( Baet al., 2016),
within a unified framework.
Second, for a typical MLP or CNN, as we train, the variables in intermediate layers (e.g.,
affine transformation outputs in MLP) may take values with widely varying magnitudes:
whether along the layers from input to output, across units in the same layer, and over
time due to our updates to the model parameters. The inventors of batch normalization
postulated informally that this drift in the distribution of such variables could hamper the
convergence of the network. Intuitively, we might conjecture that if one layer has variable
activations that are 100 times that of another layer, this might necessitate compensatory
adjustments in the learning rates. Adaptive solvers such as AdaGrad ( Duchiet al., 2011),
Adam (Kingma and Ba, 2014 ), Yogi ( Zaheeret al., 2018), or Distributed Shampoo ( Anil
etal., 2020)aimtoaddressthisfromtheviewpointofoptimization, e.g., byaddingaspects
ofsecond-ordermethods. Thealternativeistopreventtheproblemfromoccurring, simply
by adaptive normalization.
Third, deeper networks are complex and tend to be more liable to overfitting. This means
that regularization becomes more critical. A common technique for regularization is noise
injection. This has been known for a long time, e.g., with regard to noise injection for
the inputs ( Bishop, 1995 ). It also forms the basis of dropout in Section 5.6 . As it turns
out, quite serendipitously, batch normalization conveys all three benefits: preprocessing,
numerical stability, and regularization.
Batch normalization is applied to individual layers, or optionally, to all of them: In each
trainingiteration,wefirstnormalizetheinputs(ofbatchnormalization)bysubtractingtheir
meananddividingbytheirstandarddeviation,wherebothareestimatedbasedonthestatis-
tics of the current minibatch. Next, we apply a scale coefficient and an offset to recover the
294 Modern Convolutional Neural Networks
lost degrees of freedom. It is precisely due to this normalization based on batchstatistics
thatbatchnormalization derives its name.
Note that if wetried to applybatch normalization with minibatches of size 1, wewouldnot
be able to learn anything. That is because after subtracting the means, each hidden unit
would take value 0. As you might guess, since we are devoting a whole section to batch
normalization, with large enough minibatches the approach proves effective and stable.
One takeaway here is that when applying batch normalization, the choice of batch size is
even more significant than without batch normalization, or at least, suitable calibration is
needed as we might adjust batch size.
Denote byBa minibatch and let x2Bbe an input to batch normalization (BN). In this
case the batch normalization is defined as follows:
BN¬πx¬∫=ùú∏x ÀÜùùÅB
ÀÜùùàB¬∏ùú∑. (8.5.1)
In(8.5.1 ),ÀÜùùÅBisthesamplemeanand ÀÜùùàBisthesamplestandarddeviationoftheminibatch
B. Afterapplyingstandardization,theresultingminibatchhaszeromeanandunitvariance.
Thechoiceofunitvariance(ratherthansomeothermagicnumber)isarbitrary. Werecover
thisdegreeoffreedombyincludinganelementwise scaleparameter ùú∏andshiftparameter
ùú∑that have the same shape as x. Both are parameters that need to be learned as part of
model training.
The variable magnitudes for intermediate layers cannot diverge during training since batch
normalizationactivelycentersandrescalesthembacktoagivenmeanandsize(via ÀÜùùÅBand
ÀÜùùàB). Practical experience confirms that, as alluded to when discussing feature rescaling,
batch normalization seems to allow for more aggressive learning rates. We calculate ÀÜùùÅB
and ÀÜùùàBin(8.5.1 )as follows:
ÀÜùùÅB=1
jBj√ï
x2Bxand ÀÜùùà2
B=1
jBj√ï
x2B¬πx ÀÜùùÅB¬∫2¬∏ùúñ. (8.5.2)
Note that we add a small constant ùúñ > 0to the variance estimate to ensure that we never
attempt division by zero, even in cases where the empirical variance estimate might be
verysmallorvanish. Theestimates ÀÜùùÅBand ÀÜùùàBcounteractthescalingissuebyusingnoisy
estimates of mean and variance. You might think that this noisiness should be a problem.
On the contrary, it is actually beneficial.
This turns out to be a recurring theme in deep learning. For reasons that are not yet well-
characterized theoretically, various sources of noise in optimization often lead to faster
training and less overfitting: this variation appears to act as a form of regularization. Teye
etal.(2018)andLuo etal.(2018)relatedthepropertiesofbatchnormalizationtoBayesian
priors and penalties, respectively. In particular, this sheds some light on the puzzle of why
batch normalization works best for moderate minibatch sizes in the 50‚Äì100 range. This
particular size of minibatch seems to inject just the ‚Äúright amount‚Äù of noise per layer, both
intermsofscalevia ÀÜùùà, and intermsofoffsetvia ÀÜùùÅ: a largerminibatchregularizeslessdue
to the more stable estimates, whereas tiny minibatches destroy useful signal due to high
variance. Exploring this direction further, considering alternative types of preprocessing
and filtering may yet lead to other effective types of regularization.
295 Batch Normalization
Fixing a trained model, you might think that we would prefer using the entire dataset to
estimate the mean and variance. Once training is complete, why would we want the same
image to be classified differently, depending on the batch in which it happens to reside?
During training, such exact calculation is infeasible because the intermediate variables for
all data examples change every time we update our model. However, once the model is
trained, we can calculate the means and variances of each layer‚Äôs variables based on the
entire dataset. Indeed this is standard practice for models employing batch normalization;
thusbatchnormalizationlayersfunctiondifferentlyin trainingmode (normalizingbymini-
batchstatistics)thanin predictionmode (normalizingbydatasetstatistics). Inthisformthey
closely resemble the behavior of dropout regularization of Section 5.6 , where noise is only
injected during training.
8.5.2BatchNormalizationLayers
Batch normalization implementations for fully connected layers and convolutional layers
are slightly different. One key difference between batch normalization and other layers is
that because the former operates on a full minibatch at a time, we cannot just ignore the
batch dimension as we did before when introducing other layers.
FullyConnected Layers
Whenapplyingbatchnormalizationtofullyconnectedlayers, IoffeandSzegedy( 2015), in
their original paper inserted batch normalization after the affine transformation and before
the nonlinear activation function. Later applications experimented with inserting batch
normalization right afteractivation functions. Denoting the input to the fully connected
layer by x, the affine transformation by Wx¬∏b(with the weight parameter Wand the
bias parameter b), and the activation function by ùúô, we can express the computation of a
batch-normalization-enabled, fully connected layer output has follows:
h=ùúô¬πBN¬πWx¬∏b¬∫¬∫. (8.5.3)
Recall that mean and variance are computed on the sameminibatch on which the transfor-
mation is applied.
ConvolutionalLayers
Similarly,withconvolutionallayers,wecanapplybatchnormalizationaftertheconvolution
but before the nonlinear activation function. The key difference from batch normalization
in fully connected layers is that we apply the operation on a per-channel basis across all
locations . This is compatible with our assumption of translation invariance that led to
convolutions: we assumed that the specific location of a pattern within an image was not
critical for the purpose of understanding.
Assume that our minibatches contain ùëöexamples and that for each channel, the output
of the convolution has height ùëùand widthùëû. For convolutional layers, we carry out each
batch normalization over the ùëöùëùùëûelements per output channel simultaneously. Thus,
296 Modern Convolutional Neural Networks
we collect the values over all spatial locations when computing the mean and variance and
consequently apply the same mean and variance within a given channel to normalize the
value at each spatial location. Each channel has its own scale and shift parameters, both of
which are scalars.
LayerNormalization
Note that in the context of convolutions the batch normalization is well defined even for
minibatches of size 1: after all, we have all the locations across an image to average. Con-
sequently, mean and variance are well defined, even if it is just within a single observation.
This consideration led Ba et al.(2016) to introduce the notion of layer normalization . It
works just like a batch norm, only that it is applied to one observation at a time. Conse-
quently both the offset and the scaling factor are scalars. For an ùëõ-dimensional vector x,
layer norms are given by
x!LN¬πx¬∫=x ÀÜùúá
ÀÜùúé, (8.5.4)
where scaling and offset are applied coefficient-wise and given by
ÀÜùúádef=1
ùëõùëõ√ï
ùëñ=1ùë•ùëñand ÀÜùúé2def=1
ùëõùëõ√ï
ùëñ=1¬πùë•ùëñ ÀÜùúá¬∫2¬∏ùúñ. (8.5.5)
Asbeforeweaddasmalloffset ùúñ >0topreventdivisionbyzero. Oneofthemajorbenefits
of using layer normalization is that it prevents divergence. After all, ignoring ùúñ, the output
ofthelayernormalizationisscaleindependent. Thatis, wehaveLN ¬πx¬∫LN¬πùõºx¬∫forany
choice ofùõº‚â†0. This becomes an equality for jùõºj!1(the approximate equality is due
to the offsetùúñfor the variance).
Another advantage of the layer normalization is that it does not depend on the minibatch
size. Itisalsoindependentofwhetherweareintrainingortestregime. Inotherwords,itis
simplyadeterministictransformationthatstandardizestheactivationstoagivenscale. This
canbeverybeneficialinpreventingdivergenceinoptimization. Weskipfurtherdetailsand
recommend that interested readers consult the original paper.
BatchNormalization During Prediction
Aswementionedearlier,batchnormalizationtypicallybehavesdifferentlyintrainingmode
thaninpredictionmode. First,thenoiseinthesamplemeanandthesamplevariancearising
fromestimatingeachonminibatchesisnolongerdesirableoncewehavetrainedthemodel.
Second, we might not have the luxury of computing per-batch normalization statistics. For
example, we might need to apply our model to make one prediction at a time.
Typically, after training, we use the entire dataset to compute stable estimates of the vari-
able statistics and then fix them at prediction time. Hence, batch normalization behaves
differently during training than at test time. Recall that dropout also exhibits this charac-
teristic.
297 Batch Normalization
8.5.3Implementation fromScratch
To see how batch normalization works in practice, we implement one from scratch be-
low.
def batch_norm (X, gamma, beta, moving_mean, moving_var, eps, momentum):
# Use is_grad_enabled to determine whether we are in training mode
ifnot torch .is_grad_enabled():
# In prediction mode, use mean and variance obtained by moving average
X_hat =(X-moving_mean) /torch .sqrt(moving_var +eps)
else :
assert len(X.shape) in(2,4)
iflen(X.shape) ==2:
# When using a fully connected layer, calculate the mean and
# variance on the feature dimension
mean =X.mean(dim =0)
var =((X -mean) **2).mean(dim =0)
else :
# When using a two-dimensional convolutional layer, calculate the
# mean and variance on the channel dimension (axis=1). Here we
# need to maintain the shape of X, so that the broadcasting
# operation can be carried out later
mean =X.mean(dim =(0,2,3), keepdim =True )
var =((X -mean) **2).mean(dim =(0,2,3), keepdim =True )
# In training mode, the current mean and variance are used
X_hat =(X-mean) /torch .sqrt(var +eps)
# Update the mean and variance using moving average
moving_mean =(1.0 -momentum) *moving_mean +momentum *mean
moving_var =(1.0 -momentum) *moving_var +momentum *var
Y=gamma *X_hat +beta # Scale and shift
return Y, moving_mean .data, moving_var .data
Wecannowcreateaproper BatchNorm layer. Ourlayerwillmaintainproperparametersfor
scale gammaand shift beta, both of which will be updated in the course of training. Addi-
tionally,ourlayerwillmaintainmovingaveragesofthemeansandvariancesforsubsequent
use during model prediction.
Puttingasidethealgorithmicdetails,notethedesignpatternunderlyingourimplementation
of the layer. Typically, we define the mathematics in a separate function, say batch_norm .
Wethenintegratethisfunctionalityintoacustomlayer,whosecodemostlyaddressesbook-
keepingmatters, suchasmovingdatatotherightdevicecontext, allocatingandinitializing
any required variables, keeping track of moving averages (here for mean and variance),
and so on. This pattern enables a clean separation of mathematics from boilerplate code.
Also note that for the sake of convenience we did not worry about automatically inferring
the input shape here; thus we need to specify the number of features throughout. By now
all modern deep learning frameworks offer automatic detection of size and shape in the
high-level batch normalization APIs (in practice we will use this instead).
class BatchNorm (nn.Module):
# num_features: the number of outputs for a fully connected layer or the
# number of output channels for a convolutional layer. num_dims: 2 for a
(continues on next page)
298 Modern Convolutional Neural Networks
(continued from previous page)
# fully connected layer and 4 for a convolutional layer
def __init__ (self , num_features, num_dims):
super ().__init__ ()
ifnum_dims ==2:
shape =(1, num_features)
else :
shape =(1, num_features, 1,1)
# The scale parameter and the shift parameter (model parameters) are
# initialized to 1 and 0, respectively
self .gamma =nn.Parameter(torch .ones(shape))
self .beta =nn.Parameter(torch .zeros(shape))
# The variables that are not model parameters are initialized to 0 and
# 1
self .moving_mean =torch .zeros(shape)
self .moving_var =torch .ones(shape)
def forward (self , X):
# If X is not on the main memory, copy moving_mean and moving_var to
# the device where X is located
ifself .moving_mean .device !=X.device:
self .moving_mean =self .moving_mean .to(X .device)
self .moving_var =self .moving_var .to(X .device)
# Save the updated moving_mean and moving_var
Y,self .moving_mean, self .moving_var =batch_norm(
X,self .gamma, self .beta, self .moving_mean,
self .moving_var, eps =1e-5 , momentum =0.1)
return Y
We used momentum to govern the aggregation over past mean and variance estimates. This
is somewhat of a misnomer as it has nothing whatsoever to do with the momentum term of
optimization. Nonetheless, itisthecommonlyadoptednameforthistermandindeference
to API naming convention we use the same variable name in our code.
8.5.4LeNetwith BatchNormalization
To see how to apply BatchNorm in context, below we apply it to a traditional LeNet model
(Section 7.6 ). Recall that batch normalization is applied after the convolutional layers or
fully connected layers but before the corresponding activation functions.
class BNLeNetScratch (d2l .Classifier):
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nn.LazyConv2d( 6, kernel_size =5), BatchNorm( 6, num_dims =4),
nn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),
nn.LazyConv2d( 16, kernel_size =5), BatchNorm( 16, num_dims =4),
nn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),
nn.Flatten(), nn .LazyLinear( 120),
BatchNorm( 120, num_dims =2), nn .Sigmoid(), nn .LazyLinear( 84),
BatchNorm( 84, num_dims =2), nn .Sigmoid(),
nn.LazyLinear(num_classes))
299 Batch Normalization
As before, we will train our network on the Fashion-MNIST dataset. This code is virtually
identical to that when we first trained LeNet.
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128)
model =BNLeNetScratch(lr =0.1)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
Let‚Äôs have a look at the scale parameter gammaand the shift parameter betalearned from
the first batch normalization layer.
model .net[ 1].gamma .reshape(( -1,)), model .net[ 1].beta .reshape(( -1,))
(tensor([ 1.4334 ,1.9905 ,1.8584 ,2.0740 ,2.0522 ,1.8877 ], device ='cuda:0 ',
grad_fn =<ViewBackward0 >),
tensor([ 0.7354 ,-1.3538 ,-0.2567 ,-0.9991 ,-0.3028 ,1.3125 ], device ='cuda:0
‚Ü©!',
grad_fn =<ViewBackward0 >))
8.5.5Concise Implementation
Compared with the BatchNorm class, which we just defined ourselves, we can use the
BatchNorm class defined in high-level APIs from the deep learning framework directly.
The code looks virtually identical to our implementation above, except that we no longer
need to provide additional arguments for it to get the dimensions right.
class BNLeNet (d2l .Classifier):
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential(
nn.LazyConv2d( 6, kernel_size =5), nn .LazyBatchNorm2d(),
nn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),
nn.LazyConv2d( 16, kernel_size =5), nn .LazyBatchNorm2d(),
nn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),
nn.Flatten(), nn .LazyLinear( 120), nn .LazyBatchNorm1d(),
(continues on next page)
300 Modern Convolutional Neural Networks
(continued from previous page)
nn.Sigmoid(), nn .LazyLinear( 84), nn .LazyBatchNorm1d(),
nn.Sigmoid(), nn .LazyLinear(num_classes))
Below, we use the same hyperparameters to train our model. Note that as usual, the high-
level API variant runs much faster because its code has been compiled to C++ or CUDA
while our custom implementation must be interpreted by Python.
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128)
model =BNLeNet(lr =0.1)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
8.5.6Discussion
Intuitively, batch normalization is thought to make the optimization landscape smoother.
However, we must be careful to distinguish between speculative intuitions and true expla-
nations for the phenomena that we observe when training deep models. Recall that we do
notevenknowwhysimplerdeepneuralnetworks(MLPsandconventionalCNNs)general-
ize well in the first place. Even with dropout and weight decay, they remain so flexible that
their ability to generalize to unseen data likely needs significantly more refined learning-
theoretic generalization guarantees.
Theoriginalpaperproposingbatchnormalization( IoffeandSzegedy, 2015 ), inadditionto
introducingapowerfulandusefultool,offeredanexplanationforwhyitworks: byreducing
internal covariate shift . Presumably by internal covariate shift they meant something like
the intuition expressed above‚Äîthe notion that the distribution of variable values changes
overthecourseoftraining. However,thereweretwoproblemswiththisexplanation: i)This
drift is very different from covariate shift , rendering the name a misnomer. If anything, it
is closer to concept drift. ii) The explanation offers an under-specified intuition but leaves
the question of whypreciselythistechniqueworks an open question wanting for a rigorous
explanation. Throughoutthisbook,weaimtoconveytheintuitionsthatpractitionersuseto
guide their development of deep neural networks. However, we believe that it is important
to separate these guiding intuitions from established scientific fact. Eventually, when you
301 Batch Normalization
master this material and start writing your own research papers you will want to be clear to
delineate between technical claims and hunches.
Followingthesuccessofbatchnormalization,itsexplanationintermsof internalcovariate
shifthas repeatedly surfaced in debates in the technical literature and broader discourse
about how to present machine learning research. In a memorable speech given while ac-
cepting a Test of Time Award at the 2017 NeurIPS conference, Ali Rahimi used internal
covariateshift asafocalpointinanargumentlikeningthemodernpracticeofdeeplearning
to alchemy. Subsequently, the example was revisited in detail in a position paper outlining
troubling trends in machine learning ( Lipton and Steinhardt, 2018 ). Other authors have
proposed alternative explanations for the success of batch normalization, some ( Santurkar
etal.,2018)claimingthatbatchnormalization‚Äôssuccesscomesdespiteexhibitingbehavior
that is in some ways opposite to those claimed in the original paper.
We note that the internal covariate shift is no more worthy of criticism than any of thou-
sandsofsimilarlyvagueclaimsmadeeveryyearinthetechnicalmachinelearningliterature.
Likely, its resonance as a focal point of these debates owes to its broad recognizability for
the target audience. Batch normalization has proven an indispensable method, applied in
nearlyalldeployedimageclassifiers,earningthepaperthatintroducedthetechniquetensof
thousandsofcitations. Weconjecture, though, thattheguidingprinciplesofregularization
through noise injection, acceleration through rescaling and lastly preprocessing may well
lead to further inventions of layers and techniques in the future.
On a more practical note, there are a number of aspects worth remembering about batch
normalization:
Duringmodeltraining,batchnormalizationcontinuouslyadjuststheintermediateoutput
of the network by utilizing the mean and standard deviation of the minibatch, so that
the values of the intermediate output in each layer throughout the neural network are
more stable.
Batchnormalizationisslightlydifferentforfullyconnectedlayersthanforconvolutional
layers. In fact, for convolutional layers, layer normalization can sometimes be used as
an alternative.
Likeadropoutlayer,batchnormalizationlayershavedifferentbehaviorsintrainingmode
than in prediction mode.
Batchnormalizationisusefulforregularizationandimprovingconvergenceinoptimiza-
tion. By contrast, the original motivation of reducing internal covariate shift seems
not to be a valid explanation.
For more robust models that are less sensitive to input perturbations, consider removing
batch normalization ( Wangetal., 2022).
8.5.7Exercises
1.Shouldweremovethebiasparameterfromthefullyconnectedlayerortheconvolutional
layer before the batch normalization? Why?
302 Modern Convolutional Neural Networks
1322.Compare the learning rates for LeNet with and without batch normalization.
1.Plot the increase in validation accuracy.
2.Howlargecanyoumakethelearningratebeforetheoptimizationfailsinbothcases?
3.Do we need batch normalization in every layer? Experiment with it.
4.Implement a ‚Äúlite‚Äù version of batch normalization that only removes the mean, or alter-
natively one that only removes the variance. How does it behave?
5.Fix the parameters betaandgamma. Observe and analyze the results.
6.Can you replace dropout by batch normalization? How does the behavior change?
7.Research ideas: think of other normalization transforms that you can apply:
1.Can you apply the probability integral transform?
2.Can you use a full-rank covariance estimate? Why should you probably not do that?
3.Can you use other compact matrix variants (block-diagonal, low-displacement rank,
Monarch, etc.)?
4.Does a sparsification compression act as a regularizer?
5.Are there other projections (e.g., convex cone, symmetry group-specific transforms)
that you can use?
Discussions132.
8.6ResidualNetworks(ResNet)and ResNeXt
As we design ever deeper networks it becomes imperative to understand how adding layers
can increase the complexity and expressiveness of the network. Even more important is
theabilitytodesignnetworkswhereaddinglayersmakesnetworksstrictlymoreexpressive
rather than just different. To make some progress we need a bit of mathematics.
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
8.6.1Function Classes
ConsiderF,theclassoffunctionsthataspecificnetworkarchitecture(togetherwithlearn-
ing rates and other hyperparameter settings) can reach. That is, for all ùëì2Fthere exists
some set of parameters (e.g., weights and biases) that can be obtained through training on
a suitable dataset. Let‚Äôs assume that ùëìis the ‚Äútruth‚Äù function that we really would like to
303 Residual Networks (ResNet) and ResNeXt
find. If it is inF, we are in good shape but typically we will not be quite so lucky. Instead,
we will try to find some ùëì
Fwhich is our best bet within F. For instance, given a dataset
with features Xand labels y, we might try finding it by solving the following optimization
problem:
ùëì
Fdef=argmin
ùëìùêø¬πX,y, ùëì¬∫subject toùëì2F. (8.6.1)
We know that regularization ( Morozov, 1984 ,Tikhonov and Arsenin, 1977 ) may control
complexityofFandachieveconsistency,soalargersizeoftrainingdatagenerallyleadsto
betterùëì
F. It is only reasonable to assume that if we design a different and more powerful
architectureF0we should arrive at a better outcome. In other words, we would expect
thatùëì
F0is ‚Äúbetter‚Äù than ùëì
F. However, ifF‚äàF0there is no guarantee that this should
even happen. In fact, ùëì
F0might well be worse. As illustrated by Fig. 8.6.1 , for non-nested
functionclasses, alargerfunctionclassdoesnotalwaysmoveclosertothe‚Äútruth‚Äùfunction
ùëì. Forinstance, ontheleftof Fig.8.6.1 , thoughF3isclosertoùëìthanF1,F6movesaway
and there is no guarantee that further increasing the complexity can reduce the distance
fromùëì. With nested function classes where F1F 6on the right of Fig. 8.6.1 , we
can avoid the aforementioned issue from the non-nested function classes.
tFig. 8.6.1 For non-nested function classes, a larger (indicated by area) function class does not
guarantee we will get closer to the ‚Äútruth‚Äù function ( f). This does not happen in nested
function classes.
Thus,onlyiflargerfunctionclassescontainthesmalleronesareweguaranteedthatincreas-
ing them strictly increases the expressive power of the network. For deep neural networks,
if we can train the newly-added layer into an identity function ùëì¬πx¬∫=x, the new model
will be as effective as the original model. As the new model may get a better solution to fit
the training dataset, the added layer might make it easier to reduce training errors.
This is the question that He et al.(2016) considered when working on very deep com-
puter vision models. At the heart of their proposed residual network (ResNet) is the idea
that every additional layer should more easily contain the identity function as one of its
elements. These considerations are rather profound but they led to a surprisingly simple
solution, a residualblock . With it, ResNet won the ImageNet Large Scale Visual Recogni-
tion Challenge in 2015. The design had a profound influence on how to build deep neural
networks. For instance, residual blocks have been added to recurrent networks ( Kimetal.,
2017,Prakashet al., 2016). Likewise, Transformers ( Vaswaniet al., 2017) use them to
304 Modern Convolutional Neural Networks
stack many layers of networks efficiently. It is also used in graph neural networks ( Kipf
andWelling, 2016 )and, asabasicconcept, ithasbeenusedextensivelyincomputervision
(Redmonand Farhadi, 2018 ,Renetal., 2015). Notethat residual networksare predated by
highwaynetworks( Srivastava etal.,2015)thatsharesomeofthemotivation,albeitwithout
the elegant parametrization around the identity function.
8.6.2ResidualBlocks
Let‚Äôs focus on a local part of a neural network, as depicted in Fig. 8.6.2 . Denote the input
byx. Weassumethat ùëì¬πx¬∫,thedesiredunderlyingmappingwewanttoobtainbylearning,
istobeusedasinputtotheactivationfunctiononthetop. Ontheleft,theportionwithinthe
dotted-line box must directly learn ùëì¬πx¬∫. On the right, the portion within the dotted-line
box needs to learn the residualmapping ùëî¬πx¬∫=ùëì¬πx¬∫ x, which is how the residual block
derives its name. If the identity mapping ùëì¬πx¬∫=xis the desired underlying mapping, the
residualmappingamountsto ùëî¬πx¬∫=0anditisthuseasiertolearn: weonlyneedtopushthe
weights and biases of the upper weight layer (e.g., fully connected layer and convolutional
layer) within the dotted-line box to zero. The right figure illustrates the residual block of
ResNet, where the solid line carrying the layer input xto the addition operator is called
aresidual connection (orshortcut connection ). With residual blocks, inputs can forward
propagate faster through the residual connections across layers. In fact, the residual block
canbethoughtofasaspecialcaseofthemulti-branchInceptionblock: ithastwobranches
one of which is the identity mapping.
tFig. 8.6.2 In a regular block (left), the portion within the dotted-line box must directly learn the
mapping f¬πx¬∫. In a residual block (right), the portion within the dotted-line box needs to
learn the residual mapping g¬πx¬∫=f¬πx¬∫ x, making the identity mapping f¬πx¬∫=xeasier
to learn.
ResNet has VGG‚Äôs full 33convolutional layer design. The residual block has two 33
convolutional layers with the same number of output channels. Each convolutional layer
is followed by a batch normalization layer and a ReLU activation function. Then, we skip
thesetwoconvolutionoperationsandaddtheinputdirectlybeforethefinalReLUactivation
function. This kind of design requires that the output of the twoconvolutional layers has to
be of the same shape as the input, so that they can be added together. If we want to change
the number of channels, we need to introduce an additional 11convolutional layer to
305 Residual Networks (ResNet) and ResNeXt
transform the input into the desired shape for the addition operation. Let‚Äôs have a look at
the code below.
class Residual (nn.Module): #@save
"""The Residual block of ResNet models."""
def __init__ (self , num_channels, use_1x1conv =False , strides =1):
super ().__init__ ()
self .conv1 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1,
stride =strides)
self .conv2 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1)
ifuse_1x1conv:
self .conv3 =nn.LazyConv2d(num_channels, kernel_size =1,
stride =strides)
else :
self .conv3 =None
self .bn1 =nn.LazyBatchNorm2d()
self .bn2 =nn.LazyBatchNorm2d()
def forward (self , X):
Y=F.relu( self .bn1( self .conv1(X)))
Y=self .bn2( self .conv2(Y))
ifself .conv3:
X=self .conv3(X)
Y+=X
return F.relu(Y)
Thiscodegeneratestwotypesofnetworks: onewhereweaddtheinputtotheoutputbefore
applying the ReLU nonlinearity whenever use_1x1conv=False ; and one where we adjust
channelsandresolutionbymeansofa 11convolutionbeforeadding. Fig.8.6.3 illustrates
this.
tFig. 8.6.3 ResNet block with and without 1 1 convolution, which transforms the input into the
desired shape for the addition operation.
Now let‚Äôs look at a situation where the input and output are of the same shape, where 11
convolution is not needed.
306 Modern Convolutional Neural Networks
blk =Residual( 3)
X=torch .randn( 4,3,6,6)
blk(X) .shape
torch .Size([ 4,3,6,6])
We also have the option to halve the output height and width while increasing the number
of output channels. In this case we use 11convolutions via use_1x1conv=True . This
comes in handy at the beginning of each ResNet block to reduce the spatial dimensionality
viastrides=2 .
blk =Residual( 6, use_1x1conv =True , strides =2)
blk(X) .shape
torch .Size([ 4,6,3,3])
8.6.3ResNetModel
ThefirsttwolayersofResNetarethesameasthoseoftheGoogLeNetwedescribedbefore:
the77convolutional layer with 64 output channels and a stride of 2 is followed by the
33max-pooling layer with a stride of 2. The difference is the batch normalization layer
added after each convolutional layer in ResNet.
class ResNet (d2l .Classifier):
def b1(self ):
return nn.Sequential(
nn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),
nn.LazyBatchNorm2d(), nn .ReLU(),
nn.MaxPool2d(kernel_size =3, stride =2, padding =1))
GoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four
modules made up of residual blocks, each of which uses several residual blocks with the
same number of output channels. The number of channels in the first module is the same
as the number of input channels. Since a max-pooling layer with a stride of 2 has already
been used, it is not necessary to reduce the height and width. In the first residual block for
each of the subsequent modules, the number of channels is doubled compared with that of
the previous module, and the height and width are halved.
@d2l .add_to_class(ResNet)
def block (self , num_residuals, num_channels, first_block =False ):
blk =[]
for iinrange (num_residuals):
ifi==0and not first_block:
blk.append(Residual(num_channels, use_1x1conv =True , strides =2))
else :
(continues on next page)
307 Residual Networks (ResNet) and ResNeXt
(continued from previous page)
blk.append(Residual(num_channels))
return nn.Sequential( *blk)
Then, we add all the modules to ResNet. Here, two residual blocks are used for each mod-
ule. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the
fully connected layer output.
@d2l .add_to_class(ResNet)
def __init__ (self , arch, lr =0.1, num_classes =10):
super (ResNet, self ).__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential( self .b1())
for i, b inenumerate (arch):
self .net.add_module( f'b{i+2}',self .block( *b, first_block =(i==0)))
self .net.add_module( 'last ', nn .Sequential(
nn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),
nn.LazyLinear(num_classes)))
self .net.apply(d2l .init_cnn)
Therearefourconvolutionallayersineachmodule(excludingthe 11convolutionallayer).
Togetherwiththefirst 77convolutionallayerandthefinalfullyconnectedlayer,thereare
18layersintotal. Therefore,thismodeliscommonlyknownasResNet-18. Byconfiguring
different numbers of channels and residual blocks in the module, we can create different
ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture
ofResNetissimilartothatofGoogLeNet, ResNet‚Äôsstructureissimplerandeasiertomod-
ify. All these factors have resulted in the rapid and widespread use of ResNet. Fig. 8.6.4
depicts the full ResNet-18.
tFig. 8.6.4 The ResNet-18 architecture.
BeforetrainingResNet,let‚Äôsobservehowtheinputshapechangesacrossdifferentmodules
in ResNet. As in all the previous architectures, the resolution decreases while the number
of channels increases up until the point where a global average pooling layer aggregates all
features.
class ResNet18 (ResNet):
def __init__ (self , lr =0.1, num_classes =10):
super ().__init__ (((2,64), ( 2,128), ( 2,256), ( 2,512)),
lr, num_classes)
308 Modern Convolutional Neural Networks
ResNet18() .layer_summary(( 1,1,96,96))
Sequential output shape: torch .Size([ 1,64,24,24])
Sequential output shape: torch .Size([ 1,64,24,24])
Sequential output shape: torch .Size([ 1,128,12,12])
Sequential output shape: torch .Size([ 1,256,6,6])
Sequential output shape: torch .Size([ 1,512,3,3])
Sequential output shape: torch .Size([ 1,10])
8.6.4Training
We train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a pow-
erful and flexible architecture. The plot capturing training and validation loss illustrates a
significant gap between both graphs, with the training loss being considerably lower. For
a network of this flexibility, more training data would offer distinct benefit in closing the
gap and improving accuracy.
model =ResNet18(lr =0.01 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(96,96))
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model, data)
8.6.5ResNeXt
One of the challenges one encounters in the design of ResNet is the trade-off between non-
linearity and dimensionality within a given block. That is, we could add more nonlinearity
by increasing the number of layers, or by increasing the width of the convolutions. An al-
ternative strategy is to increase the number of channels that can carry information between
blocks. Unfortunately, the latter comes with a quadratic penalty since the computational
cost of ingesting ùëêichannels and emitting ùëêochannels is proportional to O¬πùëêiùëêo¬∫(see our
discussion in Section 7.4 ).
We can take some inspiration from the Inception block of Fig. 8.4.1 which has informa-
tion flowing through the block in separate groups. Applying the idea of multiple indepen-
dent groups to the ResNet block of Fig. 8.6.3 led to the design of ResNeXt ( Xieet al.,
309 Residual Networks (ResNet) and ResNeXt
2017). Different from the smorgasbord of transformations in Inception, ResNeXt adopts
thesametransformation in all branches, thus minimizing the need for manual tuning of
each branch.
tFig. 8.6.5 The ResNeXt block. The use of grouped convolution with ggroups is gtimes faster than
a dense convolution. It is a bottleneck residual block when the number of intermediate
channels bis less than c.
Breaking up a convolution from ùëêitoùëêochannels into one of ùëîgroups of size ùëêi¬ùùëîgener-
atingùëîoutputs of size ùëêo¬ùùëîis called, quite fittingly, a groupedconvolution . The computa-
tionalcost(proportionally)isreducedfrom O¬πùëêiùëêo¬∫toO¬πùëî¬πùëêi¬ùùëî¬∫¬πùëêo¬ùùëî¬∫¬∫=O¬πùëêiùëêo¬ùùëî¬∫,
i.e.,itisùëîtimesfaster. Evenbetter,thenumberofparametersneededtogeneratetheoutput
is also reduced from a ùëêiùëêomatrix toùëîsmaller matrices of size ¬πùëêi¬ùùëî¬∫¬πùëêo¬ùùëî¬∫, again a
ùëîtimesreduction. Inwhatfollowsweassumethatboth ùëêiandùëêoaredivisibleby ùëî.
Theonlychallengeinthisdesignisthatnoinformationisexchangedbetweenthe ùëîgroups.
The ResNeXt block of Fig. 8.6.5 amends this in two ways: the grouped convolution with
a33kernel is sandwiched in between two 11convolutions. The second one serves
double duty in changing the number of channels back. The benefit is that we only pay the
O¬πùëêùëè¬∫cost for 11kernels and can make do with an O¬πùëè2¬ùùëî¬∫cost for 33kernels.
Similar to the residual block implementation in Section 8.6.2 , the residual connection is
replaced (thus generalized) by a 11convolution.
Theright-handfigurein Fig.8.6.5 providesamuchmoreconcisesummaryoftheresulting
network block. It will also play a major role in the design of generic modern CNNs in
Section 8.8 . Note that the idea of grouped convolutions dates back to the implementation
of AlexNet ( Krizhevsky et al., 2012). When distributing the network across two GPUs
with limited memory, the implementation treated each GPU as its own channel with no ill
effects.
The following implementation of the ResNeXtBlock class takes as argument groups(ùëî),
with bot_channels (ùëè)intermediate(bottleneck)channels. Lastly,whenweneedtoreduce
310 Modern Convolutional Neural Networks
theheightandwidthoftherepresentation,weaddastrideof 2bysetting use_1x1conv=True,
strides=2 .
class ResNeXtBlock (nn.Module): #@save
"""The ResNeXt block."""
def __init__ (self , num_channels, groups, bot_mul, use_1x1conv =False ,
strides =1):
super ().__init__ ()
bot_channels =int(round (num_channels *bot_mul))
self .conv1 =nn.LazyConv2d(bot_channels, kernel_size =1, stride =1)
self .conv2 =nn.LazyConv2d(bot_channels, kernel_size =3,
stride =strides, padding =1,
groups =bot_channels //groups)
self .conv3 =nn.LazyConv2d(num_channels, kernel_size =1, stride =1)
self .bn1 =nn.LazyBatchNorm2d()
self .bn2 =nn.LazyBatchNorm2d()
self .bn3 =nn.LazyBatchNorm2d()
ifuse_1x1conv:
self .conv4 =nn.LazyConv2d(num_channels, kernel_size =1,
stride =strides)
self .bn4 =nn.LazyBatchNorm2d()
else :
self .conv4 =None
def forward (self , X):
Y=F.relu( self .bn1( self .conv1(X)))
Y=F.relu( self .bn2( self .conv2(Y)))
Y=self .bn3( self .conv3(Y))
ifself .conv4:
X=self .bn4( self .conv4(X))
return F.relu(Y +X)
Itsuseisentirelyanalogoustothatofthe ResNetBlock discussedpreviously. Forinstance,
when using ( use_1x1conv=False, strides=1 ), the input and output are of the same
shape. Alternatively, setting use_1x1conv=True, strides=2 halves the output height
and width.
blk =ResNeXtBlock( 32,16,1)
X=torch .randn( 4,32,96,96)
blk(X) .shape
torch .Size([ 4,32,96,96])
8.6.6Summaryand Discussion
Nested function classes are desirable since they allow us to obtain strictly more power-
fulrather than also subtly different function classes when adding capacity. One way of
accomplishing this is by letting additional layers to simply pass through the input to the
output. Residual connections allow for this. As a consequence, this changes the inductive
bias from simple functions being of the form ùëì¬πx¬∫=0to simple functions looking like
ùëì¬πx¬∫=x.
311 Residual Networks (ResNet) and ResNeXt
The residual mapping can learn the identity function more easily, such as pushing param-
eters in the weight layer to zero. We can train an effective deepneural network by having
residualblocks. Inputscanforwardpropagatefasterthroughtheresidualconnectionsacross
layers. Asaconsequence, wecanthustrainmuchdeepernetworks. Forinstance, theorigi-
nal ResNetpaper ( Heetal., 2016) allowedforup to 152 layers. Another benefit of residual
networks is that it allows us to add layers, initialized as the identity function, duringthe
training process. After all, the default behavior of a layer is to let the data pass through
unchanged. This can accelerate the training of very large networks in some cases.
Prior to residual connections, bypassing paths with gating units were introduced to effec-
tively train highway networks with over 100 layers ( Srivastava etal., 2015). Using identity
functions as bypassing paths, ResNet performed remarkably well on multiple computer vi-
sion tasks. Residual connections had a major influence on the design of subsequent deep
neural networks, of either convolutional or sequential nature. As we will introduce later,
the Transformer architecture ( Vaswaniet al., 2017) adopts residual connections (together
with other design choices) and is pervasive in areas as diverse as language, vision, speech,
and reinforcement learning.
ResNeXt is an example for how the design of convolutional neural networks has evolved
over time: by being more frugal with computation and trading it off against the size of the
activations (number of channels), it allows for faster and more accurate networks at lower
cost. An alternative way of viewing grouped convolutions is to think of a block-diagonal
matrix for the convolutional weights. Note that there are quite a few such ‚Äútricks‚Äù that lead
to more efficient networks. For instance, ShiftNet ( Wuet al., 2018) mimicks the effects of
a33convolution,simplybyaddingshiftedactivationstothechannels,offeringincreased
function complexity, this time without any computational cost.
A common feature of the designs we have discussed so far is that the network design is
fairlymanual, primarilyrelyingontheingenuityofthedesignertofindthe‚Äúright‚Äùnetwork
hyperparameters. While clearly feasible, it is also very costly in terms of human time and
thereisnoguaranteethattheoutcomeisoptimalinanysense. In Section8.8 wewilldiscuss
a number of strategies for obtaining high quality networks in a more automated fashion. In
particular, we will review the notion of network design spaces that led to the RegNetX/Y
models ( Radosavovic etal., 2020).
8.6.7Exercises
1.WhatarethemajordifferencesbetweentheInceptionblockin Fig.8.4.1 andtheresidual
block? How do they compare in terms of computation, accuracy, and the classes of
functions they can describe?
2.Refer to Table 1 in the ResNet paper ( Heetal., 2016) to implement different variants of
the network.
3.For deeper networks, ResNet introduces a ‚Äúbottleneck‚Äù architecture to reduce model
complexity. Try to implement it.
4.In subsequent versions of ResNet, the authors changed the ‚Äúconvolution, batch normal-
312 Modern Convolutional Neural Networks
133ization, and activation‚Äù structure to the ‚Äúbatch normalization, activation, and convolu-
tion‚Äù structure. Make this improvement yourself. See Figure 1 in He et al.(2016) for
details.
5.Why can‚Äôt we just increase the complexity of functions without bound, even if the func-
tion classes are nested?
Discussions133.
8.7DenselyConnected Networks(DenseNet)
ResNet significantly changed the view of how to parametrize the functions in deep net-
works.DenseNet (dense convolutional network) is to some extent the logical extension of
this(Huangetal.,2017). DenseNetischaracterizedbyboththeconnectivitypatternwhere
each layer connects to all the preceding layers and the concatenation operation (rather than
the addition operator in ResNet) to preserve and reuse features from earlier layers. To un-
derstand how to arrive at it, let‚Äôs take a small detour to mathematics.
import torch
from torch import nn
from d2l import torch asd2l
8.7.1FromResNetto DenseNet
Recall the Taylor expansion for functions. At the point ùë•=0it can be written as
ùëì¬πùë•¬∫=ùëì¬π0¬∫¬∏ùë•
ùëì0¬π0¬∫¬∏ùë•ùëì00¬π0¬∫
2!¬∏ùë•ùëì000¬π0¬∫
3!¬∏
. (8.7.1)
The key point is that it decomposes a function into terms of increasingly higher order. In a
similar vein, ResNet decomposes functions into
ùëì¬πx¬∫=x¬∏ùëî¬πx¬∫. (8.7.2)
Thatis,ResNetdecomposes ùëìintoasimplelineartermandamorecomplexnonlinearone.
What if we wanted to capture (not necessarily add) information beyond two terms? One
such solution is DenseNet ( Huangetal., 2017).
tFig. 8.7.1 The main difference between ResNet (left) and DenseNet (right) in cross-layer
connections: use of addition and use of concatenation.
313 Densely Connected Networks (DenseNet)
As shown in Fig. 8.7.1 , the key difference between ResNet and DenseNet is that in the
latter case outputs are concatenated (denoted by¬ª,¬º) rather than added. As a result, we
perform a mapping from xto its values after applying an increasingly complex sequence
of functions:
x!¬ªx, ùëì1¬πx¬∫, ùëì2¬π¬ªx, ùëì1¬πx¬∫¬º¬∫, ùëì3¬π¬ªx, ùëì1¬πx¬∫, ùëì2¬π¬ªx, ùëì1¬πx¬∫¬º¬∫¬º¬∫,...¬º. (8.7.3)
Intheend,allthesefunctionsarecombinedinMLPtoreducethenumberoffeaturesagain.
In terms of implementation this is quite simple: rather than adding terms, we concatenate
them. ThenameDenseNetarisesfromthefactthatthedependencygraphbetweenvariables
becomes quite dense. The final layer of such a chain is densely connected to all previous
layers. The dense connections are shown in Fig. 8.7.2 .
tFig. 8.7.2 Dense connections in DenseNet. Note how the dimensionality increases with depth.
ThemaincomponentsthatcompriseaDenseNetare denseblocks andtransitionlayers . The
formerdefinehowtheinputsandoutputsareconcatenated,whilethelattercontrolthenum-
berofchannelssothatitisnottoolarge,sincetheexpansion x!¬ªx, ùëì1¬πx¬∫, ùëì2¬π¬ªx, ùëì1¬πx¬∫¬º¬∫,...¬º
can be quite high-dimensional.
8.7.2DenseBlocks
DenseNet uses the modified ‚Äúbatch normalization, activation, and convolution‚Äù structure
of ResNet (see the exercise in Section 8.6 ). First, we implement this convolution block
structure.
def conv_block (num_channels):
return nn.Sequential(
nn.LazyBatchNorm2d(), nn .ReLU(),
nn.LazyConv2d(num_channels, kernel_size =3, padding =1))
Adense block consists of multiple convolution blocks, each using the same number of
output channels. In the forward propagation, however, weconcatenate the input and output
of each convolution block on the channel dimension. Lazy evaluation allows us to adjust
the dimensionality automatically.
class DenseBlock (nn.Module):
def __init__ (self , num_convs, num_channels):
super (DenseBlock, self ).__init__ ()
layer =[]
for iinrange (num_convs):
layer .append(conv_block(num_channels))
self .net =nn.Sequential( *layer)
(continues on next page)
314 Modern Convolutional Neural Networks
(continued from previous page)
def forward (self , X):
for blk inself .net:
Y=blk(X)
# Concatenate input and output of each block along the channels
X=torch .cat((X, Y), dim =1)
return X
Inthefollowingexample,wedefinea DenseBlock instancewithtwoconvolutionblocksof
10 output channels. When using an input with three channels, we will get an output with
3¬∏10¬∏10=23channels. The number of convolution block channels controls the growth
in the number of output channels relative to the number of input channels. This is also
referred to as the growthrate .
blk =DenseBlock( 2,10)
X=torch .randn( 4,3,8,8)
Y=blk(X)
Y.shape
torch .Size([ 4,23,8,8])
8.7.3TransitionLayers
Sinceeachdenseblockwillincreasethenumberofchannels,addingtoomanyofthemwill
lead to an excessively complex model. A transitionlayer is used to control the complexity
of the model. It reduces the number of channels by using a 11convolution. Moreover, it
halves the height and width via average pooling with a stride of 2.
def transition_block (num_channels):
return nn.Sequential(
nn.LazyBatchNorm2d(), nn .ReLU(),
nn.LazyConv2d(num_channels, kernel_size =1),
nn.AvgPool2d(kernel_size =2, stride =2))
Apply a transition layer with 10 channels to the output of the dense block in the previous
example. This reduces the number of output channels to 10, and halves the height and
width.
blk =transition_block( 10)
blk(Y) .shape
torch .Size([ 4,10,4,4])
8.7.4DenseNetModel
315 Densely Connected Networks (DenseNet)
Next, we will construct a DenseNet model. DenseNet first uses the same single convolu-
tional layer and max-pooling layer as in ResNet.
class DenseNet (d2l .Classifier):
def b1(self ):
return nn.Sequential(
nn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),
nn.LazyBatchNorm2d(), nn .ReLU(),
nn.MaxPool2d(kernel_size =3, stride =2, padding =1))
Then, similar to the four modules made up of residual blocks that ResNet uses, DenseNet
usesfourdenseblocks. AswithResNet,wecansetthenumberofconvolutionallayersused
in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in Section
8.6. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional
layers in the dense block to 32, so 128 channels will be added to each dense block.
In ResNet, the height and width are reduced between each module by a residual block with
a stride of 2. Here, we use the transition layer to halve the height and width and halve the
number of channels. Similar to ResNet, a global pooling layer and a fully connected layer
are connected at the end to produce the output.
@d2l .add_to_class(DenseNet)
def __init__ (self , num_channels =64, growth_rate =32, arch =(4,4,4,4),
lr=0.1, num_classes =10):
super (DenseNet, self ).__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential( self .b1())
for i, num_convs inenumerate (arch):
self .net.add_module( f'dense_blk {i+1}', DenseBlock(num_convs,
growth_rate))
# The number of output channels in the previous dense block
num_channels +=num_convs *growth_rate
# A transition layer that halves the number of channels is added
# between the dense blocks
ifi!=len(arch) -1:
num_channels //=2
self .net.add_module( f'tran_blk {i+1}', transition_block(
num_channels))
self .net.add_module( 'last ', nn .Sequential(
nn.LazyBatchNorm2d(), nn .ReLU(),
nn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),
nn.LazyLinear(num_classes)))
self .net.apply(d2l .init_cnn)
8.7.5Training
Since we are using a deeper network here, in this section, we will reduce the input height
and width from 224 to 96 to simplify the computation.
316 Modern Convolutional Neural Networks
134model =DenseNet(lr =0.01 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(96,96))
trainer .fit(model, data)
8.7.6Summaryand Discussion
The main components that comprise DenseNet are dense blocks and transition layers. For
the latter, we need to keep the dimensionality under control when composing the net-
work by adding transition layers that shrink the number of channels again. In terms of
cross-layer connections, in contrast to ResNet, where inputs and outputs are added to-
gether, DenseNet concatenates inputs and outputs on the channel dimension. Although
these concatenation operations reuse features to achieve computational efficiency, unfortu-
nately they lead to heavy GPU memory consumption. As a result, applying DenseNet may
require more memory-efficient implementations that may increase training time ( Pleisset
al., 2017).
8.7.7Exercises
1.Why do we use average pooling rather than max-pooling in the transition layer?
2.OneoftheadvantagesmentionedintheDenseNetpaperisthatitsmodelparametersare
smaller than those of ResNet. Why is this the case?
3.One problem for which DenseNet has been criticized is its high memory consumption.
1.Is this really the case? Try to change the input shape to 224224to compare the
actual GPU memory consumption empirically.
2.Can you think of an alternative means of reducing the memory consumption? How
would you need to change the framework?
4.Implement the various DenseNet versions presented in Table 1 of the DenseNet paper
(Huangetal., 2017).
5.Design an MLP-based model by applying the DenseNet idea. Apply it to the housing
price prediction task in Section 5.7 .
Discussions134.
317 Designing Convolution Network Architectures
8.8DesigningConvolutionNetworkArchitectures
The previous sections have taken us on a tour of modern network design for computer
vision. Common to all the work we covered was that it greatly relied on the intuition of
scientists. Many of the architectures are heavily informed by human creativity and to a
much lesser extent by systematic exploration of the design space that deep networks offer.
Nonetheless, this networkengineering approachhasbeentremendouslysuccessful.
Ever since AlexNet ( Section 8.1 ) beat conventional computer vision models on ImageNet,
it has become popular to construct very deep networks by stacking blocks of convolutions,
all designed according to the same pattern. In particular, 33convolutions were popular-
ized by VGG networks ( Section 8.2 ). NiN ( Section 8.3 ) showed that even 11convolu-
tionscouldbebeneficialbyaddinglocalnonlinearities. Moreover,NiNsolvedtheproblem
of aggregating information at the head of a network by aggregating across all locations.
GoogLeNet ( Section 8.4 ) added multiple branches of different convolution width, combin-
ing the advantages of VGG and NiN in its Inception block. ResNets ( Section 8.6 ) changed
the inductive bias towards the identity mapping (from ùëì¬πùë•¬∫=0). This allowed for very
deep networks. Almost a decade later, the ResNet design is still popular, a testament to
its design. Lastly, ResNeXt ( Section 8.6.5 ) added grouped convolutions, offering a better
trade-off between parameters and computation. A precursor to Transformers for vision,
the Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer be-
tween locations ( Huet al., 2018). This was accomplished by computing a per-channel
global attention function.
Up to now we have omitted networks obtained via neural architecture search (NAS) (Liu
et al., 2018,Zoph and Le, 2016 ). We chose to do so since their cost is usually enormous,
relying on brute-force search, genetic algorithms, reinforcement learning, or some other
form of hyperparameter optimization. Given a fixed search space, NAS uses a search strat-
egy to automatically select an architecture based on the returned performance estimation.
The outcome of NAS is a single network instance. EfficientNets are a notable outcome of
this search ( Tan and Le, 2019 ).
In the following we discuss an idea that is quite different to the quest for the single best
network. It is computationally relatively inexpensive, it leads to scientific insights on the
way, and it is quite effective in terms of the quality of outcomes. Let‚Äôs review the strategy
by Radosavovic et al.(2020) todesign network design spaces . The strategy combines the
strength of manual design and NAS. It accomplishes this by operating on distributions of
networks and optimizing the distributions in a way to obtain good performance for entire
families of networks. The outcome of it are RegNets, specifically RegNetX and RegNetY,
plus a range of guiding principles for the design of performant CNNs.
import torch
from torch import nn
(continues on next page)
318 Modern Convolutional Neural Networks
(continued from previous page)
from torch .nnimport functional asF
from d2l import torch asd2l
8.8.1TheAnyNetDesign Space
ThedescriptionbelowcloselyfollowsthereasoninginRadosavovic etal.(2020)withsome
abbreviations to make it fit in the scope of the book. To begin, we need a template for the
family of networks to explore. One of the commonalities of the designs in this chapter is
that the networks consist of a stem, abodyand ahead. The stem performs initial image
processing, often through convolutions with a larger window size. The body consists of
multipleblocks,carryingoutthebulkofthetransformationsneededtogofromrawimages
to object representations. Lastly, the head converts this into the desired outputs, such as
viaasoftmaxregressorformulticlassclassification. Thebody, inturn, consistsofmultiple
stages, operating on the image at decreasing resolutions. In fact, both the stem and each
subsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more
blocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the
design of generic AnyNet networks, Radosavovic etal.(2020) used the ResNeXt block of
Fig. 8.6.5 .
tFig. 8.8.1 The AnyNet design space. The numbers ¬πc,r¬∫along each arrow indicate the number of
channels c and the resolution rrof the images at that point. From left to right: generic
network structure composed of stem, body, and head; body composed of four stages;
detailed structure of a stage; two alternative structures for blocks, one without
downsampling and one that halves the resolution in each dimension. Design choices
include depth di, the number of output channels ci, the number of groups gi, and
bottleneck ratio kifor any stage i.
Let‚Äôsreviewthestructureoutlinedin Fig.8.8.1 indetail. Asmentioned,anAnyNetconsists
of a stem, body, and head. The stem takes as its input RGB images (3 channels), using a
33convolution with a stride of 2, followed by a batch norm, to halve the resolution from
ùëüùëütoùëü¬ù2ùëü¬ù2. Moreover,itgenerates ùëê0channelsthatserveasinputtothebody.
319 Designing Convolution Network Architectures
Since the network is designed to work well with ImageNet images of shape 2242243,
the body serves to reduce this to 77ùëê4through 4 stages (recall that 224¬ù21¬∏4=7),
each with an eventual stride of 2. Lastly, the head employs an entirely standard design via
global average pooling, similar to NiN ( Section 8.3 ), followed by a fully connected layer to
emit anùëõ-dimensional vector for ùëõ-class classification.
Mostoftherelevantdesigndecisionsareinherenttothebodyofthenetwork. Itproceedsin
stages, where each stage is composed of the same type of ResNeXt blocks as we discussed
inSection 8.6.5 . The design there is again entirely generic: we begin with a block that
halves the resolution by using a stride of 2(the rightmost in Fig. 8.8.1 ). To match this, the
residualbranchoftheResNeXtblockneedstopassthrougha 11convolution. Thisblock
is followed by a variable number of additional ResNeXt blocks that leave both resolution
and the number of channels unchanged. Note that a common design practice is to add
a slight bottleneck in the design of convolutional blocks. As such, with bottleneck ratio
ùëòùëñ1we afford some number of channels, ùëêùëñ¬ùùëòùëñ, within each block for stage ùëñ(as the
experiments show, this is not really effective and should be skipped). Lastly, since we are
dealing with ResNeXt blocks, we also need to pick the number of groups ùëîùëñfor grouped
convolutions at stage ùëñ.
This seemingly generic design space provides us nonetheless with many parameters: we
can set the block width (number of channels) ùëê0,...ùëê 4, the depth (number of blocks) per
stageùëë1,...ùëë 4, the bottleneck ratios ùëò1,...ùëò 4, and the group widths (numbers of groups)
ùëî1,...ùëî 4. Intotalthisaddsupto17parameters, resultinginanunreasonablylargenumber
of configurations that would warrant exploring. We need some tools to reduce this huge
design space effectively. This is where the conceptual beauty of design spaces comes in.
Before we do so, let‚Äôs implement the generic design first.
class AnyNet (d2l .Classifier):
def stem (self , num_channels):
return nn.Sequential(
nn.LazyConv2d(num_channels, kernel_size =3, stride =2, padding =1),
nn.LazyBatchNorm2d(), nn .ReLU())
Each stage consists of depthResNeXt blocks, where num_channels specifies the block
width. Note that the first block halves the height and width of input images.
@d2l .add_to_class(AnyNet)
def stage (self , depth, num_channels, groups, bot_mul):
blk =[]
for iinrange (depth):
ifi==0:
blk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul,
use_1x1conv =True , strides =2))
else :
blk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul))
return nn.Sequential( *blk)
Putting the network stem, body, and head together, we complete the implementation of
AnyNet.
320 Modern Convolutional Neural Networks
@d2l .add_to_class(AnyNet)
def __init__ (self , arch, stem_channels, lr =0.1, num_classes =10):
super (AnyNet, self ).__init__ ()
self .save_hyperparameters()
self .net =nn.Sequential( self .stem(stem_channels))
for i, s inenumerate (arch):
self .net.add_module( f'stage {i+1}',self .stage( *s))
self .net.add_module( 'head ', nn .Sequential(
nn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),
nn.LazyLinear(num_classes)))
self .net.apply(d2l .init_cnn)
8.8.2Distributions and Parametersof Design Spaces
As just discussed in Section 8.8.1 , parameters of a design space are hyperparameters of
networks in that design space. Consider the problem of identifying good parameters in the
AnyNet design space. We could try finding the single best parameter choice for a given
amount of computation (e.g., FLOPs and compute time). If we allowed for even only two
possible choices for each parameter, we would have to explore 217=131072combinations
to find the best solution. This is clearly infeasible because of its exorbitant cost. Even
worse,wedonotreallylearnanythingfromthisexerciseintermsofhowoneshoulddesign
anetwork. Nexttimeweadd,say,anX-stage,orashiftoperation,orsimilar,wewouldneed
to start from scratch. Even worse, due to the stochasticity in training (rounding, shuffling,
bit errors), no two runs are likely to produce exactly the same results. A better strategy
would be to try to determine general guidelines of how the choices of parameters should
be related. For instance, the bottleneck ratio, the number of channels, blocks, groups, or
theirchangebetweenlayersshouldideallybegovernedbyacollectionofsimplerules. The
approach in Radosavovic etal.(2019) relies on the following four assumptions:
1.We assume that general design principles actually exist, so that many networks satis-
fying these requirements should offer good performance. Consequently, identifying a
distribution over networks can be a sensible strategy. In other words, we assume that
there are many good needles in the haystack.
2.We need not train networks to convergence before we can assess whether a network is
good. Instead, it is sufficient to use the intermediate results as reliable guidance for
final accuracy. Using (approximate) proxies to optimize an objective is referred to as
multi-fidelityoptimization( Forrester etal.,2007). Consequently,designoptimizationis
carried out, based on the accuracy achieved after only a few passes through the dataset,
reducing the cost significantly.
3.Resultsobtainedatasmallerscale(forsmallernetworks)generalizetolargerones. Con-
sequently, optimization is carried out for networks that are structurally similar, but with
a smaller number of blocks, fewer channels, etc. Only in the end will we need to verify
that the so-found networks also offer good performance at scale.
4.Aspects of the design can be approximately factorized so that it is possible to infer
321 Designing Convolution Network Architectures
their effect on the quality of the outcome somewhat independently. In other words, the
optimization problem is moderately easy.
These assumptions allow us to test many networks cheaply. In particular, we can sample
uniformly from the space of configurations and evaluate their performance. Subsequently,
we can evaluate the quality of the choice of parameters by reviewing the distribution of
error/accuracy that can be achieved with said networks. Denote by ùêπ¬πùëí¬∫the cumulative
distribution function (CDF) for errors committed by networks of a given design space,
drawn using probability disribution ùëù. That is,
ùêπ¬πùëí,ùëù¬∫def=ùëÉnetùëùfùëí¬πnet¬∫ùëíg. (8.8.1)
Our goal is now to find a distribution ùëùovernetworks such that most networks have a very
low error rate and where the support of ùëùis concise. Of course, this is computationally
infeasible to perform accurately. We resort to a sample of networks Zdef=fnet1,...netùëõg
(witherrorsùëí1,...,ùëíùëõ,respectively)from ùëùandusetheempiricalCDF ÀÜùêπ¬πùëí,Z¬∫instead:
ÀÜùêπ¬πùëí,Z¬∫=1
ùëõùëõ√ï
ùëñ=11¬πùëíùëñùëí¬∫. (8.8.2)
Whenever the CDF for one set of choices majorizes (or matches) another CDF it follows
that its choice of parameters is superior (or indifferent). Accordingly Radosavovic et al.
(2020) experimented with a shared network bottleneck ratio ùëòùëñ=ùëòfor all stages ùëñof the
network. This gets rid of three of the four parameters governing the bottleneck ratio. To
assess whether this (negatively) affects the performance one can draw networks from the
constrained and from the unconstrained distribution and compare the corresonding CDFs.
It turns out that this constraint does not affect the accuracy of the distribution of networks
at all, as can be seen in the first panel of Fig. 8.8.2 . Likewise, we could choose to pick
the same group width ùëîùëñ=ùëîoccurring at the various stages of the network. Again, this
does not affect performance, as can be seen in the second panel of Fig. 8.8.2 . Both steps
combined reduce the number of free parameters by six.
tFig. 8.8.2 Comparing error empirical distribution functions of design spaces. AnyNetAis the
original design space; AnyNetBties the bottleneck ratios, AnyNetCalso ties group
widths,AnyNetDincreases the network depth across stages. From left to right: (i) tying
bottleneck ratios has no effect on performance; (ii) tying group widths has no effect on
performance; (iii) increasing network widths (channels) across stages improves
performance; (iv) increasing network depths across stages improves performance. Figure
courtesy of Radosavovic et al. ( 2020 ).
Nextwelookforwaystoreducethemultitudeofpotentialchoicesforwidthanddepthofthe
322 Modern Convolutional Neural Networks
stages. It is a reasonable assumption that, as we go deeper, the number of channels should
increase, i.e., ùëêùëñùëêùëñ 1(ùë§ùëñ¬∏1ùë§ùëñper their notation in Fig. 8.8.2 ), yielding AnyNetXùê∑.
Likewise,itisequallyreasonabletoassumethatasthestagesprogress,theyshouldbecome
deeper,i.e.,ùëëùëñùëëùëñ 1,yieldingAnyNetXùê∏. Thiscanbeexperimentallyverifiedinthethird
and fourth panel of Fig. 8.8.2 , respectively.
8.8.3RegNet
TheresultingAnyNetXùê∏designspaceconsistsofsimplenetworksfollowingeasy-to-interpret
design principles:
Share the bottleneck ratio ùëòùëñ=ùëòfor all stages ùëñ;
Share the group width ùëîùëñ=ùëîfor all stages ùëñ;
Increase network width across stages: ùëêùëñùëêùëñ¬∏1;
Increase network depth across stages: ùëëùëñùëëùëñ¬∏1.
This leaves us with a final set of choices: how to pick the specific values for the above
parameters of the eventual AnyNetXùê∏design space. By studying the best-performing
networks from the distribution in AnyNetXùê∏one can observe the following: the width
of the network ideally increases linearly with the block index across the network, i.e.,
ùëêùëóùëê0¬∏ùëêùëéùëó, whereùëóis the block index and slope ùëêùëé>0. Given that we get to choose a
different block width only per stage, wearrive at a piecewise constantfunction, engineered
to match this dependence. Furthermore, experiments also show that a bottleneck ratio of
ùëò=1performs best, i.e., we are advised not to use bottlenecks at all.
We recommend the interested reader reviews further details in the design of specific net-
works for different amounts of computation by perusing Radosavovic et al.(2020). For
instance, an effective 32-layer RegNetX variant is given by ùëò=1(no bottleneck), ùëî=16
(group width is 16), ùëê1=32andùëê2=80channels for the first and second stage, respec-
tively, chosen to be ùëë1=4andùëë2=6blocks deep. The astonishing insight from the
design is that it still applies, even when investigating networks at a larger scale. Even bet-
ter, it even holds for Squeeze-and-Excitation (SE) network designs (RegNetY) that have a
global channel activation ( Huetal., 2018).
class RegNetX32 (AnyNet):
def __init__ (self , lr =0.1, num_classes =10):
stem_channels, groups, bot_mul =32,16,1
depths, channels =(4,6), ( 32,80)
super ().__init__ (
((depths[ 0], channels[ 0], groups, bot_mul),
(depths[ 1], channels[ 1], groups, bot_mul)),
stem_channels, lr, num_classes)
We can see that each RegNetX stage progressively reduces resolution and increases output
channels.
323 Designing Convolution Network Architectures
RegNetX32() .layer_summary(( 1,1,96,96))
Sequential output shape: torch .Size([ 1,32,48,48])
Sequential output shape: torch .Size([ 1,32,24,24])
Sequential output shape: torch .Size([ 1,80,12,12])
Sequential output shape: torch .Size([ 1,10])
8.8.4Training
Training the 32-layer RegNetX on the Fashion-MNIST dataset is just like before.
model =RegNetX32(lr =0.05 )
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(96,96))
trainer .fit(model, data)
8.8.5Discussion
With desirable inductive biases (assumptions or preferences) like locality and translation
invariance( Section7.1 )forvision,CNNshavebeenthedominantarchitecturesinthisarea.
This remained the case from LeNet up until Transformers ( Section 11.7 ) (Dosovitskiy et
al.,2021,Touvronetal.,2021)startedsurpassingCNNsintermsofaccuracy. Whilemuch
of the recent progress in terms of vision Transformers canbe backported into CNNs ( Liu
et al., 2022), it is only possible at a higher computational cost. Just as importantly, recent
hardwareoptimizations(NVIDIAAmpereandHopper)haveonlywidenedthegapinfavor
of Transformers.
It is worth noting that Transformers have a significantly lower degree of inductive bias to-
wards locality and translation invariance than CNNs. That learned structures prevailed is
due, not least, to the availability of large image collections, such as LAION-400m and
LAION-5B ( Schuhmann et al., 2022) with up to 5 billion images. Quite surprisingly,
some of the more relevant work in this context even includes MLPs ( Tolstikhin et al.,
2021).
In sum, vision Transformers ( Section 11.8 ) by now lead in terms of state-of-the-art perfor-
mance in large-scale image classification, showing that scalabilitytrumpsinductivebiases
324 Modern Convolutional Neural Networks
135(Dosovitskiy et al., 2021). This includes pretraining large-scale Transformers ( Section
11.9) with multi-head self-attention ( Section 11.5 ). We invite the readers to dive into these
chapters for a much more detailed discussion.
8.8.6Exercises
1.Increase the number of stages to four. Can you design a deeper RegNetX that performs
better?
2.De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How
does your new model perform?
3.Implementmultipleinstancesofa‚ÄúVioNet‚Äùfamilyby violating thedesignprinciplesof
RegNetX. How do they perform? Which of ( ùëëùëñ,ùëêùëñ,ùëîùëñ,ùëèùëñ) is the most important factor?
4.Your goal is to design the ‚Äúperfect‚Äù MLP. Can you use the design principles introduced
above to find good architectures? Is it possible to extrapolate from small to large net-
works?
Discussions135.
9 Recurrent Neural Networks
Upuntilnow,wehavefocusedprimarilyonfixed-lengthdata. Whenintroducinglinearand
logistic regression in Chapter 3 andChapter 4 and multilayer perceptrons in Chapter 5 , we
werehappytoassumethateachfeaturevector xùëñconsistedofafixednumberofcomponents
ùë•1,...,ùë•ùëë, where each numerical feature ùë•ùëócorresponded to a particular attribute. These
datasets are sometimes called tabular, because they can be arranged in tables, where each
exampleùëñgets its own row, and each attribute gets its own column. Crucially, with tabular
data, we seldom assume any particular structure over the columns.
Subsequently, in Chapter 7 , we moved on to image data, where inputs consist of the raw
pixelvaluesateachcoordinateinanimage. Imagedatahardlyfittedthebillofaprotypical
tabular dataset. There, we needed to call upon convolutional neural networks (CNNs) to
handle the hierarchical structure and invariances. However, our data were still of fixed
length. Every Fashion-MNIST image is represented as a 2828grid of pixel values.
Moreover,ourgoalwastodevelopamodelthatlookedatjustoneimageandthenoutputted
a single prediction. But what should we do when faced with a sequence of images, as in a
video, or when tasked with producing a sequentiallystructured prediction, as in the case of
image captioning?
Agreatmanylearningtasksrequiredealingwithsequentialdata. Imagecaptioning,speech
synthesis, and music generation all require that models produce outputs consisting of se-
quences. In other domains, such as time series prediction, video analysis, and musical
information retrieval, a model must learn from inputs that are sequences. These demands
often arise simultaneously: tasks such as translating passages of text from one natural lan-
guage to another, engaging in dialogue, or controlling a robot, demand that models both
ingest and output sequentially structured data.
Recurrent neural networks (RNNs) are deep learning models that capture the dynamics of
sequences via recurrent connections, which can be thought of as cycles in the network of
nodes. This might seem counterintuitive at first. After all, it is the feedforward nature of
neural networks that makes the order of computation unambiguous. However, recurrent
edges are defined in a precise way that ensures that no such ambiguity can arise. Recurrent
neural networks are unrolled across time steps (or sequence steps), with the sameunder-
lying parameters applied at each step. While the standard connections are applied syn-
chronously to propagate each layer‚Äôs activations to the subsequent layer at the same time
step,therecurrentconnectionsare dynamic ,passinginformationacrossadjacenttimesteps.
As the unfolded view in Fig. 9.1 reveals, RNNs can be thought of as feedforward neural
325
326 Recurrent Neural Networks
networkswhereeachlayer‚Äôsparameters(bothconventionalandrecurrent)aresharedacross
time steps.
tFig. 9.1 On the left recurrent connections are depicted via cyclic edges. On the right, we unfold
the RNN over time steps. Here, recurrent edges span adjacent time steps, while
conventional connections are computed synchronously.
Like neural networks more broadly, RNNs have a long discipline-spanning history, origi-
nating as models of the brain popularized by cognitive scientists and subsequently adopted
as practical modeling tools employed by the machine learning community. As we do for
deeplearningmorebroadly,inthisbookweadoptthemachinelearningperspective,focus-
ing on RNNs as practical tools that rose to popularity in the 2010s owing to breakthrough
results on such diverse tasks as handwriting recognition ( Graveset al., 2008), machine
translation( Sutskever etal.,2014),andrecognizingmedicaldiagnoses( Liptonetal.,2016).
Wepointthereaderinterestedinmorebackgroundmaterialtoapubliclyavailablecompre-
hensivereview( Liptonetal.,2015). WealsonotethatsequentialityisnotuniquetoRNNs.
Forexample,theCNNsthatwealreadyintroducedcanbeadaptedtohandledataofvarying
length, e.g., images of varying resolution. Moreover, RNNs have recently ceded consider-
able market share to Transformer models, which will be covered in Chapter 11 . However,
RNNs rose to prominence as the default models for handling complex sequential structure
in deep learning, and remain staple models for sequential modeling to this day. The stories
of RNNs and of sequence modeling are inextricably linked, and this is as much a chapter
about the ABCs of sequence modeling problems as it is a chapter about RNNs.
One key insight paved the way for a revolution in sequence modeling. While the inputs
and targets for many fundamental tasks in machine learning cannot easily be represented
as fixed-length vectors, they can often nevertheless be represented as varying-length se-
quences of fixed-length vectors. For example, documents can be represented as sequences
of words; medical records can often be represented as sequences of events (encounters,
medications,procedures,labtests,diagnoses); videoscanberepresentedasvarying-length
sequences of still images.
Whilesequencemodelshavepoppedupinnumerousapplicationareas,basicresearchinthe
areahasbeendrivenpredominantlybyadvancesoncoretasksinnaturallanguageprocess-
ing. Thus, throughout this chapter, we will focus our exposition and examples on text data.
If you get the hang of these examples, then applying the models to other data modalities
should be relatively straightforward. In the next few sections, we introduce basic notation
forsequencesandsomeevaluationmeasuresforassessingthequalityofsequentiallystruc-
turedmodeloutputs. Afterthat,wediscussbasicconceptsofalanguagemodelandusethis
327 Working with Sequences
discussion to motivate our first RNN models. Finally, we describe the method for calculat-
ing gradients when backpropagating through RNNs and explore some challenges that are
oftenencounteredwhentrainingsuchnetworks, motivatingthemodernRNNarchitectures
that will follow in Chapter 10 .
9.1Workingwith Sequences
Up until now, we have focused on models whose inputs consisted of a single feature vector
x2Rùëë. The main change of perspective when developing models capable of processing
sequences is that we now focus on inputs that consist of an ordered list of feature vec-
torsx1,...,xùëá, where each feature vector xùë°is indexed by a time step ùë°2Z¬∏lying in
Rùëë.
Some datasets consist of a single massive sequence. Consider, for example, the extremely
long streams of sensor readings that might be available to climate scientists. In such cases,
we might create training datasets by randomly sampling subsequences of some predeter-
mined length. More often, our data arrives as a collection of sequences. Consider the
followingexamples: (i)acollectionofdocuments, eachrepresentedasitsownsequenceof
words,andeachhavingitsownlength ùëáùëñ;(ii)sequencerepresentationofpatientstaysinthe
hospital, where each stay consists of a number of events and the sequence length depends
roughly on the length of the stay.
Previously, when dealing with individual inputs, we assumed that they were sampled inde-
pendently from the same underlying distribution ùëÉ¬πùëã¬∫. While we still assume that entire
sequences (e.g., entire documents or patient trajectories) are sampled independently, we
cannot assume that the data arriving at each time step are independent of each other. For
example, the words that likely to appear later in a document depend heavily on words oc-
curring earlier in the document. The medicine a patient is likely to receive on the 10th day
of a hospital visit depends heavily on what transpired in the previous nine days.
This should come as no surprise. If we did not believe that the elements in a sequence
were related, we would not have bothered to model them as a sequence in the first place.
Considertheusefulnessoftheauto-fillfeaturesthatarepopularonsearchtoolsandmodern
email clients. They are useful precisely because it is often possible to predict (imperfectly,
but better than random guessing) what the likely continuations of a sequence might be,
given some initial prefix. For most sequence models, we do not require independence, or
even stationarity, of our sequences. Instead, we require only that the sequences themselves
are sampled from some fixed underlying distribution over entire sequences.
This flexible approach allows for such phenomena as (i) documents looking significantly
differentatthebeginningthanattheend;or(ii)patientstatusevolvingeithertowardsrecov-
eryortowardsdeathoverthecourseofahospitalstay;or(iii)customertasteevolvinginpre-
dictable ways over the course of continued interaction with a recommender system.
328 Recurrent Neural Networks
Wesometimeswishtopredictafixedtarget ùë¶givensequentiallystructuredinput(e.g.,sen-
timentclassificationbasedonamoviereview). Atothertimes,wewishtopredictasequen-
tially structured target ( ùë¶1,...,ùë¶ùëá) given a fixed input (e.g., image captioning). Still other
times, our goal is to predict sequentially structured targets based on sequentially structured
inputs (e.g., machine translation or video captioning). Such sequence-to-sequence tasks
take two forms: (i) aligned: where the input at each time step aligns with a correspond-
ing target (e.g., part of speech tagging); (ii) unaligned : where the input and target do not
necessarily exhibit a step-for-step correspondence (e.g., machine translation).
Beforeweworryabouthandlingtargetsofanykind,wecantacklethemoststraightforward
problem: unsupervised density modeling (also called sequence modeling ). Here, given a
collection of sequences, our goal is to estimate the probability mass function that tells us
how likely we are to see any given sequence, i.e., ùëù¬πx1,...,xùëá¬∫.
%matplotlib inline
import torch
from torch import nn
from d2l import torch asd2l
9.1.1AutoregressiveModels
Before introducing specialized neural networks designed to handle sequentially structured
data, let‚Äôs take a look at some actual sequence data and build up some basic intuitions
and statistical tools. In particular, we will focus on stock price data from the FTSE 100
index (Fig. 9.1.1 ). At each time stepùë°2Z¬∏, we observe the price, ùë•ùë°, of the index at that
time.
tFig. 9.1.1 FTSE 100 index over about 30 years.
Now suppose that a trader would like to make short-term trades, strategically getting into
or out of the index, depending on whether they believe that it will rise or decline in the
subsequent time step. Absent any other features (news, financial reporting data, etc.), the
329 Working with Sequences
only available signal for predicting the subsequent value is the history of prices to date.
The trader is thus interested in knowing the probability distribution
ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫ (9.1.1)
overpricesthattheindexmighttakeinthesubsequenttimestep. Whileestimatingtheentire
distribution over a continuously valued random variable can be difficult, the trader would
be happy to focus on a few key statistics of the distribution, particularly the expected value
and the variance. One simple strategy for estimating the conditional expectation
E¬ª¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫¬º, (9.1.2)
would be to apply a linear regression model (recall Section 3.1 ). Such models that regress
the value of a signal on the previous values of that same signal are naturally called au-
toregressive models . There is just one major problem: the number of inputs, ùë•ùë° 1,...,ùë• 1
varies, depending on ùë°. In other words, the number of inputs increases with the amount of
data that we encounter. Thus if we want to treat our historical data as a training set, we
are left with the problem that each example has a different number of features. Much of
whatfollowsinthischapterwillrevolvearoundtechniquesforovercomingthesechallenges
when engaging in such autoregressive modeling problems where the object of interest is
ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫or some statistic(s) of this distribution.
Afewstrategiesrecurfrequently. Firstofall,wemightbelievethatalthoughlongsequences
ùë•ùë° 1,...,ùë• 1are available, it may not be necessary to look back so far in the history when
predicting the near future. In this case we might content ourselves to condition on some
window of length ùúèand only use ùë•ùë° 1,...,ùë•ùë° ùúèobservations. The immediate benefit is
that now the number of arguments is always the same, at least for ùë° > ùúè. This allows us to
trainanylinearmodelordeepnetworkthatrequiresfixed-lengthvectorsasinputs. Second,
we might develop models that maintain some summary ‚Ñéùë°of the past observations (see
Fig. 9.1.2 ) and at the same time update ‚Ñéùë°in addition to the prediction ÀÜùë•ùë°. This leads to
models that estimate not only ùë•ùë°with ÀÜùë•ùë°=ùëÉ¬πùë•ùë°j‚Ñéùë°¬∫but also updates of the form ‚Ñéùë°=
ùëî¬π‚Ñéùë° 1,ùë•ùë° 1¬∫. Since‚Ñéùë°isneverobserved,thesemodelsarealsocalled latentautoregressive
models.
tFig. 9.1.2 A latent autoregressive model.
To construct training data from historical data, one typically creates examples by sampling
windows randomly. In general, we do not expect time to stand still. However, we often
assume that while the specific values of ùë•ùë°might change, the dynamics according to which
each subsequent observation is generated given the previous observations do not. Statisti-
cians call dynamics that do not change stationary .
330 Recurrent Neural Networks
9.1.2SequenceModels
Sometimes,especiallywhenworkingwithlanguage,wewishtoestimatethejointprobabil-
ity of an entire sequence. This is a common task when working with sequences composed
of discrete tokens, such as words. Generally, these estimated functions are called sequence
modelsand for natural language data, they are called language models . The field of se-
quence modeling has been driven so much by natural language processing, that we often
describe sequence models as ‚Äúlanguage models‚Äù, even when dealing with non-language
data. Language models prove useful for all sorts of reasons. Sometimes we want to evalu-
ate the likelihood of sentences. For example, we might wish to compare the naturalness of
twocandidateoutputsgeneratedbyamachinetranslationsystemorbyaspeechrecognition
system. But language modeling gives us not only the capacity to evaluate likelihood, but
theabilityto samplesequences, andeventooptimizeforthemostlikelysequences.
While language modeling might not, at first glance, look like an autoregressive problem,
we can reduce language modeling to autoregressive prediction by decomposing the joint
density of a sequence ùëù¬πùë•1,...,ùë•ùëá¬∫into the product of conditional densities in a left-to-
right fashion by applying the chain rule of probability:
ùëÉ¬πùë•1,...,ùë•ùëá¬∫=ùëÉ¬πùë•1¬∫ùëá√ñ
ùë°=2ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫. (9.1.3)
Note that if we are working with discrete signals such as words, then the autoregressive
model must be a probabilistic classifier, outputting a full probability distribution over the
vocabulary for whatever word will come next, given the leftwards context.
MarkovModels
Now suppose that we wish to employ the strategy mentioned above, where we condition
onlyontheùúèprevioustimesteps,i.e., ùë•ùë° 1,...,ùë•ùë° ùúè,ratherthantheentiresequencehistory
ùë•ùë° 1,...,ùë• 1. Wheneverwecanthrowawaythehistorybeyondtheprevious ùúèstepswithout
anylossinpredictivepower,wesaythatthesequencesatisfiesa Markovcondition ,i.e.,that
the future is conditionally independent of the past, given the recent history . Whenùúè=1,
we say that the data is characterized by a first-order Markov model , and whenùúè=ùëò, we
say that the data is characterized by a ùëòth-order Markov model. For when the first-order
Markovconditionholds( ùúè=1)thefactorizationofourjointprobabilitybecomesaproduct
of probabilities of each word given the previous word:
ùëÉ¬πùë•1,...,ùë•ùëá¬∫=ùëÉ¬πùë•1¬∫ùëá√ñ
ùë°=2ùëÉ¬πùë•ùë°jùë•ùë° 1¬∫. (9.1.4)
WeoftenfinditusefultoworkwithmodelsthatproceedasthoughaMarkovconditionwere
satisfied,evenwhenweknowthatthisisonly approximately true. Withrealtextdocuments
we continue to gain information as we include more and more leftwards context. But these
gains diminish rapidly. Thus, sometimes we compromise, obviating computational and
statistical difficulties by training models whose validity depends on a ùëòth-order Markov
condition. Even today‚Äôs massive RNN- and Transformer-based language models seldom
incorporate more than thousands of words of context.
331 Working with Sequences
With discrete data, a true Markov model simplycounts the number of times that each word
has occurred in each context, producing the relative frequency estimate of ùëÉ¬πùë•ùë°jùë•ùë° 1¬∫.
Whenever the data assumes only discrete values (as in language), the most likely sequence
of words can be computed efficiently using dynamic programming.
The Orderof Decoding
Youmaybewonderingwhywerepresentedthefactorizationofatextsequence ùëÉ¬πùë•1,...,ùë•ùëá¬∫
as a left-to-right chain of conditional probabilities. Why not right-to-left or some other,
seeminglyrandomorder? Inprinciple,thereisnothingwrongwithunfolding ùëÉ¬πùë•1,...,ùë•ùëá¬∫
in reverse order. The result is a valid factorization:
ùëÉ¬πùë•1,...,ùë•ùëá¬∫=ùëÉ¬πùë•ùëá¬∫1√ñ
ùë°=ùëá 1ùëÉ¬πùë•ùë°jùë•ùë°¬∏1,...,ùë•ùëá¬∫. (9.1.5)
However, there are many reasons why factorizing text in the same direction in which we
readit(left-to-rightformostlanguages,butright-to-leftforArabicandHebrew)ispreferred
for the task of language modeling. First, this is just a more natural direction for us to think
about. After all we all read text every day, and this process is guided by our ability to
anticipate which words and phrases are likely to come next. Just think of how many times
youhavecompletedsomeoneelse‚Äôssentence. Thus,evenifwehadnootherreasontoprefer
such in-order decodings, they would be useful if only because we have better intuitions for
what should be likely when predicting in this order.
Second, by factorizing in order, we can assign probabilities to arbitrarily long sequences
usingthesamelanguagemodel. Toconvertaprobabilityoversteps 1throughùë°intoonethat
extendstoword ùë°¬∏1wesimplymultiplybytheconditionalprobabilityoftheadditionalto-
kengiventhepreviousones: ùëÉ¬πùë•ùë°¬∏1,...,ùë• 1¬∫=ùëÉ¬πùë•ùë°,...,ùë• 1¬∫ùëÉ¬πùë•ùë°¬∏1jùë•ùë°,...,ùë• 1¬∫.
Third, we have stronger predictive models for predicting adjacent words than words at ar-
bitrary other locations. While all orders of factorization are valid, they do not necessarily
allrepresentequallyeasypredictivemodelingproblems. Thisistruenotonlyforlanguage,
but for other kinds of data as well, e.g., when the data is causally structured. For example,
we believe that future events cannot influence the past. Hence, if we change ùë•ùë°, we may be
able to influence what happens for ùë•ùë°¬∏1going forward but not the converse. That is, if we
changeùë•ùë°, the distribution over past events will not change. In some contexts, this makes
it easier to predict ùëÉ¬πùë•ùë°¬∏1jùë•ùë°¬∫than to predict ùëÉ¬πùë•ùë°jùë•ùë°¬∏1¬∫. For instance, in some cases,
we can findùë•ùë°¬∏1=ùëì¬πùë•ùë°¬∫¬∏ùúñfor some additive noise ùúñ, whereas the converse is not true
(Hoyeret al., 2009). This is great news, since it is typically the forward direction that we
are interested in estimating. The book by Peters et al.(2017) contains more on this topic.
We barely scratch the surface of it.
9.1.3Training
Before we focus our attention on text data, let‚Äôs first try this out with some continuous-
valued synthetic data.
Here, our 1000 synthetic data will follow the trigonometric sinfunction, applied to 0.01
332 Recurrent Neural Networks
times the time step. To make the problem a little more interesting, we corrupt each sample
with additive noise. From this sequence we extract training examples, each consisting of
features and a label.
class Data (d2l .DataModule):
def __init__ (self , batch_size =16, T=1000 , num_train =600, tau =4):
self .save_hyperparameters()
self .time =torch .arange( 1, T +1, dtype =torch .float32)
self .x=torch .sin( 0.01 *self .time) +torch .randn(T) *0.2
data =Data()
d2l.plot(data .time, data .x,'time ','x', xlim =[1,1000 ], figsize =(6,3))
To begin, we try a model that acts as if the data satisfied a ùúèth-order Markov condition,
and thus predicts ùë•ùë°using only the past ùúèobservations. Thus for each time step we have an
examplewithlabel ùë¶=ùë•ùë°andfeatures xùë°=¬ªùë•ùë° ùúè,...,ùë•ùë° 1¬º. Theastutereadermighthave
noticedthatthisresultsin 1000 ùúèexamples,sincewelacksufficienthistoryfor ùë¶1,...,ùë¶ùúè.
While we could pad the first ùúèsequences with zeros, to keep things simple, we drop them
for now. The resulting dataset contains ùëá ùúèexamples, where each input to the model has
sequence length ùúè. We create a data iterator on the first 600 examples, covering a period of
the sin function.
@d2l .add_to_class(Data)
def get_dataloader (self , train):
features =[self .x[i : self .T-self .tau+i]for iinrange (self .tau)]
self .features =torch .stack(features, 1)
self .labels =self .x[self .tau:] .reshape(( -1,1))
i=slice (0,self .num_train) iftrain else slice (self .num_train, None )
return self .get_tensorloader([ self .features, self .labels], train, i)
In this example our model will be a standard linear regression.
model =d2l.LinearRegression(lr =0.01 )
trainer =d2l.Trainer(max_epochs =5)
trainer .fit(model, data)
333 Working with Sequences
9.1.4Prediction
Toevaluateourmodel,wefirstcheckhowwellitperformsatone-step-aheadprediction.
onestep_preds =model(data .features) .detach() .numpy()
d2l.plot(data .time[data .tau:], [data .labels, onestep_preds], 'time ','x',
legend =['labels ','1-step preds '], figsize =(6,3))
These predictions look good, even near the end at ùë°=1000.
But what if we only observed sequence data up until time step 604 ( n_train + tau ) and
wished to make predictions several stepsinto the future? Unfortunately, we cannot directly
compute the one-step-ahead prediction for time step 609, because we do not know the cor-
respondinginputs,havingseenonlyupto ùë•604. Wecanaddressthisproblembypluggingin
ourearlierpredictionsasinputstoourmodelformakingsubsequentpredictions,projecting
forward, one step at a time, until reaching the desired time step:
ÀÜùë•605=ùëì¬πùë•601,ùë•602,ùë•603,ùë•604¬∫,
ÀÜùë•606=ùëì¬πùë•602,ùë•603,ùë•604,ÀÜùë•605¬∫,
ÀÜùë•607=ùëì¬πùë•603,ùë•604,ÀÜùë•605,ÀÜùë•606¬∫,
ÀÜùë•608=ùëì¬πùë•604,ÀÜùë•605,ÀÜùë•606,ÀÜùë•607¬∫,
ÀÜùë•609=ùëì¬πÀÜùë•605,ÀÜùë•606,ÀÜùë•607,ÀÜùë•608¬∫,
...(9.1.6)
Generally, for an observed sequence ùë•1,...,ùë•ùë°, its predicted output ÀÜùë•ùë°¬∏ùëòat time stepùë°¬∏ùëò
334 Recurrent Neural Networks
is called the ùëò-step-ahead prediction . Since we have observed up to ùë•604, itsùëò-step-ahead
prediction is ÀÜùë•604¬∏ùëò. In other words, we will have to keep on using our own predictions to
make multistep-ahead predictions. Let‚Äôs see how well this goes.
multistep_preds =torch .zeros(data .T)
multistep_preds[:] =data .x
for iinrange (data .num_train +data .tau, data .T):
multistep_preds[i] =model(
multistep_preds[i -data .tau:i] .reshape(( 1,-1)))
multistep_preds =multistep_preds .detach() .numpy()
d2l.plot([data .time[data .tau:], data .time[data .num_train +data .tau:]],
[onestep_preds, multistep_preds[data .num_train +data .tau:]], 'time ',
'x', legend =['1-step preds ','multistep preds '], figsize =(6,3))
Unfortunately, in this case we fail spectacularly. The predictions decay to a constant pretty
quickly after a few steps. Why did the algorithm perform so much worse when predicting
further into the future? Ultimately, this is down to the fact that errors build up. Let‚Äôs say
that after step 1 we have some error ùúñ1=¬Øùúñ. Now the inputfor step 2 is perturbed by ùúñ1,
hence we suffer some error in the order of ùúñ2=¬Øùúñ¬∏ùëêùúñ1for some constant ùëê, and so on. The
predictions can diverge rapidly from the true observations. You may already be familiar
with this common phenomenon. For instance, weather forecasts for the next 24 hours tend
to be pretty accurate but beyond that, accuracy declines rapidly. We will discuss methods
for improving this throughout this chapter and beyond.
Let‚Äôs take a closer look at the difficulties in ùëò-step-ahead predictions by computing predic-
tions on the entire sequence for ùëò=1,4,16,64.
def k_step_pred (k):
features =[]
for iinrange (data .tau):
features .append(data .x[i : i +data .T-data .tau-k+1])
# The (i+tau)-th element stores the (i+1)-step-ahead predictions
for iinrange (k):
preds =model(torch .stack(features[i : i +data .tau], 1))
features .append(preds .reshape( -1))
return features[data .tau:]
335 Working with Sequences
steps =(1,4,16,64)
preds =k_step_pred(steps[ -1])
d2l.plot(data .time[data .tau+steps[ -1]-1:],
[preds[k -1].detach() .numpy() for kinsteps], 'time ','x',
legend =[f'{k}-step preds 'for kinsteps], figsize =(6,3))
Thisclearlyillustrateshowthequalityofthepredictionchangesaswetrytopredictfurther
into the future. While the 4-step-ahead predictions still look good, anything beyond that is
almost useless.
9.1.5Summary
There is quite a difference in difficulty between interpolation and extrapolation. Conse-
quently,ifyouhaveasequence,alwaysrespectthetemporalorderofthedatawhentraining,
i.e.,nevertrainonfuturedata. Giventhiskindofdata,sequencemodelsrequirespecialized
statistical tools for estimation. Two popular choices are autoregressive models and latent-
variable autoregressive models. For causal models (e.g., time going forward), estimating
the forward direction is typically a lot easier than the reverse direction. For an observed
sequenceuptotimestep ùë°, itspredictedoutputattimestep ùë°¬∏ùëòistheùëò-step-aheadpredic-
tion. As we predict further in time by increasing ùëò, the errors accumulate and the quality
of the prediction degrades, often dramatically.
9.1.6Exercises
1.Improve the model in the experiment of this section.
1.Incorporate more than the past four observations? How many do you really need?
2.How many past observations would you need if there was no noise? Hint: you can
write sinandcosas a differential equation.
3.Can you incorporate older observations while keeping the total number of features
constant? Does this improve accuracy? Why?
4.Changetheneuralnetworkarchitectureandevaluatetheperformance. Youmaytrain
the new model with more epochs. What do you observe?
336 Recurrent Neural Networks
136
1372.An investor wants to find a good security to buy. They look at past returns to decide
which one is likely to do well. What could possibly go wrong with this strategy?
3.Does causality also apply to text? To which extent?
4.Give an example for when a latent autoregressive model might be needed to capture the
dynamic of the data.
Discussions136.
9.2ConvertingRawTextinto SequenceData
Throughoutthisbook,wewilloftenworkwithtextdatarepresentedassequencesofwords,
characters, or word pieces. To get going, we will need some basic tools for converting raw
text into sequences of the appropriate form. Typical preprocessing pipelines execute the
following steps:
1.Load text as strings into memory.
2.Split the strings into tokens (e.g., words or characters).
3.Build a vocabulary dictionary to associate each vocabulary element with a numerical
index.
4.Convert the text into sequences of numerical indices.
import collections
import random
import re
import torch
from d2l import torch asd2l
9.2.1Readingthe Dataset
Here, we will work with H. G. Wells‚Äô The Time Machine137, a book containing just over
30,000 words. While real applications will typically involve significantly larger datasets,
this is sufficient to demonstrate the preprocessing pipeline. The following _download
method reads the raw text into a string.
class TimeMachine (d2l .DataModule): #@save
"""The Time Machine dataset."""
def _download (self ):
fname =d2l.download(d2l .DATA_URL +'timemachine.txt ',self .root,
'090b5e7e70c295757f55df93cb0a180b9691891a ')
with open (fname) asf:
return f.read()
(continues on next page)
337 Converting Raw Text into Sequence Data
(continued from previous page)
data =TimeMachine()
raw_text =data ._download()
raw_text[: 60]
'The Time Machine, by H. G. Wells [1898]nnnnnInnnThe Time Tra'
Forsimplicity,weignorepunctuationandcapitalizationwhenpreprocessingtherawtext.
@d2l .add_to_class(TimeMachine) #@save
def _preprocess (self , text):
return re.sub( '[^A-Za-z]+ ','', text) .lower()
text =data ._preprocess(raw_text)
text[: 60]
'the time machine by h g wells i the time traveller for so it '
9.2.2Tokenization
Tokensare the atomic (indivisible) units of text. Each time step corresponds to 1 token,
but what precisely constitutes a token is a design choice. For example, we could represent
the sentence ‚ÄúBaby needs a new pair of shoes‚Äù as a sequence of 7 words, where the set of
all words comprise a large vocabulary (typically tens or hundreds of thousands of words).
Or we would represent the same sentence as a much longer sequence of 30 characters,
usingamuchsmallervocabulary(thereareonly256distinctASCIIcharacters). Below,we
tokenize our preprocessed text into a sequence of characters.
@d2l .add_to_class(TimeMachine) #@save
def _tokenize (self , text):
return list (text)
tokens =data ._tokenize(text)
','.join(tokens[: 30])
't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '
9.2.3Vocabulary
These tokens are still strings. However, the inputs to our models must ultimately consist of
numericalinputs. Next,weintroduceaclassforconstructing vocabularies ,i.e.,objectsthat
associateeachdistincttokenvaluewithauniqueindex. First,wedeterminethesetofunique
tokensinourtraining corpus. Wethenassignanumericalindextoeachuniquetoken. Rare
vocabularyelementsareoftendroppedforconvenience. Wheneverweencounteratokenat
trainingor testtime that had notbeen previouslyseen or wasdropped fromthe vocabulary,
werepresentitbyaspecial‚Äú<unk>‚Äùtoken,signifyingthatthisisan unknown value.
338 Recurrent Neural Networks
class Vocab :#@save
"""Vocabulary for text."""
def __init__ (self , tokens =[], min_freq =0, reserved_tokens =[]):
# Flatten a 2D list if needed
iftokens and isinstance (tokens[ 0],list ):
tokens =[token for line intokens for token inline]
# Count token frequencies
counter =collections .Counter(tokens)
self .token_freqs =sorted (counter .items(), key =lambda x: x[ 1],
reverse =True )
# The list of unique tokens
self .idx_to_token =list (sorted (set(['<unk> ']+reserved_tokens +[
token for token, freq inself .token_freqs iffreq >=min_freq])))
self .token_to_idx ={token: idx
for idx, token inenumerate (self .idx_to_token)}
def __len__ (self ):
return len(self .idx_to_token)
def __getitem__ (self , tokens):
ifnot isinstance (tokens, ( list ,tuple )):
return self .token_to_idx .get(tokens, self .unk)
return [self .__getitem__ (token) for token intokens]
def to_tokens (self , indices):
ifhasattr (indices, '__len__ ')and len(indices) >1:
return [self .idx_to_token[ int(index)] for index inindices]
return self .idx_to_token[indices]
@property
def unk(self ): # Index for the unknown token
return self .token_to_idx[ '<unk> ']
We now construct a vocabulary for our dataset, converting the sequence of strings into a
list of numerical indices. Note that we have not lost any information and can easily convert
our dataset back to its original (string) representation.
vocab =Vocab(tokens)
indices =vocab[tokens[: 10]]
print ('indices: ', indices)
print ('words: ', vocab .to_tokens(indices))
indices: [ 21,9,6,0,21,10,14,6,0,14]
words: [ 't','h','e','','t','i','m','e','','m']
9.2.4PuttingIt All Together
Using the above classes and methods, we package everything into the following build
methodofthe TimeMachine class,whichreturns corpus,alistoftokenindices,and vocab,
the vocabulary of The Time Machine corpus. The modifications we did here are: (i) we
tokenizetextintocharacters,notwords,tosimplifythetraininginlatersections;(ii) corpus
339 Converting Raw Text into Sequence Data
is a single list, not a list of token lists, since each text line in The Time Machine dataset is
not necessarily a sentence or paragraph.
@d2l .add_to_class(TimeMachine) #@save
def build (self , raw_text, vocab =None ):
tokens =self ._tokenize( self ._preprocess(raw_text))
ifvocab isNone : vocab =Vocab(tokens)
corpus =[vocab[token] for token intokens]
return corpus, vocab
corpus, vocab =data .build(raw_text)
len(corpus), len(vocab)
(173428 ,28)
9.2.5Exploratory LanguageStatistics
Usingtherealcorpusandthe Vocabclassdefinedoverwords,wecaninspectbasicstatistics
concerning word use in our corpus. Below, we construct a vocabulary from words used in
TheTimeMachine and print the ten most frequently occurring of them.
words =text .split()
vocab =Vocab(words)
vocab .token_freqs[: 10]
[('the',2261 ),
('i',1267 ),
('and',1245 ),
('of',1155 ),
('a',816),
('to',695),
('was',552),
('in',541),
('that ',443),
('my',440)]
Note that the ten most frequent words are not all that descriptive. You might even imagine
that we might see a very similar list if we had chosen any book at random. Articles like
‚Äúthe‚Äù and ‚Äúa‚Äù, pronouns like ‚Äúi‚Äù and ‚Äúmy‚Äù, and prepositions like ‚Äúof‚Äù, ‚Äúto‚Äù, and ‚Äúin‚Äù occur
often because they serve common syntactic roles. Such words that are common but not
particularly descriptive are often called stop words and, in previous generations of text
classifiers based on so-called bag-of-words representations, they were most often filtered
out. However, they carry meaning and it is not necessary to filter them out when working
with modern RNN- and Transformer-based neural models. If you look further down the
list, you will notice that word frequency decays quickly. The 10thmost frequent word is
less than 1¬ù5as common as the most popular. Word frequency tends to follow a power law
distribution(specificallytheZipfian)aswegodowntheranks. Togetabetteridea, weplot
the figure of the word frequency.
340 Recurrent Neural Networks
freqs =[freq for token, freq invocab .token_freqs]
d2l.plot(freqs, xlabel ='token: x ', ylabel ='frequency: n(x) ',
xscale ='log', yscale ='log')
Afterdealingwiththefirstfewwordsasexceptions,alltheremainingwordsroughlyfollow
a straight line on a log‚Äìlog plot. This phenomenon is captured by Zipf‚Äôs law , which states
that the frequency ùëõùëñof theùëñthmost frequent word is:
ùëõùëñ/1
ùëñùõº, (9.2.1)
which is equivalent to
logùëõùëñ= ùõºlogùëñ¬∏ùëê, (9.2.2)
whereùõºis the exponent that characterizes the distribution and ùëêis a constant. This should
already give us pause for thought if we want to model words by counting statistics. After
all, we will significantly overestimate the frequency of the tail, also known as the infre-
quent words. But what about the other word combinations, such as two consecutive words
(bigrams), three consecutive words (trigrams), and beyond? Let‚Äôs see whether the bigram
frequency behaves in the same manner as the single word (unigram) frequency.
bigram_tokens =['--'.join(pair) for pair inzip(words[: -1], words[ 1:])]
bigram_vocab =Vocab(bigram_tokens)
bigram_vocab .token_freqs[: 10]
[('of--the ',309),
('in--the ',169),
('i--had ',130),
('i--was ',112),
('and--the ',109),
('the--time ',102),
('it--was ',99),
('to--the ',85),
('as--i ',78),
('of--a ',73)]
One thing is notable here. Out of the ten most frequent word pairs, nine are composed of
bothstopwordsandonlyoneisrelevanttotheactualbook‚Äî‚Äúthetime‚Äù. Furthermore, let‚Äôs
see whether the trigram frequency behaves in the same manner.
341 Converting Raw Text into Sequence Data
trigram_tokens =['--'.join(triple) for triple inzip(
words[: -2], words[ 1:-1], words[ 2:])]
trigram_vocab =Vocab(trigram_tokens)
trigram_vocab .token_freqs[: 10]
[('the--time--traveller ',59),
('the--time--machine ',30),
('the--medical--man ',24),
('it--seemed--to ',16),
('it--was--a ',15),
('here--and--there ',15),
('seemed--to--me ',14),
('i--did--not ',14),
('i--saw--the ',13),
('i--began--to ',13)]
Now, let‚Äôs visualize the token frequency among these three models: unigrams, bigrams,
and trigrams.
bigram_freqs =[freq for token, freq inbigram_vocab .token_freqs]
trigram_freqs =[freq for token, freq intrigram_vocab .token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel ='token: x ',
ylabel ='frequency: n(x) ', xscale ='log', yscale ='log',
legend =['unigram ','bigram ','trigram '])
This figure is quite exciting. First, beyond unigram words, sequences of words also appear
to be following Zipf‚Äôs law, albeit with a smaller exponent ùõºin(9.2.1 ), depending on the
sequence length. Second, the number of distinct ùëõ-grams is not that large. This gives us
hope that there is quite a lot of structure in language. Third, many ùëõ-grams occur very
rarely. This makes certain methods unsuitable for language modeling and motivates the
use of deep learning models. We will discuss this in the next section.
9.2.6Summary
Text is among the most common forms of sequence data encountered in deep learning.
Common choices for what constitutes a token are characters, words, and word pieces. To
preprocess text, we usually (i) split text into tokens; (ii) build a vocabulary to map token
strings to numerical indices; and (iii) convert text data into token indices for models to
342 Recurrent Neural Networks
138manipulate. In practice, the frequency of words tends to follow Zipf‚Äôs law. This is true not
just for individual words (unigrams), but also for ùëõ-grams.
9.2.7Exercises
1.In the experiment of this section, tokenize text into words and vary the min_freq argu-
ment value of the Vocabinstance. Qualitatively characterize how changes in min_freq
impact the size of the resulting vocabulary.
2.EstimatetheexponentofZipfiandistributionforunigrams,bigrams,andtrigramsinthis
corpus.
3.Find some other sources of data (download a standard machine learning dataset, pick
another public domain book, scrape a website, etc). For each, tokenize the data at both
the word and character levels. How do the vocabulary sizes compare with The Time
Machine corpusatequivalentvaluesof min_freq . EstimatetheexponentoftheZipfian
distribution corresponding to the unigram and bigram distributions for these corpora.
How do they compare with the values that you observed for TheTimeMachine corpus?
Discussions138.
9.3LanguageModels
InSection 9.2 , we saw how to map text sequences into tokens, where these tokens can be
viewed as a sequence of discrete observations such as words or characters. Assume that
the tokens in a text sequence of length ùëáare in turnùë•1,ùë•2,...,ùë•ùëá. The goal of language
modelsis to estimate the joint probability of the whole sequence:
ùëÉ¬πùë•1,ùë•2,...,ùë•ùëá¬∫, (9.3.1)
where statistical tools in Section 9.1 can be applied.
Language models are incredibly useful. For instance, an ideal language model should
generate natural text on its own, simply by drawing one token at a time ùë•ùë°ùëÉ¬πùë•ùë°j
ùë•ùë° 1,...,ùë• 1¬∫. Quite unlike the monkey using a typewriter, all text emerging from such
a model would pass as natural language, e.g., English text. Furthermore, it would be suffi-
cientforgeneratingameaningfuldialog,simplybyconditioningthetextonpreviousdialog
fragments. Clearly we are still very far from designing such a system, since it would need
tounderstand the text rather than just generate grammatically sensible content.
Nonetheless, language models are of great service even in their limited form. For instance,
thephrases‚Äútorecognizespeech‚Äùand‚Äútowreckanicebeach‚Äùsoundverysimilar. Thiscan
cause ambiguity in speech recognition, which is easily resolved through a language model
that rejects the second translation as outlandish. Likewise, in a document summarization
algorithm it is worthwhile knowing that ‚Äúdog bites man‚Äù is much more frequent than ‚Äúman
343 Language Models
139bitesdog‚Äù,orthat‚ÄúIwanttoeatgrandma‚Äùisaratherdisturbingstatement,whereas‚ÄúIwant
to eat, grandma‚Äù is much more benign.
import torch
from d2l import torch asd2l
9.3.1Learning LanguageModels
The obvious question is how we should model a document, or even a sequence of tokens.
Supposethatwetokenizetextdataatthewordlevel. Let‚Äôsstartbyapplyingbasicprobability
rules:
ùëÉ¬πùë•1,ùë•2,...,ùë•ùëá¬∫=ùëá√ñ
ùë°=1ùëÉ¬πùë•ùë°jùë•1,...,ùë•ùë° 1¬∫. (9.3.2)
For example, the probability of a text sequence containing four words would be given
as:
ùëÉ¬πdeep,learning,is,fun¬∫
=ùëÉ¬πdeep¬∫ùëÉ¬πlearningjdeep¬∫ùëÉ¬πisjdeep,learning¬∫ùëÉ¬πfunjdeep,learning,is¬∫.(9.3.3)
MarkovModels and ùëõ-grams
Among those sequence model analyses in Section 9.1 , let‚Äôs apply Markov models to lan-
guage modeling. A distribution over sequences satisfies the Markov property of first order
ifùëÉ¬πùë•ùë°¬∏1jùë•ùë°,...,ùë• 1¬∫=ùëÉ¬πùë•ùë°¬∏1jùë•ùë°¬∫. Higher orders correspond to longer dependencies.
Thisleadstoanumberofapproximationsthatwecouldapplytomodelasequence:
ùëÉ¬πùë•1,ùë•2,ùë•3,ùë•4¬∫=ùëÉ¬πùë•1¬∫ùëÉ¬πùë•2¬∫ùëÉ¬πùë•3¬∫ùëÉ¬πùë•4¬∫,
ùëÉ¬πùë•1,ùë•2,ùë•3,ùë•4¬∫=ùëÉ¬πùë•1¬∫ùëÉ¬πùë•2jùë•1¬∫ùëÉ¬πùë•3jùë•2¬∫ùëÉ¬πùë•4jùë•3¬∫,
ùëÉ¬πùë•1,ùë•2,ùë•3,ùë•4¬∫=ùëÉ¬πùë•1¬∫ùëÉ¬πùë•2jùë•1¬∫ùëÉ¬πùë•3jùë•1,ùë•2¬∫ùëÉ¬πùë•4jùë•2,ùë•3¬∫.(9.3.4)
The probability formulae that involve one, two, and three variables are typically referred
to asunigram ,bigram, andtrigrammodels, respectively. In order to compute the language
model, we need to calculate the probability of words and the conditional probability of
a word given the previous few words. Note that such probabilities are language model
parameters.
WordFrequency
Here,weassumethatthetrainingdatasetisalargetextcorpus,suchasallWikipediaentries,
Project Gutenberg139, and all text posted on the web. The probability of words can be
calculated from the relative word frequency of a given word in the training dataset. For
example, theestimate ÀÜùëÉ¬πdeep¬∫canbecalculatedastheprobabilityofanysentencestarting
with the word ‚Äúdeep‚Äù. A slightly less accurate approach would be to count all occurrences
344 Recurrent Neural Networks
of the word ‚Äúdeep‚Äù and divide it by the total number of words in the corpus. This works
fairlywell,particularlyforfrequentwords. Movingon,wecouldattempttoestimate
ÀÜùëÉ¬πlearningjdeep¬∫=ùëõ¬πdeep, learning¬∫
ùëõ¬πdeep¬∫, (9.3.5)
whereùëõ¬πùë•¬∫andùëõ¬πùë•,ùë•0¬∫are the number of occurrences of singletons and consecutive word
pairs, respectively. Unfortunately, estimating the probability of a word pair is somewhat
moredifficult,sincetheoccurrencesof‚Äúdeeplearning‚Äùarealotlessfrequent. Inparticular,
for some unusual word combinations it may be tricky to find enough occurrences to get
accurate estimates. As suggested by the empirical results in Section 9.2.5 , things take a
turn for the worse for three-word combinations and beyond. There will be many plausible
three-wordcombinationsthatwelikelywillnotseeinourdataset. Unlessweprovidesome
solutiontoassignsuchwordcombinationsanonzerocount,wewillnotbeabletousethem
in a language model. If the dataset is small or if the words are very rare, we might not find
even a single one of them.
LaplaceSmoothing
A common strategy is to perform some form of Laplacesmoothing . The solution is to add
asmallconstanttoallcounts. Denoteby ùëõthetotalnumberofwordsinthetrainingsetand
ùëöthe number of unique words. This solution helps with singletons, e.g., via
ÀÜùëÉ¬πùë•¬∫=ùëõ¬πùë•¬∫¬∏ùúñ1¬ùùëö
ùëõ¬∏ùúñ1,
ÀÜùëÉ¬πùë•0jùë•¬∫=ùëõ¬πùë•,ùë•0¬∫¬∏ùúñ2ÀÜùëÉ¬πùë•0¬∫
ùëõ¬πùë•¬∫¬∏ùúñ2,
ÀÜùëÉ¬πùë•00jùë•,ùë•0¬∫=ùëõ¬πùë•,ùë•0,ùë•00¬∫¬∏ùúñ3ÀÜùëÉ¬πùë•00¬∫
ùëõ¬πùë•,ùë•0¬∫¬∏ùúñ3.(9.3.6)
Hereùúñ1,ùúñ2,andùúñ3arehyperparameters. Take ùúñ1asanexample: when ùúñ1=0,nosmoothing
is applied; when ùúñ1approaches positive infinity, ÀÜùëÉ¬πùë•¬∫approaches the uniform probability
1¬ùùëö. The above is a rather primitive variant of what other techniques can accomplish
(Woodetal., 2011).
Unfortunately, modelslikethisgetunwieldyratherquicklyforthefollowingreasons. First,
as discussed in Section 9.2.5 , manyùëõ-grams occur very rarely, making Laplace smoothing
rather unsuitable for language modeling. Second, we need to store all counts. Third, this
entirely ignores the meaning of the words. For instance, ‚Äúcat‚Äù and ‚Äúfeline‚Äù should occur in
related contexts. It is quite difficult to adjust such models to additional contexts, whereas,
deep learning based language models are well suited to take this into account. Last, long
word sequences are almost certain to be novel, hence a model that simply counts the fre-
quencyofpreviouslyseenwordsequencesisboundtoperformpoorlythere. Therefore,we
focus on using neural networks for language modeling in the rest of the chapter.
9.3.2Perplexity
345 Language Models
Next, let‚Äôs discuss about how to measure the quality of the language model, which we
will then use to evaluate our models in the subsequent sections. One way is to check how
surprising the text is. A good language model is able to predict, with high accuracy, the
tokens that come next. Consider the following continuations of the phrase ‚ÄúIt is raining‚Äù,
as proposed by different language models:
1.‚ÄúIt is raining outside‚Äù
2.‚ÄúIt is raining banana tree‚Äù
3.‚ÄúIt is raining piouw;kcj pwepoiut‚Äù
In terms of quality, Example 1 is clearly the best. The words are sensible and logically co-
herent. Whileitmightnotquiteaccuratelyreflectwhichwordfollowssemantically(‚ÄúinSan
Francisco‚Äù and ‚Äúin winter‚Äù would have been perfectly reasonable extensions), the model is
abletocapturewhichkindofwordfollows. Example2isconsiderablyworsebyproducing
a nonsensical extension. Nonetheless, at least the model has learned how to spell words
and some degree of correlation between words. Last, Example 3 indicates a poorly trained
model that does not fit data properly.
We might measure the quality of the model by computing the likelihood of the sequence.
Unfortunatelythisisanumberthatishardtounderstandanddifficulttocompare. Afterall,
shorter sequencesare muchmore likelyto occur than the longerones, hence evaluatingthe
model on Tolstoy‚Äôs magnum opus War and Peace will inevitably produce a much smaller
likelihood than, say, on Saint-Exupery‚Äôs novella The Little Prince . What is missing is the
equivalent of an average.
Information theory comes handy here. We defined entropy, surprisal, and cross-entropy
when we introduced the softmax regression ( Section 4.1.3 ). If we want to compress text,
wecanaskaboutpredictingthenexttokengiventhecurrentsetoftokens. Abetterlanguage
modelshouldallowustopredictthenexttokenmoreaccurately. Thus,itshouldallowusto
spend fewer bits in compressing the sequence. So we can measure it by the cross-entropy
loss averaged over all the ùëõtokens of a sequence:
1
ùëõùëõ√ï
ùë°=1 logùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫, (9.3.7)
whereùëÉisgivenbyalanguagemodeland ùë•ùë°istheactualtokenobservedattimestep ùë°from
the sequence. This makes the performance on documents of different lengths comparable.
For historical reasons, scientists in natural language processing prefer to use a quantity
calledperplexity . In a nutshell, it is the exponential of (9.3.7 ):
exp 
 1
ùëõùëõ√ï
ùë°=1logùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫!
. (9.3.8)
Perplexitycanbebestunderstoodasthereciprocalofthegeometricmeanofthenumberof
real choices that we have when deciding which token to pick next. Let‚Äôs look at a number
of cases:
346 Recurrent Neural Networks
In the best case scenario, the model always perfectly estimates the probability of the
target token as 1. In this case the perplexity of the model is 1.
In the worst case scenario, the model always predicts the probability of the target token
as 0. In this situation, the perplexity is positive infinity.
Atthe baseline, the model predicts a uniform distribution overall the availabletokensof
the vocabulary. In this case, the perplexity equals the number of unique tokens of the
vocabulary. In fact, if we were to store the sequence without any compression, this
wouldbe thebestwecould doforencodingit. Hence, this providesa nontrivialupper
bound that any useful model must beat.
9.3.3PartitioningSequences
We will design language models using neural networks and use perplexity to evaluate how
good the model is at predicting the next token given the current set of tokens in text se-
quences. Before introducing the model, let‚Äôs assume that it processes a minibatch of se-
quences with predefined length at a time. Now the question is how to read minibatches of
input sequences and target sequences at random.
Suppose that the dataset takes the form of a sequence of ùëátoken indices in corpus. We
will partition it into subsequences, where each subsequence has ùëõtokens (time steps). To
iterateover(almost)allthetokensoftheentiredatasetforeachepochandobtainallpossible
length-ùëõsubsequences, we can introduce randomness. More concretely, at the beginning
of each epoch, discard the first ùëëtokens, where ùëë2¬ª0,ùëõ¬∫is uniformly sampled at random.
Therestofthesequenceisthenpartitionedinto ùëö=b¬πùëá ùëë¬∫¬ùùëõcsubsequences. Denoteby
xùë°=¬ªùë•ùë°,...,ùë•ùë°¬∏ùëõ 1¬ºthe length-ùëõsubsequence starting from token ùë•ùë°at time stepùë°. The
resultingùëöpartitioned subsequences are xùëë,xùëë¬∏ùëõ,...,xùëë¬∏ùëõ¬πùëö 1¬∫.Each subsequence will
be used as an input sequence into the language model.
For language modeling, the goal is to predict the next token based on the tokens we have
seen so far; hence the targets (labels) are the original sequence, shifted by one token. The
target sequence for any input sequence xùë°isxùë°¬∏1with lengthùëõ.
tFig. 9.3.1 Obtaining Ô¨Åve pairs of input sequences and target sequences from partitioned length-5
subsequences.
Fig.9.3.1 showsanexampleofobtainingfivepairsofinputsequencesandtargetsequences
withùëõ=5andùëë=2.
@d2l .add_to_class(d2l .TimeMachine) #@save
def __init__ (self , batch_size, num_steps, num_train =10000 , num_val =5000 ):
super (d2l .TimeMachine, self ).__init__ ()
self .save_hyperparameters()
corpus, self .vocab =self .build( self ._download())
(continues on next page)
347 Language Models
(continued from previous page)
array =torch .tensor([corpus[i:i +num_steps +1]
for iinrange (len(corpus) -num_steps)])
self .X,self .Y=array[:,: -1], array[:, 1:]
To train language models, we will randomly sample pairs of input sequences and target
sequences in minibatches. The followingdata loader randomlygeneratesa minibatchfrom
the dataset each time. The argument batch_size specifies the number of subsequence
examples in each minibatch and num_steps is the subsequence length in tokens.
@d2l .add_to_class(d2l .TimeMachine) #@save
def get_dataloader (self , train):
idx =slice (0,self .num_train) iftrain else slice (
self .num_train, self .num_train +self .num_val)
return self .get_tensorloader([ self .X,self .Y], train, idx)
As we can see in the following, a minibatch of target sequences can be obtained by shifting
the input sequences by one token.
data =d2l.TimeMachine(batch_size =2, num_steps =10)
for X, Y indata .train_dataloader():
print ('X:', X, '\nY:', Y)
break
Downloading ../data /timemachine .txt from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/timemachine .txt...
X: tensor([[ 10,4,2,21,10,16,15,0,20,2],
[21,9,6,19,0,24,2,26,0,16]])
Y: tensor([[ 4,2,21,10,16,15,0,20,2,10],
[9,6,19,0,24,2,26,0,16,9]])
9.3.4Summaryand Discussion
Language models estimate the joint probability of a text sequence. For long sequences,
ùëõ-grams provide a convenient model by truncating the dependence. However, there is a lot
ofstructurebutnotenoughfrequencytodealefficientlywithinfrequentwordcombinations
via Laplace smoothing. Thus, we will focus on neural language modeling in subsequent
sections. To train language models, we can randomly sample pairs of input sequences
and target sequences in minibatches. After training, we will use perplexity to measure the
language model quality.
Language models can be scaled up with increased data size, model size, and amount in
training compute. Large language models can perform desired tasks by predicting output
textgiveninputtextinstructions. Aswewilldiscusslater(e.g., Section11.9 ),atthepresent
moment large language models form the basis of state-of-the-art systems across diverse
tasks.
348 Recurrent Neural Networks
1409.3.5Exercises
1.Suppose there are 100,000 words in the training dataset. How much word frequency
and multi-word adjacent frequency does a four-gram need to store?
2.How would you model a dialogue?
3.What other methods can you think of for reading long sequence data?
4.Consider our method for discarding a uniformly random number of the first few tokens
at the beginning of each epoch.
1.Doesitreallyleadtoaperfectlyuniformdistributionoverthesequencesonthedocu-
ment?
2.What would you have to do to make things even more uniform?
5.If we want a sequence example to be a complete sentence, what kind of problem does
this introduce in minibatch sampling? How can we fix it?
Discussions140.
9.4RecurrentNeuralNetworks
InSection 9.3 we described Markov models and ùëõ-grams for language modeling, where
the conditional probability of token ùë•ùë°at time step ùë°only depends on the ùëõ 1previous
tokens. If we want to incorporate the possible effect of tokens earlier than time step ùë° 
¬πùëõ 1¬∫onùë•ùë°, weneedtoincrease ùëõ. However, thenumberofmodelparameterswouldalso
increase exponentially with it, as we need to store jVjùëõnumbers for a vocabulary set V.
Hence,ratherthanmodeling ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë•ùë° ùëõ¬∏1¬∫itispreferabletousealatentvariable
model,
ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫ùëÉ¬πùë•ùë°j‚Ñéùë° 1¬∫, (9.4.1)
where‚Ñéùë° 1is ahidden state that stores the sequence information up to time step ùë° 1. In
general, the hidden state at any time step ùë°could be computed based on both the current
inputùë•ùë°and the previous hidden state ‚Ñéùë° 1:
‚Ñéùë°=ùëì¬πùë•ùë°,‚Ñéùë° 1¬∫. (9.4.2)
Forasufficientlypowerfulfunction ùëìin(9.4.2 ),thelatentvariablemodelisnotanapprox-
imation. Afterall, ‚Ñéùë°maysimplystoreallthedataithasobservedsofar. However, itcould
potentially make both computation and storage expensive.
Recallthatwehavediscussedhiddenlayerswithhiddenunitsin Chapter5 . Itisnoteworthy
that hidden layers and hidden states refer to two very different concepts. Hidden layers are,
as explained, layers that are hidden from view on the path from input to output. Hidden
349 Recurrent Neural Networks
states are technically speaking inputsto whatever we do at a given step, and they can only
be computed by looking at data at previous time steps.
Recurrent neural networks (RNNs) are neural networks with hidden states. Before intro-
ducing the RNN model, we first revisit the MLP model introduced in Section 5.1 .
import torch
from d2l import torch asd2l
9.4.1NeuralNetworkswithout Hidden States
Let‚Äôs take a look at an MLP with a single hidden layer. Let the hidden layer‚Äôs activation
function beùúô. Given a minibatch of examples X2Rùëõùëëwith batch size ùëõandùëëinputs,
the hidden layer output H2Rùëõ‚Ñéis calculated as
H=ùúô¬πXWxh¬∏bh¬∫. (9.4.3)
In(9.4.3 ), we have the weight parameter Wxh2Rùëë‚Ñé, the bias parameter bh2R1‚Ñé, and
the number of hidden units ‚Ñé, for the hidden layer. So armed, we apply broadcasting (see
Section 2.1.4 ) during the summation. Next, the hidden layer output His used as input of
the output layer, which is given by
O=HWhq¬∏bq, (9.4.4)
where O2Rùëõùëûis the output variable, Whq2R‚Ñéùëûis the weight parameter, and bq2
R1ùëûis the bias parameter of the output layer. If it is a classification problem, we can use
softmax¬πO¬∫to compute the probability distribution of the output categories.
This is entirely analogous to the regression problem we solved previously in Section 9.1 ,
hence we omit details. Suffice it to say that we can pick feature-label pairs at random and
learn the parameters of our network via automatic differentiation and stochastic gradient
descent.
9.4.2RecurrentNeuralNetworkswith Hidden States
Matters are entirely different when we have hidden states. Let‚Äôs look at the structure in
some more detail.
Assume that we have a minibatch of inputs Xùë°2Rùëõùëëat time stepùë°. In other words, for
a minibatch of ùëõsequence examples, each row of Xùë°corresponds to one example at time
stepùë°from the sequence. Next, denote by Hùë°2Rùëõ‚Ñéthe hidden layer output of time step
ùë°. UnlikewithMLP,herewesavethehiddenlayeroutput Hùë° 1fromtheprevioustimestep
andintroduceanewweightparameter Whh2R‚Ñé‚Ñétodescribehowtousethehiddenlayer
output of the previous time step in the current time step. Specifically, the calculation of the
hidden layer output of the current time step is determined by the input of the current time
step together with the hidden layer output of the previous time step:
Hùë°=ùúô¬πXùë°Wxh¬∏Hùë° 1Whh¬∏bh¬∫. (9.4.5)
350 Recurrent Neural Networks
Comparedwith (9.4.3 ),(9.4.5 )addsonemoreterm Hùë° 1Whhandthusinstantiates (9.4.2 ).
From the relationship between hidden layer outputs Hùë°andHùë° 1of adjacent time steps,
we know that these variables captured and retained the sequence‚Äôs historical information
up to their current time step, just like the state or memory of the neural network‚Äôs current
time step. Therefore, such a hidden layer output is called a hidden state . Since the hidden
state uses the same definition of the previous time step in the current time step, the compu-
tation of (9.4.5 )isrecurrent . Hence, as we said, neural networks with hidden states based
on recurrent computation are named recurrent neural networks . Layers that perform the
computation of (9.4.5 )in RNNs are called recurrentlayers .
There are many different ways for constructing RNNs. Those with a hidden state defined
by(9.4.5 )are very common. For time step ùë°, the output of the output layer is similar to the
computation in the MLP:
Oùë°=Hùë°Whq¬∏bq. (9.4.6)
Parameters of the RNN include the weights Wxh2Rùëë‚Ñé,Whh2R‚Ñé‚Ñé, and the bias bh2
R1‚Ñéof the hidden layer, together with the weights Whq2R‚Ñéùëûand the bias bq2R1ùëû
of the output layer. It is worth mentioning that even at different time steps, RNNs always
use these model parameters. Therefore, the parametrization cost of an RNN does not grow
as the number of time steps increases.
Fig. 9.4.1 illustrates the computational logic of an RNN at three adjacent time steps. At
any time step ùë°, the computation of the hidden state can be treated as: (i) concatenating the
inputXùë°at the current time step ùë°and the hidden state Hùë° 1at the previous time step ùë° 1;
(ii)feedingtheconcatenationresultintoafullyconnectedlayerwiththeactivationfunction
ùúô. Theoutputofsuchafullyconnectedlayeristhehiddenstate Hùë°ofthecurrenttimestep
ùë°. In this case, the model parameters are the concatenation of WxhandWhh, and a bias
ofbh, all from (9.4.5 ). The hidden state of the current time step ùë°,Hùë°, will participate in
computing the hidden state Hùë°¬∏1of the next time step ùë°¬∏1. What is more, Hùë°will also be
fed into the fully connected output layer to compute the output Oùë°of the current time step
ùë°.
tFig. 9.4.1 An RNN with a hidden state.
Wejustmentionedthatthecalculationof Xùë°Wxh¬∏Hùë° 1Whhforthehiddenstateisequiv-
alent to matrix multiplication of the concatenation of Xùë°andHùë° 1and the concatenation
ofWxhandWhh. Though this can be proven mathematically, in the following we just use
351 Recurrent Neural Networks
a simple code snippet as a demonstration. To begin with, we define matrices X,W_xh,H,
andW_hh, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying Xby
W_xh, and HbyW_hh, and then adding these two products, we obtain a matrix of shape (3,
4).
X, W_xh =torch .randn( 3,1), torch .randn( 1,4)
H, W_hh =torch .randn( 3,4), torch .randn( 4,4)
torch .matmul(X, W_xh) +torch .matmul(H, W_hh)
tensor([[ 1.2526 ,0.0580 ,-3.3460 ,-0.2519 ],
[-1.3064 ,1.4132 ,-0.1435 ,0.3482 ],
[3.1495 ,0.8172 ,1.5167 ,-0.9038 ]])
Now we concatenate the matrices XandHalong columns (axis 1), and the matrices W_xh
andW_hhalong rows (axis 0). These two concatenations result in matrices of shape (3, 5)
and of shape (5, 4), respectively. Multiplying these two concatenated matrices, we obtain
the same output matrix of shape (3, 4) as above.
torch .matmul(torch .cat((X, H), 1), torch .cat((W_xh, W_hh), 0))
tensor([[ 1.2526 ,0.0580 ,-3.3460 ,-0.2519 ],
[-1.3064 ,1.4132 ,-0.1435 ,0.3482 ],
[3.1495 ,0.8172 ,1.5167 ,-0.9038 ]])
9.4.3RNN-Based Character-LevelLanguageModels
Recallthatforlanguagemodelingin Section9.3 , weaimtopredictthenexttokenbasedon
the current and past tokens; thus we shift the original sequence by one token as the targets
(labels). Bengio etal.(2003)firstproposedtouseaneuralnetworkforlanguagemodeling.
In the following we illustrate how RNNs can be used to build a language model. Let the
minibatch size be one, and the sequence of the text be ‚Äúmachine‚Äù. To simplify training
in subsequent sections, we tokenize text into characters rather than words and consider a
character-level language model .Fig. 9.4.2 demonstrates how to predict the next charac-
ter based on the current and previous characters via an RNN for character-level language
modeling.
Duringthetrainingprocess,werunasoftmaxoperationontheoutputfromtheoutputlayer
for each time step, and then use the cross-entropy loss to compute the error between the
modeloutputandthetarget. Becauseoftherecurrentcomputationofthehiddenstateinthe
hidden layer, the output, O3, of time step 3 in Fig. 9.4.2 is determined by the text sequence
‚Äúm‚Äù, ‚Äúa‚Äù, and ‚Äúc‚Äù. Since the next character of the sequence in the training data is ‚Äúh‚Äù, the
lossoftimestep3willdependontheprobabilitydistributionofthenextcharactergenerated
based on the feature sequence ‚Äúm‚Äù, ‚Äúa‚Äù, ‚Äúc‚Äù and the target ‚Äúh‚Äù of this time step.
In practice, each token is represented by a ùëë-dimensional vector, and we use a batch size
352 Recurrent Neural Networks
tFig. 9.4.2 A character-level language model based on the RNN. The input and target sequences are
‚Äúmachin‚Äù and ‚Äúachine‚Äù, respectively.
141ùëõ> 1. Therefore, the input Xùë°at time stepùë°will be anùëõùëëmatrix, which is identical to
what we discussed in Section 9.4.2 .
In the following sections, we will implement RNNs for character-level language mod-
els.
9.4.4Summary
A neural network that uses recurrent computation for hidden states is called a recurrent
neural network (RNN). The hidden state of an RNN can capture historical information of
the sequence up to the current time step. With recurrent computation, the number of RNN
modelparametersdoesnotgrowasthenumberoftimestepsincreases. Asforapplications,
an RNN can be used to create character-level language models.
9.4.5Exercises
1.If we use an RNN to predict the next character in a text sequence, what is the required
dimension for any output?
2.Why can RNNs express the conditional probability of a token at some time step based
on all the previous tokens in the text sequence?
3.What happens to the gradient if you backpropagate through a long sequence?
4.What are some of the problems associated with the language model described in this
section?
Discussions141.
9.5RecurrentNeuralNetworkImplementation
fromScratch
We are now ready to implement an RNN from scratch. In particular, we will train this
RNN to function as a character-level language model (see Section 9.4 ) and train it on a
353 Recurrent Neural Network Implementation from Scratch
corpus consisting of the entire text of H. G. Wells‚Äô The Time Machine , following the data
processing steps outlined in Section 9.2 . We start by loading the dataset.
%matplotlib inline
import math
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
9.5.1RNNModel
We begin by defining a class to implement the RNN model ( Section 9.4.2 ). Note that the
number of hidden units num_hiddens is a tunable hyperparameter.
class RNNScratch (d2l .Module): #@save
"""The RNN model implemented from scratch."""
def __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .W_xh =nn.Parameter(
torch .randn(num_inputs, num_hiddens) *sigma)
self .W_hh =nn.Parameter(
torch .randn(num_hiddens, num_hiddens) *sigma)
self .b_h =nn.Parameter(torch .zeros(num_hiddens))
Theforward methodbelowdefineshowtocomputetheoutputandhiddenstateatanytime
step, given the current input and the state of the model at the previous time step. Note that
the RNN model loops through the outermost dimension of inputs, updating the hidden
state one time step at a time. The model here uses a tanhactivation function ( Section
5.1.2).
@d2l .add_to_class(RNNScratch) #@save
def forward (self , inputs, state =None ):
ifstate isNone :
# Initial state with shape: (batch_size, num_hiddens)
state =torch .zeros((inputs .shape[ 1],self .num_hiddens),
device =inputs .device)
else :
state, =state
outputs =[]
for Xininputs: # Shape of inputs: (num_steps, batch_size, num_inputs)
state =torch .tanh(torch .matmul(X, self .W_xh) +
torch .matmul(state, self .W_hh) +self .b_h)
outputs .append(state)
return outputs, state
We can feed a minibatch of input sequences into an RNN model as follows.
batch_size, num_inputs, num_hiddens, num_steps =2,16,32,100
(continues on next page)
354 Recurrent Neural Networks
(continued from previous page)
rnn =RNNScratch(num_inputs, num_hiddens)
X=torch .ones((num_steps, batch_size, num_inputs))
outputs, state =rnn(X)
Let‚Äôs check whether the RNN model produces results of the correct shapes to ensure that
the dimensionality of the hidden state remains unchanged.
def check_len (a, n): #@save
"""Check the length of a list."""
assert len(a) ==n,f'list \'s length {len(a)}!= expected length {n}'
def check_shape (a, shape): #@save
"""Check the shape of a tensor."""
assert a.shape ==shape, \
f'tensor \'s shape {a.shape }!= expected shape {shape }'
check_len(outputs, num_steps)
check_shape(outputs[ 0], (batch_size, num_hiddens))
check_shape(state, (batch_size, num_hiddens))
9.5.2RNN-BasedLanguageModel
The following RNNLMScratch class defines an RNN-based language model, where wepass
in our RNN via the rnnargument of the __init__ method. When training language mod-
els, the inputs and outputs are from the same vocabulary. Hence, they have the same di-
mension, which is equal to the vocabulary size. Note that we use perplexity to evaluate the
model. As discussed in Section 9.3.2 , this ensures that sequences of different length are
comparable.
class RNNLMScratch (d2l .Classifier): #@save
"""The RNN-based language model implemented from scratch."""
def __init__ (self , rnn, vocab_size, lr =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .init_params()
def init_params (self ):
self .W_hq =nn.Parameter(
torch .randn(
self .rnn.num_hiddens, self .vocab_size) *self .rnn.sigma)
self .b_q =nn.Parameter(torch .zeros( self .vocab_size))
def training_step (self , batch):
l=self .loss( self (*batch[: -1]), batch[ -1])
self .plot( 'ppl', torch .exp(l), train =True )
return l
def validation_step (self , batch):
l=self .loss( self (*batch[: -1]), batch[ -1])
self .plot( 'ppl', torch .exp(l), train =False )
355 Recurrent Neural Network Implementation from Scratch
One-HotEncoding
Recall that each token is represented by a numerical index indicating the position in the
vocabularyofthecorrespondingword/character/wordpiece. Youmightbetemptedtobuild
a neural network with a single input node (at each time step), where the index could be fed
in as a scalar value. This works when we are dealing with numerical inputs like price or
temperature, where any two values sufficiently close together should be treated similarly.
But this does not quite make sense. The 45thand46thwords in our vocabulary happen to
be ‚Äútheir‚Äù and ‚Äúsaid‚Äù, whose meanings are not remotely similar.
When dealing with such categorical data, the most common strategy is to represent each
item by a one-hot encoding (recall from Section 4.1.1 ). A one-hot encoding is a vector
whose length is given by the size of the vocabulary ùëÅ, where all entries are set to 0, except
for the entry corresponding to our token, which is set to 1. For example, if the vocabulary
had five elements, then the one-hot vectors corresponding to indices 0 and 2 would be the
following.
F.one_hot(torch .tensor([ 0,2]), 5)
tensor([[ 1,0,0,0,0],
[0,0,1,0,0]])
The minibatches that we sample at each iteration will take the shape (batch size, number
of time steps). Once representing each input as a one-hot vector, we can think of each
minibatch as a three-dimensional tensor, where the length along the third axis is given by
the vocabulary size ( len(vocab) ). We often transpose the input so that we will obtain an
output of shape (number of time steps, batch size, vocabulary size). This will allow us to
loop more conveniently through the outermost dimension for updating hidden states of a
minibatch, time step by time step (e.g., in the above forward method).
@d2l .add_to_class(RNNLMScratch) #@save
def one_hot (self , X):
# Output shape: (num_steps, batch_size, vocab_size)
return F.one_hot(X .T,self .vocab_size) .type(torch .float32)
TransformingRNN Outputs
The language model uses a fully connected output layer to transform RNN outputs into
token predictions at each time step.
@d2l .add_to_class(RNNLMScratch) #@save
def output_layer (self , rnn_outputs):
outputs =[torch .matmul(H, self .W_hq) +self .b_q for Hinrnn_outputs]
return torch .stack(outputs, 1)
@d2l .add_to_class(RNNLMScratch) #@save
(continues on next page)
356 Recurrent Neural Networks
(continued from previous page)
def forward (self , X, state =None ):
embs =self .one_hot(X)
rnn_outputs, _ =self .rnn(embs, state)
return self .output_layer(rnn_outputs)
Let‚Äôscheckwhethertheforwardcomputationproducesoutputswiththecorrectshape.
model =RNNLMScratch(rnn, num_inputs)
outputs =model(torch .ones((batch_size, num_steps), dtype =torch .int64))
check_shape(outputs, (batch_size, num_steps, num_inputs))
9.5.3Gradient Clipping
Whileyouarealreadyusedtothinkingofneuralnetworksas‚Äúdeep‚Äùinthesensethatmany
layers separate the input and output even within a single time step, the length of the se-
quence introduces a new notion of depth. In addition to the passing through the network
in the input-to-output direction, inputs at the first time step must pass through a chain of ùëá
layers along the time steps in order to influence the output of the model at the final time
step. Taking the backwards view, in each iteration, we backpropagate gradients through
time, resulting in a chain of matrix-products of length O¬πùëá¬∫. As mentioned in Section 5.4 ,
this can result in numerical instability, causing the gradients either to explode or vanish,
depending on the properties of the weight matrices.
Dealing with vanishing and exploding gradients is a fundamental problem when designing
RNNs and has inspired some of the biggest advances in modern neural network architec-
tures. Inthenextchapter,wewilltalkaboutspecializedarchitecturesthatweredesignedin
hopes of mitigating the vanishing gradient problem. However, even modern RNNs often
sufferfromexplodinggradients. Oneinelegantbutubiquitoussolutionistosimplyclipthe
gradients forcing the resulting ‚Äúclipped‚Äù gradients to take smaller values.
Generally speaking, when optimizing some objective by gradient descent, we iteratively
updatetheparameterofinterest,sayavector x,butpushingitinthedirectionofthenegative
gradient g(instochasticgradientdescent,wecalculatethisgradientonarandomlysampled
minibatch). Forexample,withlearningrate ùúÇ> 0,eachupdatetakestheform x x ùúÇg.
Let‚Äôs further assume that the objective function ùëìis sufficiently smooth. Formally, we say
that the objective is Lipschitz continuous with constant ùêø, meaning that for any xandy,
we have
jùëì¬πx¬∫ ùëì¬πy¬∫jùêøkx yk. (9.5.1)
As you can see, when we update the parameter vector by subtracting ùúÇg, the change in
the value of the objective depends on the learning rate, the norm of the gradient and ùêøas
follows:
jùëì¬πx¬∫ ùëì¬πx ùúÇg¬∫jùêøùúÇkgk. (9.5.2)
In other words, the objective cannot change by more than ùêøùúÇkgk. Having a small value for
357 Recurrent Neural Network Implementation from Scratch
this upper bound might be viewed as good or bad. On the downside, we are limiting the
speed at which we can reduce the value of the objective. On the bright side, this limits by
just how much we can go wrong in any one gradient step.
When we say that gradients explode, we mean that kgkbecomes excessively large. In this
worst case, we might do so much damage in a single gradient step that we could undo all
of the progress made over the course of thousands of training iterations. When gradients
can be so large, neural network training often diverges, failing to reduce the value of the
objective. At other times, training eventually converges but is unstable owing to massive
spikes in the loss.
One way to limit the size of ùêøùúÇkgkis to shrink the learning rate ùúÇto tiny values. This
has the advantage that we do not bias the updates. But what if we only rarelyget large
gradients? This drastic move slows down our progress at all steps, just to deal with the rare
exploding gradient events. A popular alternative is to adopt a gradient clipping heuristic
projecting the gradients gonto a ball of some given radius ùúÉas follows:
g min
1,ùúÉ
kgk
g. (9.5.3)
Thisensuresthatthegradientnormneverexceeds ùúÉandthattheupdatedgradientisentirely
aligned with the original direction of g. It also has the desirable side-effect of limiting the
influence any given minibatch (and within it any given sample) can exert on the parameter
vector. This bestows a certain degree of robustness to the model. To be clear, it is a hack.
Gradient clipping means that we are not always following the true gradient and it is hard to
reason analytically about the possible side effects. However, it is a very useful hack, and is
widely adopted in RNN implementations in most deep learning frameworks.
Below we define a method to clip gradients, which is invoked by the fit_epoch method
of the d2l.Trainer class (see Section 3.4 ). Note that when computing the gradient norm,
we are concatenating all model parameters, treating them as a single giant parameter vec-
tor.
@d2l .add_to_class(d2l .Trainer) #@save
def clip_gradients (self , grad_clip_val, model):
params =[pfor pinmodel .parameters() ifp.requires_grad]
norm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))
ifnorm >grad_clip_val:
for param inparams:
param .grad[:] *=grad_clip_val /norm
9.5.4Training
UsingTheTimeMachine dataset( data),wetrainacharacter-levellanguagemodel( model)
based on the RNN ( rnn) implemented from scratch. Note that we first calculate the gra-
dients, then clip them, and finally update the model parameters using the clipped gradi-
ents.
358 Recurrent Neural Networks
data =d2l.TimeMachine(batch_size =1024 , num_steps =32)
rnn =RNNScratch(num_inputs =len(data .vocab), num_hiddens =32)
model =RNNLMScratch(rnn, vocab_size =len(data .vocab), lr =1)
trainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
9.5.5Decoding
Once a language model has been learned, we can use it not only to predict the next token
but to continue predicting each subsequent one, treating the previously predicted token as
thoughitwerethenextintheinput. Sometimeswewilljustwanttogeneratetextasthough
wewerestartingatthebeginningofadocument. However,itisoftenusefultoconditionthe
languagemodelonauser-suppliedprefix. Forexample,ifweweredevelopinganautocom-
plete feature for a search engine or to assist users in writing emails, we would want to feed
in what they had written so far (the prefix), and then generate a likely continuation.
The following predict method generates a continuation, one character at a time, after
ingesting a user-provided prefix. When looping through the characters in prefix, we
keep passing the hidden state to the next time step but do not generate any output. This is
called the warm-up period. After ingesting the prefix, we are now ready to begin emitting
the subsequent characters, each of which will be fed back into the model as the input at the
next time step.
@d2l .add_to_class(RNNLMScratch) #@save
def predict (self , prefix, num_preds, vocab, device =None ):
state, outputs =None , [vocab[prefix[ 0]]]
for iinrange (len(prefix) +num_preds -1):
X=torch .tensor([[outputs[ -1]]], device =device)
embs =self .one_hot(X)
rnn_outputs, state =self .rnn(embs, state)
ifi<len(prefix) -1:# Warm-up period
outputs .append(vocab[prefix[i +1]])
else :# Predict num_preds steps
Y=self .output_layer(rnn_outputs)
outputs .append( int(Y.argmax(axis =2).reshape( 1)))
return ''.join([vocab .idx_to_token[i] for iinoutputs])
Inthefollowing,wespecifytheprefixandhaveitgenerate20additionalcharacters.
359 Recurrent Neural Network Implementation from Scratch
model .predict( 'it has ',20, data .vocab, d2l .try_gpu())
'it has in the the the the '
WhileimplementingtheaboveRNNmodelfromscratchisinstructive,itisnotconvenient.
Inthenextsection,wewillseehowtoleveragedeeplearningframeworkstowhipupRNNs
usingstandardarchitectures, andtoreapperformancegainsbyrelyingonhighlyoptimized
library functions.
9.5.6Summary
WecantrainRNN-basedlanguagemodelstogeneratetextfollowingtheuser-providedtext
prefix. A simple RNN language model consists of input encoding, RNN modeling, and
output generation. During training, gradient clipping can mitigate the problem of explod-
ing gradients but does not address the problem of vanishing gradients. In the experiment,
weimplementedasimpleRNNlanguagemodelandtraineditwithgradientclippingonse-
quences of text, tokenized at the character level. By conditioning on a prefix, we can use a
languagemodeltogeneratelikelycontinuations,whichprovesusefulinmanyapplications,
e.g., autocomplete features.
9.5.7Exercises
1.Doestheimplementedlanguagemodelpredictthenexttokenbasedonallthepasttokens
up to the very first token in TheTimeMachine ?
2.Which hyperparameter controls the length of history used for prediction?
3.Show that one-hot encoding is equivalent to picking a different embedding for each
object.
4.Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number
of time steps in a minibatch, and learning rate) to improve the perplexity. How low can
you go while sticking with this simple architecture?
5.Replace one-hot encoding with learnable embeddings. Does this lead to better perfor-
mance?
6.Conductanexperimenttodeterminehowwellthislanguagemodeltrainedon TheTime
Machine works on other books by H. G. Wells, e.g., The Warof theWorlds .
7.Conduct another experiment to evaluate the perplexity of this model on books written
by other authors.
8.Modify the prediction method so as to use sampling rather than picking the most likely
next character.
What happens?
360 Recurrent Neural Networks
142Biasthemodeltowardsmorelikelyoutputs,e.g.,bysamplingfrom ùëû¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫/
ùëÉ¬πùë•ùë°jùë•ùë° 1,...,ùë• 1¬∫ùõºforùõº> 1.
9.Run the code in this section without clipping the gradient. What happens?
10.Replace the activation function used in this section with ReLU and repeat the experi-
ments in this section. Do we still need gradient clipping? Why?
Discussions142.
9.6ConciseImplementation of RecurrentNeural
Networks
Like most of our from-scratch implementations, Section 9.5 was designed to provide in-
sight into how each component works. But when you are using RNNs every day or writing
production code, you will want to rely more on libraries that cut down on both implemen-
tationtime(bysupplyinglibrarycodeforcommonmodelsandfunctions)andcomputation
time (by optimizing the heck out of these library implementations). This section will show
you how to implement the same language model more efficiently using the high-level API
provided by your deep learning framework. We begin, as before, by loading The Time
Machine dataset.
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
9.6.1Definingthe Model
We define the following class using the RNN implemented by high-level APIs.
class RNN(d2l .Module): #@save
"""The RNN model implemented with high-level APIs."""
def __init__ (self , num_inputs, num_hiddens):
super ().__init__ ()
self .save_hyperparameters()
self .rnn =nn.RNN(num_inputs, num_hiddens)
def forward (self , inputs, H =None ):
return self .rnn(inputs, H)
Inheriting from the RNNLMScratch class inSection 9.5 , the following RNNLMclass defines
a complete RNN-based language model. Note that we need to create a separate fully con-
nected output layer.
361 Concise Implementation of Recurrent Neural Networks
class RNNLM (d2l .RNNLMScratch): #@save
"""The RNN-based language model implemented with high-level APIs."""
def init_params (self ):
self .linear =nn.LazyLinear( self .vocab_size)
def output_layer (self , hiddens):
return self .linear(hiddens) .swapaxes( 0,1)
9.6.2Trainingand Predicting
Before training the model, let‚Äôs make a prediction with a model initialized with random
weights. Given that we have not trained the network, it will generate nonsensical predic-
tions.
data =d2l.TimeMachine(batch_size =1024 , num_steps =32)
rnn =RNN(num_inputs =len(data .vocab), num_hiddens =32)
model =RNNLM(rnn, vocab_size =len(data .vocab), lr =1)
model .predict( 'it has ',20, data .vocab)
'it hasoadd dd dd dd dd dd '
Next, we train our model, leveraging the high-level API.
trainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
Comparedwith Section9.5 , thismodelachievescomparableperplexity, butrunsfasterdue
to the optimized implementations. As before, we can generate predicted tokens following
the specified prefix string.
model .predict( 'it has ',20, data .vocab, d2l .try_gpu())
'it has and the trave the t '
9.6.3Summary
362 Recurrent Neural Networks
143High-level APIs in deep learning frameworks provide implementations of standard RNNs.
Theselibrarieshelpyoutoavoidwastingtimereimplementingstandardmodels. Moreover,
framework implementations are often highly optimized, leading to significant (computa-
tional) performance gains when compared with implementations from scratch.
9.6.4Exercises
1.Can you make the RNN model overfit using the high-level APIs?
2.Implement the autoregressive model of Section 9.1 using an RNN.
Discussions143.
9.7BackpropagationThroughTime
Ifyoucompletedtheexercisesin Section9.5 , youwouldhaveseenthatgradientclippingis
vitalforpreventingtheoccasionalmassivegradientsfromdestabilizingtraining. Wehinted
that the exploding gradients stem from backpropagating across long sequences. Before in-
troducing a slew of modern RNN architectures, let‚Äôs take a closer look at how backprop-
agationworks in sequence models in mathematical detail. Hopefully, this discussion will
bring some precision to the notion of vanishing andexploding gradients. If you recall our
discussion of forward and backward propagation through computational graphs when we
introduced MLPs in Section 5.3 , then forward propagation in RNNs should be relatively
straightforward. Applying backpropagation in RNNs is called backpropagation through
time(Werbos, 1990 ). This procedure requires us to expand (or unroll) the computational
graph of an RNN one time step at a time. The unrolled RNN is essentially a feedforward
neural network with the special property that the same parameters are repeated throughout
the unrolled network, appearing at each time step. Then, just as in any feedforward neural
network, we can apply the chain rule, backpropagating gradients through the unrolled net.
The gradient with respect to each parameter must be summed across all places that the pa-
rameteroccursintheunrollednet. Handlingsuchweighttyingshouldbefamiliarfromour
chapters on convolutional neural networks.
Complications arise because sequences can be rather long. It is not unusual to work with
text sequences consisting of over a thousand tokens. Note that this poses problems both
from a computational (too much memory) and optimization (numerical instability) stand-
point. Input from the first step passes through over 1000 matrix products before arriving
at the output, and another 1000 matrix products are required to compute the gradient. We
now analyze what can go wrong and how to address it in practice.
9.7.1Analysisof Gradients in RNNs
We start with a simplified model of how an RNN works. This model ignores details about
thespecificsofthehiddenstateandhowitisupdated. Themathematicalnotationheredoes
363 Backpropagation Through Time
notexplicitlydistinguishscalars, vectors, andmatrices. Wearejusttryingtodevelopsome
intuition. In this simplified model, we denote ‚Ñéùë°as the hidden state, ùë•ùë°as input, and ùëúùë°as
output at time step ùë°. Recall our discussions in Section 9.4.2 that the input and the hidden
statecanbeconcatenatedbeforebeingmultipliedbyoneweightvariableinthehiddenlayer.
Thus, we use ùë§handùë§oto indicate the weights of the hidden layer and the output layer,
respectively. As a result, the hidden states and outputs at each time step are
‚Ñéùë°=ùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫,
ùëúùë°=ùëî¬π‚Ñéùë°,ùë§o¬∫,(9.7.1)
whereùëìandùëîare transformations of the hidden layer and the output layer, respectively.
Hence, we have a chain of values f...,¬πùë•ùë° 1,‚Ñéùë° 1,ùëúùë° 1¬∫,¬πùë•ùë°,‚Ñéùë°,ùëúùë°¬∫,...gthat depend on
each other via recurrent computation. The forward propagation is fairly straightforward.
All we need is to loop through the ¬πùë•ùë°,‚Ñéùë°,ùëúùë°¬∫triples one time step at a time. The discrep-
ancy between output ùëúùë°and the desired target ùë¶ùë°is then evaluated by an objective function
across all the ùëátime steps as
ùêø¬πùë•1,...,ùë•ùëá,ùë¶1,...,ùë¶ùëá,ùë§h,ùë§o¬∫=1
ùëáùëá√ï
ùë°=1ùëô¬πùë¶ùë°,ùëúùë°¬∫. (9.7.2)
For backpropagation, matters are a bit trickier, especially when we compute the gradients
with regard to the parameters ùë§hof the objective function ùêø. To be specific, by the chain
rule,
ùúïùêø
ùúïùë§h=1
ùëáùëá√ï
ùë°=1ùúïùëô¬πùë¶ùë°,ùëúùë°¬∫
ùúïùë§h
=1
ùëáùëá√ï
ùë°=1ùúïùëô¬πùë¶ùë°,ùëúùë°¬∫
ùúïùëúùë°ùúïùëî¬π‚Ñéùë°,ùë§o¬∫
ùúï‚Ñéùë°ùúï‚Ñéùë°
ùúïùë§h.(9.7.3)
The first and the second factors of the product in (9.7.3 )are easy to compute. The third
factorùúï‚Ñéùë°¬ùùúïùë§his where things get tricky, since we need to recurrently compute the effect
of the parameter ùë§hon‚Ñéùë°. According to the recurrent computation in (9.7.1 ),‚Ñéùë°depends
on both‚Ñéùë° 1andùë§h, where computation of ‚Ñéùë° 1also depends on ùë§h. Thus, evaluating the
total derivate of ‚Ñéùë°with respect to ùë§husing the chain rule yields
ùúï‚Ñéùë°
ùúïùë§h=ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúïùë§h¬∏ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúï‚Ñéùë° 1ùúï‚Ñéùë° 1
ùúïùë§h. (9.7.4)
To derive the above gradient, assume that we have three sequences fùëéùë°g,fùëèùë°g,fùëêùë°gsatisfy-
ingùëé0=0andùëéùë°=ùëèùë°¬∏ùëêùë°ùëéùë° 1forùë°=1,2,.... Then forùë°1, it is easy to show
ùëéùë°=ùëèùë°¬∏ùë° 1√ï
ùëñ=1¬©¬≠
¬´ùë°√ñ
ùëó=ùëñ¬∏1ùëêùëó¬™¬Æ
¬¨ùëèùëñ. (9.7.5)
364 Recurrent Neural Networks
By substituting ùëéùë°,ùëèùë°, andùëêùë°according to
ùëéùë°=ùúï‚Ñéùë°
ùúïùë§h,
ùëèùë°=ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúïùë§h,
ùëêùë°=ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúï‚Ñéùë° 1,(9.7.6)
the gradient computation in (9.7.4 )satisfiesùëéùë°=ùëèùë°¬∏ùëêùë°ùëéùë° 1. Thus, per (9.7.5 ), we can
remove the recurrent computation in (9.7.4 )with
ùúï‚Ñéùë°
ùúïùë§h=ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúïùë§h¬∏ùë° 1√ï
ùëñ=1¬©¬≠
¬´ùë°√ñ
ùëó=ùëñ¬∏1ùúïùëì¬πùë•ùëó,‚Ñéùëó 1,ùë§h¬∫
ùúï‚Ñéùëó 1¬™¬Æ
¬¨ùúïùëì¬πùë•ùëñ,‚Ñéùëñ 1,ùë§h¬∫
ùúïùë§h.(9.7.7)
While we can use the chain rule to compute ùúï‚Ñéùë°¬ùùúïùë§hrecursively, this chain can get very
long whenever ùë°is large. Let‚Äôs discuss a number of strategies for dealing with this prob-
lem.
Full Computation
One idea might be to compute the full sum in (9.7.7 ). However, this is very slow and
gradients can blow up, since subtle changes in the initial conditions can potentially affect
theoutcomealot. Thatis,wecouldseethingssimilartothebutterflyeffect,whereminimal
changes in the initial conditions lead to disproportionate changes in the outcome. This is
generally undesirable. After all, we are looking for robust estimators that generalize well.
Hence this strategy is almost never used in practice.
TruncatingTime Steps
Alternatively, we can truncate the sum in (9.7.7 )afterùúèsteps. This is what we have been
discussingsofar. Thisleadstoan approximation ofthetruegradient,simplybyterminating
the sum atùúï‚Ñéùë° ùúè¬ùùúïùë§h. In practice this works quite well. It is what is commonly referred
to as truncated backpropgation through time ( Jaeger, 2002 ). One of the consequences of
this is that the model focuses primarily on short-term influence rather than long-term con-
sequences. Thisisactually desirable ,sinceitbiasestheestimatetowardssimplerandmore
stable models.
RandomizedTruncation
Last, we can replace ùúï‚Ñéùë°¬ùùúïùë§hby a random variable which is correct in expectation but
truncates the sequence. This is achieved by using a sequence of ùúâùë°with predefined 0
ùúãùë°1, whereùëÉ¬πùúâùë°=0¬∫=1 ùúãùë°andùëÉ¬πùúâùë°=ùúã 1
ùë°¬∫=ùúãùë°, thusùê∏¬ªùúâùë°¬º=1. We use this to
replace the gradient ùúï‚Ñéùë°¬ùùúïùë§hin(9.7.4 )with
ùëßùë°=ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúïùë§h¬∏ùúâùë°ùúïùëì¬πùë•ùë°,‚Ñéùë° 1,ùë§h¬∫
ùúï‚Ñéùë° 1ùúï‚Ñéùë° 1
ùúïùë§h. (9.7.8)
365 Backpropagation Through Time
It follows from the definition of ùúâùë°thatùê∏¬ªùëßùë°¬º=ùúï‚Ñéùë°¬ùùúïùë§h. Wheneverùúâùë°=0the recurrent
computation terminates at that time step ùë°. This leads to a weighted sum of sequences of
varying lengths, where long sequences are rare but appropriately overweighted. This idea
was proposed by Tallec and Ollivier ( 2017).
ComparingStrategies
tFig. 9.7.1 Comparing strategies for computing gradients in RNNs. From top to bottom: randomized
truncation, regular truncation, and full computation.
Fig.9.7.1 illustratesthethreestrategieswhenanalyzingthefirstfewcharactersof TheTime
Machine using backpropagation through time for RNNs:
Thefirstrowistherandomizedtruncationthatpartitionsthetextintosegmentsofvarying
lengths.
The second row is the regular truncation that breaks the text into subsequences of the
same length. This is what we have been doing in RNN experiments.
The third row is the full backpropagation through time that leads to a computationally
infeasible expression.
Unfortunately, while appealing in theory, randomized truncation does not work much bet-
ter than regular truncation, most likely due to a number of factors. First, the effect of an
observation after a number of backpropagation steps into the past is quite sufficient to cap-
ture dependencies in practice. Second, the increased variance counteracts the fact that the
gradient is more accurate with more steps. Third, we actually wantmodels that have only
ashortrangeofinteractions. Hence, regularlytruncatedbackpropagationthroughtimehas
a slight regularizing effect that can be desirable.
9.7.2BackpropagationThroughTimein Detail
Afterdiscussingthegeneralprinciple,let‚Äôsdiscussbackpropagationthroughtimeindetail.
Incontrasttotheanalysisin Section9.7.1 ,inthefollowingwewillshowhowtocomputethe
gradients of the objective function with respect to all the decomposed model parameters.
To keep things simple, we consider an RNN without bias parameters, whose activation
function in the hidden layer uses the identity mapping ( ùúô¬πùë•¬∫=ùë•). For time step ùë°, let
the single example input and the target be xùë°2Rùëëandùë¶ùë°, respectively. The hidden state
hùë°2R‚Ñéand the output oùë°2Rùëûare computed as
hùë°=Whxxùë°¬∏Whhhùë° 1,
oùë°=Wqhhùë°,(9.7.9)
366 Recurrent Neural Networks
where Whx2R‚Ñéùëë,Whh2R‚Ñé‚Ñé, andWqh2Rùëû‚Ñéare the weight parameters. Denote
byùëô¬πoùë°,ùë¶ùë°¬∫the loss at time step ùë°. Our objective function, the loss over ùëátime steps from
the beginning of the sequence is thus
ùêø=1
ùëáùëá√ï
ùë°=1ùëô¬πoùë°,ùë¶ùë°¬∫. (9.7.10)
In order to visualize the dependencies among model variables and parameters during com-
putation of the RNN, we can draw a computational graph for the model, as shown in Fig.
9.7.2. For example, the computation of the hidden states of time step 3, h3, depends on
the model parameters WhxandWhh, the hidden state of the previous time step h2, and the
input of the current time step x3.
tFig. 9.7.2 Computational graph showing dependencies for an RNN model with three time steps.
Boxes represent variables (not shaded) or parameters (shaded) and circles represent
operators.
As just mentioned, the model parameters in Fig. 9.7.2 areWhx,Whh, andWqh. Gen-
erally, training this model requires gradient computation with respect to these parameters
ùúïùêø¬ùùúïWhx,ùúïùêø¬ùùúïWhh,andùúïùêø¬ùùúïWqh. Accordingtothedependenciesin Fig.9.7.2 ,wecan
traverse in the opposite direction of the arrows to calculate and store the gradients in turn.
To flexibly express the multiplication of matrices, vectors, and scalars of different shapes
in the chain rule, we continue to use the prod operator as described in Section 5.3 .
First of all, differentiating the objective function with respect to the model output at any
time stepùë°is fairly straightforward:
ùúïùêø
ùúïoùë°=ùúïùëô¬πoùë°,ùë¶ùë°¬∫
ùëáùúïoùë°2Rùëû. (9.7.11)
Now we can calculate the gradient of the objective with respect to the parameter Wqhin
the output layer: ùúïùêø¬ùùúïWqh2Rùëû‚Ñé. Based on Fig. 9.7.2 , the objective ùêødepends on Wqh
viao1,...,oùëá. Using the chain rule yields
ùúïùêø
ùúïWqh=ùëá√ï
ùë°=1prodùúïùêø
ùúïoùë°,ùúïoùë°
ùúïWqh
=ùëá√ï
ùë°=1ùúïùêø
ùúïoùë°h>
ùë°, (9.7.12)
whereùúïùêø¬ùùúïoùë°is given by (9.7.11 ).
Next, as shown in Fig. 9.7.2 , at the final time step ùëá, the objective function ùêødepends on
the hidden state hùëáonly via oùëá. Therefore, we can easily find the gradient ùúïùêø¬ùùúïhùëá2R‚Ñé
367 Backpropagation Through Time
using the chain rule:
ùúïùêø
ùúïhùëá=prodùúïùêø
ùúïoùëá,ùúïoùëá
ùúïhùëá
=W>
qhùúïùêø
ùúïoùëá. (9.7.13)
It gets trickier for any time step ùë° < ùëá, where the objective function ùêødepends on hùë°via
hùë°¬∏1andoùë°. According to the chain rule, the gradient of the hidden state ùúïùêø¬ùùúïhùë°2R‚Ñéat
any time step ùë° <ùëácan be recurrently computed as:
ùúïùêø
ùúïhùë°=prodùúïùêø
ùúïhùë°¬∏1,ùúïhùë°¬∏1
ùúïhùë°
¬∏prodùúïùêø
ùúïoùë°,ùúïoùë°
ùúïhùë°
=W>
hhùúïùêø
ùúïhùë°¬∏1¬∏W>
qhùúïùêø
ùúïoùë°.(9.7.14)
Foranalysis,expandingtherecurrentcomputationforanytimestep 1ùë°ùëágives
ùúïùêø
ùúïhùë°=ùëá√ï
ùëñ=ùë° W>
hhùëá ùëñW>
qhùúïùêø
ùúïoùëá¬∏ùë° ùëñ. (9.7.15)
We can see from (9.7.15 )that this simple linear example already exhibits some key prob-
lems of long sequence models: it involves potentially very large powers of W>
hh. In it,
eigenvalues smaller than 1 vanish and eigenvalues larger than 1 diverge. This is numer-
ically unstable, which manifests itself in the form of vanishing and exploding gradients.
Onewaytoaddressthisistotruncatethetimestepsatacomputationallyconvenientsizeas
discussedin Section9.7.1 . Inpractice,thistruncationcanalsobeeffectedbydetachingthe
gradient after a given number of time steps. Later on, we will see how more sophisticated
sequence models such as long short-term memory can alleviate this further.
Finally,Fig. 9.7.2 shows that the objective function ùêødepends on model parameters Whx
andWhhin the hidden layer via hidden states h1,...,hùëá. To compute gradients with
respect to such parameters ùúïùêø¬ùùúïWhx2R‚Ñéùëëandùúïùêø¬ùùúïWhh2R‚Ñé‚Ñé, we apply the chain
rule giving
ùúïùêø
ùúïWhx=ùëá√ï
ùë°=1prodùúïùêø
ùúïhùë°,ùúïhùë°
ùúïWhx
=ùëá√ï
ùë°=1ùúïùêø
ùúïhùë°x>
ùë°,
ùúïùêø
ùúïWhh=ùëá√ï
ùë°=1prodùúïùêø
ùúïhùë°,ùúïhùë°
ùúïWhh
=ùëá√ï
ùë°=1ùúïùêø
ùúïhùë°h>
ùë° 1,(9.7.16)
whereùúïùêø¬ùùúïhùë°which is recurrently computed by (9.7.13 )and(9.7.14 )is the key quantity
that affects the numerical stability.
Since backpropagation through time is the application of backpropagation in RNNs, as we
have explained in Section 5.3 , training RNNs alternates forward propagation with back-
propagation through time. Moreover, backpropagation through time computes and stores
the above gradients in turn. Specifically, stored intermediate values are reused to avoid du-
plicate calculations, such as storing ùúïùêø¬ùùúïhùë°to be used in computation of both ùúïùêø¬ùùúïWhx
andùúïùêø¬ùùúïWhh.
9.7.3Summary
368 Recurrent Neural Networks
144Backpropagation through time is merely an application of backpropagation to sequence
models with a hidden state. Truncation, such as regular or randomized, is needed for com-
putational convenience and numerical stability. High powers of matrices can lead to diver-
gent or vanishing eigenvalues. This manifests itself in the form of exploding or vanishing
gradients. For efficient computation, intermediate values are cached during backpropaga-
tion through time.
9.7.4Exercises
1.Assume that we have a symmetric matrix M2Rùëõùëõwith eigenvalues ùúÜùëñwhose cor-
responding eigenvectors are vùëñ(ùëñ=1,...,ùëõ). Without loss of generality, assume that
they are ordered in the order jùúÜùëñjjùúÜùëñ¬∏1j.
1.Show that Mùëòhas eigenvalues ùúÜùëò
ùëñ.
2.Provethatforarandomvector x2Rùëõ,withhighprobability Mùëòxwillbeverymuch
aligned with the eigenvector v1ofM. Formalize this statement.
3.What does the above result mean for gradients in RNNs?
2.Besides gradient clipping, can you think of any other methods to cope with gradient
explosion in recurrent neural networks?
Discussions144.
10 Modern Recurrent Neural Networks
The previous chapter introduced the key ideas behind recurrent neural networks (RNNs).
However, just as with convolutional neural networks, there has been a tremendous amount
of innovation in RNN architectures, culminating in several complex designs that have
proven successful in practice. In particular, the most popular designs feature mechanisms
for mitigating the notorious numerical instability faced by RNNs, as typified by vanishing
and exploding gradients. Recall that in Chapter 9 we dealt with exploding gradients by ap-
plying a blunt gradient clipping heuristic. Despite the efficacy of this hack, it leaves open
the problem of vanishing gradients.
Inthischapter,weintroducethekeyideasbehindthemostsuccessfulRNNarchitecturesfor
sequences, which stem from two papers. The first, Long Short-Term Memory (Hochreiter
and Schmidhuber, 1997 ), introduces the memory cell , a unit of computation that replaces
traditional nodes in the hidden layer of a network. With these memory cells, networks
are able to overcome difficulties with training encountered by earlier recurrent networks.
Intuitively, the memory cell avoids the vanishing gradient problem by keeping values in
each memory cell‚Äôs internal state cascading along a recurrent edge with weight 1 across
manysuccessivetimesteps. Asetofmultiplicativegateshelpthenetworktodeterminenot
only the inputs to allow into the memory state, but when the content of the memory state
should influence the model‚Äôs output.
The second paper, Bidirectional Recurrent Neural Networks (Schuster and Paliwal, 1997 ),
introducesanarchitectureinwhichinformationfromboththefuture(subsequenttimesteps)
and the past (preceding time steps) are used to determine the output at any point in the se-
quence. This is in contrast to previous networks, in which only past input can affect the
output. Bidirectional RNNs have become a mainstay for sequence labeling tasks in natu-
ral language processing, among a myriad of other tasks. Fortunately, the two innovations
arenotmutuallyexclusive,andhavebeensuccessfullycombinedforphonemeclassification
(GravesandSchmidhuber,2005 )andhandwritingrecognition( Gravesetal.,2008).
ThefirstsectionsinthischapterwillexplaintheLSTMarchitecture,alighter-weightversion
called the gated recurrent unit (GRU), the key ideas behind bidirectional RNNs and a brief
explanation of how RNN layers are stacked together to form deep RNNs. Subsequently,
we will explore the application of RNNs in sequence-to-sequence tasks, introducing ma-
chine translation along with key ideas such as encoder‚Äìdecoder architectures and beam
search.
369
370 Modern Recurrent Neural Networks
10.1Long Short-TermMemory (LSTM)
ShortlyafterthefirstElman-styleRNNsweretrainedusingbackpropagation( Elman,1990 ),
the problems of learning long-term dependencies (owing to vanishing and exploding gra-
dients) became salient, with Bengio and Hochreiter discussing the problem ( Bengioetal.,
1994,Hochreiter et al., 2001). Hochreiter had articulated this problem as early as 1991 in
hisMaster‚Äôsthesis,althoughtheresultswerenotwidelyknownbecausethethesiswaswrit-
teninGerman. Whilegradientclippinghelpswithexplodinggradients,handlingvanishing
gradientsappearstorequireamoreelaboratesolution. Oneofthefirstandmostsuccessful
techniquesforaddressingvanishinggradientscameintheformofthelongshort-termmem-
ory (LSTM) model due to Hochreiter and Schmidhuber ( 1997). LSTMs resemble standard
recurrent neural networks but here each ordinary recurrent node is replaced by a memory
cell. Each memory cell contains an internal state , i.e., a node with a self-connected re-
current edge of fixed weight 1, ensuring that the gradient can pass across many time steps
without vanishing or exploding.
The term ‚Äúlong short-term memory‚Äù comes from the following intuition. Simple recurrent
neuralnetworkshave long-termmemory intheformofweights. Theweightschangeslowly
during training, encoding general knowledge about the data. They also have short-term
memory in the form of ephemeral activations, which pass from each node to successive
nodes. The LSTM model introduces an intermediate type of storage via the memory cell.
A memory cell is a composite unit, built from simpler nodes in a specific connectivity
pattern, with the novel inclusion of multiplicative nodes.
import torch
from torch import nn
from d2l import torch asd2l
10.1.1Gated Memory Cell
Each memory cell is equipped with an internal state and a number of multiplicative gates
that determine whether (i) a given input should impact the internal state (the input gate ),
(ii) the internal state should be flushed to 0(theforgetgate ), and (iii) the internal state of a
given neuron should be allowed to impact the cell‚Äôs output (the outputgate).
GatedHidden State
The key distinction between vanilla RNNs and LSTMs is that the latter support gating of
the hidden state. This means that we have dedicated mechanisms for when a hidden state
should be updated and also for when it should be reset. These mechanisms are learned and
theyaddresstheconcernslistedabove. Forinstance,ifthefirsttokenisofgreatimportance
we will learn not to update the hidden state after the first observation. Likewise, we will
learn to skip irrelevant temporary observations. Last, we will learn to reset the latent state
whenever needed. We discuss this in detail below.
371 Long Short-Term Memory (LSTM)
InputGate, ForgetGate, and Output Gate
The data feeding into the LSTM gates are the input at the current time step and the hidden
state of the previous time step, as illustrated in Fig. 10.1.1 . Three fully connected layers
withsigmoidactivationfunctionscomputethevaluesoftheinput, forget, andoutputgates.
As a result of the sigmoid activation, all values of the three gates are in the range of ¬π0,1¬∫.
Additionally, we require an input node , typically computed with a tanhactivation func-
tion. Intuitively, the input gate determines how much of the input node‚Äôs value should be
addedtothecurrentmemorycellinternalstate. The forgetgate determineswhethertokeep
the current value of the memory or flush it. And the output gate determines whether the
memory cell should influence the output at the current time step.
tFig. 10.1.1 Computing the input gate, the forget gate, and the output gate in an LSTM model.
Mathematically, suppose that there are ‚Ñéhidden units, the batch size is ùëõ, and the number
of inputs isùëë. Thus, the input is Xùë°2Rùëõùëëand the hidden state of the previous time step
isHùë° 12Rùëõ‚Ñé. Correspondingly, the gates at time step ùë°are defined as follows: the input
gate is Iùë°2Rùëõ‚Ñé, the forget gate is Fùë°2Rùëõ‚Ñé, and the output gate is Oùë°2Rùëõ‚Ñé. They
are calculated as follows:
Iùë°=ùúé¬πXùë°Wxi¬∏Hùë° 1Whi¬∏bi¬∫,
Fùë°=ùúé¬πXùë°Wxf¬∏Hùë° 1Whf¬∏bf¬∫,
Oùë°=ùúé¬πXùë°Wxo¬∏Hùë° 1Who¬∏bo¬∫,(10.1.1)
where Wxi,Wxf,Wxo2Rùëë‚ÑéandWhi,Whf,Who2R‚Ñé‚Ñéare weight parameters and
bi,bf,bo2R1‚Ñéare bias parameters. Note that broadcasting (see Section 2.1.4 ) is trig-
gered during the summation. We use sigmoid functions (as introduced in Section 5.1 ) to
map the input values to the interval ¬π0,1¬∫.
InputNode
Nextwedesignthememorycell. Sincewehavenotspecifiedtheactionofthevariousgates
yet, we first introduce the input node ÀúCùë°2Rùëõ‚Ñé. Its computation is similar to that of the
three gates described above, but uses a tanhfunction with a value range for ¬π 1,1¬∫as the
activation function. This leads to the following equation at time step ùë°:
ÀúCùë°=tanh¬πXùë°Wxc¬∏Hùë° 1Whc¬∏bc¬∫, (10.1.2)
372 Modern Recurrent Neural Networks
where Wxc2Rùëë‚ÑéandWhc2R‚Ñé‚Ñéare weight parameters and bc2R1‚Ñéis a bias
parameter.
A quick illustration of the input node is shown in Fig. 10.1.2 .
tFig. 10.1.2 Computing the input node in an LSTM model.
Memory Cell Internal State
In LSTMs, the input gate Iùë°governs how much we take new data into account via ÀúCùë°and
theforgetgate Fùë°addresseshowmuchoftheoldcellinternalstate Cùë° 12Rùëõ‚Ñéweretain.
Using the Hadamard (elementwise) product operator we arrive at the following update
equation:
Cùë°=Fùë°Cùë° 1¬∏Iùë°ÀúCùë°. (10.1.3)
If the forget gate is always 1 and the input gate is always 0, the memory cell internal state
Cùë° 1will remain constant forever, passing unchanged to each subsequent time step. How-
ever, input gates and forget gates give the model the flexibility of being able to learn when
to keep this value unchanged and when to perturb it in response to subsequent inputs. In
practice, this design alleviates the vanishing gradient problem, resulting in models that are
much easier to train, especially when facing datasets with long sequence lengths.
We thus arrive at the flow diagram in Fig. 10.1.3 .
Hidden State
Last,weneedtodefinehowtocomputetheoutputofthememorycell, i.e., thehiddenstate
Hùë°2Rùëõ‚Ñé, as seen by other layers. This is where the output gate comes into play. In
LSTMs, we first apply tanhto the memory cell internal state and then apply another point-
wise multiplication, this time with the output gate. This ensures that the values of Hùë°are
always in the interval ¬π 1,1¬∫:
Hùë°=Oùë°tanh¬πCùë°¬∫. (10.1.4)
Whenever the output gate is close to 1, we allow the memory cell internal state to impact
thesubsequentlayersuninhibited, whereasforoutputgatevaluescloseto0, wepreventthe
373 Long Short-Term Memory (LSTM)
tFig. 10.1.3 Computing the memory cell internal state in an LSTM model.
current memory from impacting other layers of the network at the current time step. Note
that a memory cell can accrue information across many time steps without impacting the
rest of the network (as long as the output gate takes values close to 0), and then suddenly
impact the network at a subsequent time step as soon as the output gate flips from values
closeto0tovaluescloseto1. Fig.10.1.4 hasagraphicalillustrationofthedataflow.
tFig. 10.1.4 Computing the hidden state in an LSTM model.
10.1.2Implementation fromScratch
Now let‚Äôs implement an LSTM from scratch. As same as the experiments in Section 9.5 ,
we first load The TimeMachine dataset.
InitializingModel Parameters
Next, we need to define and initialize the model parameters. As previously, the hyperpa-
rameter num_hiddens dictates the number of hidden units. We initialize weights following
a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.
class LSTMScratch (d2l .Module):
def __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):
(continues on next page)
374 Modern Recurrent Neural Networks
(continued from previous page)
super ().__init__ ()
self .save_hyperparameters()
init_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)
triple =lambda : (init_weight(num_inputs, num_hiddens),
init_weight(num_hiddens, num_hiddens),
nn.Parameter(torch .zeros(num_hiddens)))
self .W_xi, self .W_hi, self .b_i =triple() # Input gate
self .W_xf, self .W_hf, self .b_f =triple() # Forget gate
self .W_xo, self .W_ho, self .b_o =triple() # Output gate
self .W_xc, self .W_hc, self .b_c =triple() # Input node
Theactualmodelisdefinedasdescribedabove,consistingofthreegatesandaninputnode.
Note that only the hidden state is passed to the output layer.
@d2l .add_to_class(LSTMScratch)
def forward (self , inputs, H_C =None ):
ifH_C isNone :
# Initial state with shape: (batch_size, num_hiddens)
H=torch .zeros((inputs .shape[ 1],self .num_hiddens),
device =inputs .device)
C=torch .zeros((inputs .shape[ 1],self .num_hiddens),
device =inputs .device)
else :
H, C =H_C
outputs =[]
for Xininputs:
I=torch .sigmoid(torch .matmul(X, self .W_xi) +
torch .matmul(H, self .W_hi) +self .b_i)
F=torch .sigmoid(torch .matmul(X, self .W_xf) +
torch .matmul(H, self .W_hf) +self .b_f)
O=torch .sigmoid(torch .matmul(X, self .W_xo) +
torch .matmul(H, self .W_ho) +self .b_o)
C_tilde =torch .tanh(torch .matmul(X, self .W_xc) +
torch .matmul(H, self .W_hc) +self .b_c)
C=F*C+I*C_tilde
H=O*torch .tanh(C)
outputs .append(H)
return outputs, (H, C)
Trainingand Prediction
Let‚ÄôstrainanLSTMmodelbyinstantiatingthe RNNLMScratch classfrom Section9.5 .
data =d2l.TimeMachine(batch_size =1024 , num_steps =32)
lstm =LSTMScratch(num_inputs =len(data .vocab), num_hiddens =32)
model =d2l.RNNLMScratch(lstm, vocab_size =len(data .vocab), lr =4)
trainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
10.1.3ConciseImplementation
375 Long Short-Term Memory (LSTM)
Using high-level APIs, we can directly instantiate an LSTM model. This encapsulates
all the configuration details that we made explicit above. The code is significantly faster
as it uses compiled operators rather than Python for many details that we spelled out be-
fore.
class LSTM (d2l .RNN):
def __init__ (self , num_inputs, num_hiddens):
d2l.Module .__init__ (self )
self .save_hyperparameters()
self .rnn =nn.LSTM(num_inputs, num_hiddens)
def forward (self , inputs, H_C =None ):
return self .rnn(inputs, H_C)
lstm =LSTM(num_inputs =len(data .vocab), num_hiddens =32)
model =d2l.RNNLM(lstm, vocab_size =len(data .vocab), lr =4)
trainer .fit(model, data)
model .predict( 'it has ',20, data .vocab, d2l .try_gpu())
'it has a the time travelly '
LSTMs are the prototypical latent variable autoregressive model with nontrivial state con-
trol. Many variants thereof have been proposed over the years, e.g., multiple layers, resid-
ual connections, different types of regularization. However, training LSTMs and other
376 Modern Recurrent Neural Networks
145sequence models (such as GRUs) is quite costly because of the long range dependency of
the sequence. Later we will encounter alternative models such as Transformers that can be
used in some cases.
10.1.4Summary
WhileLSTMswerepublishedin1997,theyrosetogreatprominencewithsomevictoriesin
prediction competitions in the mid-2000s, and became the dominant models for sequence
learning from 2011 until the rise of Transformer models, starting in 2017. Even Tran-
formers owe some of their key ideas to architecture design innovations introduced by the
LSTM.
LSTMshavethreetypesofgates: inputgates,forgetgates,andoutputgatesthatcontrolthe
flow of information. The hidden layer output of LSTM includes the hidden state and the
memory cell internal state. Only the hidden state is passed into the output layer while the
memory cell internal state remains entirely internal. LSTMs can alleviate vanishing and
exploding gradients.
10.1.5Exercises
1.Adjustthehyperparametersandanalyzetheirinfluenceonrunningtime,perplexity,and
the output sequence.
2.How would you need to change the model to generate proper words rather than just
sequences of characters?
3.Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hid-
den dimension. Pay special attention to the training and inference cost.
4.Since the candidate memory cell ensures that the value range is between  1and1by
using the tanhfunction, why does the hidden state need to use the tanhfunction again
to ensure that the output value range is between  1and1?
5.Implement an LSTM model for time series prediction rather than character sequence
prediction.
Discussions145.
10.2GatedRecurrentUnits(GRU)
As RNNs and particularly the LSTM architecture ( Section 10.1 ) rapidly gained popularity
during the 2010s, a number of researchers began to experiment with simplified architec-
tures in hopes of retaining the key idea of incorporating an internal state and multiplicative
gating mechanisms but with the aim of speeding up computation. The gated recurrent unit
377 Gated Recurrent Units (GRU)
(GRU) (Choet al., 2014) offered a streamlined version of the LSTM memory cell that of-
ten achieves comparable performance but with the advantage of being faster to compute
(Chungetal., 2014).
import torch
from torch import nn
from d2l import torch asd2l
10.2.1ResetGate and UpdateGate
Here, the LSTM‚Äôs three gates are replaced by two: the reset gate and theupdate gate . As
with LSTMs, these gates are given sigmoid activations, forcing their values to lie in the
interval¬π0,1¬∫. Intuitively, the reset gate controls how much of the previous state we might
still want to remember. Likewise, an update gate would allow us to control how much of
thenewstateisjustacopyoftheoldone. Fig.10.2.1 illustratestheinputsforboththereset
and update gates in a GRU, given the input of the current time step and the hidden state
of the previous time step. The outputs of the gates are given by two fully connected layers
with a sigmoid activation function.
tFig. 10.2.1 Computing the reset gate and the update gate in a GRU model.
Mathematically, for a given time step ùë°, suppose that the input is a minibatch Xùë°2Rùëõùëë
(number of examples =ùëõ; number of inputs =ùëë) and the hidden state of the previous time
step is Hùë° 12Rùëõ‚Ñé(number of hidden units =‚Ñé). Then the reset gate Rùë°2Rùëõ‚Ñéand
update gate Zùë°2Rùëõ‚Ñéare computed as follows:
Rùë°=ùúé¬πXùë°Wxr¬∏Hùë° 1Whr¬∏br¬∫,
Zùë°=ùúé¬πXùë°Wxz¬∏Hùë° 1Whz¬∏bz¬∫,(10.2.1)
whereWxr,Wxz2Rùëë‚ÑéandWhr,Whz2R‚Ñé‚Ñéareweightparametersand br,bz2R1‚Ñé
are bias parameters.
10.2.2Candidate Hidden State
378 Modern Recurrent Neural Networks
Next,weintegratetheresetgate Rùë°withtheregularupdatingmechanismin (9.4.5 ),leading
to the following candidatehidden state ÀúHùë°2Rùëõ‚Ñéat time stepùë°:
ÀúHùë°=tanh¬πXùë°Wxh¬∏¬πRùë°Hùë° 1¬∫Whh¬∏bh¬∫, (10.2.2)
whereWxh2Rùëë‚ÑéandWhh2R‚Ñé‚Ñéareweightparameters, bh2R1‚Ñéisthebias,andthe
symbolis the Hadamard (elementwise) product operator. Here we use a tanh activation
function.
The result is a candidate , since we still need to incorporate the action of the update gate.
Comparing with (9.4.5 ), the influence of the previous states can now be reduced with the
elementwise multiplication of Rùë°andHùë° 1in(10.2.2 ). Whenever the entries in the reset
gateRùë°are close to 1, we recover a vanilla RNN such as that in (9.4.5 ). For all entries of
the reset gate Rùë°that are close to 0, the candidate hidden state is the result of an MLP with
Xùë°as input. Any pre-existing hidden state is thus resetto defaults.
Fig. 10.2.2 illustrates the computational flow after applying the reset gate.
tFig. 10.2.2 Computing the candidate hidden state in a GRU model.
10.2.3HiddenState
Finally, we need to incorporate the effect of the update gate Zùë°. This determines the extent
to which the new hidden state Hùë°2Rùëõ‚Ñématches the old state Hùë° 1compared with how
much it resembles the new candidate state ÀúHùë°. The update gate Zùë°can be used for this
purpose, simply by taking elementwise convex combinations of Hùë° 1and ÀúHùë°. This leads
to the final update equation for the GRU:
Hùë°=Zùë°Hùë° 1¬∏¬π1 Zùë°¬∫ ÀúHùë°. (10.2.3)
Whenever the update gate Zùë°is close to 1, we simply retain the old state. In this case
the information from Xùë°is ignored, effectively skipping time step ùë°in the dependency
chain. By contrast, whenever Zùë°is close to 0, the new latent state Hùë°approaches the
candidate latent state ÀúHùë°.Fig. 10.2.3 shows the computational flow after the update gate is
in action.
In summary, GRUs have the following two distinguishing features:
379 Gated Recurrent Units (GRU)
tFig. 10.2.3 Computing the hidden state in a GRU model.
Reset gates help capture short-term dependencies in sequences.
Update gates help capture long-term dependencies in sequences.
10.2.4Implementation fromScratch
To gain a better understanding of the GRU model, let‚Äôs implement it from scratch.
InitializingModel Parameters
The first step is to initialize the model parameters. We draw the weights from a Gaussian
distribution with standard deviation to be sigmaand set the bias to 0. The hyperparameter
num_hiddens defines the number of hidden units. We instantiate all weights and biases
relating to the update gate, the reset gate, and the candidate hidden state.
class GRUScratch (d2l .Module):
def __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
init_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)
triple =lambda : (init_weight(num_inputs, num_hiddens),
init_weight(num_hiddens, num_hiddens),
nn.Parameter(torch .zeros(num_hiddens)))
self .W_xz, self .W_hz, self .b_z =triple() # Update gate
self .W_xr, self .W_hr, self .b_r =triple() # Reset gate
self .W_xh, self .W_hh, self .b_h =triple() # Candidate hidden state
Definingthe Model
Now we are ready to define the GRU forward computation. Its structure is the same as that
of the basic RNN cell, except that the update equations are more complex.
@d2l .add_to_class(GRUScratch)
def forward (self , inputs, H =None ):
(continues on next page)
380 Modern Recurrent Neural Networks
(continued from previous page)
ifHisNone :
# Initial state with shape: (batch_size, num_hiddens)
H=torch .zeros((inputs .shape[ 1],self .num_hiddens),
device =inputs .device)
outputs =[]
for Xininputs:
Z=torch .sigmoid(torch .matmul(X, self .W_xz) +
torch .matmul(H, self .W_hz) +self .b_z)
R=torch .sigmoid(torch .matmul(X, self .W_xr) +
torch .matmul(H, self .W_hr) +self .b_r)
H_tilde =torch .tanh(torch .matmul(X, self .W_xh) +
torch .matmul(R *H,self .W_hh) +self .b_h)
H=Z*H+(1-Z)*H_tilde
outputs .append(H)
return outputs, H
Training
Trainingalanguagemodelon TheTimeMachine datasetworksinexactlythesamemanner
as inSection 9.5 .
data =d2l.TimeMachine(batch_size =1024 , num_steps =32)
gru =GRUScratch(num_inputs =len(data .vocab), num_hiddens =32)
model =d2l.RNNLMScratch(gru, vocab_size =len(data .vocab), lr =4)
trainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
10.2.5ConciseImplementation
In high-level APIs, we can directly instantiate a GRU model. This encapsulates all the
configuration detail that we made explicit above.
class GRU(d2l .RNN):
def __init__ (self , num_inputs, num_hiddens):
d2l.Module .__init__ (self )
self .save_hyperparameters()
self .rnn =nn.GRU(num_inputs, num_hiddens)
381 Gated Recurrent Units (GRU)
146ThecodeissignificantlyfasterintrainingasitusescompiledoperatorsratherthanPython.
gru =GRU(num_inputs =len(data .vocab), num_hiddens =32)
model =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =4)
trainer .fit(model, data)
After training, we print out the perplexity on the training set and the predicted sequence
following the provided prefix.
model .predict( 'it has ',20, data .vocab, d2l .try_gpu())
'it has so it and the time '
10.2.6Summary
Compared with LSTMs, GRUs achieve similar performance but tend to be lighter com-
putationally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and
GRUs,canbettercapturedependenciesforsequenceswithlargetimestepdistances. GRUs
containbasicRNNsastheirextremecasewhenevertheresetgateisswitchedon. Theycan
also skip subsequences by turning on the update gate.
10.2.7Exercises
1.Assume that we only want to use the input at time step ùë°0to predict the output at time
stepùë° >ùë°0. What are the best values for the reset and update gates for each time step?
2.Adjustthehyperparametersandanalyzetheirinfluenceonrunningtime,perplexity,and
the output sequence.
3.Compare runtime, perplexity, and the output strings for rnn.RNN andrnn.GRU imple-
mentations with each other.
4.Whathappensifyouimplementonlyparts ofaGRU,e.g., withonlyareset gateoronly
an update gate?
Discussions146.
382 Modern Recurrent Neural Networks
10.3Deep RecurrentNeuralNetworks
Upuntilnow,wehavefocusedondefiningnetworksconsistingofasequenceinput,asingle
hidden RNN layer, and an output layer. Despite having just one hidden layer between the
inputatanytimestepandthecorrespondingoutput,thereisasenseinwhichthesenetworks
are deep. Inputs from the first time step can influence the outputs at the final time step
ùëá(often 100s or 1000s of steps later). These inputs pass through ùëáapplications of the
recurrent layer before reaching the final output. However, we often also wish to retain the
ability to express complex relationships between the inputs at a given time step and the
outputs at that same time step. Thus we often construct RNNs that are deep not only in the
timedirectionbutalsointheinput-to-outputdirection. Thisispreciselythenotionofdepth
that we have already encountered in our development of MLPs and deep CNNs.
The standard method for building this sort of deep RNN is strikingly simple: we stack
the RNNs on top of each other. Given a sequence of length ùëá, the first RNN produces a
sequence of outputs, also of length ùëá. These, in turn, constitute the inputs to the next RNN
layer. Inthisshortsection,weillustratethisdesignpatternandpresentasimpleexamplefor
howtocodeupsuchstackedRNNs. Below,in Fig.10.3.1 ,weillustrateadeepRNNwith ùêø
hidden layers. Each hidden state operates on a sequential input and produces a sequential
output. Moreover, any RNN cell (white box in Fig. 10.3.1 ) at each time step depends on
both the same layer‚Äôs value at the previous time step and the previous layer‚Äôs value at the
same time step.
tFig. 10.3.1 Architecture of a deep RNN.
Formally, suppose that we have a minibatch input Xùë°2Rùëõùëë(number of examples =ùëõ;
number of inputs in each example =ùëë) at time step ùë°. At the same time step, let the hidden
state of theùëôthhidden layer ( ùëô=1,...,ùêø) beH¬πùëô¬∫
ùë°2Rùëõ‚Ñé(number of hidden units =‚Ñé)
and the output layer variable be Oùë°2Rùëõùëû(number of outputs: ùëû). Setting H¬π0¬∫
ùë°=Xùë°,
the hidden state of the ùëôthhidden layer that uses the activation function ùúôùëôis calculated as
follows:
H¬πùëô¬∫
ùë°=ùúôùëô¬πH¬πùëô 1¬∫
ùë°W¬πùëô¬∫
xh¬∏H¬πùëô¬∫
ùë° 1W¬πùëô¬∫
hh¬∏b¬πùëô¬∫
h¬∫, (10.3.1)
383 Deep Recurrent Neural Networks
wheretheweights W¬πùëô¬∫
xh2R‚Ñé‚ÑéandW¬πùëô¬∫
hh2R‚Ñé‚Ñé, togetherwiththebias b¬πùëô¬∫
h2R1‚Ñé, are
the model parameters of the ùëôthhidden layer.
At the end, the calculation of the output layer is only based on the hidden state of the final
ùêøthhidden layer:
Oùë°=H¬πùêø¬∫
ùë°Whq¬∏bq, (10.3.2)
where the weight Whq2R‚Ñéùëûand the bias bq2R1ùëûare the model parameters of the
output layer.
JustaswithMLPs, thenumberofhiddenlayers ùêøandthenumberofhiddenunits ‚Ñéarehy-
perparametersthatwecantune. CommonRNNlayerwidths( ‚Ñé)areintherange¬π64,2056¬∫,
and common depths ( ùêø) are in the range¬π1,8¬∫. In addition, we can easily get a deep-gated
RNN by replacing the hidden state computation in (10.3.1 )with that from an LSTM or a
GRU.
import torch
from torch import nn
from d2l import torch asd2l
10.3.1Implementation fromScratch
To implement a multilayer RNN from scratch, we can treat each layer as an RNNScratch
instance with its own learnable parameters.
class StackedRNNScratch (d2l .Module):
def __init__ (self , num_inputs, num_hiddens, num_layers, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .rnns =nn.Sequential( *[d2l .RNNScratch(
num_inputs ifi==0else num_hiddens, num_hiddens, sigma)
for iinrange (num_layers)])
Themultilayerforwardcomputationsimplyperformsforwardcomputationlayerbylayer.
@d2l .add_to_class(StackedRNNScratch)
def forward (self , inputs, Hs =None ):
outputs =inputs
ifHsisNone : Hs =[None ]*self .num_layers
for iinrange (self .num_layers):
outputs, Hs[i] =self .rnns[i](outputs, Hs[i])
outputs =torch .stack(outputs, 0)
return outputs, Hs
As an example, we train a deep GRU model on The Time Machine dataset (same as in
Section 9.5 ). To keep things simple we set the number of layers to 2.
384 Modern Recurrent Neural Networks
data =d2l.TimeMachine(batch_size =1024 , num_steps =32)
rnn_block =StackedRNNScratch(num_inputs =len(data .vocab),
num_hiddens =32, num_layers =2)
model =d2l.RNNLMScratch(rnn_block, vocab_size =len(data .vocab), lr =2)
trainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
10.3.2ConciseImplementation
Fortunatelymanyof thelogisticaldetails requiredtoimplement multiplelayersofanRNN
are readily available in high-level APIs. Our concise implementation will use such built-
in functionalities. The code generalizes the one we used previously in Section 10.2 , let-
ting us specify the number of layers explicitly rather than picking the default of only one
layer.
class GRU(d2l .RNN): #@save
"""The multilayer GRU model."""
def __init__ (self , num_inputs, num_hiddens, num_layers, dropout =0):
d2l.Module .__init__ (self )
self .save_hyperparameters()
self .rnn =nn.GRU(num_inputs, num_hiddens, num_layers,
dropout =dropout)
The architectural decisions such as choosing hyperparameters are very similar to those of
Section 10.2 . We pick the same number of inputs and outputs as we have distinct tokens,
i.e.,vocab_size . Thenumberofhiddenunitsisstill32. Theonlydifferenceisthatwenow
select a nontrivial number of hidden layers by specifying the value of num_layers .
gru =GRU(num_inputs =len(data .vocab), num_hiddens =32, num_layers =2)
model =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =2)
trainer .fit(model, data)
model .predict( 'it has ',20, data .vocab, d2l .try_gpu())
'it has for and the time th '
385 Bidirectional Recurrent Neural Networks
14710.3.3Summary
In deep RNNs, the hidden state information is passed to the next time step of the current
layer and the current time step of the next layer. There exist many different flavors of
deep RNNs, such as LSTMs, GRUs, or vanilla RNNs. Conveniently, these models are
all available as parts of the high-level APIs of deep learning frameworks. Initialization of
models requires care. Overall, deep RNNs require considerable amount of work (such as
learning rate and clipping) to ensure proper convergence.
10.3.4Exercises
1.Replace the GRU by an LSTM and compare the accuracy and training speed.
2.Increase the training data to include multiple books. How low can you go on the per-
plexity scale?
3.Would you want to combine sources of different authors when modeling text? Why is
this a good idea? What could go wrong?
Discussions147.
10.4BidirectionalRecurrentNeuralNetworks
Sofar,ourworkingexampleofasequencelearningtaskhasbeenlanguagemodeling,where
we aim to predict the next token given all previous tokens in a sequence. In this scenario,
wewishonlytoconditionupontheleftwardcontext,andthustheunidirectionalchainingof
astandardRNNseemsappropriate. However,therearemanyothersequencelearningtasks
contexts where it is perfectly fine to condition the prediction at every time step on both the
leftward and the rightward context. Consider, for example, part of speech detection. Why
shouldn‚Äôt we take the context in both directions into account when assessing the part of
speech associated with a given word?
Another common task‚Äîoften useful as a pretraining exercise prior to fine-tuning a model
on an actual task of interest‚Äîis to mask out random tokens in a text document and then
386 Modern Recurrent Neural Networks
to train a sequence model to predict the values of the missing tokens. Note that depend-
ing on what comes after the blank, the likely value of the missing token changes dramati-
cally:
I am ___.
I am ___hungry.
I am ___hungry, and I can eat half a pig.
In the first sentence ‚Äúhappy‚Äù seems to be a likely candidate. The words ‚Äúnot‚Äù and ‚Äúvery‚Äù
seem plausible in the second sentence, but ‚Äúnot‚Äù seems incompatible with the third sen-
tences.
Fortunately, a simple technique transforms any unidirectional RNN into a bidirectional
RNN (Schuster and Paliwal, 1997 ). We simply implement two unidirectional RNN layers
chained together in opposite directions and acting on the same input ( Fig. 10.4.1 ). For
the first RNN layer, the first input is x1and the last input is xùëá, but for the second RNN
layer,thefirstinputis xùëáandthelastinputis x1. Toproducetheoutputofthisbidirectional
RNNlayer,wesimplyconcatenatetogetherthecorrespondingoutputsofthetwounderlying
unidirectional RNN layers.
tFig. 10.4.1 Architecture of a bidirectional RNN.
Formally for any time step ùë°, we consider a minibatch input Xùë°2Rùëõùëë(number of exam-
ples=ùëõ;numberofinputsineachexample =ùëë)andletthehiddenlayeractivationfunction
beùúô. Inthebidirectionalarchitecture, theforwardandbackwardhiddenstatesforthistime
step are  !Hùë°2Rùëõ‚Ñéand  Hùë°2Rùëõ‚Ñé, respectively, where ‚Ñéis the number of hidden units.
The forward and backward hidden state updates are as follows:
  !Hùë°=ùúô¬πXùë°W¬πùëì¬∫
xh¬∏  !Hùë° 1W¬πùëì¬∫
hh¬∏b¬πùëì¬∫
h¬∫,
  Hùë°=ùúô¬πXùë°W¬πùëè¬∫
xh¬∏  Hùë°¬∏1W¬πùëè¬∫
hh¬∏b¬πùëè¬∫
h¬∫,(10.4.1)
where the weights W¬πùëì¬∫
xh2Rùëë‚Ñé,W¬πùëì¬∫
hh2R‚Ñé‚Ñé,W¬πùëè¬∫
xh2Rùëë‚Ñé,andW¬πùëè¬∫
hh2R‚Ñé‚Ñé, and
the biases b¬πùëì¬∫
h2R1‚Ñéandb¬πùëè¬∫
h2R1‚Ñéare all the model parameters.
Next, we concatenate the forward and backward hidden states  !Hùë°and  Hùë°to obtain the
hiddenstate Hùë°2Rùëõ2‚Ñéforfeedingintotheoutputlayer. IndeepbidirectionalRNNswith
multiplehiddenlayers,suchinformationispassedonas inputtothenextbidirectionallayer.
387 Bidirectional Recurrent Neural Networks
Last, the output layer computes the output Oùë°2Rùëõùëû(number of outputs =ùëû):
Oùë°=Hùë°Whq¬∏bq. (10.4.2)
Here, the weight matrix Whq2R2‚Ñéùëûand the bias bq2R1ùëûare the model parameters
of the output layer. While technically, the two directions can have different numbers of
hidden units, this design choice is seldom made in practice. We now demonstrate a simple
implementation of a bidirectional RNN.
import torch
from torch import nn
from d2l import torch asd2l
10.4.1Implementation fromScratch
ToimplementabidirectionalRNNfromscratch,wecanincludetwounidirectional RNNScratch
instances with separate learnable parameters.
class BiRNNScratch (d2l .Module):
def __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):
super ().__init__ ()
self .save_hyperparameters()
self .f_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)
self .b_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)
self .num_hiddens *=2# The output dimension will be doubled
States of forward and backward RNNs are updated separately, while outputs of these two
RNNs are concatenated.
@d2l .add_to_class(BiRNNScratch)
def forward (self , inputs, Hs =None ):
f_H, b_H =HsifHsisnot None else (None ,None )
f_outputs, f_H =self .f_rnn(inputs, f_H)
b_outputs, b_H =self .b_rnn( reversed (inputs), b_H)
outputs =[torch .cat((f, b), -1)for f, b inzip(
f_outputs, reversed (b_outputs))]
return outputs, (f_H, b_H)
10.4.2ConciseImplementation
Usingthehigh-levelAPIs,wecanimplementbidirectionalRNNsmoreconcisely. Herewe
take a GRU model as an example.
class BiGRU (d2l .RNN):
def __init__ (self , num_inputs, num_hiddens):
d2l.Module .__init__ (self )
self .save_hyperparameters()
self .rnn =nn.GRU(num_inputs, num_hiddens, bidirectional =True )
self .num_hiddens *=2
388 Modern Recurrent Neural Networks
14810.4.3Summary
In bidirectional RNNs, the hidden state for each time step is simultaneously determined
by the data prior to and after the current time step. Bidirectional RNNs are mostly use-
ful for sequence encoding and the estimation of observations given bidirectional context.
Bidirectional RNNs are very costly to train due to long gradient chains.
10.4.4Exercises
1.If the different directions use a different number of hidden units, how will the shape of
Hùë°change?
2.Design a bidirectional RNN with multiple hidden layers.
3.Polysemy is common in natural languages. For example, the word ‚Äúbank‚Äù has different
meanings in contexts ‚Äúi went to the bank to deposit cash‚Äù and ‚Äúi went to the bank to sit
down‚Äù. How can we design a neural network model such that given a context sequence
and a word, a vector representation of the word in the correct context will be returned?
What type of neural architectures is preferred for handling polysemy?
Discussions148.
10.5MachineTranslationand the Dataset
Among the major breakthroughs that prompted widespread interest in modern RNNs was
a major advance in the applied field of statistical machine translation . Here, the model is
presented with a sentence in one language and must predict the corresponding sentence in
another. Note that here the sentences may be of different lengths, and that corresponding
words in the two sentences may not occur in the same order, owing to differences in the
two language‚Äôs grammatical structure.
Many problems have this flavor of mapping between two such ‚Äúunaligned‚Äù sequences.
Examples include mapping from dialog prompts to replies or from questions to answers.
Broadly, such problems are called sequence-to-sequence (seq2seq) problems and they are
our focus for both the remainder of this chapter and much of Chapter 11 .
In this section, we introduce the machine translation problem and an example dataset that
we will use in the subsequent examples. For decades, statistical formulations of translation
between languages had been popular ( Brownetal., 1990,Brownetal., 1988), even before
researchers got neural network approaches working (methods were often lumped together
under the term neuralmachinetranslation ).
First we will need some new code to process our data. Unlike the language modeling that
wesawin Section9.3 ,hereeachexampleconsistsoftwoseparatetextsequences,oneinthe
source language and another (the translation) in the target language. The following code
snippets will show how to load the preprocessed data into minibatches for training.
389 Machine Translation and the Dataset
149import os
import torch
from d2l import torch asd2l
10.5.1Downloadingand Preprocessingthe Dataset
To begin, we download an English‚ÄìFrench dataset that consists of bilingual sentence pairs
from the Tatoeba Project149. Each line in the dataset is a tab-delimited pair consisting
of an English text sequence (the source) and the translated French text sequence (the tar-
get). Note that each text sequence can be just one sentence, or a paragraph of multiple
sentences.
class MTFraEng (d2l .DataModule): #@save
"""The English-French dataset."""
def _download (self ):
d2l.extract(d2l .download(
d2l.DATA_URL +'fra-eng.zip ',self .root,
'94646ad1522d915e7b0f9296181140edcf86a4f5 '))
with open (self .root +'/fra-eng/fra.txt ', encoding ='utf-8 ')asf:
return f.read()
data =MTFraEng()
raw_text =data ._download()
print (raw_text[: 75])
Downloading ../data/fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.
‚Ü©!com/fra-eng.zip...
Go. Va !
Hi. Salut !
Run! Cours !
Run! Courez !
Who? Qui ?
Wow! √áa alors !
Afterdownloadingthedataset,weproceedwithseveralpreprocessingstepsfortherawtext
data. For instance, we replace non-breaking space with space, convert uppercase letters to
lowercase ones, and insert space between words and punctuation marks.
@d2l .add_to_class(MTFraEng) #@save
def _preprocess (self , text):
# Replace non-breaking space with space
text =text .replace( '\u202f ','').replace( '\xa0 ','')
# Insert space between words and punctuation marks
no_space =lambda char, prev_char: char in',.!? 'and prev_char !=''
out =[''+char ifi>0and no_space(char, text[i -1])else char
for i, char inenumerate (text .lower())]
return ''.join(out)
390 Modern Recurrent Neural Networks
text =data ._preprocess(raw_text)
print (text[: 80])
go . va !
hi . salut !
run ! cours !
run ! courez !
who ? qui ?
wow ! √ßa alors !
10.5.2Tokenization
Unlike the character-level tokenization in Section 9.3 , for machine translation we prefer
word-level tokenization here (today‚Äôs state-of-the-art models use more complex tokeniza-
tion techniques). The following _tokenize method tokenizes the first max_examples text
sequence pairs, where each token is either a word or a punctuation mark. We append the
special ‚Äú<eos>‚Äù token to the end of every sequence to indicate the end of the sequence.
When a model is predicting by generating a sequence token after token, the generation of
the‚Äú<eos>‚Äùtokencansuggestthattheoutputsequenceiscomplete. Intheend,themethod
below returns two lists of token lists: srcandtgt. Specifically, src[i]is a list of tokens
from theùëñthtext sequence in the source language (English here) and tgt[i]is that in the
target language (French here).
@d2l .add_to_class(MTFraEng) #@save
def _tokenize (self , text, max_examples =None ):
src, tgt =[], []
for i, line inenumerate (text .split( '\n')):
ifmax_examples and i>max_examples: break
parts =line .split( '\t')
iflen(parts) ==2:
# Skip empty tokens
src.append([t for tinf'{parts[ 0]}<eos> '.split( '')ift])
tgt.append([t for tinf'{parts[ 1]}<eos> '.split( '')ift])
return src, tgt
src, tgt =data ._tokenize(text)
src[: 6], tgt[: 6]
([['go','.','<eos> '],
['hi','.','<eos> '],
['run','!','<eos> '],
['run','!','<eos> '],
['who','?','<eos> '],
['wow','!','<eos> ']],
[['va','!','<eos> '],
['salut ','!','<eos> '],
['cours ','!','<eos> '],
['courez ','!','<eos> '],
(continues on next page)
391 Machine Translation and the Dataset
(continued from previous page)
['qui','?','<eos> '],
['√ßa','alors ','!','<eos> ']])
Let‚Äôs plot the histogram of the number of tokens per text sequence. In this simple English‚Äì
French dataset, most of the text sequences have fewer than 20 tokens.
#@save
def show_list_len_pair_hist (legend, xlabel, ylabel, xlist, ylist):
"""Plot the histogram for list length pairs."""
d2l.set_figsize()
_, _, patches =d2l.plt.hist(
[[len(l) for linxlist], [ len(l) for linylist]])
d2l.plt.xlabel(xlabel)
d2l.plt.ylabel(ylabel)
for patch inpatches[ 1].patches:
patch .set_hatch( '/')
d2l.plt.legend(legend)
show_list_len_pair_hist([ 'source ','target '],'# tokens per sequence ',
'count ', src, tgt);
10.5.3LoadingSequencesof FixedLength
Recallthat in languagemodeling eachexamplesequence, either a segment of one sentence
or a span over multiple sentences, had a fixed length. This was specified by the num_steps
(number of time steps or tokens) argument from Section 9.3 . In machine translation, each
example is a pair of source and target text sequences, where the two text sequences may
have different lengths.
Forcomputationalefficiency,wecanstillprocessaminibatchoftextsequencesatonetime
bytruncation andpadding. Supposethateverysequenceinthesameminibatchshouldhave
the same length num_steps . If a text sequence has fewer than num_steps tokens, we will
keep appending the special ‚Äú<pad>‚Äù token to its end until its length reaches num_steps .
Otherwise, wewilltruncatethetextsequencebyonlytakingitsfirst num_steps tokensand
discarding the remaining. In this way, every text sequence will have the same length to be
loaded in minibatches of the same shape. Furthermore, we also record length of the source
392 Modern Recurrent Neural Networks
sequence excluding padding tokens. This information will be needed by some models that
we will cover later.
Since the machine translation dataset consists of pairs of languages, we can build two vo-
cabulariesforboththesourcelanguageandthetargetlanguageseparately. Withword-level
tokenization, the vocabulary size will be significantly larger than that using character-level
tokenization. To alleviate this, here we treat infrequent tokens that appear less than twice
as the same unknown (‚Äú<unk>‚Äù) token. As we will explain later ( Fig. 10.7.1 ), when train-
ing with target sequences, the decoder output (label tokens) can be the same decoder input
(targettokens),shiftedbyonetoken; andthespecialbeginning-of-sequence‚Äú<bos>‚Äùtoken
will be used as the first input token for predicting the target sequence ( Fig. 10.7.3 ).
@d2l .add_to_class(MTFraEng) #@save
def __init__ (self , batch_size, num_steps =9, num_train =512, num_val =128):
super (MTFraEng, self ).__init__ ()
self .save_hyperparameters()
self .arrays, self .src_vocab, self .tgt_vocab =self ._build_arrays(
self ._download())
@d2l .add_to_class(MTFraEng) #@save
def _build_arrays (self , raw_text, src_vocab =None , tgt_vocab =None ):
def _build_array (sentences, vocab, is_tgt =False ):
pad_or_trim =lambda seq, t: (
seq[:t] iflen(seq) >telse seq +['<pad> ']*(t-len(seq)))
sentences =[pad_or_trim(s, self .num_steps) for sinsentences]
ifis_tgt:
sentences =[['<bos> ']+sfor sinsentences]
ifvocab isNone :
vocab =d2l.Vocab(sentences, min_freq =2)
array =torch .tensor([vocab[s] for sinsentences])
valid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)
return array, vocab, valid_len
src, tgt =self ._tokenize( self ._preprocess(raw_text),
self .num_train +self .num_val)
src_array, src_vocab, src_valid_len =_build_array(src, src_vocab)
tgt_array, tgt_vocab, _ =_build_array(tgt, tgt_vocab, True )
return ((src_array, tgt_array[:,: -1], src_valid_len, tgt_array[:, 1:]),
src_vocab, tgt_vocab)
10.5.4Readingthe Dataset
Finally, we define the get_dataloader method to return the data iterator.
@d2l .add_to_class(MTFraEng) #@save
def get_dataloader (self , train):
idx =slice (0,self .num_train) iftrain else slice (self .num_train, None )
return self .get_tensorloader( self .arrays, train, idx)
Let‚Äôs read the first minibatch from the English‚ÄìFrench dataset.
393 Machine Translation and the Dataset
data =MTFraEng(batch_size =3)
src, tgt, src_valid_len, label =next (iter (data .train_dataloader()))
print ('source: ', src .type(torch .int32))
print ('decoder input: ', tgt .type(torch .int32))
print ('source len excluding pad: ', src_valid_len .type(torch .int32))
print ('label: ', label .type(torch .int32))
source: tensor([[ 117,182, 0, 3, 4, 4, 4, 4, 4],
[62,72, 2, 3, 4, 4, 4, 4, 4],
[57,124, 0, 3, 4, 4, 4, 4, 4]], dtype =torch .int32)
decoder input : tensor([[ 3,37,100,58,160, 0, 4, 5, 5],
[3, 6, 2, 4, 5, 5, 5, 5, 5],
[3,180, 0, 4, 5, 5, 5, 5, 5]], dtype =torch .int32)
source len excluding pad: tensor([ 4,4,4], dtype =torch .int32)
label: tensor([[ 37,100,58,160, 0, 4, 5, 5, 5],
[6, 2, 4, 5, 5, 5, 5, 5, 5],
[180, 0, 4, 5, 5, 5, 5, 5, 5]], dtype =torch .int32)
We show a pair of source and target sequences processed by the above _build_arrays
method (in the string format).
@d2l .add_to_class(MTFraEng) #@save
def build (self , src_sentences, tgt_sentences):
raw_text ='\n'.join([src +'\t'+tgt for src, tgt inzip(
src_sentences, tgt_sentences)])
arrays, _, _ =self ._build_arrays(
raw_text, self .src_vocab, self .tgt_vocab)
return arrays
src, tgt, _, _ =data .build([ 'hi . '], [ 'salut . '])
print ('source: ', data .src_vocab .to_tokens(src[ 0].type(torch .int32)))
print ('target: ', data .tgt_vocab .to_tokens(tgt[ 0].type(torch .int32)))
source: [ 'hi','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','<pad> ','
‚Ü©!<pad> ']
target: [ '<bos> ','salut ','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','
‚Ü©!<pad> ']
10.5.5Summary
Innaturallanguageprocessing, machinetranslation referstothetaskofautomaticallymap-
pingfromasequencerepresentingastringoftextina sourcelanguagetoastringrepresent-
ing a plausible translation in a targetlanguage. Using word-level tokenization, the vocab-
ulary size will be significantly larger than that using character-level tokenization, but the
sequence lengths will be much shorter. To mitigate the large vocabulary size, we can treat
infrequent tokens as some ‚Äúunknown‚Äù token. We can truncate and pad text sequences so
that all of them will have the same length to be loaded in minibatches. Modern implemen-
394 Modern Recurrent Neural Networks
150tationsoftenbucketsequenceswithsimilarlengthstoavoidwastingexcessivecomputation
on padding.
10.5.6Exercises
1.Try different values of the max_examples argument in the _tokenize method. How
does this affect the vocabulary sizes of the source language and the target language?
2.Text in some languages such as Chinese and Japanese does not have word boundary
indicators(e.g.,space). Isword-leveltokenizationstillagoodideaforsuchcases? Why
or why not?
Discussions150.
10.6TheEncoder Decoder Architecture
In general sequence-to-sequence problems like machine translation ( Section 10.5 ), inputs
and outputs are of varying lengths that are unaligned. The standard approach to handling
this sort of data is to design an encoder‚Äìdecoder architecture ( Fig. 10.6.1 ) consisting of
two major components: an encoder that takes a variable-length sequence as input, and a
decoder that acts as a conditional language model, taking in the encoded input and the
leftwards context of the target sequence and predicting the subsequent token in the target
sequence.
tFig. 10.6.1 The encoder‚Äìdecoder architecture.
Let‚Äôs take machine translation from English to French as an example. Given an input
sequence in English: ‚ÄúThey‚Äù, ‚Äúare‚Äù, ‚Äúwatching‚Äù, ‚Äú.‚Äù, this encoder‚Äìdecoder architecture
first encodes the variable-length input into a state, then decodes the state to generate the
translated sequence, token by token, as output: ‚ÄúIls‚Äù, ‚Äúregardent‚Äù, ‚Äú.‚Äù. Since the encoder‚Äì
decoder architecture forms the basis of different sequence-to-sequence models in subse-
quent sections, this section will convert this architecture into an interface that will be im-
plemented later.
from torch import nn
from d2l import torch asd2l
10.6.1Encoder
Intheencoderinterface,wejustspecifythattheencodertakesvariable-lengthsequencesas
input X. The implementation will be provided by any model that inherits this base Encoder
class.
395 The Encoder Decoder Architecture
class Encoder (nn.Module): #@save
"""The base encoder interface for the encoder--decoder architecture."""
def __init__ (self ):
super ().__init__ ()
# Later there can be additional arguments (e.g., length excluding padding)
def forward (self , X, *args):
raise NotImplementedError
10.6.2Decoder
Inthefollowingdecoderinterface,weaddanadditional init_state methodtoconvertthe
encoder output ( enc_all_outputs ) into the encoded state. Note that this step may require
extra inputs, such as the valid length of the input, which was explained in Section 10.5 .
To generate a variable-length sequence token by token, every time the decoder may map
an input (e.g., the generated token at the previous time step) and the encoded state into an
output token at the current time step.
class Decoder (nn.Module): #@save
"""The base decoder interface for the encoder--decoder architecture."""
def __init__ (self ):
super ().__init__ ()
# Later there can be additional arguments (e.g., length excluding padding)
def init_state (self , enc_all_outputs, *args):
raise NotImplementedError
def forward (self , X, state):
raise NotImplementedError
10.6.3Puttingthe Encoder and Decoder Together
In the forward propagation, the output of the encoder is used to produce the encoded state,
and this state will be further used by the decoder as one of its input.
class EncoderDecoder (d2l .Classifier): #@save
"""The base class for the encoder--decoder architecture."""
def __init__ (self , encoder, decoder):
super ().__init__ ()
self .encoder =encoder
self .decoder =decoder
def forward (self , enc_X, dec_X, *args):
enc_all_outputs =self .encoder(enc_X, *args)
dec_state =self .decoder .init_state(enc_all_outputs, *args)
# Return decoder output only
return self .decoder(dec_X, dec_state)[ 0]
Inthenextsection,wewillseehowtoapplyRNNstodesignsequence-to-sequencemodels
based on this encoder‚Äìdecoder architecture.
396 Modern Recurrent Neural Networks
15110.6.4Summary
Encoder-decoder architectures can handle inputs and outputs that both consist of variable-
lengthsequencesandthusaresuitableforsequence-to-sequenceproblemssuchasmachine
translation. The encoder takes a variable-length sequence as input and transforms it into a
statewitha fixedshape. Thedecoder mapsthe encodedstateofa fixedshapeto avariable-
length sequence.
10.6.5Exercises
1.Suppose that we use neural networks to implement the encoder‚Äìdecoder architecture.
Do the encoder and the decoder have to be the same type of neural network?
2.Besides machine translation, can you think of another application where the encoder‚Äì
decoder architecture can be applied?
Discussions151.
10.7Sequence-to-SequenceLearning forMachine
Translation
In so-called sequence-to-sequence problems such as machine translation (as discussed in
Section10.5 ),whereinputsandoutputseachconsistofvariable-lengthunalignedsequences,
we generally rely on encoder‚Äìdecoder architectures ( Section 10.6 ). In this section, we will
demonstrate the application of an encoder‚Äìdecoder architecture, where both the encoder
and decoder are implemented as RNNs, to the task of machine translation ( Choet al.,
2014,Sutskever etal., 2014).
Here, the encoder RNN will take a variable-length sequence as input and transform it into
a fixed-shape hidden state. Later, in Chapter 11 , we will introduce attention mechanisms,
which allow us to access encoded inputs without having to compress the entire input into a
single fixed-length representation.
Then to generate the output sequence, one token at a time, the decoder model, consisting
of a separate RNN, will predict each successive target token given both the input sequence
and the preceding tokens in the output. During training, the decoder will typically be con-
ditioned upon the preceding tokens in the official ‚Äúground truth‚Äù label. However, at test
time, we will want to condition each output of the decoder on the tokens already predicted.
Note that if we ignore the encoder, the decoder in a sequence-to-sequence architecture be-
haves just like a normal language model. Fig. 10.7.1 illustrates how to use two RNNs for
sequence-to-sequence learning in machine translation.
InFig. 10.7.1 , the special ‚Äú<eos>‚Äù token marks the end of the sequence. Our model can
stop making predictions once this token is generated. At the initial time step of the RNN
decoder, there are two special design decisions to be aware of: First, we begin every input
397 Sequence-to-Sequence Learning for Machine Translation
tFig. 10.7.1 Sequence-to-sequence learning with an RNN encoder and an RNN decoder.
withaspecialbeginning-of-sequence‚Äú<bos>‚Äùtoken. Second,wemayfeedthefinalhidden
state of the encoder into the decoder at every single decoding time step ( Choet al., 2014).
In some other designs, such as that of Sutskever et al.(2014), the final hidden state of the
RNN encoder is used to initiate the hidden state of the decoder only at the first decoding
step.
import collections
import math
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
10.7.1TeacherForcing
While running the encoder on the input sequence is relatively straightforward, handling
the input and output of the decoder requires more care. The most common approach is
sometimes called teacher forcing . Here, the original target sequence (token labels) is fed
into the decoder as input. More concretely, the special beginning-of-sequence token and
the original target sequence, excluding the final token, are concatenated as input to the
decoder,whilethedecoderoutput(labelsfortraining)istheoriginaltargetsequence,shifted
by one token: ‚Äú<bos>‚Äù, ‚ÄúIls‚Äù, ‚Äúregardent‚Äù, ‚Äú.‚Äù !‚ÄúIls‚Äù, ‚Äúregardent‚Äù, ‚Äú.‚Äù, ‚Äú<eos>‚Äù ( Fig.
10.7.1).
Our implementation in Section 10.5.3 prepared training data for teacher forcing, where
shifting tokens for self-supervised learning is similar to the training of language models in
Section 9.3 . An alternative approach is to feed the predicted token from the previous time
step as the current input to the decoder.
In the following, we explain the design depicted in Fig. 10.7.1 in greater detail. We will
train this model for machine translation on the English‚ÄìFrench dataset as introduced in
Section 10.5 .
10.7.2Encoder
Recall that the encoder transforms an input sequence of variable length into a fixed-shape
contextvariable c(seeFig. 10.7.1 ).
Considerasinglesequenceexample(batchsize1). Supposetheinputsequenceis ùë•1,...,ùë•ùëá,
such thatùë•ùë°is theùë°thtoken. At time step ùë°, the RNN transforms the input feature vector xùë°
398 Modern Recurrent Neural Networks
forùë•ùë°andthehiddenstate hùë° 1fromtheprevioustimestepintothecurrenthiddenstate hùë°.
Wecanuseafunction ùëìtoexpressthetransformationoftheRNN‚Äôsrecurrentlayer:
hùë°=ùëì¬πxùë°,hùë° 1¬∫. (10.7.1)
In general, the encoder transforms the hidden states at all time steps into a context variable
through a customized function ùëû:
c=ùëû¬πh1,...,hùëá¬∫. (10.7.2)
For example, in Fig. 10.7.1 , the context variable is just the hidden state hùëácorrespond-
ing to the encoder RNN‚Äôs representation after processing the final token of the input se-
quence.
Inthisexample,wehaveusedaunidirectionalRNNtodesigntheencoder,wherethehidden
state only depends on the input subsequence at and before the time step of the hidden state.
We can also construct encoders using bidirectional RNNs. In this case, a hidden state
dependsonthesubsequencebeforeandafterthetimestep(includingtheinputatthecurrent
time step), which encodes the information of the entire sequence.
Now let‚Äôs implement the RNN encoder. Note that we use an embedding layer to obtain
the feature vector for each token in the input sequence. The weight of an embedding
layer is a matrix, where the number of rows corresponds to the size of the input vocab-
ulary ( vocab_size ) and number of columns corresponds to the feature vector‚Äôs dimension
(embed_size ). Foranyinputtokenindex ùëñ,theembeddinglayerfetchesthe ùëñthrow(starting
from 0) of the weight matrix to return its feature vector. Here we implement the encoder
with a multilayer GRU.
def init_seq2seq (module): #@save
"""Initialize weights for sequence-to-sequence learning."""
iftype (module) ==nn.Linear:
nn.init .xavier_uniform_(module .weight)
iftype (module) ==nn.GRU:
for param inmodule ._flat_weights_names:
if"weight "inparam:
nn.init .xavier_uniform_(module ._parameters[param])
class Seq2SeqEncoder (d2l .Encoder): #@save
"""The RNN encoder for sequence-to-sequence learning."""
def __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,
dropout =0):
super ().__init__ ()
self .embedding =nn.Embedding(vocab_size, embed_size)
self .rnn =d2l.GRU(embed_size, num_hiddens, num_layers, dropout)
self .apply(init_seq2seq)
def forward (self , X, *args):
# X shape: (batch_size, num_steps)
embs =self .embedding(X .t().type(torch .int64))
# embs shape: (num_steps, batch_size, embed_size)
outputs, state =self .rnn(embs)
(continues on next page)
399 Sequence-to-Sequence Learning for Machine Translation
(continued from previous page)
# outputs shape: (num_steps, batch_size, num_hiddens)
# state shape: (num_layers, batch_size, num_hiddens)
return outputs, state
Let‚Äôs use a concrete example to illustrate the above encoder implementation. Below, we
instantiateatwo-layerGRUencoderwhosenumberofhiddenunitsis16. Givenaminibatch
of sequence inputs X(batch size =4; number of time steps =9), the hidden states of the
final layer at all the time steps ( enc_outputs returned by the encoder‚Äôs recurrent layers)
are a tensor of shape (number of time steps, batch size, number of hidden units).
vocab_size, embed_size, num_hiddens, num_layers =10,8,16,2
batch_size, num_steps =4,9
encoder =Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
X=torch .zeros((batch_size, num_steps))
enc_outputs, enc_state =encoder(X)
d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))
Since we are using a GRU here, the shape of the multilayer hidden states at the final time
step is (number of hidden layers, batch size, number of hidden units).
d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))
10.7.3Decoder
Givenatargetoutputsequence ùë¶1,ùë¶2,...,ùë¶ùëá0foreachtimestep ùë°0(weuseùë°0todifferentiate
from the input sequence time steps), the decoder assigns a predicted probability to each
possible token occurring at step ùë¶ùë°0¬∏1conditioned upon the previous tokens in the target
ùë¶1,...,ùë¶ùë°0and the context variable c, i.e.,ùëÉ¬πùë¶ùë°0¬∏1jùë¶1,...,ùë¶ùë°0,c¬∫.
To predict the subsequent token ùë°0¬∏1in the target sequence, the RNN decoder takes the
previous step‚Äôs target token ùë¶ùë°0, the hidden RNN state from the previous time step sùë°0 1,
and the context variable cas its input, and transforms them into the hidden state sùë°0at the
current time step. We can use a function ùëîto express the transformation of the decoder‚Äôs
hidden layer:
sùë°0=ùëî¬πùë¶ùë°0 1,c,sùë°0 1¬∫. (10.7.3)
After obtaining the hidden state of the decoder, we can use an output layer and the softmax
operation to compute the predictive distribution ùëù¬πùë¶ùë°0¬∏1jùë¶1,...,ùë¶ùë°0,c¬∫over the subse-
quent output token ùë°0¬∏1.
Following Fig. 10.7.1 , when implementing the decoder as follows, we directly use the hid-
den state at the final time step of the encoder to initialize the hidden state of the decoder.
This requires that the RNN encoder and the RNN decoder have the same number of lay-
ers and hidden units. To further incorporate the encoded input sequence information, the
context variable is concatenated with the decoder input at all the time steps. To predict the
400 Modern Recurrent Neural Networks
probability distribution of the output token, we use a fully connected layer to transform the
hidden state at the final layer of the RNN decoder.
class Seq2SeqDecoder (d2l .Decoder):
"""The RNN decoder for sequence to sequence learning."""
def __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,
dropout =0):
super ().__init__ ()
self .embedding =nn.Embedding(vocab_size, embed_size)
self .rnn =d2l.GRU(embed_size +num_hiddens, num_hiddens,
num_layers, dropout)
self .dense =nn.LazyLinear(vocab_size)
self .apply(init_seq2seq)
def init_state (self , enc_all_outputs, *args):
return enc_all_outputs
def forward (self , X, state):
# X shape: (batch_size, num_steps)
# embs shape: (num_steps, batch_size, embed_size)
embs =self .embedding(X .t().type(torch .int32))
enc_output, hidden_state =state
# context shape: (batch_size, num_hiddens)
context =enc_output[ -1]
# Broadcast context to (num_steps, batch_size, num_hiddens)
context =context .repeat(embs .shape[ 0],1,1)
# Concat at the feature dimension
embs_and_context =torch .cat((embs, context), -1)
outputs, hidden_state =self .rnn(embs_and_context, hidden_state)
outputs =self .dense(outputs) .swapaxes( 0,1)
# outputs shape: (batch_size, num_steps, vocab_size)
# hidden_state shape: (num_layers, batch_size, num_hiddens)
return outputs, [enc_output, hidden_state]
To illustrate the implemented decoder, below we instantiate it with the same hyperparam-
eters from the aforementioned encoder. As we can see, the output shape of the decoder
becomes (batch size, number of time steps, vocabulary size), where the final dimension of
the tensor stores the predicted token distribution.
decoder =Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)
state =decoder .init_state(encoder(X))
dec_outputs, state =decoder(X, state)
d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))
d2l.check_shape(state[ 1], (num_layers, batch_size, num_hiddens))
ThelayersintheaboveRNNencoder‚Äìdecodermodelaresummarizedin Fig.10.7.2 .
10.7.4Encoder‚ÄìDecoderforSequence-to-SequenceLearning
Putting it all together in code yields the following:
401 Sequence-to-Sequence Learning for Machine Translation
tFig. 10.7.2 Layers in an RNN encoder‚Äìdecoder model.
class Seq2Seq (d2l .EncoderDecoder): #@save
"""The RNN encoder--decoder for sequence to sequence learning."""
def __init__ (self , encoder, decoder, tgt_pad, lr):
super ().__init__ (encoder, decoder)
self .save_hyperparameters()
def validation_step (self , batch):
Y_hat =self (*batch[: -1])
self .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )
def configure_optimizers (self ):
# Adam optimizer is used here
return torch .optim .Adam( self .parameters(), lr =self .lr)
10.7.5LossFunction with Masking
At each time step, the decoder predicts a probability distribution for the output tokens.
As with language modeling, we can apply softmax to obtain the distribution and calculate
the cross-entropy loss for optimization. Recall from Section 10.5 that the special padding
tokens are appended to the end of sequences and so sequences of varying lengths can be
efficientlyloadedinminibatchesofthesameshape. However,predictionofpaddingtokens
should be excluded from loss calculations. To this end, we can mask irrelevant entries
with zero values so that multiplication of any irrelevant prediction with zero equates to
zero.
@d2l .add_to_class(Seq2Seq)
def loss (self , Y_hat, Y):
l=super (Seq2Seq, self ).loss(Y_hat, Y, averaged =False )
mask =(Y.reshape( -1)!=self .tgt_pad) .type(torch .float32)
return (l*mask) .sum() /mask .sum()
10.7.6Training
Now we can create and train an RNN encoder‚Äìdecoder model for sequence-to-sequence
learning on the machine translation dataset.
data =d2l.MTFraEng(batch_size =128)
embed_size, num_hiddens, num_layers, dropout =256,256,2,0.2
(continues on next page)
402 Modern Recurrent Neural Networks
(continued from previous page)
encoder =Seq2SeqEncoder(
len(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder =Seq2SeqDecoder(
len(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model =Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],
lr=0.005 )
trainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
10.7.7Prediction
Topredicttheoutputsequenceateachstep,thepredictedtokenfromtheprevioustimestep
is fed into the decoder as an input. One simple strategy is to sample whichever token that
has been assigned by the decoder the highest probability when predicting at each step. As
intraining,attheinitialtimestepthebeginning-of-sequence(‚Äú<bos>‚Äù)tokenisfedintothe
decoder. This prediction process is illustrated in Fig. 10.7.3 . When the end-of-sequence
(‚Äú<eos>‚Äù) token is predicted, the prediction of the output sequence is complete.
tFig. 10.7.3 Predicting the output sequence token by token using an RNN encoder‚Äìdecoder.
In the next section, we will introduce more sophisticated strategies based on beam search
(Section 10.8 ).
@d2l .add_to_class(d2l .EncoderDecoder) #@save
def predict_step (self , batch, device, num_steps,
save_attention_weights =False ):
batch =[a.to(device) for ainbatch]
src, tgt, src_valid_len, _ =batch
enc_all_outputs =self .encoder(src, src_valid_len)
dec_state =self .decoder .init_state(enc_all_outputs, src_valid_len)
(continues on next page)
403 Sequence-to-Sequence Learning for Machine Translation
(continued from previous page)
outputs, attention_weights =[tgt[:, ( 0)].unsqueeze( 1), ], []
for _inrange (num_steps):
Y, dec_state =self .decoder(outputs[ -1], dec_state)
outputs .append(Y .argmax( 2))
# Save attention weights (to be covered later)
ifsave_attention_weights:
attention_weights .append( self .decoder .attention_weights)
return torch .cat(outputs[ 1:], 1), attention_weights
10.7.8Evaluationof PredictedSequences
Wecanevaluateapredictedsequencebycomparingitwiththetargetsequence(theground
truth). Butwhatpreciselyistheappropriatemeasureforcomparingsimilaritybetweentwo
sequences?
Bilingual Evaluation Understudy (BLEU), though originally proposed for evaluating ma-
chinetranslationresults( Papinenietal., 2002), hasbeenextensivelyusedinmeasuringthe
qualityofoutputsequencesfordifferentapplications. Inprinciple,forany ùëõ-gram(Section
9.3.1)inthepredictedsequence,BLEUevaluateswhetherthis ùëõ-gramappearsinthetarget
sequence.
Denote byùëùùëõthe precision of an ùëõ-gram, defined as the ratio of the number of matched
ùëõ-grams in the predicted and target sequences to the number of ùëõ-grams in the predicted
sequence. To explain, given a target sequence ùê¥,ùêµ,ùê∂,ùê∑,ùê∏,ùêπ, and a predicted sequence
ùê¥,ùêµ,ùêµ,ùê∂,ùê∑, we haveùëù1=4¬ù5,ùëù2=3¬ù4,ùëù3=1¬ù3, andùëù4=0. Now let len label
and len predbe the numbers of tokens in the target sequence and the predicted sequence,
respectively. Then, BLEU is defined as
exp
min
0,1 lenlabel
lenpredùëò√ñ
ùëõ=1ùëù1¬ù2ùëõ
ùëõ, (10.7.4)
whereùëòis the longest ùëõ-gram for matching.
Based on the definition of BLEU in (10.7.4 ), whenever the predicted sequence is the same
as the target sequence, BLEU is 1. Moreover, since matching longer ùëõ-grams is more diffi-
cult,BLEUassignsagreaterweightwhenalonger ùëõ-gramhashighprecision. Specifically,
whenùëùùëõis fixed,ùëù1¬ù2ùëõ
ùëõincreases as ùëõgrows (the original paper uses ùëù1¬ùùëõ
ùëõ). Furthermore,
since predicting shorter sequences tends to yield a higher ùëùùëõvalue, the coefficient before
the multiplication term in (10.7.4 )penalizes shorter predicted sequences. For example,
whenùëò=2, given the target sequence ùê¥,ùêµ,ùê∂,ùê∑,ùê∏,ùêπand the predicted sequence ùê¥,ùêµ,
althoughùëù1=ùëù2=1, the penalty factor exp¬π1 6¬ù2¬∫0.14lowers the BLEU.
We implement the BLEU measure as follows.
def bleu (pred_seq, label_seq, k): #@save
"""Compute the BLEU."""
pred_tokens, label_tokens =pred_seq .split( ''), label_seq .split( '')
(continues on next page)
404 Modern Recurrent Neural Networks
(continued from previous page)
len_pred, len_label =len(pred_tokens), len(label_tokens)
score =math .exp( min(0,1-len_label /len_pred))
for ninrange (1,min(k, len_pred) +1):
num_matches, label_subs =0, collections .defaultdict( int)
for iinrange (len_label -n+1):
label_subs[ ''.join(label_tokens[i: i +n])] +=1
for iinrange (len_pred -n+1):
iflabel_subs[ ''.join(pred_tokens[i: i +n])] >0:
num_matches +=1
label_subs[ ''.join(pred_tokens[i: i +n])] -=1
score *=math .pow(num_matches /(len_pred -n+1), math .pow( 0.5, n))
return score
In the end, we use the trained RNN encoder‚Äìdecoder to translate a few English sentences
into French and compute the BLEU of the results.
engs =['go . ','i lost . ','he\'s calm . ','i\'m home . ']
fras =['va ! ','j\'ai perdu . ','il est calme . ','je suis chez moi . ']
preds, _ =model .predict_step(
data .build(engs, fras), d2l .try_gpu(), data .num_steps)
for en, fr, p inzip(engs, fras, preds):
translation =[]
for token indata .tgt_vocab .to_tokens(p):
iftoken =='<eos> ':
break
translation .append(token)
print (f'{en}=>{translation }, bleu, '
f'{bleu( "".join(translation), fr, k=2):.3f}')
go.=>['va','!'], bleu, 1.000
i lost .=>["j'ai",'perdu ','.'], bleu, 1.000
he's calm . => [ 'elle ','court ','.'], bleu,0.000
i'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000
10.7.9Summary
Following the design of the encoder‚Äìdecoder architecture, we can use two RNNs to design
a model for sequence-to-sequence learning. In encoder‚Äìdecoder training, the teacher forc-
ing approach feeds original output sequences (in contrast to predictions) into the decoder.
When implementing the encoder and the decoder, we can use multilayer RNNs. We can
usemaskstofilteroutirrelevantcomputations, suchaswhencalculatingtheloss. Foreval-
uating output sequences, BLEU is a popular measure that matches ùëõ-grams between the
predicted sequence and the target sequence.
10.7.10Exercises
1.Can you adjust the hyperparameters to improve the translation results?
2.Rerun the experiment without using masks in the loss calculation. What results do you
observe? Why?
405 Beam Search
1523.If the encoder and the decoder differ in the number of layers or the number of hidden
units, how can we initialize the hidden state of the decoder?
4.In training, replace teacher forcing with feeding the prediction at the previous time step
into the decoder. How does this influence the performance?
5.Rerun the experiment by replacing GRU with LSTM.
6.Are there any other ways to design the output layer of the decoder?
Discussions152.
10.8Beam Search
InSection 10.7 , we introduced the encoder‚Äìdecoder architecture, and the standard tech-
niques for training them end-to-end. However, when it came to test-time prediction, we
mentioned only the greedystrategy, where we select at each time step the token given the
highest predicted probability of coming next, until, at some time step, we find that we have
predicted the special end-of-sequence ‚Äú<eos>‚Äù token. In this section, we will begin by for-
malizing this greedysearch strategy and identifying some problems that practitioners tend
toruninto. Subsequently,wecomparethisstrategywithtwoalternatives: exhaustivesearch
(illustrative but not practical) and beamsearch (the standard method in practice).
Let‚Äôs begin by setting up our mathematical notation, borrowing conventions from Section
10.7. At any time step ùë°0, the decoder outputs predictions representing the probability of
each token in the vocabulary coming next in the sequence (the likely value of ùë¶ùë°0¬∏1), con-
ditioned on the previous tokens ùë¶1,...,ùë¶ùë°0and the context variable c, produced by the
encoder to represent the input sequence. To quantify computational cost, denote by Ythe
output vocabulary (including the special end-of-sequence token ‚Äú<eos>‚Äù). Let‚Äôs also spec-
ify the maximum number of tokens of an output sequence as ùëá0. Our goal is to search for
an ideal output from all O¬πjYjùëá0¬∫possible output sequences. Note that this slightly over-
estimates the number of distinct outputs because there are no subsequent tokens once the
‚Äú<eos>‚Äù token occurs. However, for our purposes, this number roughly captures the size
of the search space.
10.8.1GreedySearch
Consider the simple greedy search strategy from Section 10.7 . Here, at any time step ùë°0,
we simply select the token with the highest conditional probability from Y, i.e.,
ùë¶ùë°0=argmax
ùë¶2YùëÉ¬πùë¶jùë¶1,...,ùë¶ùë°0 1,c¬∫.(10.8.1)
Onceourmodeloutputs‚Äú<eos>‚Äù(orwereachthemaximumlength ùëá0)theoutputsequence
is completed.
406 Modern Recurrent Neural Networks
Thisstrategymightlookreasonable,andinfactitisnotsobad! Consideringhowcomputa-
tionallyundemandingitis,you‚Äôdbehardpressedtogetmorebangforyourbuck. However,
ifweputasideefficiencyforaminute,itmightseemmorereasonabletosearchforthe most
likelysequence , not the sequence of (greedily selected) mostlikelytokens . It turns out that
thesetwoobjectscanbequitedifferent. Themostlikelysequenceistheonethatmaximizes
the expression√éùëá0
ùë°0=1ùëÉ¬πùë¶ùë°0jùë¶1,...,ùë¶ùë°0 1,c¬∫. In our machine translation example, if the
decoder truly recovered the probabilities of the underlying generative process, then this
would give us the most likely translation. Unfortunately, there is no guarantee that greedy
search will give us this sequence.
Let‚Äôs illustrate it with an example. Suppose that there are four tokens ‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, and
‚Äú<eos>‚Äùintheoutputdictionary. In Fig.10.8.1 ,thefournumbersundereachtimesteprep-
resent the conditional probabilities of generating ‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, and ‚Äú<eos>‚Äù respectively,
at that time step.
tFig. 10.8.1 At each time step, greedy search selects the token with the highest conditional probability.
At each time step, greedy search selects the token with the highest conditional probability.
Therefore, the output sequence ‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, and ‚Äú<eos>‚Äù will be predicted ( Fig. 10.8.1 ).
Theconditionalprobabilityofthisoutputsequenceis 0.50.40.40.6=0.048.
Next, let‚Äôs look at another example in Fig. 10.8.2 . Unlike in Fig. 10.8.1 , at time step 2 we
select the token ‚ÄúC‚Äù, which has the secondhighest conditional probability.
tFig. 10.8.2 The four numbers under each time step represent the conditional probabilities of
generating ‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, and ‚Äú<eos>‚Äù at that time step. At time step 2, the token ‚ÄúC‚Äù,
which has the second highest conditional probability, is selected.
Since the output subsequences at time steps 1 and 2, on which time step 3 is based, have
changed from ‚ÄúA‚Äù and ‚ÄúB‚Äù in Fig. 10.8.1 to ‚ÄúA‚Äù and ‚ÄúC‚Äù in Fig. 10.8.2 , the conditional
probability of each token at time step 3 has also changed in Fig. 10.8.2 . Suppose that
we choose the token ‚ÄúB‚Äù at time step 3. Now time step 4 is conditional on the output
subsequence at the first three time steps ‚ÄúA‚Äù, ‚ÄúC‚Äù, and ‚ÄúB‚Äù, which has changed from ‚ÄúA‚Äù,
‚ÄúB‚Äù,and‚ÄúC‚Äùin Fig.10.8.1 . Therefore,theconditionalprobabilityofgeneratingeachtoken
at time step 4 in Fig. 10.8.2 is also different from that in Fig. 10.8.1 . As a result, the
conditional probability of the output sequence ‚ÄúA‚Äù, ‚ÄúC‚Äù, ‚ÄúB‚Äù, and ‚Äú<eos>‚Äù in Fig. 10.8.2
is0.50.30.60.6=0.054, which is greater than that of greedy search in Fig. 10.8.1 .
407 Beam Search
In this example, the output sequence ‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, and ‚Äú<eos>‚Äù obtained by the greedy
search is not optimal.
10.8.2ExhaustiveSearch
If the goal is to obtain the most likely sequence, we may consider using exhaustivesearch :
enumerate all the possible output sequences with their conditional probabilities, and then
output the one that scores the highest predicted probability.
While this would certainly give us what we desire, it would come at a prohibitive com-
putational cost ofO¬πjYjùëá0¬∫, exponential in the sequence length and with an enormous
base given by the vocabulary size. For example, when jYj=10000andùëá0=10, both
small numbers when compared with ones in real applications, we will need to evaluate
1000010=1040sequences, which is already beyond the capabilities of any foreseeable
computers. Ontheotherhand,thecomputationalcostofgreedysearchis O¬πjYjùëá0¬∫: mirac-
ulously cheap but far from optimal. For example, when jYj=10000andùëá0=10, we only
need to evaluate 1000010=105sequences.
10.8.3Beam Search
You could view sequence decoding strategies as lying on a spectrum, with beam search
striking a compromise between the efficiency of greedy search and the optimality of ex-
haustive search. The most straightforward version of beam search is characterized by a
single hyperparameter, the beam size ,ùëò. Let‚Äôs explain this terminology. At time step 1,
we select the ùëòtokens with the highest predicted probabilities. Each of them will be the
first token of ùëòcandidate output sequences, respectively. At each subsequent time step,
based on the ùëòcandidate output sequences at the previous time step, we continue to select
ùëòcandidate output sequences with the highest predicted probabilities from ùëòjYjpossible
choices.
tFig. 10.8.3 The process of beam search (beam size =2; maximum length of an output sequence =3).
The candidate output sequences are A,C,AB,CE,ABD , and CED .
Fig. 10.8.3 demonstrates the process of beam search with an example. Suppose that the
output vocabulary contains only five elements: Y=fùê¥,ùêµ,ùê∂,ùê∑,ùê∏g, where one of them is
408 Modern Recurrent Neural Networks
153‚Äú<eos>‚Äù. Letthebeamsizebetwoandthemaximumlengthofanoutputsequencebethree.
At time step 1, suppose that the tokens with the highest conditional probabilities ùëÉ¬πùë¶1jc¬∫
areùê¥andùê∂. At time step 2, for all ùë¶22Y,we compute
ùëÉ¬πùê¥,ùë¶ 2jc¬∫=ùëÉ¬πùê¥jc¬∫ùëÉ¬πùë¶2jùê¥,c¬∫,
ùëÉ¬πùê∂,ùë¶ 2jc¬∫=ùëÉ¬πùê∂jc¬∫ùëÉ¬πùë¶2jùê∂,c¬∫,(10.8.2)
and pick the largest two among these ten values, say ùëÉ¬πùê¥,ùêµjc¬∫andùëÉ¬πùê∂,ùê∏jc¬∫. Then at
time step 3, for all ùë¶32Y, we compute
ùëÉ¬πùê¥,ùêµ,ùë¶ 3jc¬∫=ùëÉ¬πùê¥,ùêµjc¬∫ùëÉ¬πùë¶3jùê¥,ùêµ,c¬∫,
ùëÉ¬πùê∂,ùê∏,ùë¶ 3jc¬∫=ùëÉ¬πùê∂,ùê∏jc¬∫ùëÉ¬πùë¶3jùê∂,ùê∏,c¬∫,(10.8.3)
and pick the largest two among these ten values, say ùëÉ¬πùê¥,ùêµ,ùê∑jc¬∫andùëÉ¬πùê∂,ùê∏,ùê∑jc¬∫.
As a result, we get six candidates output sequences: (i) ùê¥; (ii)ùê∂; (iii)ùê¥,ùêµ; (iv)ùê∂,ùê∏; (v)
ùê¥,ùêµ,ùê∑; and (vi)ùê∂,ùê∏,ùê∑.
In the end, we obtain the set of final candidate output sequences based on these six se-
quences (e.g., discard portions including and after ‚Äú<eos>‚Äù). Then we choose the output
sequence which maximizes the following score:
1
ùêøùõºlogùëÉ¬πùë¶1,...,ùë¶ùêøjc¬∫=1
ùêøùõºùêø√ï
ùë°0=1logùëÉ¬πùë¶ùë°0jùë¶1,...,ùë¶ùë°0 1,c¬∫; (10.8.4)
hereùêøis the length of the final candidate sequence and ùõºis usually set to 0.75. Since a
longer sequence has more logarithmic terms in the summation of (10.8.4 ), the termùêøùõºin
the denominator penalizes long sequences.
The computational cost of beam search is O¬πùëòjYjùëá0¬∫. This result is in between that of
greedy search and that of exhaustive search. Greedy search can be treated as a special case
of beam search arising when the beam size is set to 1.
10.8.4Summary
Sequence searching strategies include greedy search, exhaustive search, and beam search.
Beam search provides a trade-off between accuracy and computational cost via the flexible
choice of the beam size.
10.8.5Exercises
1.Can we treat exhaustive search as a special type of beam search? Why or why not?
2.Apply beam search in the machine translation problem in Section 10.7 . How does the
beam size affect the translation results and the prediction speed?
3.Weusedlanguagemodelingforgeneratingtextfollowinguser-providedprefixesin Sec-
tion 9.5. Which kind of search strategy does it use? Can you improve it?
Discussions153.
11 Attention Mechanisms and Transformers
The earliest years of the deep learning boom were driven primarily by results produced us-
ing the multilayer perceptron, convolutional network, and recurrent network architectures.
Remarkably, the model architectures that underpinned many of deep learning‚Äôs break-
throughsinthe2010shadchangedremarkablylittlerelativetotheirantecedentsdespitethe
lapse of nearly 30 years. While plenty of new methodological innovations made their way
into most practitioner‚Äôs toolkits‚ÄîReLU activations, residual layers, batch normalization,
dropout, and adaptive learning rate schedules come to mind‚Äîthe core underlying archi-
tectures were clearly recognizable as scaled-up implementations of classic ideas. Despite
thousandsofpapersproposingalternativeideas,modelsresemblingclassicalconvolutional
neural networks ( Chapter 7 ) retained state-of-the-art status in computer vision and models
resemblingSeppHochreiter‚ÄôsoriginaldesignfortheLSTMrecurrentneuralnetwork( Sec-
tion 10.1 ), dominated most applications in natural language processing. Arguably, to that
point, the rapid emergence of deep learning appeared to be primarily attributable to shifts
in the available computational resources (thanks to innovations in parallel computing with
GPUs) and the availability of massive data resources (thanks to cheap storage and Internet
services). While these factors may indeed remain the primary drivers behind this technol-
ogy‚Äôs increasing power we are also witnessing, at long last, a sea change in the landscape
of dominant architectures.
At the present moment, the dominant models for nearly all natural language processing
tasks are based on the Transformer architecture. Given any new task in natural language
processing, the default first-pass approach is to grab a large Transformer-based pretrained
model, (e.g., BERT ( Devlinet al., 2018), ELECTRA ( Clarket al., 2020), RoBERTa ( Liu
et al., 2019), or Longformer ( Beltagyet al., 2020)) adapting the output layers as neces-
sary, and fine-tuning the model on the available data for the downstream task. If you have
been paying attention to the last few years of breathless news coverage centered on Ope-
nAI‚Äôs large language models, then you have been tracking a conversation centered on the
GPT-2 and GPT-3 Transformer-based models ( Brownet al., 2020,Radfordet al., 2019).
Meanwhile,thevisionTransformerhasemergedasadefaultmodelfordiversevisiontasks,
includingimagerecognition, objectdetection, semanticsegmentation, andsuperresolution
(Dosovitskiy et al., 2021,Liuet al., 2021). Transformers also showed up as competitive
methods for speech recognition ( Gulatiet al., 2020), reinforcement learning ( Chenet al.,
2021), and graph neural networks ( Dwivedi and Bresson, 2020 ).
The core idea behind the Transformer model is the attention mechanism , an innovation
that was originally envisioned as an enhancement for encoder‚Äìdecoder RNNs applied to
409
410 Attention Mechanisms and Transformers
sequence-to-sequence applications, such as machine translations ( Bahdanau et al., 2014).
You might recall that in the first sequence-to-sequence models for machine translation
(Sutskever etal., 2014), theentireinputwascompressedbytheencoderintoasinglefixed-
length vector to be fed into the decoder. The intuition behind attention is that rather than
compressing the input, it might be better for the decoder to revisit the input sequence at
every step. Moreover, rather than always seeing the same representation of the input, one
might imagine that the decoder should selectively focus on particular parts of the input se-
quence at particular decoding steps. Bahdanau‚Äôs attention mechanism provided a simple
meansbywhichthedecodercoulddynamically attendtodifferentpartsoftheinputateach
decoding step. The high-level idea is that the encoder could produce a representation of
length equal to the original input sequence. Then, at decoding time, the decoder can (via
some control mechanism) receive as input a context vector consisting of a weighted sum
of the representations on the input at each time step. Intuitively, the weights determine the
extent to which each step‚Äôs context ‚Äúfocuses‚Äù on each input token, and the key is to make
this process for assigning the weights differentiable so that it can be learned along with all
of the other neural network parameters.
Initially, the idea was a remarkably successful enhancement to the recurrent neural net-
works that already dominated machine translation applications. The models performed
betterthantheoriginalencoder‚Äìdecodersequence-to-sequencearchitectures. Furthermore,
researchers noted that some nice qualitative insights sometimes emerged from inspecting
the pattern of attention weights. In translation tasks, attention models often assigned high
attention weights to cross-lingual synonyms when generating the corresponding words in
the target language. For example, when translating the sentence ‚Äúmy feet hurt‚Äù to ‚Äúj‚Äôai mal
au pieds‚Äù, the neural network might assign high attention weights to the representation of
‚Äúfeet‚Äù when generating the corresponding French word ‚Äúpieds‚Äù. These insights spurred
claims that attention models confer ‚Äúinterpretability‚Äù although what precisely the atten-
tion weights mean‚Äîi.e., how, if at all, they should be interpreted remains a hazy research
topic.
However, attention mechanisms soon emerged as more significant concerns, beyond their
usefulnessasanenhancementforencoder‚Äìdecoderrecurrentneuralnetworksandtheirpu-
tative usefulness for picking out salient inputs. Vaswani et al.(2017) proposed the Trans-
former architecture for machine translation, dispensing with recurrent connections alto-
gether, and instead relying on cleverly arranged attention mechanisms to capture all rela-
tionshipsamonginputandoutputtokens. Thearchitectureperformedremarkablywell,and
by 2018 the Transformer began showing up in the majority of state-of-the-art natural lan-
guageprocessingsystems. Moreover,atthesametime,thedominantpracticeinnaturallan-
guage processing became to pretrain large-scale models on enormous generic background
corpora to optimize some self-supervised pretraining objective, and then to fine-tune these
modelsusingtheavailabledownstreamdata. ThegapbetweenTransformersandtraditional
architectures grew especially wide when applied in this pretraining paradigm, and thus the
ascendance of Transformers coincided with the ascendence of such large-scale pretrained
models, now sometimes called foundationmodels (Bommasani etal., 2021).
In this chapter, we introduce attention models, starting with the most basic intuitions and
411 Queries, Keys, and Values
the simplest instantiations of the idea. We then work our way up to the Transformer archi-
tecture,thevisionTransformer,andthelandscapeofmodernTransformer-basedpretrained
models.
11.1Queries, Keys,and Values
So far all the networks we have reviewed crucially relied on the input being of a well-
defined size. For instance, the images in ImageNet are of size 224224pixels and CNNs
are specifically tuned to this size. Even in natural language processing the input size for
RNNs is well defined and fixed. Variable size is addressed by sequentially processing one
token at a time, or by specially designed convolution kernels ( Kalchbrenner et al., 2014).
This approach can lead to significant problems when the input is truly of varying size with
varyinginformationcontent,suchasin Section10.7 inthetransformationoftext( Sutskever
et al., 2014). In particular, for long sequences it becomes quite difficult to keep track of
everything that has already been generated or even viewed by the network. Even explicit
trackingheuristicssuchasproposedbyYang etal.(2016)onlyofferlimitedbenefit.
Comparethistodatabases. Intheirsimplestformtheyarecollectionsofkeys( ùëò)andvalues
(ùë£). For instance, our database Dmight consist of tuples {(‚ÄúZhang‚Äù, ‚ÄúAston‚Äù), (‚ÄúLipton‚Äù,
‚ÄúZachary‚Äù), (‚ÄúLi‚Äù, ‚ÄúMu‚Äù), (‚ÄúSmola‚Äù, ‚ÄúAlex‚Äù), (‚ÄúHu‚Äù, ‚ÄúRachel‚Äù), (‚ÄúWerness‚Äù, ‚ÄúBrent‚Äù)}
with the last name being the key and the first name being the value. We can operate on
D, for instance with the exact query ( ùëû) for ‚ÄúLi‚Äù which would return the value ‚ÄúMu‚Äù. If
(‚ÄúLi‚Äù, ‚ÄúMu‚Äù) was not a record in D, there would be no valid answer. If we also allowed for
approximate matches, we would retrieve (‚ÄúLipton‚Äù, ‚ÄúZachary‚Äù) instead. This quite simple
and trivial example nonetheless teaches us a number of useful things:
We can design queries ùëûthat operate on ( ùëò,ùë£) pairs in such a manner as to be valid
regardless of the database size.
The same query can receive different answers, according to the contents of the database.
The‚Äúcode‚Äùbeingexecutedforoperatingonalargestatespace(thedatabase)canbequite
simple (e.g., exact match, approximate match, top- ùëò).
There is no need to compress or simplify the database to make the operations effective.
Clearly we would not have introduced a simple database here if it wasn‚Äôt for the purpose of
explainingdeeplearning. Indeed,thisleadstooneofthemostexcitingconceptsintroduced
in deep learning in the past decade: the attention mechanism (Bahdanau et al., 2014). We
will cover the specifics of its application to machine translation later. For now, simply
consider the following: denote by Ddef=f¬πk1,v1¬∫,...¬πkùëö,vùëö¬∫ga database of ùëötuples of
keysandvalues. Moreover, denote by qaquery. Then we can define the attention overD
412 Attention Mechanisms and Transformers
as
Attention¬πq,D¬∫def=ùëö√ï
ùëñ=1ùõº¬πq,kùëñ¬∫vùëñ, (11.1.1)
whereùõº¬πq,kùëñ¬∫ 2R(ùëñ=1,...,ùëö) are scalar attention weights. The operation itself is
typically referred to as attentionpooling . The name attention derives from the fact that the
operation pays particular attention to the terms for which the weight ùõºis significant (i.e.,
large). As such, the attention over Dgenerates a linear combination of values contained in
the database. In fact, this contains the above example as a special case where all but one
weight is zero. We have a number of special cases:
Theweights ùõº¬πq,kùëñ¬∫arenonnegative. Inthiscasetheoutputoftheattentionmechanism
is contained in the convex cone spanned by the values vùëñ.
Theweights ùõº¬πq,kùëñ¬∫formaconvexcombination,i.e.,√ç
ùëñùõº¬πq,kùëñ¬∫=1andùõº¬πq,kùëñ¬∫0
for allùëñ. This is the most common setting in deep learning.
Exactlyoneoftheweights ùõº¬πq,kùëñ¬∫is1,whileallothersare 0. Thisisakintoatraditional
database query.
All weights are equal, i.e., ùõº¬πq,kùëñ¬∫=1
ùëöfor allùëñ. This amounts to averaging across the
entire database, also called average pooling in deep learning.
Acommonstrategyforensuringthattheweightssumupto 1istonormalizethemvia
ùõº¬πq,kùëñ¬∫=ùõº¬πq,kùëñ¬∫√ç
ùëóùõº¬πq,kùëó¬∫. (11.1.2)
In particular, to ensure that the weights are also nonnegative, one can resort to exponenti-
ation. This means that we can now pick anyfunctionùëé¬πq,k¬∫and then apply the softmax
operation used for multinomial models to it via
ùõº¬πq,kùëñ¬∫=exp¬πùëé¬πq,kùëñ¬∫¬∫√ç
ùëóexp¬πùëé¬πq,kùëó¬∫¬∫. (11.1.3)
This operation is readily available in all deep learning frameworks. It is differentiable and
its gradient never vanishes, all of which are desirable properties in a model. Note though,
the attention mechanism introduced above is not the only option. For instance, we can
designanon-differentiableattentionmodelthatcanbetrainedusingreinforcementlearning
methods( Mnihetal., 2014). Asonewouldexpect, trainingsuchamodelisquitecomplex.
Consequentlythebulkofmodernattentionresearchfollowstheframeworkoutlinedin Fig.
11.1.1. We thus focus our exposition on this family of differentiable mechanisms.
Whatisquiteremarkableisthattheactual‚Äúcode‚Äùforexecutingonthesetofkeysandvalues,
namely the query, can be quite concise, even though the space to operate on is significant.
Thisisadesirablepropertyforanetworklayerasitdoesnotrequiretoomanyparametersto
learn. Just as convenient is the fact that attention can operate on arbitrarily large databases
without the need to change the way the attention pooling operation is performed.
413 Queries, Keys, and Values
tFig. 11.1.1 The attention mechanism computes a linear combination over values vivia attention
pooling, where weights are derived according to the compatibility between a query qand
keyski.
import torch
from d2l import torch asd2l
11.1.1Visualization
One of the benefits of the attention mechanism is that it can be quite intuitive, particularly
when the weights are nonnegative and sum to 1. In this case we might interpret large
weights as a way for the model to select components of relevance. While this is a good
intuition, it is important to remember that it is just that, an intuition . Regardless, we may
want to visualize its effect on the given set of keys when applying a variety of different
queries. This function will come in handy later.
Wethusdefinethe show_heatmaps function. Notethatitdoesnottakeamatrix(ofattention
weights) as its input but rather a tensor with four axes, allowing for an array of different
queries and weights. Consequently the input matrices has the shape (number of rows
for display, number of columns for display, number of queries, number of keys). This
will come in handy later on when we want to visualize the workings that are to design
Transformers.
#@save
def show_heatmaps (matrices, xlabel, ylabel, titles =None , figsize =(2.5,2.5),
cmap ='Reds '):
"""Show heatmaps of matrices."""
d2l.use_svg_display()
num_rows, num_cols, _, _ =matrices .shape
fig, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize,
sharex =True , sharey =True , squeeze =False )
for i, (row_axes, row_matrices) inenumerate (zip(axes, matrices)):
for j, (ax, matrix) inenumerate (zip(row_axes, row_matrices)):
pcm =ax.imshow(matrix .detach() .numpy(), cmap =cmap)
ifi==num_rows -1:
ax.set_xlabel(xlabel)
ifj==0:
ax.set_ylabel(ylabel)
iftitles:
(continues on next page)
414 Attention Mechanisms and Transformers
(continued from previous page)
ax.set_title(titles[j])
fig.colorbar(pcm, ax =axes, shrink =0.6);
As a quick sanity check let‚Äôs visualize the identity matrix, representing a case where the
attention weight is 1only when the query and the key are the same.
attention_weights =torch .eye( 10).reshape(( 1,1,10,10))
show_heatmaps(attention_weights, xlabel ='Keys ', ylabel ='Queries ')
11.1.2Summary
The attention mechanism allows us to aggregate data from many (key, value) pairs. So
far our discussion was quite abstract, simply describing a way to pool data. We have not
explained yet where those mysterious queries, keys, and values might arise from. Some
intuition might help here: for instance, in a regression setting, the query might correspond
to the location where the regression should be carried out. The keys are the locations
where past data was observed and the values are the (regression) values themselves. This
is the so-called Nadaraya‚ÄìWatson estimator ( Nadaraya, 1964 ,Watson, 1964 ) that we will
be studying in the next section.
By design, the attention mechanism provides a differentiable means of control by which a
neural network can select elements from a set and to construct an associated weighted sum
over representations.
11.1.3Exercises
1.Suppose that you wanted to reimplement approximate (key, query) matches as used in
classical databases, which attention function would you pick?
2.Suppose that the attention function is given by ùëé¬πq,kùëñ¬∫=q>kùëñand that kùëñ=vùëñfor
ùëñ=1,...,ùëö. Denote by ùëù¬πkùëñ;q¬∫the probability distribution over keys when using the
softmax normalization in (11.1.3 ). Prove thatrqAttention¬πq,D¬∫=Covùëù¬πkùëñ;q¬∫¬ªkùëñ¬º.
3.Design a differentiable search engine using the attention mechanism.
4.ReviewthedesignoftheSqueezeandExcitationNetworks( Huetal.,2018)andinterpret
them through the lens of the attention mechanism.
415 Attention Pooling by Similarity
154
155Discussions154.
11.2Attention PoolingbySimilarity
Now that we have introduced the primary components of the attention mechanism, let‚Äôs
use them in a rather classical setting, namely regression and classification via kernel den-
sity estimation ( Nadaraya, 1964 ,Watson, 1964 ). This detour simply provides additional
background: it is entirely optional and can be skipped if needed. At their core, Nadaraya‚Äì
Watsonestimatorsrelyonsomesimilaritykernel ùõº¬πq,k¬∫relatingqueries qtokeys k. Some
common kernels are
ùõº¬πq,k¬∫=exp
 1
2kq kk2
Gaussian;
ùõº¬πq,k¬∫=1ifkq kk1 Boxcar;
ùõº¬πq,k¬∫=max¬π0,1 kq kk¬∫Epanechikov.(11.2.1)
There are many more choices that we could pick. See a Wikipedia article155for a more
extensivereviewandhowthechoiceofkernelsisrelatedtokerneldensityestimation,some-
times also called Parzen Windows (Parzen, 1957 ). All of the kernels are heuristic and can
be tuned. For instance, we can adjust the width, not only on a global basis but even on a
per-coordinate basis. Regardless, all of them lead to the following equation for regression
and classification alike:
ùëì¬πq¬∫=√ï
ùëñvùëñùõº¬πq,kùëñ¬∫√ç
ùëóùõº¬πq,kùëó¬∫. (11.2.2)
Inthecaseofa(scalar)regressionwithobservations ¬πxùëñ,ùë¶ùëñ¬∫forfeaturesandlabelsrespec-
tively, vùëñ=ùë¶ùëñare scalars, kùëñ=xùëñare vectors, and the query qdenotes the new location
whereùëìshould be evaluated. In the case of (multiclass) classification, we use one-hot-
encoding of ùë¶ùëñto obtain vùëñ. One of the convenient properties of this estimator is that it re-
quiresnotraining. Evenmoreso, ifwesuitablynarrowthekernelwithincreasingamounts
of data, the approach is consistent ( Mack and Silverman, 1982 ), i.e., it will converge to
some statistically optimal solution. Let‚Äôs start by inspecting some kernels.
import numpy asnp
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
d2l.use_svg_display()
11.2.1Kernelsand Data
All the kernels ùõº¬πk,q¬∫defined in this section are translation and rotation invariant ; that
is, if we shift and rotate kandqin the same manner, the value of ùõºremains unchanged.
416 Attention Mechanisms and Transformers
For simplicity we thus pick scalar arguments ùëò,ùëû2Rand pick the key ùëò=0as the origin.
This yields:
# Define some kernels
def gaussian (x):
return torch .exp( -x**2/2)
def boxcar (x):
return torch .abs(x) <1.0
def constant (x):
return 1.0 +0*x
def epanechikov (x):
return torch .max( 1-torch .abs(x), torch .zeros_like(x))
fig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))
kernels =(gaussian, boxcar, constant, epanechikov)
names =('Gaussian ','Boxcar ','Constant ','Epanechikov ')
x=torch .arange( -2.5,2.5,0.1)
for kernel, name, ax inzip(kernels, names, axes):
ax.plot(x .detach() .numpy(), kernel(x) .detach() .numpy())
ax.set_xlabel(name)
d2l.plt.show()
Different kernels correspond to different notions of range and smoothness. For instance,
the boxcar kernel only attends to observations within a distance of 1(or some otherwise
defined hyperparameter) and does so indiscriminately.
To see Nadaraya‚ÄìWatson estimation in action, let‚Äôs define some training data. In the fol-
lowing we use the dependency
ùë¶ùëñ=2 sin¬πùë•ùëñ¬∫¬∏ùë•ùëñ¬∏ùúñ, (11.2.3)
whereùúñis drawn from a normal distribution with zero mean and unit variance. We draw
40 training examples.
417 Attention Pooling by Similarity
def f(x):
return 2*torch .sin(x) +x
n=40
x_train, _ =torch .sort(torch .rand(n) *5)
y_train =f(x_train) +torch .randn(n)
x_val =torch .arange( 0,5,0.1)
y_val =f(x_val)
11.2.2AttentionPoolingvia Nadaraya‚ÄìWatsonRegression
Now that we have data and kernels, all we need is a function that computes the kernel
regression estimates. Note that we also want to obtain the relative kernel weights in order
toperformsomeminordiagnostics. Hencewefirstcomputethekernelbetweenalltraining
features(covariates) x_train andallvalidationfeatures x_val. Thisyieldsamatrix,which
we subsequently normalize. When multiplied with the training labels y_train we obtain
the estimates.
Recall attention pooling in (11.1.1 ). Let each validation feature be a query, and each
training feature‚Äìlabel pair be a key‚Äìvalue pair. As a result, the normalized relative ker-
nel weights ( attention_w below) are the attentionweights .
def nadaraya_watson (x_train, y_train, x_val, kernel):
dists =x_train .reshape(( -1,1))-x_val .reshape(( 1,-1))
# Each column/row corresponds to each query/key
k=kernel(dists) .type(torch .float32)
# Normalization over keys for each query
attention_w =k/k.sum( 0)
y_hat =y_train @attention_w
return y_hat, attention_w
Let‚Äôs have a look at the kind of estimates that the different kernels produce.
def plot (x_train, y_train, x_val, y_val, kernels, names, attention =False ):
fig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))
for kernel, name, ax inzip(kernels, names, axes):
y_hat, attention_w =nadaraya_watson(x_train, y_train, x_val, kernel)
ifattention:
pcm =ax.imshow(attention_w .detach() .numpy(), cmap ='Reds ')
else :
ax.plot(x_val, y_hat)
ax.plot(x_val, y_val, 'm--')
ax.plot(x_train, y_train, 'o', alpha =0.5);
ax.set_xlabel(name)
ifnot attention:
ax.legend([ 'y_hat ','y'])
ifattention:
fig.colorbar(pcm, ax =axes, shrink =0.7)
418 Attention Mechanisms and Transformers
plot(x_train, y_train, x_val, y_val, kernels, names)
The first thing that stands out is that all three nontrivial kernels (Gaussian, Boxcar, and
Epanechikov) produce fairly workable estimates that are not too far from the true function.
Only the constant kernel that leads to the trivial estimate ùëì¬πùë•¬∫=1
ùëõ√ç
ùëñùë¶ùëñproduces a rather
unrealistic result. Let‚Äôs inspect the attention weighting a bit more closely:
plot(x_train, y_train, x_val, y_val, kernels, names, attention =True )
The visualization clearly shows why the estimates for Gaussian, Boxcar, and Epanechikov
are very similar: after all, they are derived from very similar attention weights, despite the
different functional form of the kernel. This raises the question as to whether this is always
the case.
11.2.3AdaptingAttentionPooling
We could replace the Gaussian kernel with one of a different width. That is, we could use
ùõº¬πq,k¬∫=exp
 1
2ùúé2kq kk2
whereùúé2determines the width of the kernel. Let‚Äôs see
whether this affects the outcomes.
sigmas =(0.1,0.2,0.5,1)
names =['Sigma '+str(sigma) for sigma insigmas]
def gaussian_with_width (sigma):
return (lambda x: torch .exp( -x**2/(2*sigma **2)))
kernels =[gaussian_with_width(sigma) for sigma insigmas]
plot(x_train, y_train, x_val, y_val, kernels, names)
419 Attention Pooling by Similarity
Clearly, the narrower the kernel, the less smooth the estimate. At the same time, it adapts
better to the local variations. Let‚Äôs look at the corresponding attention weights.
plot(x_train, y_train, x_val, y_val, kernels, names, attention =True )
As we would expect, the narrower the kernel, the narrower the range of large attention
weights. It is also clear that picking the same width might not be ideal. In fact, Silverman
(1986) proposed a heuristic that depends on the local density. Many more such ‚Äútricks‚Äù
have been proposed. For instance, Norelli et al.(2022) used a similar nearest-neighbor
interpolation technique for designing cross-modal image and text representations.
Theastutereadermightwonderwhyweareprovidingthisdeepdiveforamethodthatisover
halfacenturyold. First,itisoneoftheearliestprecursorsofmodernattentionmechanisms.
Second,itisgreatforvisualization. Third,andjustasimportantly,itdemonstratesthelimits
of hand-crafted attention mechanisms. A much better strategy is to learnthe mechanism,
by learning the representations for queries and keys. This is what we will embark on in the
following sections.
11.2.4Summary
Nadaraya‚ÄìWatson kernel regression is an early precursor of the current attention mecha-
nisms. It can be used directly with little to no training or tuning, either for classification or
regression. The attention weight is assigned according to the similarity (or distance) be-
tweenqueryandkey,andaccordingtohowmanysimilarobservationsareavailable.
11.2.5Exercises
420 Attention Mechanisms and Transformers
1561.Parzen windows density estimates are given by ÀÜùëù¬πx¬∫=1
ùëõ√ç
ùëñùëò¬πx,xùëñ¬∫. Prove that for
binary classification the function ÀÜùëù¬πx,ùë¶=1¬∫  ÀÜùëù¬πx,ùë¶= 1¬∫, as obtained by Parzen
windows is equivalent to Nadaraya‚ÄìWatson classification.
2.ImplementstochasticgradientdescenttolearnagoodvalueforkernelwidthsinNadaraya‚Äì
Watson regression.
1.Whathappensifyoujustusetheaboveestimatestominimize ¬πùëì¬πxi¬∫ ùë¶ùëñ¬∫2directly?
Hint:ùë¶ùëñis part of the terms used to compute ùëì.
2.Remove¬πxùëñ,ùë¶ùëñ¬∫from the estimate for ùëì¬πxùëñ¬∫and optimize over the kernel widths.
Do you still observe overfitting?
3.Assume that all xlie on the unit sphere, i.e., all satisfy kxk=1. Can you simplify the
kx xùëñk2termintheexponential? Hint: wewilllaterseethatthisisverycloselyrelated
to dot product attention.
4.RecallthatMackandSilverman( 1982)provedthatNadaraya‚ÄìWatsonestimationiscon-
sistent. Howquicklyshouldyoureducethescalefortheattentionmechanismasyouget
more data? Provide some intuition for your answer. Does it depend on the dimension-
ality of the data? How?
Discussions156.
11.3AttentionScoring Functions
InSection11.2 ,weusedanumberofdifferentdistance-basedkernels,includingaGaussian
kernel to model interactions between queries and keys. As it turns out, distance functions
are slightly more expensive to compute than dot products. As such, with the softmax op-
eration to ensure nonnegative attention weights, much of the work has gone into attention
scoringfunctions ùëéin(11.1.3 )andFig. 11.3.1 that are simpler to compute.
tFig. 11.3.1 Computing the output of attention pooling as a weighted average of values, where weights
are computed with the attention scoring function aand the softmax operation.
421 Attention Scoring Functions
import math
import torch
from torch import nn
from d2l import torch asd2l
11.3.1DotProductAttention
Let‚Äôs review the attention function (without exponentiation) from the Gaussian kernel for
a moment:
ùëé¬πq,kùëñ¬∫= 1
2kq kùëñk2=q>kùëñ 1
2kkùëñk2 1
2kqk2. (11.3.1)
First, note that the final term depends on qonly. As such it is identical for all ¬πq,kùëñ¬∫
pairs. Normalizing the attention weights to 1, as is done in (11.1.3 ), ensures that this term
disappears entirely. Second, note that both batch and layer normalization (to be discussed
later) lead to activations that have well-bounded, and often constant, norms kkùëñk. This is
the case, for instance, whenever the keys kùëñwere generated by a layer norm. As such, we
can drop it from the definition of ùëéwithout any major change in the outcome.
Last, we need to keep the order of magnitude of the arguments in the exponential function
under control. Assume that all the elements of the query q2Rùëëand the key kùëñ2Rùëë
are independent and identically drawn random variables with zero mean and unit variance.
The dot product between both vectors has zero mean and a variance of ùëë. To ensure that
thevarianceofthedotproductstillremains 1regardlessofvectorlength, weusethe scaled
dot product attention scoring function. That is, we rescale the dot product by 1¬ùp
ùëë. We
thus arrive at the first commonly used attention function that is used, e.g., in Transformers
(Vaswanietal., 2017):
ùëé¬πq,kùëñ¬∫=q>kùëñ¬ùp
ùëë. (11.3.2)
Note that attention weights ùõºstill need normalizing. We can simplify this further via
(11.1.3 )by using the softmax operation:
ùõº¬πq,kùëñ¬∫=softmax¬πùëé¬πq,kùëñ¬∫¬∫=exp¬πq>kùëñ¬ùp
ùëë¬∫
√ç
ùëó=1exp¬πq>kùëó¬ùp
ùëë¬∫. (11.3.3)
As it turns out, all popular attention mechanisms use the softmax, hence we will limit
ourselves to that in the remainder of this chapter.
11.3.2ConvenienceFunctions
Weneedafewfunctionstomaketheattentionmechanismefficienttodeploy. Thisincludes
toolsfordealingwithstringsofvariablelengths(commonfornaturallanguageprocessing)
and tools for efficient evaluation on minibatches (batch matrix multiplication).
MaskedSoftmax Operation
One of the most popular applications of the attention mechanism is to sequence models.
Hence we need to be able to deal with sequences of different lengths. In some cases, such
422 Attention Mechanisms and Transformers
sequences may end up in the same minibatch, necessitating padding with dummy tokens
for shorter sequences (see Section 10.5 for an example). These special tokens do not carry
meaning. For instance, assume that we have the following three sentences:
Dive into Deep Learning
Learn to code <blank >
Hello world <blank ><blank >
Sincewedonotwantblanksinourattentionmodelwesimplyneedtolimit√çùëõ
ùëñ=1ùõº¬πq,kùëñ¬∫vùëñ
to√çùëô
ùëñ=1ùõº¬πq,kùëñ¬∫vùëñforhoweverlong, ùëôùëõ,theactualsentenceis. Sinceitissuchacommon
problem, it has a name: the maskedsoftmax operation .
Let‚Äôsimplementit. Actually,theimplementationcheatseversoslightlybysettingthevalues
ofvùëñ, forùëñ >ùëô, to zero. Moreover, it sets the attention weights to a large negative number,
suchas 106,inordertomaketheircontributiontogradientsandvaluesvanishinpractice.
This is done since linear algebra kernels and operators are heavily optimized for GPUs and
it is faster to be slightly wasteful in computation rather than to have code with conditional
(if then else) statements.
def masked_softmax (X, valid_lens): #@save
"""Perform softmax operation by masking elements on the last axis."""
# X: 3D tensor, valid_lens: 1D or 2D tensor
def _sequence_mask (X, valid_len, value =0):
maxlen =X.size( 1)
mask =torch .arange((maxlen), dtype =torch .float32,
device =X.device)[ None , :] <valid_len[:, None ]
X[~mask] =value
return X
ifvalid_lens isNone :
return nn.functional .softmax(X, dim =-1)
else :
shape =X.shape
ifvalid_lens .dim() ==1:
valid_lens =torch .repeat_interleave(valid_lens, shape[ 1])
else :
valid_lens =valid_lens .reshape( -1)
# On the last axis, replace masked elements with a very large negative
# value, whose exponentiation outputs 0
X=_sequence_mask(X .reshape( -1, shape[ -1]), valid_lens, value =-1e6)
return nn.functional .softmax(X .reshape(shape), dim =-1)
To illustrate how this function works, consider a minibatch of two examples of size 24,
where their valid lengths are 2and3, respectively. As a result of the masked softmax oper-
ation,valuesbeyondthevalidlengthsforeachpairofvectorsareallmaskedaszero.
masked_softmax(torch .rand( 2,2,4), torch .tensor([ 2,3]))
tensor([[[ 0.4448 ,0.5552 ,0.0000 ,0.0000 ],
[0.4032 ,0.5968 ,0.0000 ,0.0000 ]],
(continues on next page)
423 Attention Scoring Functions
(continued from previous page)
[[0.2795 ,0.2805 ,0.4400 ,0.0000 ],
[0.2798 ,0.3092 ,0.4110 ,0.0000 ]]])
If we need more fine-grained control to specify the valid length for each of the two vec-
tors of every example, we simply use a two-dimensional tensor of valid lengths. This
yields:
masked_softmax(torch .rand( 2,2,4), torch .tensor([[ 1,3], [ 2,4]]))
tensor([[[ 1.0000 ,0.0000 ,0.0000 ,0.0000 ],
[0.4109 ,0.2794 ,0.3097 ,0.0000 ]],
[[0.3960 ,0.6040 ,0.0000 ,0.0000 ],
[0.2557 ,0.1833 ,0.2420 ,0.3190 ]]])
BatchMatrix Multiplication
Another commonly used operation is to multiply batches of matrices by one another. This
comes in handy when we have minibatches of queries, keys, and values. More specifically,
assume that
Q=¬ªQ1,Q2,...,Qùëõ¬º2Rùëõùëéùëè,
K=¬ªK1,K2,...,Kùëõ¬º2Rùëõùëèùëê.(11.3.4)
Then the batch matrix multiplication (BMM) computes the elementwise product
BMM¬πQ,K¬∫=¬ªQ1K1,Q2K2,...,QùëõKùëõ¬º2Rùëõùëéùëê. (11.3.5)
Let‚Äôs see this in action in a deep learning framework.
Q=torch .ones(( 2,3,4))
K=torch .ones(( 2,4,6))
d2l.check_shape(torch .bmm(Q, K), ( 2,3,6))
11.3.3Scaled DotProductAttention
Let‚Äôs return to the dot product attention introduced in (11.3.2 ). In general, it requires that
both the query and the key have the same vector length, say ùëë, even though this can be
addressed easily by replacing q>kwithq>Mkwhere Mis a matrix suitably chosen for
translating between both spaces. For now assume that the dimensions match.
In practice, we often think of minibatches for efficiency, such as computing attention for
ùëõqueries andùëökey-value pairs, where queries and keys are of length ùëëand values are of
lengthùë£. The scaled dot product attention of queries Q2Rùëõùëë, keysK2Rùëöùëë, and
424 Attention Mechanisms and Transformers
values V2Rùëöùë£thus can be written as
softmaxQK>
p
ùëë
V2Rùëõùë£. (11.3.6)
Notethatwhenapplyingthistoaminibatch, weneedthebatchmatrixmultiplicationintro-
duced in (11.3.5 ). In the following implementation of the scaled dot product attention, we
use dropout for model regularization.
class DotProductAttention (nn.Module): #@save
"""Scaled dot product attention."""
def __init__ (self , dropout):
super ().__init__ ()
self .dropout =nn.Dropout(dropout)
# Shape of queries: (batch_size, no. of queries, d)
# Shape of keys: (batch_size, no. of key-value pairs, d)
# Shape of values: (batch_size, no. of key-value pairs, value dimension)
# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)
def forward (self , queries, keys, values, valid_lens =None ):
d=queries .shape[ -1]
# Swap the last two dimensions of keys with keys.transpose(1, 2)
scores =torch .bmm(queries, keys .transpose( 1,2))/math .sqrt(d)
self .attention_weights =masked_softmax(scores, valid_lens)
return torch .bmm( self .dropout( self .attention_weights), values)
To illustrate how the DotProductAttention class works, we use the same keys, values,
and valid lengths from the earlier toy example for additive attention. For the purpose of
our example we assume that we have a minibatch size of 2, a total of 10keys and values,
and that the dimensionality of the values is 4. Lastly, we assume that the valid length per
observationis 2and6respectively. Giventhat,weexpecttheoutputtobea 214tensor,
i.e., one row per example of the minibatch.
queries =torch .normal( 0,1, (2,1,2))
keys =torch .normal( 0,1, (2,10,2))
values =torch .normal( 0,1, (2,10,4))
valid_lens =torch .tensor([ 2,6])
attention =DotProductAttention(dropout =0.5)
attention .eval()
d2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))
Let‚Äôs check whether the attention weights actually vanish for anything beyond the second
and sixth column respectively (because of setting the valid length to 2and6).
d2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),
xlabel ='Keys ', ylabel ='Queries ')
11.3.4AdditiveAttention
Whenqueries qandkeys karevectorsofdifferentdimension,wecaneitheruseamatrixto
address the mismatch via q>Mk, or we can use additive attention as the scoring function.
425 Attention Scoring Functions
Another benefit is that, as its name indicates, the attention is additive. This can lead to
some minor computational savings. Given a query q2Rùëûand a key k2Rùëò, theadditive
attention scoring function ( Bahdanau etal., 2014) is given by
ùëé¬πq,k¬∫=w>
ùë£tanh¬πWùëûq¬∏Wùëòk¬∫2R, (11.3.7)
where Wùëû2R‚Ñéùëû,Wùëò2R‚Ñéùëò, andwùë£2R‚Ñéare the learnable parameters. This term
is then fed into a softmax to ensure both nonnegativity and normalization. An equivalent
interpretation of (11.3.7 )is that the query and key are concatenated and fed into an MLP
with a single hidden layer. Using tanhas the activation function and disabling bias terms,
we implement additive attention as follows:
class AdditiveAttention (nn.Module): #@save
"""Additive attention."""
def __init__ (self , num_hiddens, dropout, **kwargs):
super (AdditiveAttention, self ).__init__ (**kwargs)
self .W_k =nn.LazyLinear(num_hiddens, bias =False )
self .W_q =nn.LazyLinear(num_hiddens, bias =False )
self .w_v =nn.LazyLinear( 1, bias =False )
self .dropout =nn.Dropout(dropout)
def forward (self , queries, keys, values, valid_lens):
queries, keys =self .W_q(queries), self .W_k(keys)
# After dimension expansion, shape of queries: (batch_size, no. of
# queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of
# key-value pairs, num_hiddens). Sum them up with broadcasting
features =queries .unsqueeze( 2)+keys .unsqueeze( 1)
features =torch .tanh(features)
# There is only one output of self.w_v, so we remove the last
# one-dimensional entry from the shape. Shape of scores: (batch_size,
# no. of queries, no. of key-value pairs)
scores =self .w_v(features) .squeeze( -1)
self .attention_weights =masked_softmax(scores, valid_lens)
# Shape of values: (batch_size, no. of key-value pairs, value
# dimension)
return torch .bmm( self .dropout( self .attention_weights), values)
Let‚Äôs see how AdditiveAttention works. In our toy example we pick queries, keys and
valuesofsize¬π2,1,20¬∫,¬π2,10,2¬∫and¬π2,10,4¬∫,respectively. Thisisidenticaltoourchoice
forDotProductAttention , except that now the queries are 20-dimensional. Likewise, we
pick¬π2,6¬∫as the valid lengths for the sequences in the minibatch.
queries =torch .normal( 0,1, (2,1,20))
(continues on next page)
426 Attention Mechanisms and Transformers
157
158(continued from previous page)
attention =AdditiveAttention(num_hiddens =8, dropout =0.1)
attention .eval()
d2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))
When reviewing the attention function we see a behavior that is qualitatively quite similar
to that of DotProductAttention . That is, onlyterms within the chosenvalid length ¬π2,6¬∫
are nonzero.
d2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),
xlabel ='Keys ', ylabel ='Queries ')
11.3.5Summary
Inthissectionweintroducedthetwokeyattentionscoringfunctions: dotproductandaddi-
tive attention. They are effective tools for aggregating across sequences of variable length.
Inparticular,thedotproductattentionisthemainstayofmodernTransformerarchitectures.
When queries and keys are vectors of different lengths, we can use the additive attention
scoring function instead. Optimizing these layers is one of the key areas of advance in re-
cent years. For instance, NVIDIA‚Äôs Transformer Library157and Megatron ( Shoeybietal.,
2019) crucially rely on efficient variants of the attention mechanism. We will dive into this
in quite a bit more detail as we review Transformers in later sections.
11.3.6Exercises
1.Implementdistance-basedattentionbymodifyingthe DotProductAttention code. Note
that you only need the squared norms of the keys kkùëñk2for an efficient implementation.
2.Modify the dot product attention to allow for queries and keys of different dimension-
alities by employing a matrix to adjust dimensions.
3.How does the computational cost scale with the dimensionality of the keys, queries,
values, and their number? What about the memory bandwidth requirements?
Discussions158.
427 The Bahdanau Attention Mechanism
11.4TheBahdanau AttentionMechanism
Whenweencounteredmachinetranslationin Section10.7 ,wedesignedanencoder‚Äìdecoder
architectureforsequence-to-sequencelearningbasedontwoRNNs( Sutskever etal.,2014).
Specifically, the RNN encoder transforms a variable-length sequence into a fixed-shape
context variable. Then, the RNN decoder generates the output (target) sequence token by
token based on the generated tokens and the context variable.
RecallFig. 10.7.2 which we repeat ( Fig. 11.4.1 ) with some additional detail. Convention-
ally, in an RNN all relevant information about a source sequence is translated into some
internalfixed-dimensional state representation by the encoder. It is this very state that is
used by the decoder as the complete and exclusive source of information for generating the
translated sequence. In other words, the sequence-to-sequence mechanism treats the inter-
mediate state as a sufficient statistic of whatever string might have served as input.
tFig. 11.4.1 Sequence-to-sequence model. The state, as generated by the encoder, is the only piece of
information shared between the encoder and the decoder.
Whilethisisquitereasonableforshortsequences,itisclearthatitisinfeasibleforlongones,
suchasabookchapterorevenjustaverylongsentence. Afterall,beforetoolongtherewill
simplynotbeenough‚Äúspace‚Äùintheintermediaterepresentationtostoreallthatisimportant
in the source sequence. Consequently the decoder will fail to translate long and complex
sentences. One of the first to encounter this was Graves ( 2013) who tried to design an
RNNtogeneratehandwrittentext. Sincethesourcetexthasarbitrarylengththeydesigneda
differentiableattentionmodeltoaligntextcharacterswiththemuchlongerpentrace,where
the alignment moves only in one direction. This, in turn, draws on decoding algorithms in
speech recognition, e.g., hidden Markov models ( Rabiner and Juang, 1993 ).
Inspired by the idea of learning to align, Bahdanau et al.(2014) proposed a differentiable
attention model withoutthe unidirectional alignment limitation. When predicting a token,
ifnotalltheinputtokensarerelevant,themodelaligns(orattends)onlytopartsoftheinput
sequencethataredeemedrelevanttothecurrentprediction. Thisisthenusedtoupdatethe
currentstatebeforegeneratingthenexttoken. Whilequiteinnocuousinitsdescription,this
Bahdanau attention mechanism has arguably turned into one of the most influential ideas
of the past decade in deep learning, giving rise to Transformers ( Vaswanietal., 2017) and
many related new architectures.
428 Attention Mechanisms and Transformers
import torch
from torch import nn
from d2l import torch asd2l
11.4.1Model
We follow the notation introduced by the sequence-to-sequence architecture of Section
10.7, in particular (10.7.3 ). The key idea is that instead of keeping the state, i.e., the con-
text variable csummarizing the source sentence, as fixed, we dynamically update it, as a
function of both the original text (encoder hidden states hùë°) and the text that was already
generated (decoder hidden states sùë°0 1). This yields cùë°0, which is updated after any decod-
ing time step ùë°0. Suppose that the input sequence is of length ùëá. In this case the context
variable is the output of attention pooling:
cùë°0=ùëá√ï
ùë°=1ùõº¬πsùë°0 1,hùë°¬∫hùë°. (11.4.1)
We used sùë°0 1as the query, and hùë°as both the key and the value. Note that cùë°0is then
used to generate the state sùë°0and to generate a new token: see (10.7.3 ). In particular, the
attention weight ùõºis computed as in (11.3.3 )using the additive attention scoring function
definedby (11.3.7 ). ThisRNNencoder‚Äìdecoderarchitectureusingattentionisdepictedin
Fig. 11.4.2 . Note that later this model was modified so as to include the already generated
tokensinthedecoderasfurthercontext(i.e., theattentionsumdoesnotstopat ùëábutrather
it proceeds up to ùë°0 1). For instance, see Chan et al.(2015) for a description of this
strategy, as applied to speech recognition.
tFig. 11.4.2 Layers in an RNN encoder‚Äìdecoder model with the Bahdanau attention mechanism.
11.4.2Defining the Decoder with Attention
To implement the RNN encoder‚Äìdecoder with attention, we only need to redefine the de-
coder (omitting the generated symbols from the attention function simplifies the design).
Let‚Äôs begin with the base interface for decoders with attention by defining the quite unsur-
prisingly named AttentionDecoder class.
429 The Bahdanau Attention Mechanism
class AttentionDecoder (d2l .Decoder): #@save
"""The base attention-based decoder interface."""
def __init__ (self ):
super ().__init__ ()
@property
def attention_weights (self ):
raise NotImplementedError
We need to implement the RNN decoder in the Seq2SeqAttentionDecoder class. The
state of the decoder is initialized with (i) the hidden states of the last layer of the encoder
at all time steps, used as keys and values for attention; (ii) the hidden state of the encoder
at all layers at the final time step, which serves to initialize the hidden state of the decoder;
and (iii) the valid length of the encoder, to exclude the padding tokens in attention pooling.
At each decoding time step, the hidden state of the final layer of the decoder, obtained at
the previous time step, is used as the query of the attention mechanism. Both the output of
the attention mechanism and the input embedding are concatenated to serve as the input of
the RNN decoder.
class Seq2SeqAttentionDecoder (AttentionDecoder):
def __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,
dropout =0):
super ().__init__ ()
self .attention =d2l.AdditiveAttention(num_hiddens, dropout)
self .embedding =nn.Embedding(vocab_size, embed_size)
self .rnn =nn.GRU(
embed_size +num_hiddens, num_hiddens, num_layers,
dropout =dropout)
self .dense =nn.LazyLinear(vocab_size)
self .apply(d2l .init_seq2seq)
def init_state (self , enc_outputs, enc_valid_lens):
# Shape of outputs: (num_steps, batch_size, num_hiddens).
# Shape of hidden_state: (num_layers, batch_size, num_hiddens)
outputs, hidden_state =enc_outputs
return (outputs .permute( 1,0,2), hidden_state, enc_valid_lens)
def forward (self , X, state):
# Shape of enc_outputs: (batch_size, num_steps, num_hiddens).
# Shape of hidden_state: (num_layers, batch_size, num_hiddens)
enc_outputs, hidden_state, enc_valid_lens =state
# Shape of the output X: (num_steps, batch_size, embed_size)
X=self .embedding(X) .permute( 1,0,2)
outputs, self ._attention_weights =[], []
for xinX:
# Shape of query: (batch_size, 1, num_hiddens)
query =torch .unsqueeze(hidden_state[ -1], dim =1)
# Shape of context: (batch_size, 1, num_hiddens)
context =self .attention(
query, enc_outputs, enc_outputs, enc_valid_lens)
# Concatenate on the feature dimension
x=torch .cat((context, torch .unsqueeze(x, dim =1)), dim =-1)
(continues on next page)
430 Attention Mechanisms and Transformers
(continued from previous page)
# Reshape x as (1, batch_size, embed_size + num_hiddens)
out, hidden_state =self .rnn(x .permute( 1,0,2), hidden_state)
outputs .append(out)
self ._attention_weights .append( self .attention .attention_weights)
# After fully connected layer transformation, shape of outputs:
# (num_steps, batch_size, vocab_size)
outputs =self .dense(torch .cat(outputs, dim =0))
return outputs .permute( 1,0,2), [enc_outputs, hidden_state,
enc_valid_lens]
@property
def attention_weights (self ):
return self ._attention_weights
In the following, we test the implemented decoder with attention using a minibatch of four
sequences, each of which are seven time steps long.
vocab_size, embed_size, num_hiddens, num_layers =10,8,16,2
batch_size, num_steps =4,7
encoder =d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
decoder =Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,
num_layers)
X=torch .zeros((batch_size, num_steps), dtype =torch .long)
state =decoder .init_state(encoder(X), None )
output, state =decoder(X, state)
d2l.check_shape(output, (batch_size, num_steps, vocab_size))
d2l.check_shape(state[ 0], (batch_size, num_steps, num_hiddens))
d2l.check_shape(state[ 1][0], (batch_size, num_hiddens))
11.4.3Training
Now that we specified the new decoder we can proceed analogously to Section 10.7.6 :
specify the hyperparameters, instantiate a regular encoder and a decoder with attention,
and train this model for machine translation.
data =d2l.MTFraEng(batch_size =128)
embed_size, num_hiddens, num_layers, dropout =256,256,2,0.2
encoder =d2l.Seq2SeqEncoder(
len(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder =Seq2SeqAttentionDecoder(
len(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],
lr=0.005 )
trainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
After the model is trained, we use it to translate a few English sentences into French and
compute their BLEU scores.
431 The Bahdanau Attention Mechanism
engs =['go . ','i lost . ','he\'s calm . ','i\'m home . ']
fras =['va ! ','j\'ai perdu . ','il est calme . ','je suis chez moi . ']
preds, _ =model .predict_step(
data .build(engs, fras), d2l .try_gpu(), data .num_steps)
for en, fr, p inzip(engs, fras, preds):
translation =[]
for token indata .tgt_vocab .to_tokens(p):
iftoken =='<eos> ':
break
translation .append(token)
print (f'{en}=>{translation }, bleu, '
f'{d2l.bleu( "".join(translation), fr, k=2):.3f}')
go.=>['va','!'], bleu, 1.000
i lost .=>["j'ai",'perdu ','.'], bleu, 1.000
he's calm . => [ 'il','court ','.'], bleu,0.000
i'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000
Let‚ÄôsvisualizetheattentionweightswhentranslatingthelastEnglishsentence. Weseethat
each query assigns non-uniform weights over key‚Äìvalue pairs. It shows that at each decod-
ing step, different parts of the input sequences are selectively aggregated in the attention
pooling.
_, dec_attention_weights =model .predict_step(
data .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )
attention_weights =torch .cat(
[step[ 0][0][0]for step indec_attention_weights], 0)
attention_weights =attention_weights .reshape(( 1,1,-1, data .num_steps))
# Plus one to include the end-of-sequence token
d2l.show_heatmaps(
attention_weights[:, :, :, : len(engs[ -1].split()) +1].cpu(),
xlabel ='Key positions ', ylabel ='Query positions ')
11.4.4Summary
Whenpredictingatoken,ifnotalltheinputtokensarerelevant,theRNNencoder‚Äìdecoder
with the Bahdanau attention mechanism selectively aggregates different parts of the input
432 Attention Mechanisms and Transformers
159sequence. This is achieved by treating the state (context variable) as an output of additive
attention pooling. In the RNN encoder‚Äìdecoder, the Bahdanau attention mechanism treats
the decoder hidden state at the previous time step as the query, and the encoder hidden
states at all the time steps as both the keys and values.
11.4.5Exercises
1.Replace GRU with LSTM in the experiment.
2.Modifytheexperimenttoreplacetheadditiveattentionscoringfunctionwiththescaled
dot-product. How does it influence the training efficiency?
Discussions159.
11.5Multi-HeadAttention
In practice, given the same set of queries, keys, and values we may want our model to
combine knowledge from different behaviors of the same attention mechanism, such as
capturingdependenciesofvariousranges(e.g., shorter-rangevs.longer-range)withinase-
quence. Thus,itmaybebeneficialtoallowourattentionmechanismtojointlyusedifferent
representation subspaces of queries, keys, and values.
To this end, instead of performing a single attention pooling, queries, keys, and values can
be transformed with ‚Ñéindependently learned linear projections. Then these ‚Ñéprojected
queries, keys, and values are fed into attention pooling in parallel. In the end, ‚Ñéattention-
poolingoutputsareconcatenatedandtransformedwithanotherlearnedlinearprojectionto
produce the final output. This design is called multi-head attention , where each of the ‚Ñé
attention pooling outputs is a head(Vaswaniet al., 2017). Using fully connected layers to
performlearnablelineartransformations, Fig.11.5.1 describesmulti-headattention.
import math
import torch
from torch import nn
from d2l import torch asd2l
433 Multi-Head Attention
tFig. 11.5.1 Multi-head attention, where multiple heads are concatenated then linearly transformed.
11.5.1Model
Before providing the implementation of multi-head attention, let‚Äôs formalize this model
mathematically. Given a query q2Rùëëùëû, a key k2Rùëëùëò, and a value v2Rùëëùë£, each
attention head hùëñ(ùëñ=1,...,‚Ñé) is computed as
hùëñ=ùëì¬πW¬πùëû¬∫
ùëñq,W¬πùëò¬∫
ùëñk,W¬πùë£¬∫
ùëñv¬∫2Rùëùùë£, (11.5.1)
where W¬πùëû¬∫
ùëñ2Rùëùùëûùëëùëû,W¬πùëò¬∫
ùëñ2Rùëùùëòùëëùëò, andW¬πùë£¬∫
ùëñ2Rùëùùë£ùëëùë£are learnable parameters
andùëìis attention pooling, such as additive attention and scaled dot product attention in
Section11.3 . Themulti-headattentionoutputisanotherlineartransformationvialearnable
parameters Wùëú2Rùëùùëú‚Ñéùëùùë£of the concatenation of ‚Ñéheads:
Wùëú2666664h1
...
h‚Ñé37777752Rùëùùëú. (11.5.2)
Based on this design, each head may attend to different parts of the input. More sophisti-
cated functions than the simple weighted average can be expressed.
11.5.2Implementation
In our implementation, we choose the scaled dot product attention for each head of the
multi-head attention. To avoid significant growth of computational cost and parametriza-
tion cost, we set ùëùùëû=ùëùùëò=ùëùùë£=ùëùùëú¬ù‚Ñé. Note that‚Ñéheads can be computed in parallel
if we set the number of outputs of linear transformations for the query, key, and value to
ùëùùëû‚Ñé=ùëùùëò‚Ñé=ùëùùë£‚Ñé=ùëùùëú. Inthefollowingimplementation, ùëùùëúisspecifiedviatheargument
num_hiddens .
class MultiHeadAttention (d2l .Module): #@save
"""Multi-head attention."""
def __init__ (self , num_hiddens, num_heads, dropout, bias =False ,**kwargs):
super ().__init__ ()
self .num_heads =num_heads
self .attention =d2l.DotProductAttention(dropout)
self .W_q =nn.LazyLinear(num_hiddens, bias =bias)
self .W_k =nn.LazyLinear(num_hiddens, bias =bias)
self .W_v =nn.LazyLinear(num_hiddens, bias =bias)
(continues on next page)
434 Attention Mechanisms and Transformers
(continued from previous page)
self .W_o =nn.LazyLinear(num_hiddens, bias =bias)
def forward (self , queries, keys, values, valid_lens):
# Shape of queries, keys, or values:
# (batch_size, no. of queries or key-value pairs, num_hiddens)
# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)
# After transposing, shape of output queries, keys, or values:
# (batch_size * num_heads, no. of queries or key-value pairs,
# num_hiddens / num_heads)
queries =self .transpose_qkv( self .W_q(queries))
keys =self .transpose_qkv( self .W_k(keys))
values =self .transpose_qkv( self .W_v(values))
ifvalid_lens isnot None :
# On axis 0, copy the first item (scalar or vector) for num_heads
# times, then copy the next item, and so on
valid_lens =torch .repeat_interleave(
valid_lens, repeats =self .num_heads, dim =0)
# Shape of output: (batch_size * num_heads, no. of queries,
# num_hiddens / num_heads)
output =self .attention(queries, keys, values, valid_lens)
# Shape of output_concat: (batch_size, no. of queries, num_hiddens)
output_concat =self .transpose_output(output)
return self .W_o(output_concat)
Toallowforparallelcomputationofmultipleheads, theabove MultiHeadAttention class
uses two transposition methods as defined below. Specifically, the transpose_output
method reverses the operation of the transpose_qkv method.
@d2l .add_to_class(MultiHeadAttention) #@save
def transpose_qkv (self , X):
"""Transposition for parallel computation of multiple attention heads."""
# Shape of input X: (batch_size, no. of queries or key-value pairs,
# num_hiddens). Shape of output X: (batch_size, no. of queries or
# key-value pairs, num_heads, num_hiddens / num_heads)
X=X.reshape(X .shape[ 0], X .shape[ 1],self .num_heads, -1)
# Shape of output X: (batch_size, num_heads, no. of queries or key-value
# pairs, num_hiddens / num_heads)
X=X.permute( 0,2,1,3)
# Shape of output: (batch_size * num_heads, no. of queries or key-value
# pairs, num_hiddens / num_heads)
return X.reshape( -1, X.shape[ 2], X .shape[ 3])
@d2l .add_to_class(MultiHeadAttention) #@save
def transpose_output (self , X):
"""Reverse the operation of transpose_qkv."""
X=X.reshape( -1,self .num_heads, X .shape[ 1], X .shape[ 2])
X=X.permute( 0,2,1,3)
return X.reshape(X .shape[ 0], X .shape[ 1],-1)
Let‚Äôs test our implemented MultiHeadAttention class using a toy example where keys
435 Self-Attention and Positional Encoding
160and values are the same. As a result, the shape of the multi-head attention output is
(batch_size ,num_queries ,num_hiddens ).
num_hiddens, num_heads =100,5
attention =MultiHeadAttention(num_hiddens, num_heads, 0.5)
batch_size, num_queries, num_kvpairs =2,4,6
valid_lens =torch .tensor([ 3,2])
X=torch .ones((batch_size, num_queries, num_hiddens))
Y=torch .ones((batch_size, num_kvpairs, num_hiddens))
d2l.check_shape(attention(X, Y, Y, valid_lens),
(batch_size, num_queries, num_hiddens))
11.5.3Summary
Multi-headattentioncombinesknowledgeofthesameattentionpoolingviadifferentrepre-
sentation subspaces of queries, keys, and values. To compute multiple heads of multi-head
attention in parallel, proper tensor manipulation is needed.
11.5.4Exercises
1.Visualize attention weights of multiple heads in this experiment.
2.Suppose that we have a trained model based on multi-head attention and we want to
prune less important attention heads to increase the prediction speed. How can we de-
sign experiments to measure the importance of an attention head?
Discussions160.
11.6Self-Attentionand PositionalEncoding
In deep learning, we often use CNNs or RNNs to encode sequences. Now with attention
mechanisms in mind, imagine feeding a sequence of tokens into an attention mechanism
suchthatateverystep,eachtokenhasitsownquery,keys,andvalues. Here,whencomput-
ingthevalueofatoken‚Äôsrepresentationatthenextlayer,thetokencanattend(viaitsquery
vector) to any other‚Äôs token (matching based on their key vectors). Using the full set of
query-key compatibility scores, we can compute, for each token, a representation by build-
ing the appropriate weighted sum over the other tokens. Because every token is attending
to each other token (unlike the case where decoder steps attend to encoder steps), such
architectures are typically described as self-attention models ( Linet al., 2017,Vaswaniet
al., 2017), and elsewhere described as intra-attention model (Chenget al., 2016,Parikh
et al., 2016,Pauluset al., 2017). In this section, we will discuss sequence encoding using
self-attention, including using additional information for the sequence order.
436 Attention Mechanisms and Transformers
import math
import torch
from torch import nn
from d2l import torch asd2l
11.6.1Self-Attention
Given a sequence of input tokens x1,...,xùëõwhere any xùëñ2Rùëë(1ùëñùëõ), its self-
attention outputs a sequence of the same length y1,...,yùëõ, where
yùëñ=ùëì¬πxùëñ,¬πx1,x1¬∫,...,¬πxùëõ,xùëõ¬∫¬∫2Rùëë(11.6.1)
according to the definition of attention pooling in (11.1.1 ). Using multi-head attention,
the following code snippet computes the self-attention of a tensor with shape (batch size,
number of time steps or sequence length in tokens, ùëë). The output tensor has the same
shape.
num_hiddens, num_heads =100,5
attention =d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)
batch_size, num_queries, valid_lens =2,4, torch .tensor([ 3,2])
X=torch .ones((batch_size, num_queries, num_hiddens))
d2l.check_shape(attention(X, X, X, valid_lens),
(batch_size, num_queries, num_hiddens))
11.6.2ComparingCNNs, RNNs, and Self-Attention
Let‚Äôs compare architectures for mapping a sequence of ùëõtokens to another one of equal
length, where each input or output token is represented by a ùëë-dimensional vector. Specif-
ically, we will consider CNNs, RNNs, and self-attention. We will compare their computa-
tional complexity, sequential operations, and maximum path lengths. Note that sequential
operations prevent parallel computation, while a shorter path between any combination of
sequence positions makes it easier to learn long-range dependencies within the sequence
(Hochreiter etal., 2001).
Let‚Äôs regard any text sequence as a ‚Äúone-dimensional image‚Äù. Similarly, one-dimensional
CNNscanprocesslocalfeaturessuchas ùëõ-gramsintext. Givenasequenceoflength ùëõ,con-
sider a convolutional layer whose kernel size is ùëò, and whose numbers of input and output
channels are both ùëë. The computational complexity of the convolutional layer is O¬πùëòùëõùëë2¬∫.
AsFig. 11.6.1 shows, CNNs are hierarchical, so there are O¬π1¬∫sequential operations and
the maximum path length is O¬πùëõ¬ùùëò¬∫. For example, x1andx5are within the receptive field
of a two-layer CNN with kernel size 3 in Fig. 11.6.1 .
WhenupdatingthehiddenstateofRNNs,multiplicationofthe ùëëùëëweightmatrixandthe
ùëë-dimensional hidden state has a computational complexity of O¬πùëë2¬∫. Since the sequence
length isùëõ, the computational complexity of the recurrent layer is O¬πùëõùëë2¬∫. According
toFig. 11.6.1 , there areO¬πùëõ¬∫sequential operations that cannot be parallelized and the
maximum path length is also O¬πùëõ¬∫.
437 Self-Attention and Positional Encoding
tFig. 11.6.1 Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.
In self-attention, the queries, keys, and values are all ùëõùëëmatrices. Consider the scaled
dotproductattentionin (11.3.6 ),whereanùëõùëëmatrixismultipliedbya ùëëùëõmatrix,then
theoutputùëõùëõmatrixismultipliedbyan ùëõùëëmatrix. Asaresult, theself-attentionhasa
O¬πùëõ2ùëë¬∫computational complexity. As we can see from Fig. 11.6.1 , each token is directly
connectedtoanyothertokenviaself-attention. Therefore,computationcanbeparallelwith
O¬π1¬∫sequential operations and the maximum path length is also O¬π1¬∫.
All in all, both CNNs and self-attention enjoy parallel computation and self-attention has
the shortest maximum path length. However, the quadratic computational complexity with
respect to the sequence length makes self-attention prohibitively slow for very long se-
quences.
11.6.3Positional Encoding
Unlike RNNs, which recurrently process tokens of a sequence one-by-one, self-attention
ditches sequential operations in favor of parallel computation. Note that self-attention by
itself does not preserve the order of the sequence. What do we do if it really matters that
the model knows in which order the input sequence arrived?
Thedominantapproachforpreservinginformationabouttheorderoftokensistorepresent
this to the model as an additional input associated with each token. These inputs are called
positional encodings , and they can either be learned or fixed a priori. We now describe a
simple scheme for fixed positional encodings based on sine and cosine functions ( Vaswani
etal., 2017).
Suppose that the input representation X2Rùëõùëëcontains the ùëë-dimensional embeddings
forùëõtokens of a sequence. The positional encoding outputs X¬∏Pusing a positional
embedding matrix P2Rùëõùëëof the same shape, whose element on the ùëñthrow and the
438 Attention Mechanisms and Transformers
¬π2ùëó¬∫thor the¬π2ùëó¬∏1¬∫thcolumn is
ùëùùëñ,2ùëó=sinùëñ
100002ùëó¬ùùëë
,
ùëùùëñ,2ùëó¬∏1=cosùëñ
100002ùëó¬ùùëë
.(11.6.2)
Atfirstglance,thistrigonometricfunctiondesignlooksweird. Beforewegiveexplanations
of this design, let‚Äôs first implement it in the following PositionalEncoding class.
class PositionalEncoding (nn.Module): #@save
"""Positional encoding."""
def __init__ (self , num_hiddens, dropout, max_len =1000 ):
super ().__init__ ()
self .dropout =nn.Dropout(dropout)
# Create a long enough P
self .P=torch .zeros(( 1, max_len, num_hiddens))
X=torch .arange(max_len, dtype =torch .float32) .reshape(
-1,1)/torch .pow( 10000 , torch .arange(
0, num_hiddens, 2, dtype =torch .float32) /num_hiddens)
self .P[:, :, 0::2]=torch .sin(X)
self .P[:, :, 1::2]=torch .cos(X)
def forward (self , X):
X=X+self .P[:, :X .shape[ 1], :] .to(X .device)
return self .dropout(X)
In the positional embedding matrix P, rows correspond to positions within a sequence and
columns represent different positional encoding dimensions. In the example below, we
can see that the 6thand the 7thcolumns of the positional embedding matrix have a higher
frequencythanthe 8thandthe 9thcolumns. Theoffsetbetweenthe 6thandthe 7th(samefor
the8thand the 9th) columns is due to the alternation of sine and cosine functions.
encoding_dim, num_steps =32,60
pos_encoding =PositionalEncoding(encoding_dim, 0)
X=pos_encoding(torch .zeros(( 1, num_steps, encoding_dim)))
P=pos_encoding .P[:, :X .shape[ 1], :]
d2l.plot(torch .arange(num_steps), P[ 0, :, 6:10].T, xlabel ='Row (position) ',
figsize =(6,2.5), legend =["Col %d"%dfor dintorch .arange( 6,10)])

439 Self-Attention and Positional Encoding
AbsolutePositionalInformation
To see how the monotonically decreased frequency along the encoding dimension relates
to absolute positional information, let‚Äôs print out the binary representations of 0,1,..., 7.
As we can see, the lowest bit, the second-lowest bit, and the third-lowest bit alternate on
every number, every two numbers, and every four numbers, respectively.
for iinrange (8):
print (f'{i}in binary is {i:>03b }')
0inbinary is000
1inbinary is001
2inbinary is010
3inbinary is011
4inbinary is100
5inbinary is101
6inbinary is110
7inbinary is111
In binary representations, a higher bit has a lower frequency than a lower bit. Similarly,
as demonstrated in the heat map below, the positional encoding decreases frequencies
alongtheencodingdimensionbyusingtrigonometricfunctions. Sincetheoutputsarefloat
numbers, such continuous representations are more space-efficient than binary representa-
tions.
P=P[0, :, :] .unsqueeze( 0).unsqueeze( 0)
d2l.show_heatmaps(P, xlabel ='Column (encoding dimension) ',
ylabel ='Row (position) ', figsize =(3.5,4), cmap ='Blues ')
RelativePositionalInformation
Besides capturing absolute positional information, the above positional encoding also al-
lows a model to easily learn to attend by relative positions. This is because for any fixed
440 Attention Mechanisms and Transformers
161position offset ùõø, the positional encoding at position ùëñ¬∏ùõøcan be represented by a linear
projection of that at position ùëñ.
This projection can be explained mathematically. Denoting ùúîùëó=1¬ù100002ùëó¬ùùëë, any pair
of¬πùëùùëñ,2ùëó,ùëùùëñ,2ùëó¬∏1¬∫in(11.6.2 )can be linearly projected to ¬πùëùùëñ¬∏ùõø,2ùëó,ùëùùëñ¬∏ùõø,2ùëó¬∏1¬∫for any fixed
offsetùõø:
cos¬πùõøùúîùëó¬∫ sin¬πùõøùúîùëó¬∫
 sin¬πùõøùúîùëó¬∫cos¬πùõøùúîùëó¬∫ ùëùùëñ,2ùëó
ùëùùëñ,2ùëó¬∏1
=cos¬πùõøùúîùëó¬∫sin¬πùëñùúîùëó¬∫¬∏sin¬πùõøùúîùëó¬∫cos¬πùëñùúîùëó¬∫
 sin¬πùõøùúîùëó¬∫sin¬πùëñùúîùëó¬∫¬∏cos¬πùõøùúîùëó¬∫cos¬πùëñùúîùëó¬∫
=sin ¬πùëñ¬∏ùõø¬∫ùúîùëó
cos ¬πùëñ¬∏ùõø¬∫ùúîùëó
=ùëùùëñ¬∏ùõø,2ùëó
ùëùùëñ¬∏ùõø,2ùëó¬∏1
,
(11.6.3)
where the 22projection matrix does not depend on any position index ùëñ.
11.6.4Summary
In self-attention, the queries, keys, and values all come from the same place. Both CNNs
and self-attention enjoy parallel computation and self-attention has the shortest maximum
pathlength. However,thequadraticcomputationalcomplexitywithrespecttothesequence
lengthmakesself-attentionprohibitivelyslowforverylongsequences. Tousethesequence
order information, we can inject absolute or relative positional information by adding po-
sitional encoding to the input representations.
11.6.5Exercises
1.Suppose that we design a deep architecture to represent a sequence by stacking self-
attention layers with positional encoding. What could the possible issues be?
2.Can you design a learnable positional encoding method?
3.Canweassigndifferentlearnedembeddingsaccordingtodifferentoffsetsbetweenqueries
and keys that are compared in self-attention? Hint: you may refer to relative position
embeddings ( Huangetal., 2018,Shawetal., 2018).
Discussions161.
11.7The TransformerArchitecture
We have compared CNNs, RNNs, and self-attention in Section 11.6.2 . Notably, self-
attention enjoys both parallel computation and the shortest maximum path length. There-
fore, it is appealing to design deep architectures by using self-attention. Unlike earlier
self-attention models that still rely on RNNs for input representations ( Chenget al., 2016,
441 The Transformer Architecture
Linet al., 2017,Pauluset al., 2017), the Transformer model is solely based on attention
mechanisms without any convolutional or recurrent layer ( Vaswaniet al., 2017). Though
originallyproposedforsequence-to-sequencelearningontextdata,Transformershavebeen
pervasiveinawiderangeofmoderndeeplearningapplications, suchasinareastodowith
language, vision, speech, and reinforcement learning.
import math
import pandas aspd
import torch
from torch import nn
from d2l import torch asd2l
11.7.1Model
As an instance of the encoder‚Äìdecoder architecture, the overall architecture of the Trans-
former is presented in Fig. 11.7.1 . As we can see, the Transformer is composed of an en-
coder and a decoder. In contrast to Bahdanau attention for sequence-to-sequence learning
inFig. 11.4.2 , the input (source) and output (target) sequence embeddings are added with
positional encoding before being fed into the encoder and the decoder that stack modules
based on self-attention.
NowweprovideanoverviewoftheTransformerarchitecturein Fig.11.7.1 . Atahighlevel,
the Transformer encoder is a stack of multiple identical layers, where each layer has two
sublayers (either is denoted as sublayer). The first is a multi-head self-attention pooling
and the second is a positionwise feed-forward network. Specifically, in the encoder self-
attention, queries, keys, and values are all from the outputs of the previous encoder layer.
Inspired by the ResNet design of Section 8.6 , a residual connection is employed around
both sublayers. In the Transformer, for any input x2Rùëëat any position of the sequence,
we require that sublayer ¬πx¬∫2Rùëëso that the residual connection x¬∏sublayer¬πx¬∫2Rùëëis
feasible. This addition from the residual connection is immediately followed by layer nor-
malization ( Baetal., 2016). As a result, the Transformer encoder outputs a ùëë-dimensional
vector representation for each position of the input sequence.
The Transformer decoder is also a stack of multiple identical layers with residual connec-
tions and layer normalizations. As well as the two sublayers described in the encoder, the
decoder inserts a third sublayer, known as the encoder‚Äìdecoder attention, between these
two. In the encoder‚Äìdecoder attention, queries are from the outputs of the decoder‚Äôs self-
attention sublayer, and the keys and values are from the Transformer encoder outputs. In
thedecoderself-attention, queries, keys, andvaluesareallfromtheoutputsoftheprevious
decoder layer. However, each position in the decoder is allowed only to attend to all posi-
tions in the decoder up to that position. This maskedattention preserves the autoregressive
property, ensuring that the prediction only depends on those output tokens that have been
generated.
Wehavealreadydescribedandimplementedmulti-headattentionbasedonscaleddotprod-
ucts inSection 11.5 and positional encoding in Section 11.6.3 . In the following, we will
implement the rest of the Transformer model.
442 Attention Mechanisms and Transformers
tFig. 11.7.1 The Transformer architecture.
11.7.2PositionwiseFeed-ForwardNetworks
The positionwise feed-forward network transforms the representation at all the sequence
positions using the same MLP. This is why we call it positionwise . In the implementation
below,theinput Xwithshape(batchsize,numberoftimestepsorsequencelengthintokens,
number of hidden units or feature dimension) will be transformed by a two-layer MLP into
an output tensor of shape (batch size, number of time steps, ffn_num_outputs ).
class PositionWiseFFN (nn.Module): #@save
"""The positionwise feed-forward network."""
def __init__ (self , ffn_num_hiddens, ffn_num_outputs):
super ().__init__ ()
self .dense1 =nn.LazyLinear(ffn_num_hiddens)
self .relu =nn.ReLU()
self .dense2 =nn.LazyLinear(ffn_num_outputs)
def forward (self , X):
return self .dense2( self .relu( self .dense1(X)))
Thefollowingexampleshowsthattheinnermostdimensionofatensorchangestothenum-
443 The Transformer Architecture
ber of outputs in the positionwise feed-forward network. Since the same MLP transforms
atallthepositions,whentheinputsatallthesepositionsarethesame,theiroutputsarealso
identical.
ffn =PositionWiseFFN( 4,8)
ffn.eval()
ffn(torch .ones(( 2,3,4)))[ 0]
tensor([[ 0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.
‚Ü©!5163 ],
[0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.
‚Ü©!5163 ],
[0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.
‚Ü©!5163 ]],
grad_fn =<SelectBackward0 >)
11.7.3Residual Connection and LayerNormalization
Now let‚Äôs focus on the ‚Äúadd & norm‚Äù component in Fig. 11.7.1 . As we described at the
beginning of this section, this is a residual connection immediately followed by layer nor-
malization. Both are key to effective deep architectures.
InSection 8.5 , we explained how batch normalization recenters and rescales across the
exampleswithinaminibatch. Asdiscussedin Section8.5.2 ,layernormalizationisthesame
asbatchnormalizationexceptthattheformernormalizesacrossthefeaturedimension,thus
enjoyingbenefitsofscaleindependenceandbatchsizeindependence. Despiteitspervasive
applications in computer vision, batch normalization is usually empirically less effective
than layer normalization in natural language processing tasks, where the inputs are often
variable-length sequences.
The following code snippet compares the normalization across different dimensions by
layer normalization and batch normalization.
ln=nn.LayerNorm( 2)
bn=nn.LazyBatchNorm1d()
X=torch .tensor([[ 1,2], [ 2,3]], dtype =torch .float32)
# Compute mean and variance from X in the training mode
print ('layer norm: ', ln(X), '\nbatch norm: ', bn(X))
layer norm: tensor([[ -1.0000 ,1.0000 ],
[-1.0000 ,1.0000 ]], grad_fn =<NativeLayerNormBackward0 >)
batch norm: tensor([[ -1.0000 ,-1.0000 ],
[1.0000 ,1.0000 ]], grad_fn =<NativeBatchNormBackward0 >)
Now we can implement the AddNorm class using a residual connection followed by layer
normalization. Dropout is also applied for regularization.
444 Attention Mechanisms and Transformers
class AddNorm (nn.Module): #@save
"""The residual connection followed by layer normalization."""
def __init__ (self , norm_shape, dropout):
super ().__init__ ()
self .dropout =nn.Dropout(dropout)
self .ln=nn.LayerNorm(norm_shape)
def forward (self , X, Y):
return self .ln(self .dropout(Y) +X)
Theresidualconnectionrequiresthatthetwoinputsareofthesameshapesothattheoutput
tensor also has the same shape after the addition operation.
add_norm =AddNorm( 4,0.5)
shape =(2,3,4)
d2l.check_shape(add_norm(torch .ones(shape), torch .ones(shape)), shape)
11.7.4Encoder
With all the essential components to assemble the Transformer encoder, let‚Äôs start by im-
plementing a single layer within the encoder. The following TransformerEncoderBlock
class contains two sublayers: multi-head self-attention and positionwise feed-forward net-
works, where a residual connection followed by layer normalization is employed around
both sublayers.
class TransformerEncoderBlock (nn.Module): #@save
"""The Transformer encoder block."""
def __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias =False ):
super ().__init__ ()
self .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout, use_bias)
self .addnorm1 =AddNorm(num_hiddens, dropout)
self .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self .addnorm2 =AddNorm(num_hiddens, dropout)
def forward (self , X, valid_lens):
Y=self .addnorm1(X, self .attention(X, X, X, valid_lens))
return self .addnorm2(Y, self .ffn(Y))
As we can see, no layer in the Transformer encoder changes the shape of its input.
X=torch .ones(( 2,100,24))
valid_lens =torch .tensor([ 3,2])
encoder_blk =TransformerEncoderBlock( 24,48,8,0.5)
encoder_blk .eval()
d2l.check_shape(encoder_blk(X, valid_lens), X .shape)
In the following Transformer encoder implementation, we stack num_blks instances of the
above TransformerEncoderBlock classes. Since we use the fixed positional encoding
445 The Transformer Architecture
whose values are always between  1and1, we multiply values of the learnable input em-
beddings by the square root of the embedding dimension to rescale before summing up the
input embedding and the positional encoding.
class TransformerEncoder (d2l .Encoder): #@save
"""The Transformer encoder."""
def __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_blks, dropout, use_bias =False ):
super ().__init__ ()
self .num_hiddens =num_hiddens
self .embedding =nn.Embedding(vocab_size, num_hiddens)
self .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)
self .blks =nn.Sequential()
for iinrange (num_blks):
self .blks .add_module( "block "+str(i), TransformerEncoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))
def forward (self , X, valid_lens):
# Since positional encoding values are between -1 and 1, the embedding
# values are multiplied by the square root of the embedding dimension
# to rescale before they are summed up
X=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))
self .attention_weights =[None ]*len(self .blks)
for i, blk inenumerate (self .blks):
X=blk(X, valid_lens)
self .attention_weights[
i]=blk.attention .attention .attention_weights
return X
Belowwespecifyhyperparameterstocreateatwo-layerTransformerencoder. Theshapeof
theTransformerencoderoutputis(batchsize,numberoftimesteps, num_hiddens ).
encoder =TransformerEncoder( 200,24,48,8,2,0.5)
d2l.check_shape(encoder(torch .ones(( 2,100), dtype =torch .long), valid_lens),
(2,100,24))
11.7.5Decoder
As shown in Fig. 11.7.1 , the Transformer decoder is composed of multiple identical lay-
ers. Each layer is implemented in the following TransformerDecoderBlock class, which
contains three sublayers: decoder self-attention, encoder‚Äìdecoder attention, and position-
wise feed-forward networks. These sublayers employ a residual connection around them
followed by layer normalization.
As we described earlier in this section, in the masked multi-head decoder self-attention
(the first sublayer), queries, keys, and values all come from the outputs of the previous
decoder layer. When training sequence-to-sequence models, tokens at all the positions
(time steps) of the output sequence are known. However, during prediction the output
sequence is generated token by token; thus, at any decoder time step only the generated
tokenscanbeusedinthedecoderself-attention. Topreserveautoregressioninthedecoder,
446 Attention Mechanisms and Transformers
its masked self-attention specifies dec_valid_lens so that any query only attends to all
positions in the decoder up to the query position.
class TransformerDecoderBlock (nn.Module):
# The i-th block in the Transformer decoder
def __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout, i):
super ().__init__ ()
self .i=i
self .attention1 =d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout)
self .addnorm1 =AddNorm(num_hiddens, dropout)
self .attention2 =d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout)
self .addnorm2 =AddNorm(num_hiddens, dropout)
self .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self .addnorm3 =AddNorm(num_hiddens, dropout)
def forward (self , X, state):
enc_outputs, enc_valid_lens =state[ 0], state[ 1]
# During training, all the tokens of any output sequence are processed
# at the same time, so state[2][self.i] is None as initialized. When
# decoding any output sequence token by token during prediction,
# state[2][self.i] contains representations of the decoded output at
# the i-th block up to the current time step
ifstate[ 2][self .i]isNone :
key_values =X
else :
key_values =torch .cat((state[ 2][self .i], X), dim =1)
state[ 2][self .i]=key_values
ifself .training:
batch_size, num_steps, _ =X.shape
# Shape of dec_valid_lens: (batch_size, num_steps), where every
# row is [1, 2, ..., num_steps]
dec_valid_lens =torch .arange(
1, num_steps +1, device =X.device) .repeat(batch_size, 1)
else :
dec_valid_lens =None
# Self-attention
X2=self .attention1(X, key_values, key_values, dec_valid_lens)
Y=self .addnorm1(X, X2)
# Encoder-decoder attention. Shape of enc_outputs:
# (batch_size, num_steps, num_hiddens)
Y2=self .attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
Z=self .addnorm2(Y, Y2)
return self .addnorm3(Z, self .ffn(Z)), state
To facilitate scaled dot product operations in the encoder‚Äìdecoder attention and addition
operationsintheresidualconnections,thefeaturedimension( num_hiddens )ofthedecoder
is the same as that of the encoder.
decoder_blk =TransformerDecoderBlock( 24,48,8,0.5,0)
X=torch .ones(( 2,100,24))
state =[encoder_blk(X, valid_lens), valid_lens, [ None ]]
d2l.check_shape(decoder_blk(X, state)[ 0], X .shape)
447 The Transformer Architecture
Now we construct the entire Transformer decoder composed of num_blks instances of
TransformerDecoderBlock . In the end, a fully connected layer computes the prediction
for all the vocab_size possible output tokens. Both of the decoder self-attention weights
and the encoder‚Äìdecoder attention weights are stored for later visualization.
class TransformerDecoder (d2l .AttentionDecoder):
def __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_blks, dropout):
super ().__init__ ()
self .num_hiddens =num_hiddens
self .num_blks =num_blks
self .embedding =nn.Embedding(vocab_size, num_hiddens)
self .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)
self .blks =nn.Sequential()
for iinrange (num_blks):
self .blks .add_module( "block "+str(i), TransformerDecoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, i))
self .dense =nn.LazyLinear(vocab_size)
def init_state (self , enc_outputs, enc_valid_lens):
return [enc_outputs, enc_valid_lens, [ None ]*self .num_blks]
def forward (self , X, state):
X=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))
self ._attention_weights =[[None ]*len(self .blks) for _inrange (2)]
for i, blk inenumerate (self .blks):
X, state =blk(X, state)
# Decoder self-attention weights
self ._attention_weights[ 0][
i]=blk.attention1 .attention .attention_weights
# Encoder-decoder attention weights
self ._attention_weights[ 1][
i]=blk.attention2 .attention .attention_weights
return self .dense(X), state
@property
def attention_weights (self ):
return self ._attention_weights
11.7.6Training
Let‚Äôs instantiate an encoder‚Äìdecoder model by following the Transformer architecture.
Here we specify that both the Transformer encoder and the Transformer decoder have two
layers using 4-head attention. As in Section 10.7.6 , we train the Transformer model for
sequence-to-sequence learning on the English‚ÄìFrench machine translation dataset.
data =d2l.MTFraEng(batch_size =128)
num_hiddens, num_blks, dropout =256,2,0.2
ffn_num_hiddens, num_heads =64,4
encoder =TransformerEncoder(
len(data .src_vocab), num_hiddens, ffn_num_hiddens, num_heads,
num_blks, dropout)
decoder =TransformerDecoder(
(continues on next page)
448 Attention Mechanisms and Transformers
(continued from previous page)
len(data .tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,
num_blks, dropout)
model =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],
lr=0.001 )
trainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)
trainer .fit(model, data)
After training, we use the Transformer model to translate a few English sentences into
French and compute their BLEU scores.
engs =['go . ','i lost . ','he\'s calm . ','i\'m home . ']
fras =['va ! ','j\'ai perdu . ','il est calme . ','je suis chez moi . ']
preds, _ =model .predict_step(
data .build(engs, fras), d2l .try_gpu(), data .num_steps)
for en, fr, p inzip(engs, fras, preds):
translation =[]
for token indata .tgt_vocab .to_tokens(p):
iftoken =='<eos> ':
break
translation .append(token)
print (f'{en}=>{translation }, bleu, '
f'{d2l.bleu( "".join(translation), fr, k=2):.3f}')
go.=>['va','!'], bleu, 1.000
i lost .=>['je','perdu ','.'], bleu, 0.687
he's calm . => [ 'il','est','mouill√© ','.'], bleu,0.658
i'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000
Let‚Äôs visualize the Transformer attention weights when translating the final English sen-
tence into French. The shape of the encoder self-attention weights is (number of encoder
layers, numberofattentionheads, num_steps ornumberofqueries, num_steps ornumber
of key-value pairs).
_, dec_attention_weights =model .predict_step(
data .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )
enc_attention_weights =torch .cat(model .encoder .attention_weights, 0)
shape =(num_blks, num_heads, -1, data .num_steps)
enc_attention_weights =enc_attention_weights .reshape(shape)
(continues on next page)
449 The Transformer Architecture
(continued from previous page)
d2l.check_shape(enc_attention_weights,
(num_blks, num_heads, data .num_steps, data .num_steps))
In the encoder self-attention, both queries and keys come from the same input sequence.
Since padding tokens do not carry meaning, with specified valid length of the input se-
quence no query attends to positions of padding tokens. In the following, two layers of
multi-head attention weights are presented row by row. Each head independently attends
based on a separate representation subspace of queries, keys, and values.
d2l.show_heatmaps(
enc_attention_weights .cpu(), xlabel ='Key positions ',
ylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],
figsize =(7,3.5))
To visualize the decoder self-attention weights and the encoder‚Äìdecoder attention weights,
we need more data manipulations. For example, we fill the masked attention weights
with zero. Note that the decoder self-attention weights and the encoder‚Äìdecoder atten-
tion weights both have the same queries: the beginning-of-sequence token followed by the
output tokens and possibly end-of-sequence tokens.
dec_attention_weights_2d =[head[ 0].tolist()
for step indec_attention_weights
for attn instep for blk inattn for head inblk]
dec_attention_weights_filled =torch .tensor(
pd.DataFrame(dec_attention_weights_2d) .fillna( 0.0).values)
shape =(-1,2, num_blks, num_heads, data .num_steps)
dec_attention_weights =dec_attention_weights_filled .reshape(shape)
dec_self_attention_weights, dec_inter_attention_weights =\
dec_attention_weights .permute( 1,2,3,0,4)
d2l.check_shape(dec_self_attention_weights,
(num_blks, num_heads, data .num_steps, data .num_steps))
d2l.check_shape(dec_inter_attention_weights,
(num_blks, num_heads, data .num_steps, data .num_steps))
450 Attention Mechanisms and Transformers
Because of the autoregressive property of the decoder self-attention, no query attends to
key‚Äìvalue pairs after the query position.
d2l.show_heatmaps(
dec_self_attention_weights[:, :, :, :],
xlabel ='Key positions ', ylabel ='Query positions ',
titles =['Head %d'%ifor iinrange (1,5)], figsize =(7,3.5))
Similar to the case in the encoder self-attention, via the specified valid length of the input
sequence,noqueryfromtheoutputsequenceattendstothosepaddingtokensfromtheinput
sequence.
d2l.show_heatmaps(
dec_inter_attention_weights, xlabel ='Key positions ',
ylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],
figsize =(7,3.5))
Although the Transformer architecture was originally proposed for sequence-to-sequence
learning, aswewilldiscoverlaterinthebook, eithertheTransformerencoderortheTrans-
former decoder is often individually used for different deep learning tasks.
11.7.7Summary
451 Transformers for Vision
162The Transformer is an instance of the encoder‚Äìdecoder architecture, though either the en-
coder or the decoder can be used individually in practice. In the Transformer architec-
ture, multi-head self-attention is used for representing the input sequence and the output
sequence, though the decoder has to preserve the autoregressive property via a masked
version. Both the residual connections and the layer normalization in the Transformer are
important for training a very deep model. The positionwise feed-forward network in the
Transformer model transforms the representation at all the sequence positions using the
same MLP.
11.7.8Exercises
1.Train a deeper Transformer in the experiments. How does it affect the training speed
and the translation performance?
2.Is it a good idea to replace scaled dot product attention with additive attention in the
Transformer? Why?
3.Forlanguagemodeling,shouldweusetheTransformerencoder,decoder,orboth? How
would you design this method?
4.What challenges can Transformers face if input sequences are very long? Why?
5.How would you improve the computational and memory efficiency of Transformers?
Hint: you may refer to the survey paper by Tay etal.(2020).
Discussions162.
11.8TransformersforVision
The Transformer architecture was initially proposed for sequence-to-sequence learning,
with a focus on machine translation. Subsequently, Transformers emerged as the model
of choice in various natural language processing tasks ( Brownet al., 2020,Devlinet al.,
2018,Radfordetal.,2018,Radfordetal.,2019,Raffeletal.,2020). However,inthefieldof
computer vision the dominant architecture has remained the CNN ( Chapter 8 ). Naturally,
researchers started to wonder if it might be possible to do better by adapting Transformer
modelstoimagedata. Thisquestionsparkedimmenseinterestinthecomputervisioncom-
munity. Recently, Ramachandran et al.(2019) proposed a scheme for replacing convolu-
tion with self-attention. However, its use of specialized patterns in attention makes it hard
to scale up models on hardware accelerators. Then, Cordonnier et al.(2020) theoretically
proved that self-attention can learn to behave similarly to convolution. Empirically, 22
patches were taken from images as inputs, but the small patch size makes the model only
applicable to image data with low resolutions.
Without specific constraints on patch size, visionTransformers (ViTs) extract patches from
images and feed them into a Transformer encoder to obtain a global representation, which
452 Attention Mechanisms and Transformers
willfinallybetransformedforclassification( Dosovitskiy etal.,2021). Notably,Transform-
ers show better scalability than CNNs: and when training larger models on larger datasets,
vision Transformers outperform ResNets by a significant margin. Similar to the landscape
of network architecture design in natural language processing, Transformers have also be-
come a game-changer in computer vision.
import torch
from torch import nn
from d2l import torch asd2l
11.8.1Model
Fig.11.8.1 depictsthemodelarchitectureofvisionTransformers. Thisarchitectureconsists
of a stem that patchifies images, a body based on the multilayer Transformer encoder, and
a head that transforms the global representation into the output label.
tFig. 11.8.1 The vision Transformer architecture. In this example, an image is split into nine patches.
A special ‚Äú<cls>‚Äù token and the nine Ô¨Çattened image patches are transformed via patch
embedding and nTransformer encoder blocks into ten representations, respectively. The
‚Äú<cls>‚Äù representation is further transformed into the output label.
Consider an input image with height ‚Ñé, widthùë§, andùëêchannels. Specifying the patch
height and width both as ùëù, the image is split into a sequence of ùëö=‚Ñéùë§¬ùùëù2patches,
where each patch is flattened to a vector of length ùëêùëù2. In this way, image patches can be
453 Transformers for Vision
treated similarly to tokens in text sequences by Transformer encoders. A special ‚Äú<cls>‚Äù
(class) token and the ùëöflattened image patches are linearly projected into a sequence of
ùëö¬∏1vectors, summed with learnable positional embeddings. The multilayer Transformer
encoder transforms ùëö¬∏1input vectors into the same number of output vector representa-
tionsofthesamelength. ItworksexactlythesamewayastheoriginalTransformerencoder
inFig. 11.7.1 , only differing in the position of normalization. Since the ‚Äú<cls>‚Äù token at-
tends to all the image patches via self-attention (see Fig. 11.6.1 ), its representation from
the Transformer encoder output will be further transformed into the output label.
11.8.2PatchEmbedding
To implement a vision Transformer, let‚Äôs start with patch embedding in Fig. 11.8.1 . Split-
ting an image into patches and linearly projecting these flattened patches can be simplified
as a single convolution operation, where both the kernel size and the stride size are set to
the patch size.
class PatchEmbedding (nn.Module):
def __init__ (self , img_size =96, patch_size =16, num_hiddens =512):
super ().__init__ ()
def _make_tuple (x):
ifnot isinstance (x, ( list ,tuple )):
return (x, x)
return x
img_size, patch_size =_make_tuple(img_size), _make_tuple(patch_size)
self .num_patches =(img_size[ 0]//patch_size[ 0])*(
img_size[ 1]//patch_size[ 1])
self .conv =nn.LazyConv2d(num_hiddens, kernel_size =patch_size,
stride =patch_size)
def forward (self , X):
# Output shape: (batch size, no. of patches, no. of channels)
return self .conv(X) .flatten( 2).transpose( 1,2)
In the following example, taking images with height and width of img_size as inputs, the
patchembeddingoutputs (img_size//patch_size)**2 patchesthatarelinearlyprojected
to vectors of length num_hiddens .
img_size, patch_size, num_hiddens, batch_size =96,16,512,4
patch_emb =PatchEmbedding(img_size, patch_size, num_hiddens)
X=torch .zeros(batch_size, 3, img_size, img_size)
d2l.check_shape(patch_emb(X),
(batch_size, (img_size //patch_size) **2, num_hiddens))
11.8.3VisionTransformerEncoder
TheMLPofthevisionTransformerencoderisslightlydifferentfromthepositionwiseFFN
oftheoriginalTransformerencoder(see Section11.7.2 ). First,heretheactivationfunction
usestheGaussianerrorlinearunit(GELU),whichcanbeconsideredasasmootherversion
of the ReLU ( Hendrycks and Gimpel, 2016 ). Second, dropout is applied to the output of
each fully connected layer in the MLP for regularization.
454 Attention Mechanisms and Transformers
class ViTMLP (nn.Module):
def __init__ (self , mlp_num_hiddens, mlp_num_outputs, dropout =0.5):
super ().__init__ ()
self .dense1 =nn.LazyLinear(mlp_num_hiddens)
self .gelu =nn.GELU()
self .dropout1 =nn.Dropout(dropout)
self .dense2 =nn.LazyLinear(mlp_num_outputs)
self .dropout2 =nn.Dropout(dropout)
def forward (self , x):
return self .dropout2( self .dense2( self .dropout1( self .gelu(
self .dense1(x)))))
The vision Transformer encoder block implementation just follows the pre-normalization
design in Fig. 11.8.1 , where normalization is applied right beforemulti-head attention or
the MLP. In contrast to post-normalization (‚Äúadd & norm‚Äù in Fig. 11.7.1 ), where normal-
izationisplacedright afterresidualconnections, pre-normalizationleadstomoreeffective
or efficient training for Transformers ( Baevski and Auli, 2018 ,Wangetal., 2019,Xionget
al., 2020).
class ViTBlock (nn.Module):
def __init__ (self , num_hiddens, norm_shape, mlp_num_hiddens,
num_heads, dropout, use_bias =False ):
super ().__init__ ()
self .ln1 =nn.LayerNorm(norm_shape)
self .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout, use_bias)
self .ln2 =nn.LayerNorm(norm_shape)
self .mlp =ViTMLP(mlp_num_hiddens, num_hiddens, dropout)
def forward (self , X, valid_lens =None ):
X=X+self .attention( *([self .ln1(X)] *3), valid_lens)
return X+self .mlp( self .ln2(X))
Justasin Section11.7.4 ,novisionTransformerencoderblockchangesitsinputshape.
X=torch .ones(( 2,100,24))
encoder_blk =ViTBlock( 24,24,48,8,0.5)
encoder_blk .eval()
d2l.check_shape(encoder_blk(X), X .shape)
11.8.4PuttingIt All Together
The forward pass of vision Transformers below is straightforward. First, input images are
fedintoan PatchEmbedding instance,whoseoutputisconcatenatedwiththe‚Äú<cls>‚Äùtoken
embedding. Theyaresummedwithlearnablepositionalembeddingsbeforedropout. Then
the output is fed into the Transformer encoder that stacks num_blks instances of the ViT-
Blockclass. Finally, the representation of the ‚Äú<cls>‚Äù token is projected by the network
head.
455 Transformers for Vision
class ViT(d2l .Classifier):
"""Vision Transformer."""
def __init__ (self , img_size, patch_size, num_hiddens, mlp_num_hiddens,
num_heads, num_blks, emb_dropout, blk_dropout, lr =0.1,
use_bias =False , num_classes =10):
super ().__init__ ()
self .save_hyperparameters()
self .patch_embedding =PatchEmbedding(
img_size, patch_size, num_hiddens)
self .cls_token =nn.Parameter(torch .zeros( 1,1, num_hiddens))
num_steps =self .patch_embedding .num_patches +1# Add the cls token
# Positional embeddings are learnable
self .pos_embedding =nn.Parameter(
torch .randn( 1, num_steps, num_hiddens))
self .dropout =nn.Dropout(emb_dropout)
self .blks =nn.Sequential()
for iinrange (num_blks):
self .blks .add_module( f"{i}", ViTBlock(
num_hiddens, num_hiddens, mlp_num_hiddens,
num_heads, blk_dropout, use_bias))
self .head =nn.Sequential(nn .LayerNorm(num_hiddens),
nn.Linear(num_hiddens, num_classes))
def forward (self , X):
X=self .patch_embedding(X)
X=torch .cat(( self .cls_token .expand(X .shape[ 0],-1,-1), X), 1)
X=self .dropout(X +self .pos_embedding)
for blk inself .blks:
X=blk(X)
return self .head(X[:, 0])
11.8.5Training
Training a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were
trained in Chapter 8 .
img_size, patch_size =96,16
num_hiddens, mlp_num_hiddens, num_heads, num_blks =512,2048 ,8,2
emb_dropout, blk_dropout, lr =0.1,0.1,0.1
model =ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,
num_blks, emb_dropout, blk_dropout, lr)
trainer =d2l.Trainer(max_epochs =10, num_gpus =1)
data =d2l.FashionMNIST(batch_size =128, resize =(img_size, img_size))
trainer .fit(model, data)
11.8.6Summary and Discussion
Youmayhavenoticed thatforsmall datasetslikeFashion-MNIST,ourimplementedvision
Transformer does not outperform the ResNet in Section 8.6 . Similar observations can be
madeevenontheImageNetdataset(1.2millionimages). ThisisbecauseTransformers lack
those useful principles in convolution, such as translation invariance and locality ( Section
7.1). However, the picture changes when training larger models on larger datasets (e.g.,
456 Attention Mechanisms and Transformers
163300 million images), where vision Transformers outperform ResNets by a large margin
in image classification, demonstrating intrinsic superiority of Transformers in scalability
(Dosovitskiy et al., 2021). The introduction of vision Transformers has changed the land-
scapeofnetworkdesignformodelingimagedata. Theyweresoonshowntobeeffectiveon
the ImageNet dataset with data-efficient training strategies of DeiT ( Touvronet al., 2021).
However, the quadratic complexity of self-attention ( Section 11.6 ) makes the Transformer
architecture less suitable for higher-resolution images. Towards a general-purpose back-
bone network in computer vision, Swin Transformers addressed the quadratic computa-
tional complexity with respect to image size ( Section 11.6.2 ) and reinstated convolution-
like priors, extending the applicability of Transformers to a range of computer vision tasks
beyond image classification with state-of-the-art results ( Liuetal., 2021).
11.8.7Exercises
1.How does the value of img_size affect training time?
2.Instead of projecting the ‚Äú<cls>‚Äù token representation to the output, how would you
projecttheaveragedpatchrepresentations? Implementthischangeandseehowitaffects
the accuracy.
3.Can you modify hyperparameters to improve the accuracy of the vision Transformer?
Discussions163.
11.9Large-ScalePretrainingwith Transformers
So far in our image classification and machine translation experiments, models have been
trained on datasets with input‚Äìoutput examples fromscratch to perform specific tasks. For
example, a Transformer was trained with English‚ÄìFrench pairs ( Section 11.7 ) so that this
model can translate input English text into French. As a result, each model becomes a
specific expert that is sensitive to even a slight shift in data distribution ( Section 4.7 ). For
better generalized models, or even more competent generalists that can perform multiple
tasks with or without adaptation, pretraining models on large data has been increasingly
common.
457 Large-Scale Pretraining with Transformers
Given larger data for pretraining, the Transformer architecture performs better with an in-
creasedmodelsizeandtrainingcompute,demonstratingsuperior scalingbehavior. Specif-
ically, performance of Transformer-based language models scales as a power law with the
amount of model parameters, training tokens, and training compute ( Kaplanet al., 2020).
The scalability of Transformers is also evidenced by the significantly boosted performance
from larger vision Transformers trained on larger data (discussed in Section 11.8 ). More
recent success stories include Gato, a generalist model that can play Atari, caption im-
ages, chat, and act as a robot ( Reedet al., 2022). Gato is a single Transformer that scales
well when pretrained on diverse modalities, including text, images, joint torques, and but-
ton presses. Notably, all such multimodal data is serialized into a flat sequence of tokens,
which can be processed akin to text tokens ( Section 11.7 ) or image patches ( Section 11.8 )
by Transformers.
Prior to the compelling success of pretraining Transformers for multimodal data, Trans-
formerswereextensivelypretrainedwithawealthoftext. Originallyproposedformachine
translation,theTransformerarchitecturein Fig.11.7.1 consistsofanencoderforrepresent-
inginputsequencesandadecoderforgeneratingtargetsequences. Primarily,Transformers
can be used in three different modes: encoder-only ,encoder‚Äìdecoder , anddecoder-only .
To conclude this chapter, we will review these three modes and explain the scalability in
pretraining Transformers.
11.9.1Encoder-Only
WhenonlytheTransformerencoderisused,asequenceofinputtokensisconvertedintothe
same number of representations that can be further projected into output (e.g., classifica-
tion). ATransformerencoderconsistsofself-attentionlayers,whereallinputtokensattend
to each other. For example, vision Transformers depicted in Fig. 11.8.1 are encoder-only,
converting a sequence of input image patches into the representation of a special ‚Äú<cls>‚Äù
token. Since this representation depends on all input tokens, it is further projected into
classification labels. This design was inspired by an earlier encoder-only Transformer pre-
trainedontext: BERT(BidirectionalEncoderRepresentationsfromTransformers)( Devlin
etal., 2018).
PretrainingBERT
BERT is pretrained on text sequences using masked language modeling : input text with
randomly masked tokens is fed into a Transformer encoder to predict the masked tokens.
As illustrated in Fig. 11.9.1 , an original text sequence ‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚Äúthis‚Äù, ‚Äúred‚Äù, ‚Äúcar‚Äù is
prepended with the ‚Äú<cls>‚Äù token, and the ‚Äú<mask>‚Äù token randomly replaces ‚Äúlove‚Äù;
then the cross-entropy loss between the masked token ‚Äúlove‚Äù and its prediction is to be
minimized during pretraining. Note that there is no constraint in the attention pattern of
Transformer encoders (right of Fig. 11.9.1 ) so all tokens can attend to each other. Thus,
prediction of ‚Äúlove‚Äù depends on input tokens before and after it in the sequence. This is
whyBERTis a ‚Äúbidirectional encoder‚Äù. Withoutneed formanual labeling, large-scaletext
data from books and Wikipedia can be used for pretraining BERT.
458 Attention Mechanisms and Transformers
tFig. 11.9.1 Left: Pretraining BERT with masked language modeling. Prediction of the masked ‚Äúlove‚Äù
token depends on all input tokens before and after ‚Äúlove‚Äù. Right: Attention pattern in the
Transformer encoder. Each token along the vertical axis attends to all input tokens along
the horizontal axis.
Fine-TuningBERT
ThepretrainedBERTcanbe fine-tuned todownstreamencodingtasksinvolvingsingletext
or text pairs. During fine-tuning, additional layers can be added to BERT with randomized
parameters: these parameters and those pretrained BERT parameters will be updated to fit
training data of downstream tasks.
tFig. 11.9.2 Fine-tuning BERT for sentiment analysis.
Fig.11.9.2 illustratesfine-tuningofBERTforsentimentanalysis. TheTransformerencoder
is a pretrained BERT, which takes a text sequence as input and feeds the ‚Äú<cls>‚Äù represen-
tation(globalrepresentationoftheinput)intoanadditionalfullyconnectedlayertopredict
the sentiment. During fine-tuning, the cross-entropy loss between the prediction and the
label on sentiment analysis data is minimized via gradient-based algorithms, where the
additional layer is trained from scratch while pretrained parameters of BERT are updated.
BERT does more than sentiment analysis. The general language representations learned
by the 350-million-parameter BERT from 250 billion training tokens advanced the state of
theartfornaturallanguagetaskssuchassingletextclassification, textpairclassificationor
regression, text tagging, and question answering.
Youmaynotethatthesedownstreamtasksincludetextpairunderstanding. BERTpretrain-
ing has another loss for predicting whether one sentence immediately follows the other.
However, this loss was later found to be less useful when pretraining RoBERTa, a BERT
variant of the same size, on 2000 billion tokens ( Liuet al., 2019). Other derivatives of
459 Large-Scale Pretraining with Transformers
BERT improved model architectures or pretraining objectives, such as ALBERT (enforc-
ingparametersharing)( Lanetal.,2019),SpanBERT(representingandpredictingspansof
text) (Joshiet al., 2020), DistilBERT (lightweight via knowledge distillation) ( Sanhet al.,
2019),andELECTRA(replacedtokendetection)( Clarketal.,2020). Moreover,BERTin-
spiredTransformerpretrainingincomputervision,suchaswithvisionTransformers( Doso-
vitskiyetal.,2021),SwinTransformers( Liuetal.,2021),andMAE(maskedautoencoders)
(Heetal., 2022).
11.9.2Encoder‚ÄìDecoder
Since a Transformer encoder converts a sequence of input tokens into the same number
of output representations, the encoder-only mode cannot generate a sequence of arbitrary
lengthasinmachinetranslation. Asoriginallyproposedformachinetranslation,theTrans-
former architecture can be outfitted with a decoder that autoregressively predicts the tar-
get sequence of arbitrary length, token by token, conditional on both encoder output and
decoder output: (i) for conditioning on encoder output, encoder‚Äìdecoder cross-attention
(multi-head attention of decoder in Fig. 11.7.1 ) allows target tokens to attend to allinput
tokens; (ii) conditioning on decoder output is achieved by a so-called causalattention (this
name is common in the literature but is misleading as it has little connection to the proper
study of causality) pattern (masked multi-head attention of decoder in Fig. 11.7.1 ), where
any target token can only attend to pastandpresenttokens in the target sequence.
Topretrainencoder‚ÄìdecoderTransformersbeyondhuman-labeledmachinetranslationdata,
BART (Lewiset al., 2019) and T5 ( Raffelet al., 2020) are two concurrently proposed
encoder‚Äìdecoder Transformers pretrained on large-scale text corpora. Both attempt to re-
construct original text in their pretraining objectives, while the former emphasizes noising
input(e.g.,masking,deletion,permutation,androtation)andthelatterhighlightsmultitask
unification with comprehensive ablation studies.
PretrainingT5
As an example of the pretrained Transformer encoder‚Äìdecoder, T5 (Text-to-Text Transfer
Transformer) unifies many tasks as the same text-to-text problem: for any task, the input
of the encoder is a task description (e.g., ‚ÄúSummarize‚Äù, ‚Äú:‚Äù) followed by task input (e.g.,
a sequence of tokens from an article), and the decoder predicts the task output (e.g., a
sequenceoftokenssummarizingtheinputarticle). Toperformastext-to-text, T5istrained
to generate some target text conditional on input text.
To obtain input and output from any original text, T5 is pretrained to predict consecu-
tive spans. Specifically, tokens from text are randomly replaced by special tokens where
each consecutive span is replaced by the same special token. Consider the example in Fig.
11.9.3, where the original text is ‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚Äúthis‚Äù, ‚Äúred‚Äù, ‚Äúcar‚Äù. Tokens ‚Äúlove‚Äù, ‚Äúred‚Äù,
‚Äúcar‚Äù are randomly replaced by special tokens. Since ‚Äúred‚Äù and ‚Äúcar‚Äù are a consecutive
span, they are replaced by the same special token. As a result, the input sequence is ‚ÄúI‚Äù,
‚Äú<X>‚Äù, ‚Äúthis‚Äù, ‚Äú<Y>‚Äù, and the target sequence is ‚Äú<X>‚Äù, ‚Äúlove‚Äù, ‚Äú<Y>‚Äù, ‚Äúred‚Äù, ‚Äúcar‚Äù,
‚Äú<Z>‚Äù, where ‚Äú<Z>‚Äù is another special token marking the end. As shown in Fig. 11.9.3 ,
460 Attention Mechanisms and Transformers
tFig. 11.9.3 Left: Pretraining T5 by predicting consecutive spans. The original sentence is ‚ÄúI‚Äù, ‚Äúlove‚Äù,
‚Äúthis‚Äù, ‚Äúred‚Äù, ‚Äúcar‚Äù, where ‚Äúlove‚Äù is replaced by a special ‚Äú<X>‚Äù token, and consecutive
‚Äúred‚Äù, ‚Äúcar‚Äù are replaced by a special ‚Äú<Y>‚Äù token. The target sequence ends with a
special ‚Äú<Z>‚Äù token. Right: Attention pattern in the Transformer encoder‚Äìdecoder. In the
encoder self-attention (lower square), all input tokens attend to each other; In the
encoder‚Äìdecoder cross-attention (upper rectangle), each target token attends to all input
tokens; In the decoder self-attention (upper triangle), each target token attends to present
and past target tokens only (causal).
the decoder has a causal attention pattern to prevent itself from attending to future tokens
during sequence prediction.
In T5, predicting consecutive span is also referred to as reconstructing corrupted text.
With this objective, T5 is pretrained with 1000 billion tokens from the C4 (Colossal Clean
Crawled Corpus) data, which consists of clean English text from the web ( Raffelet al.,
2020).
Fine-TuningT5
SimilartoBERT,T5needstobefine-tuned(updatingT5parameters)ontask-specifictrain-
ing data to perform this task. Major differences from BERT fine-tuning include: (i) T5
input includes task descriptions; (ii) T5 can generate sequences with arbitrary length with
its Transformer decoder; (iii) No additional layers are required.
Fig. 11.9.4 explains fine-tuning T5 using text summarization as an example. In this down-
stream task, the task description tokens ‚ÄúSummarize‚Äù, ‚Äú:‚Äù followed by the article tokens
are input to the encoder.
Afterfine-tuning,the11-billion-parameterT5(T5-11B)achievedstate-of-the-artresultson
multiple encoding (e.g., classification) and generation (e.g., summarization) benchmarks.
Since released, T5 has been extensively used in later research. For example, switch Trans-
formersaredesignedbasedonT5toactivateasubsetoftheparametersforbettercomputa-
tional efficiency ( Fedusetal., 2022). In a text-to-image model called Imagen, text is input
to a frozen T5 encoder (T5-XXL) with 4.6 billion parameters ( Sahariaet al., 2022). The
461 Large-Scale Pretraining with Transformers
tFig. 11.9.4 Fine-tuning T5 for text summarization. Both the task description and article tokens are
fed into the Transformer encoder for predicting the summary.
photorealistic text-to-image examples in Fig. 11.9.5 suggest that the T5 encoder alone may
effectively represent text even without fine-tuning.
tFig. 11.9.5 Text-to-image examples by the Imagen model, whose text encoder is from T5 (Ô¨Ågures
taken from Saharia et al. ( 2022 )).
11.9.3Decoder-Only
Wehavereviewedencoder-onlyandencoder‚ÄìdecoderTransformers. Alternatively,decoder-
only Transformers remove the entire encoder and the decoder sublayer with the encoder‚Äì
decoder cross-attention from the original encoder‚Äìdecoder architecture depicted in Fig.
11.7.1. Nowadays, decoder-only Transformers have been the defacto architecture in large-
scalelanguagemodeling( Section9.3 ),whichleveragestheworld‚Äôsabundantunlabeledtext
corpora via self-supervised learning.
GPT and GPT-2
Usinglanguagemodelingasthetrainingobjective,theGPT(generativepre-training)model
chooses a Transformer decoder as its backbone ( Radfordetal., 2018).
Following the autoregressive language model training as described in Section 9.3.3 ,Fig.
11.9.6illustratesGPTpretrainingwithaTransformerencoder,wherethetargetsequenceis
the input sequence shifted by one token. Note that the attention pattern in the Transformer
462 Attention Mechanisms and Transformers
tFig. 11.9.6 Left: Pretraining GPT with language modeling. The target sequence is the input sequence
shifted by one token. Both ‚Äú<bos>‚Äù and ‚Äú<eos>‚Äù are special tokens marking the
beginning and end of sequences, respectively. Right: Attention pattern in the Transformer
decoder. Each token along the vertical axis attends to only its past tokens along the
horizontal axis (causal).
decoder enforces that each token can only attend to its past tokens (future tokens cannot be
attended to because they have not yet been chosen).
GPT has 100 million parameters and needs to be fine-tuned for individual downstream
tasks. A much larger Transformer-decoder language model, GPT-2, was introduced one
year later ( Radfordetal., 2019). Compared with the original Transformer decoder in GPT,
pre-normalization (discussed in Section 11.8.3 ) and improved initialization and weight-
scalingwereadoptedinGPT-2. Pretrainedon40GBoftext,the1.5-billion-parameterGPT-
2 obtained the state-of-the-art results on language modeling benchmarks and promising
results on multiple other tasks withoutupdating theparametersor architecture .
GPT-3and Beyond
GPT-2demonstratedpotentialofusingthesamelanguagemodelformultipletaskswithout
updatingthemodel. Thisismorecomputationallyefficientthanfine-tuning,whichrequires
model updates via gradient computation.
Before explaining the more computationally efficient use of language models without pa-
rameter update, recall Section 9.5 that a language model can be trained to generate a text
sequenceconditionalonsomeprefixtextsequence. Thus,apretrainedlanguagemodelmay
generate the task output as a sequence without parameter update , conditional on an input
sequencewiththetaskdescription,task-specificinput‚Äìoutputexamples,andaprompt(task
input). This learning paradigm is called in-context learning (Brownet al., 2020), which
can be further categorized into zero-shot ,one-shot , andfew-shot , when there is no, one,
and a few task-specific input‚Äìoutput examples ( Fig. 11.9.7 ).
These three settings were tested in GPT-3 ( Brownetal., 2020), whose largest version uses
data and model size about two orders of magnitude larger than those in GPT-2. GPT-3
usesthesameTransformerdecoderarchitectureasitsdirectpredecessorGPT-2exceptthat
attention patterns (at the right in Fig. 11.9.6 ) are sparser at alternating layers. Pretrained
463 Large-Scale Pretraining with Transformers
tFig. 11.9.7 Zero-shot, one-shot, few-shot in-context learning with language models (Transformer
decoders). No parameter update is needed.
tFig. 11.9.8 Aggregate performance of GPT-3 for all 42 accuracy-denominated benchmarks (caption
adapted and Ô¨Ågure taken from Brown et al. ( 2020 )).
with 300 billion tokens, GPT-3 performs better with larger model size, where few-shot
performance increases most rapidly ( Fig. 11.9.8 ).
The subsequent GPT-4 model did not fully disclose technical details in its report ( OpenAI,
2023). By contrast with its predecessors, GPT-4 is a large-scale, multimodal model that
can take both text and images as input and generate text output.
11.9.4Scalability
Fig. 11.9.8 empirically demonstrates scalability of Transformers in the GPT-3 language
model. For language modeling, more comprehensive empirical studies on the scalability
464 Attention Mechanisms and Transformers
of Transformers have led researchers to see promise in training larger Transformers with
more data and compute ( Kaplanetal., 2020).
tFig. 11.9.9 Transformer language model performance improves smoothly as we increase the model
size, dataset size, and amount of compute used for training. For optimal performance all
three factors must be scaled up in tandem. Empirical performance has a power-law
relationship with each individual factor when not bottlenecked by the other two (caption
adapted and Ô¨Ågure taken from Kaplan et al. ( 2020 )).
As shown in Fig. 11.9.9 ,power-law scaling can be observed in the performance with re-
spect to the model size (number of parameters, excluding embedding layers), dataset size
(numberoftrainingtokens), andamountoftrainingcompute(PetaFLOP/s-days, excluding
embedding layers). In general, increasing all these three factors in tandem leads to better
performance. However, howto increase them in tandem still remains a matter of debate
(Hoffmann etal., 2022).
tFig. 11.9.10 Transformer language model training runs (Ô¨Ågure taken from Kaplan et al. ( 2020 )).
As well as increased performance, large models also enjoy better sample efficiency than
small models. Fig. 11.9.10 shows that large models need fewer training samples (tokens
processed) to perform at the same level achieved by small models, and performance is
scaled smoothly with compute.
The empirical scaling behaviors in Kaplan et al.(2020) have been tested in subsequent
large Transformer models. For example, GPT-3 supported this hypothesis with two more
orders of magnitude in Fig. 11.9.11 .
465 Large-Scale Pretraining with Transformers
tFig. 11.9.11 GPT-3 performance (cross-entropy validation loss) follows a power-law trend with the
amount of compute used for training. The power-law behavior observed in Kaplan et al.
(2020 ) continues for an additional two orders of magnitude with only small deviations
from the predicted curve. Embedding parameters are excluded from compute and
parameter counts (caption adapted and Ô¨Ågure taken from Brown et al. ( 2020 )).
11.9.5LargeLanguageModels
The scalability of Transformers in the GPT series has inspired subsequent large language
models. The GPT-2 Transformer decoder was used for training the 530-billion-parameter
Megatron-Turing NLG ( Smithet al., 2022) with 270 billion training tokens. Following
theGPT-2design, the280-billion-parameterGopher( Raeetal., 2021)pretrainedwith300
billion tokens, performed competitively across diverse tasks. Inheriting the same architec-
ture and using the same compute budget of Gopher, Chinchilla ( Hoffmann et al., 2022)
is a substantially smaller (70 billion parameters) model that trains for much longer (1.4
trillion training tokens), outperforming Gopher on many tasks and with more emphasis on
the number of tokens than on the number of parameters. To continue the scaling line of
language modeling, PaLM (Pathway Language Model) ( Chowdhery et al., 2022), a 540-
billion-parameter Transformer decoder with modified designs pretrained on 780 billion to-
kens,outperformedaveragehumanperformanceontheBIG-Benchbenchmark( Srivastava
etal.,2022). Itslaterversion,PaLM2( Aniletal.,2023),scaleddataandmodelroughly1:1
and improved multilingual and reasoning capabilities. Other large language models, such
as Minerva ( Lewkowycz et al., 2022) that further trains a generalist (PaLM) and Galac-
tica (Tayloret al., 2022) that is not trained on a general corpus, have shown promising
quantitative and scientific reasoning capabilities.
Open-sourced releases, such as OPT (Open Pretrained Transformers) ( Zhangetal., 2022),
BLOOM ( Scaoet al., 2022), and FALCON ( Penedoet al., 2023), democratized research
and use of large language models. Focusing on computational efficiency at inference time,
the open-sourced Llama 1 ( Touvronet al., 2023a) outperformed much larger models by
training on more tokens than had been typically used. The updated Llama 2 ( Touvronet
al.,2023b)furtherincreasedthepretrainingcorpusby40%,leadingtoproductmodelsthat
may match the performance of competitive close-sourced models.
466 Attention Mechanisms and Transformers
164Weietal.(2022) discussed emergent abilities of large language models that are present in
larger models, but not in smaller models. However, simply increasing model size does not
inherently make models follow human instructions better. Sanh et al.(2021), Weiet al.
(2021) have found that fine-tuning large language models on a range of datasets described
viainstructions canimprovezero-shotperformanceonheld-outtasks. Using reinforcement
learningfromhumanfeedback , Ouyang etal.(2022) fine-tuned GPT-3 to follow a diverse
setofinstructions. FollowingtheresultantInstructGPTwhichalignslanguagemodelswith
human intent via fine-tuning ( Ouyanget al., 2022),ChatGPT164can generate human-like
responses (e.g., code debugging and creative writing) based on conversations with humans
andcanperformmanynaturallanguageprocessingtaskszero-shot( Qinetal.,2023). Baiet
al.(2022)replacedhumaninputs(e.g.,human-labeleddata)withmodeloutputstopartially
automate the instruction tuning process, which is also known as reinforcement learning
fromAI feedback .
Largelanguagemodelsofferanexcitingprospectofformulatingtextinputtoinducemodels
to perform desired tasks via in-context learning, which is also known as prompting . No-
tably,chain-of-thought prompting (Weiet al., 2022), an in-context learning method with
few-shot ‚Äúquestion, intermediate reasoning steps, answer‚Äù demonstrations, elicits the com-
plex reasoning capabilities of large language models in order to solve mathematical, com-
monsense, and symbolic reasoning tasks. Sampling multiple reasoning paths ( Wangetal.,
2023), diversifying few-shot demonstrations ( Zhanget al., 2023), and reducing complex
problems to sub-problems ( Zhouet al., 2023) can all improve the reasoning accuracy. In
fact, with simple prompts like ‚ÄúLet‚Äôs think step by step‚Äù just before each answer, large lan-
guagemodelscanevenperform zero-shot chain-of-thoughtreasoningwithdecentaccuracy
(Kojimaet al., 2022). Even for multimodal inputs consisting of both text and images, lan-
guage models can perform multimodal chain-of-thought reasoning with higher accuracy
than using text input only ( Zhangetal., 2023).
11.9.6Summary and Discussion
Transformers have been pretrained as encoder-only (e.g., BERT), encoder‚Äìdecoder (e.g.,
T5), and decoder-only (e.g., GPT series). Pretrained models may be adapted to perform
different tasks with model update (e.g., fine-tuning) or not (e.g., few-shot). Scalability of
Transformers suggests that better performance benefits from larger models, more training
data, and more training compute. Since Transformers were first designed and pretrained
for text data, this section leans slightly towards natural language processing. Nonetheless,
those models discussed above can be often found in more recent models across multiple
modalities. For example, (i) Chinchilla ( Hoffmann et al., 2022) was further extended to
Flamingo ( Alayracetal., 2022), a visual language model for few-shot learning; (ii) GPT-2
(Radfordet al., 2019) and the vision Transformer encode text and images in CLIP (Con-
trastive Language-Image Pre-training) ( Radfordet al., 2021), whose image and text em-
beddings were later adopted in the DALL-E 2 text-to-image system ( Rameshetal., 2022).
Although there have been no systematic studies on Transformer scalability in multimodal
pretrainingyet,anall-Transformertext-to-imagemodelcalledParti( Yuetal.,2022)shows
potential of scalability across modalities: a larger Parti is more capable of high-fidelity
image generation and content-rich text understanding ( Fig. 11.9.12 ).
467 Large-Scale Pretraining with Transformers
tFig. 11.9.12 Image examples generated from the same text by the Parti model of increasing sizes
(350M, 750M, 3B, 20B) (examples taken from Yu et al. ( 2022 )).
16511.9.7Exercises
1.Is it possible to fine-tune T5 using a minibatch consisting of different tasks? Why or
why not? How about for GPT-2?
2.Given a powerful language model, what applications can you think of?
3.Say that you are asked to fine-tune a language model to perform text classification by
adding additional layers. Where will you add them? Why?
4.Consider sequence-to-sequence problems (e.g., machine translation) where the input
sequence is always available throughout the target sequence prediction. What could be
limitations of modeling with decoder-only Transformers? Why?
Discussions165.
12 Optimization Algorithms
Ifyoureadthebookinsequenceuptothispointyoualreadyusedanumberofoptimization
algorithms to train deep learning models. They were the tools that allowed us to continue
updating model parameters and to minimize the value of the loss function, as evaluated on
the training set. Indeed, anyone content with treating optimization as a black box device
to minimize objective functions in a simple setting might well content oneself with the
knowledge that there exists an array of incantations of such a procedure (with names such
as ‚ÄúSGD‚Äù and ‚ÄúAdam‚Äù).
To do well, however, some deeper knowledge is required. Optimization algorithms are
important for deep learning. On the one hand, training a complex deep learning model can
take hours, days, or even weeks. The performance of the optimization algorithm directly
affects the model‚Äôs training efficiency. On the other hand, understanding the principles
of different optimization algorithms and the role of their hyperparameters will enable us to
tunethehyperparametersinatargetedmannertoimprovetheperformanceofdeeplearning
models.
In this chapter, we explore common deep learning optimization algorithms in depth. Al-
most all optimization problems arising in deep learning are nonconvex . Nonetheless, the
designandanalysisofalgorithmsinthecontextof convexproblemshaveproventobevery
instructive. It is for that reason that this chapter includes a primer on convex optimization
and the proof fora very simple stochastic gradient descent algorithm on a convexobjective
function.
12.1Optimizationand Deep Learning
In this section, we will discuss the relationship between optimization and deep learning as
wellasthechallengesofusingoptimizationindeeplearning. Foradeeplearningproblem,
we will usually define a loss function first. Once we have the loss function, we can use an
optimization algorithm in attempt to minimize the loss. In optimization, a loss function is
oftenreferredtoasthe objectivefunction oftheoptimizationproblem. Bytraditionandcon-
vention most optimization algorithms are concerned with minimization . If we ever need to
maximize an objective there is a simple solution: just flip the sign on the objective.
468
469 Optimization and Deep Learning
12.1.1Goal of Optimization
Although optimization provides a way to minimize the loss function for deep learning,
in essence, the goals of optimization and deep learning are fundamentally different. The
formerisprimarilyconcernedwithminimizinganobjectivewhereasthelatterisconcerned
withfindingasuitablemodel,givenafiniteamountofdata. In Section3.6 ,wediscussedthe
differencebetweenthesetwogoalsindetail. Forinstance,trainingerrorandgeneralization
errorgenerallydiffer: sincetheobjectivefunctionoftheoptimizationalgorithmisusuallya
lossfunctionbasedonthetrainingdataset,thegoalofoptimizationistoreducethetraining
error. However,thegoalofdeeplearning(ormorebroadly,statisticalinference)istoreduce
the generalization error. To accomplish the latter we need to pay attention to overfitting in
addition to using the optimization algorithm to reduce the training error.
%matplotlib inline
import numpy asnp
import torch
from mpl_toolkits import mplot3d
from d2l import torch asd2l
To illustrate the aforementioned different goals, let‚Äôs consider the empirical risk and the
risk. As described in Section 4.7.3 , the empirical risk is an average loss on the training
datasetwhiletheriskistheexpectedlossontheentirepopulationofdata. Belowwedefine
two functions: the risk function fand the empirical risk function g. Suppose that we have
only a finite amount of training data. As a result, here gis less smooth than f.
def f(x):
return x*torch .cos(np .pi*x)
def g(x):
return f(x) +0.2 *torch .cos( 5*np.pi*x)
The graph below illustrates that the minimum of the empirical risk on a training dataset
may be at a different location from the minimum of the risk (generalization error).
def annotate (text, xy, xytext): #@save
d2l.plt.gca() .annotate(text, xy =xy, xytext =xytext,
arrowprops =dict (arrowstyle ='->'))
x=torch .arange( 0.5,1.5,0.01 )
d2l.set_figsize(( 4.5,2.5))
d2l.plot(x, [f(x), g(x)], 'x','risk ')
annotate( 'min of \nempirical risk ', (1.0,-1.2), ( 0.5,-1.1))
annotate( 'min of risk ', (1.1,-1.05 ), ( 0.95 ,-0.5))
12.1.2OptimizationChallengesin Deep Learning
In this chapter, we are going to focus specifically on the performance of optimization algo-
rithms in minimizing the objective function, rather than a model‚Äôs generalization error. In
Section 3.1 we distinguished between analytical solutions and numerical solutions in opti-
470 Optimization Algorithms
mization problems. In deep learning, most objective functions are complicated and do not
have analytical solutions. Instead, we must use numerical optimization algorithms. The
optimization algorithms in this chapter all fall into this category.
There are many challenges in deep learning optimization. Some of the most vexing ones
arelocalminima, saddlepoints, andvanishinggradients. Let‚Äôshavealookatthem.
LocalMinima
For any objective function ùëì¬πùë•¬∫, if the value of ùëì¬πùë•¬∫atùë•is smaller than the values of ùëì¬πùë•¬∫
at any other points in the vicinity of ùë•, thenùëì¬πùë•¬∫could be a local minimum. If the value
ofùëì¬πùë•¬∫atùë•is the minimum of the objective function over the entire domain, then ùëì¬πùë•¬∫is
the global minimum.
For example, given the function
ùëì¬πùë•¬∫=ùë•cos¬πùúãùë•¬∫for 1.0ùë•2.0, (12.1.1)
we can approximate the local minimum and global minimum of this function.
x=torch .arange( -1.0,2.0,0.01 )
d2l.plot(x, [f(x), ], 'x','f(x) ')
annotate( 'local minimum ', (-0.3,-0.25 ), ( -0.77 ,-1.0))
annotate( 'global minimum ', (1.1,-0.95 ), ( 0.6,0.8))
The objective function of deep learning models usually has many local optima. When the
numerical solution of an optimization problem is near the local optimum, the numerical
471 Optimization and Deep Learning
solution obtained by the final iteration may only minimize the objective function locally,
rather than globally, as the gradient of the objective function‚Äôs solutions approaches or
becomes zero. Only some degree of noise might knock the parameter out of the local
minimum. In fact, this is one of the beneficial properties of minibatch stochastic gradient
descent where the natural variation of gradients over minibatches is able to dislodge the
parameters from local minima.
Saddle Points
Besides local minima, saddle points are another reason for gradients to vanish. A saddle
pointis any location where all gradients of a function vanish but which is neither a global
nor a local minimum. Consider the function ùëì¬πùë•¬∫=ùë•3. Its first and second derivative van-
ishforùë•=0. Optimizationmightstallatthispoint,eventhoughitisnotaminimum.
x=torch .arange( -2.0,2.0,0.01 )
d2l.plot(x, [x **3],'x','f(x) ')
annotate( 'saddle point ', (0,-0.2), ( -0.52 ,-5.0))
Saddle points in higher dimensions are even more insidious, as the example below shows.
Considerthefunction ùëì¬πùë•,ùë¶¬∫=ùë•2 ùë¶2. Ithasitssaddlepointat ¬π0,0¬∫. Thisisamaximum
with respect to ùë¶and a minimum with respect to ùë•. Moreover, it lookslike a saddle, which
is where this mathematical property got its name.
x, y =torch .meshgrid(
torch .linspace( -1.0,1.0,101), torch .linspace( -1.0,1.0,101))
z=x**2-y**2
ax=d2l.plt.figure() .add_subplot( 111, projection ='3d')
ax.plot_wireframe(x, y, z, **{'rstride ':10,'cstride ':10})
ax.plot([ 0], [ 0], [ 0],'rx')
ticks =[-1,0,1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'y');
We assume that the input of a function is a ùëò-dimensional vector and its output is a scalar,
472 Optimization Algorithms
so its Hessian matrix will have ùëòeigenvalues. The solution of the function could be a local
minimum, a local maximum, or a saddle point at a position where the function gradient is
zero:
When the eigenvalues of the function‚Äôs Hessian matrix at the zero-gradient position are
all positive, we have a local minimum for the function.
When the eigenvalues of the function‚Äôs Hessian matrix at the zero-gradient position are
all negative, we have a local maximum for the function.
When the eigenvalues of the function‚Äôs Hessian matrix at the zero-gradient position are
negative and positive, we have a saddle point for the function.
Forhigh-dimensionalproblemsthelikelihoodthatatleast someoftheeigenvaluesareneg-
ativeisquitehigh. Thismakessaddlepointsmorelikelythanlocalminima. Wewilldiscuss
some exceptions to this situation in the next section when introducing convexity. In short,
convex functions are those where the eigenvalues of the Hessian are never negative. Sadly,
though,mostdeeplearningproblemsdonotfallintothiscategory. Nonethelessitisagreat
tool to study optimization algorithms.
VanishingGradients
Probably the most insidious problem to encounter is the vanishing gradient. Recall our
commonly-used activation functions and their derivatives in Section 5.1.2 . For instance,
assumethatwewanttominimizethefunction ùëì¬πùë•¬∫=tanh¬πùë•¬∫andwehappentogetstarted
atùë•=4. As we can see, the gradient of ùëìis close to nil. More specifically, ùëì0¬πùë•¬∫=
1 tanh2¬πùë•¬∫and thusùëì0¬π4¬∫=0.0013. Consequently, optimization will get stuck for a
long time before we make progress. This turns out to be one of the reasons that training
deep learning models was quite tricky prior to the introduction of the ReLU activation
function.
x=torch .arange( -2.0,5.0,0.01 )
d2l.plot(x, [torch .tanh(x)], 'x','f(x) ')
annotate( 'vanishing gradient ', (4,1), ( 2,0.0))
As we saw, optimization for deep learning is full of challenges. Fortunately there exists a
robust range of algorithms that perform well and that are easy to use even for beginners.
473 Optimization and Deep Learning
166Furthermore, it is not really necessary to find thebest solution. Local optima or even ap-
proximate solutions thereof are still very useful.
12.1.3Summary
Minimizing the training error does notguarantee that we find the best set of parameters
to minimize the generalization error.
The optimization problems may have many local minima.
Theproblemmayhaveevenmoresaddlepoints,asgenerallytheproblemsarenotconvex.
Vanishing gradients can cause optimization to stall. Often a reparametrization of the
problem helps. Good initialization of the parameters can be beneficial, too.
12.1.4Exercises
1.Consider a simple MLP with a single hidden layer of, say, ùëëdimensions in the hid-
den layer and a single output. Show that for any local minimum there are at least ùëë!
equivalent solutions that behave identically.
2.Assume that we have a symmetric random matrix Mwhere the entries ùëÄùëñùëó=ùëÄùëóùëñare
each drawn from some probability distribution ùëùùëñùëó. Furthermore assume that ùëùùëñùëó¬πùë•¬∫=
ùëùùëñùëó¬π ùë•¬∫, i.e., that the distribution is symmetric (see e.g., Wigner ( 1958) for details).
1.Provethatthedistributionovereigenvaluesisalsosymmetric. Thatis,foranyeigen-
vector vthe probability that the associated eigenvalue ùúÜsatisfiesùëÉ¬πùúÜ> 0¬∫=ùëÉ¬πùúÜ<
0¬∫.
2.Why does the above notimplyùëÉ¬πùúÜ> 0¬∫=0.5?
3.What other challenges involved in deep learning optimization can you think of?
4.Assume that you want to balance a (real) ball on a (real) saddle.
1.Why is this hard?
2.Can you exploit this effect also for optimization algorithms?
Discussions166.
474 Optimization Algorithms
12.2Convexity
Convexity plays a vital role in the design of optimization algorithms. This is largely due
to the fact that it is much easier to analyze and test algorithms in such a context. In other
words, if the algorithm performs poorly even in the convex setting, typically we should not
hopetoseegreatresultsotherwise. Furthermore,eventhoughtheoptimizationproblemsin
deep learning are generally nonconvex, they often exhibit some properties of convex ones
near local minima. This can lead to exciting new optimization variants such as ( Izmailov
etal., 2018).
%matplotlib inline
import numpy asnp
import torch
from mpl_toolkits import mplot3d
from d2l import torch asd2l
12.2.1Definitions
Before convex analysis, we need to define convex sets andconvex functions . They lead to
mathematical tools that are commonly applied to machine learning.
ConvexSets
Sets are the basis of convexity. Simply put, a set Xin a vector space is convexif for any
ùëé,ùëè2Xthe line segment connecting ùëéandùëèis also inX. In mathematical terms this
means that for all ùúÜ2¬ª0,1¬ºwe have
ùúÜùëé¬∏¬π1 ùúÜ¬∫ùëè2Xwheneverùëé,ùëè2X. (12.2.1)
Thissoundsabitabstract. Consider Fig.12.2.1 . Thefirstsetisnotconvexsincethereexist
linesegmentsthatarenotcontainedinit. Theothertwosetssuffernosuchproblem.
tFig. 12.2.1 The Ô¨Årst set is nonconvex and the other two are convex.
Definitionsontheirownarenotparticularlyusefulunlessyoucandosomethingwiththem.
In this case we can look at intersections as shown in Fig. 12.2.2 . Assume thatXandYare
convex sets. ThenX\Yis also convex. To see this, consider any ùëé,ùëè2X\Y . SinceX
andYare convex, the line segments connecting ùëéandùëèare contained in both XandY.
Given that, they also need to be contained in X\Y, thus proving our theorem.
475 Convexity
tFig. 12.2.2 The intersection between two convex sets is convex.
Wecanstrengthenthisresultwithlittleeffort: givenconvexsets Xùëñ, theirintersection\ùëñXùëñ
is convex. To see that the converse is not true, consider two disjoint sets X\Y =;. Now
pickùëé2Xandùëè2Y. Thelinesegmentin Fig.12.2.3 connectingùëéandùëèneedstocontain
some part that is neither in Xnor inY, since we assumed that X\Y =;. Hence the line
segment is not inX[Yeither, thus proving that in general unions of convex sets need not
be convex.
tFig. 12.2.3 The union of two convex sets need not be convex.
Typically the problems in deep learning are defined on convex sets. For instance, Rùëë, the
setofùëë-dimensionalvectorsofrealnumbers,isaconvexset(afterall,thelinebetweenany
two points in Rùëëremains in Rùëë). In some cases we work with variables of bounded length,
such as balls of radius ùëüas defined byfxjx2Rùëëandkxkùëüg.
ConvexFunctions
Now that we have convex sets we can introduce convexfunctions ùëì. Given a convex set X,
a functionùëì:X! Risconvexif for allùë•,ùë•02Xand for allùúÜ2¬ª0,1¬ºwe have
ùúÜùëì¬πùë•¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πùë•0¬∫ùëì¬πùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•0¬∫. (12.2.2)
To illustrate this let‚Äôs plot a few functions and check which ones satisfy the requirement.
Below we define a few functions, both convex and nonconvex.
f=lambda x:0.5 *x**2# Convex
g=lambda x: torch .cos(np .pi*x) # Nonconvex
h=lambda x: torch .exp( 0.5 *x) # Convex
x, segment =torch .arange( -2,2,0.01 ), torch .tensor([ -1.5,1])
d2l.use_svg_display()
_, axes =d2l.plt.subplots( 1,3, figsize =(9,3))
for ax, func inzip(axes, [f, g, h]):
d2l.plot([x, segment], [func(x), func(segment)], axes =ax)
476 Optimization Algorithms
As expected, the cosine function is nonconvex , whereas the parabola and the exponential
functionare. Notethattherequirementthat Xisaconvexsetisnecessaryforthecondition
tomakesense. Otherwisetheoutcomeof ùëì¬πùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•0¬∫mightnotbewelldefined.
Jensen‚ÄôsInequality
Givenaconvexfunction ùëì,oneofthemostusefulmathematicaltoolsis Jensen‚Äôsinequality .
It amounts to a generalization of the definition of convexity:
√ï
ùëñùõºùëñùëì¬πùë•ùëñ¬∫ùëì √ï
ùëñùõºùëñùë•ùëñ!
andùê∏ùëã¬ªùëì¬πùëã¬∫¬ºùëì¬πùê∏ùëã¬ªùëã¬º¬∫, (12.2.3)
whereùõºùëñare nonnegative real numbers such that√ç
ùëñùõºùëñ=1andùëãis a random variable. In
other words, the expectation of a convex function is no less than the convex function of an
expectation, where the latter is usually a simpler expression. To prove the first inequality
we repeatedly apply the definition of convexity to one term in the sum at a time.
One of the common applications of Jensen‚Äôs inequality is to bound a more complicated
expression by a simpler one. For example, its application can be with regard to the log-
likelihood of partially observed random variables. That is, we use
ùê∏ùëåùëÉ¬πùëå¬∫¬ª logùëÉ¬πùëãjùëå¬∫¬º  logùëÉ¬πùëã¬∫, (12.2.4)
since¬Ø
ùëÉ¬πùëå¬∫ùëÉ¬πùëãjùëå¬∫ùëëùëå=ùëÉ¬πùëã¬∫. This can be used in variational methods. Here ùëå
is typically the unobserved random variable, ùëÉ¬πùëå¬∫is the best guess of how it might be
distributed, and ùëÉ¬πùëã¬∫is the distribution with ùëåintegrated out. For instance, in clustering
ùëåmight be the cluster labels and ùëÉ¬πùëãjùëå¬∫is the generative model when applying cluster
labels.
12.2.2Properties
Convex functions have many useful properties. We describe a few commonly-used ones
below.
477 Convexity
LocalMinima AreGlobal Minima
First and foremost, the local minima of convex functions are also the global minima. We
can prove it by contradiction as follows.
Consider a convex function ùëìdefined on a convex set X. Suppose that ùë•2Xis a local
minimum: thereexistsasmallpositivevalue ùëùsothatforùë•2Xthatsatisfies 0<jùë• ùë•j
ùëùwe haveùëì¬πùë•¬∫< ùëì¬πùë•¬∫.
Assume that the local minimum ùë•is not the global minimum of ùëì: there exists ùë•02X
for whichùëì¬πùë•0¬∫< ùëì¬πùë•¬∫. There also exists ùúÜ2 ¬ª0,1¬∫such asùúÜ=1 ùëù
jùë• ùë•0jso that
0<jùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•0 ùë•jùëù.
However, according to the definition of convex functions, we have
ùëì¬πùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•0¬∫ùúÜùëì¬πùë•¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πùë•0¬∫
<ùúÜùëì¬πùë•¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πùë•¬∫
=ùëì¬πùë•¬∫,(12.2.5)
which contradicts with our statement that ùë•is a local minimum. Therefore, there does
not existùë•02 Xfor whichùëì¬πùë•0¬∫< ùëì¬πùë•¬∫. The local minimum ùë•is also the global
minimum.
For instance, the convex function ùëì¬πùë•¬∫=¬πùë• 1¬∫2has a local minimum at ùë•=1, which is
also the global minimum.
f=lambda x: (x -1)**2
d2l.set_figsize()
d2l.plot([x, segment], [f(x), f(segment)], 'x','f(x) ')
The fact that the local minima for convex functions are also the global minima is very
convenient. It means that if we minimize functions we cannot ‚Äúget stuck‚Äù. Note, though,
that this does not mean that there cannot be more than one global minimum or that there
mightevenexistone. Forinstance,thefunction ùëì¬πùë•¬∫=max¬πjùë•j 1,0¬∫attainsitsminimum
value over the interval ¬ª 1,1¬º. Conversely, the function ùëì¬πùë•¬∫=exp¬πùë•¬∫does not attain a
minimum value on R: forùë•! 1it asymptotes to 0, but there is no ùë•for whichùëì¬πùë•¬∫=
0.
478 Optimization Algorithms
BelowSetsof ConvexFunctions AreConvex
We can conveniently define convex sets via below sets of convex functions. Concretely,
given a convex function ùëìdefined on a convex set X, any below set
Sùëèdef=fùë•jùë•2Xandùëì¬πùë•¬∫ùëèg (12.2.6)
is convex.
Let‚Äôsprovethisquickly. Recallthatforany ùë•,ùë•02Sùëèweneedtoshowthat ùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•02
Sùëèas long asùúÜ2¬ª0,1¬º. Sinceùëì¬πùë•¬∫ùëèandùëì¬πùë•0¬∫ùëè, by the definition of convexity we
have
ùëì¬πùúÜùë•¬∏¬π1 ùúÜ¬∫ùë•0¬∫ùúÜùëì¬πùë•¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πùë•0¬∫ùëè. (12.2.7)
Convexityand Second Derivatives
Whenever the second derivative of a function ùëì:Rùëõ!Rexists it is very easy to check
whetherùëìis convex. All we need to do is check whether the Hessian of ùëìis positive
semidefinite:r2ùëì0, i.e., denoting the Hessian matrix r2ùëìbyH,x>Hx0for all
x2Rùëõ. Forinstance, thefunction ùëì¬πx¬∫=1
2kxk2isconvexsincer2ùëì=1, i.e., itsHessian
is an identity matrix.
Formally, atwice-differentiableone-dimensionalfunction ùëì:R!Risconvexifandonly
if its second derivative ùëì000. For any twice-differentiable multidimensional function
ùëì:Rùëõ!R, it is convex if and only if its Hessian r2ùëì0.
First,weneedtoprovetheone-dimensionalcase. Toseethatconvexityof ùëìimpliesùëì000
we use the fact that
1
2ùëì¬πùë•¬∏ùúñ¬∫¬∏1
2ùëì¬πùë• ùúñ¬∫ùëìùë•¬∏ùúñ
2¬∏ùë• ùúñ
2
=ùëì¬πùë•¬∫. (12.2.8)
Sincethesecondderivativeisgivenbythelimitoverfinitedifferencesitfollowsthat
ùëì00¬πùë•¬∫=lim
ùúñ!0ùëì¬πùë•¬∏ùúñ¬∫¬∏ùëì¬πùë• ùúñ¬∫ 2ùëì¬πùë•¬∫
ùúñ20. (12.2.9)
To see thatùëì000implies that ùëìis convex we use the fact that ùëì000implies that ùëì0
is a monotonically nondecreasing function. Let ùëé < ùë• < ùëè be three points in R, where
ùë•=¬π1 ùúÜ¬∫ùëé¬∏ùúÜùëèandùúÜ2 ¬π0,1¬∫. According to the mean value theorem, there exist
ùõº2¬ªùëé,ùë•¬ºandùõΩ2¬ªùë•,ùëè¬ºsuch that
ùëì0¬πùõº¬∫=ùëì¬πùë•¬∫ ùëì¬πùëé¬∫
ùë• ùëéandùëì0¬πùõΩ¬∫=ùëì¬πùëè¬∫ ùëì¬πùë•¬∫
ùëè ùë•. (12.2.10)
By monotonicity ùëì0¬πùõΩ¬∫ùëì0¬πùõº¬∫, hence
ùë• ùëé
ùëè ùëéùëì¬πùëè¬∫¬∏ùëè ùë•
ùëè ùëéùëì¬πùëé¬∫ùëì¬πùë•¬∫. (12.2.11)
Sinceùë•=¬π1 ùúÜ¬∫ùëé¬∏ùúÜùëè, we have
ùúÜùëì¬πùëè¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πùëé¬∫ùëì¬π¬π1 ùúÜ¬∫ùëé¬∏ùúÜùëè¬∫, (12.2.12)
479 Convexity
thus proving convexity.
Second,weneedalemmabeforeprovingthemultidimensionalcase: ùëì:Rùëõ!Risconvex
if and only if for all x,y2Rùëõ
ùëî¬πùëß¬∫def=ùëì¬πùëßx¬∏¬π1 ùëß¬∫y¬∫whereùëß2¬ª0,1¬º (12.2.13)
is convex.
Toprovethatconvexityof ùëìimpliesthatùëîisconvex,wecanshowthatforall ùëé,ùëè,ùúÜ2¬ª0,1¬º
(thus 0ùúÜùëé¬∏¬π1 ùúÜ¬∫ùëè1)
ùëî¬πùúÜùëé¬∏¬π1 ùúÜ¬∫ùëè¬∫
=ùëì¬π¬πùúÜùëé¬∏¬π1 ùúÜ¬∫ùëè¬∫x¬∏¬π1 ùúÜùëé ¬π1 ùúÜ¬∫ùëè¬∫y¬∫
=ùëì¬πùúÜ¬πùëéx¬∏¬π1 ùëé¬∫y¬∫¬∏¬π1 ùúÜ¬∫¬πùëèx¬∏¬π1 ùëè¬∫y¬∫¬∫
ùúÜùëì¬πùëéx¬∏¬π1 ùëé¬∫y¬∫¬∏¬π1 ùúÜ¬∫ùëì¬πùëèx¬∏¬π1 ùëè¬∫y¬∫
=ùúÜùëî¬πùëé¬∫¬∏¬π 1 ùúÜ¬∫ùëî¬πùëè¬∫.(12.2.14)
To prove the converse, we can show that for all ùúÜ2¬ª0,1¬º
ùëì¬πùúÜx¬∏¬π1 ùúÜ¬∫y¬∫
=ùëî¬πùúÜ1¬∏¬π1 ùúÜ¬∫0¬∫
ùúÜùëî¬π1¬∫¬∏¬π 1 ùúÜ¬∫ùëî¬π0¬∫
=ùúÜùëì¬πx¬∫¬∏¬π 1 ùúÜ¬∫ùëì¬πy¬∫.(12.2.15)
Finally,usingthelemmaaboveandtheresultoftheone-dimensionalcase,themultidimen-
sional case can be proven as follows. A multidimensional function ùëì:Rùëõ!Ris convex
if and only if for all x,y2Rùëõùëî¬πùëß¬∫def=ùëì¬πùëßx¬∏¬π1 ùëß¬∫y¬∫, whereùëß2¬ª0,1¬º, is convex. Ac-
cording to the one-dimensional case, this holds if and only if ùëî00=¬πx y¬∫>H¬πx y¬∫0
(Hdef=r2ùëì) for all x,y2Rùëõ, which is equivalent to H0per the definition of positive
semidefinite matrices.
12.2.3Constraints
Oneofthenicepropertiesofconvexoptimizationisthatitallowsustohandleconstraintsef-
ficiently. Thatis,itallowsustosolve constrainedoptimization problemsoftheform:
minimize
xùëì¬πx¬∫
subject toùëêùëñ¬πx¬∫0for allùëñ2f1,...,ùëõg,(12.2.16)
whereùëìistheobjectiveandthefunctions ùëêùëñareconstraintfunctions. Toseewhatthisdoes
consider the case where ùëê1¬πx¬∫=kxk2 1. In this case the parameters xare constrained to
the unit ball. If a second constraint is ùëê2¬πx¬∫=v>x¬∏ùëè, then this corresponds to all xlying
on a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of
a ball.
480 Optimization Algorithms
Lagrangian
In general, solving a constrained optimization problem is difficult. One way of addressing
it stems from physics with a rather simple intuition. Imagine a ball inside a box. The ball
will roll to the place that is lowest and the forces of gravity will be balanced out with the
forcesthatthesidesoftheboxcanimposeontheball. Inshort,thegradientoftheobjective
function(i.e.,gravity)willbeoffsetbythegradientoftheconstraintfunction(theballneed
to remain inside the box by virtue of the walls ‚Äúpushing back‚Äù). Note that some constraints
may not be active: the walls that are not touched by the ball will not be able to exert any
force on the ball.
Skipping over the derivation of the Lagrangian ùêø, the above reasoning can be expressed
via the following saddle point optimization problem:
ùêø¬πx,ùõº1,...,ùõºùëõ¬∫=ùëì¬πx¬∫¬∏ùëõ√ï
ùëñ=1ùõºùëñùëêùëñ¬πx¬∫whereùõºùëñ0. (12.2.17)
Here the variables ùõºùëñ(ùëñ=1,...,ùëõ) are the so-called Lagrange multipliers that ensure that
constraintsareproperlyenforced. Theyarechosenjustlargeenoughtoensurethat ùëêùëñ¬πx¬∫
0for allùëñ. For instance, for any xwhereùëêùëñ¬πx¬∫<0naturally, we‚Äôd end up picking ùõºùëñ=0.
Moreover, thisisasaddlepointoptimizationproblemwhereonewantsto maximizeùêøwith
respect to all ùõºùëñand simultaneously minimize it with respect to x. There is a rich body of
literature explaining how to arrive at the function ùêø¬πx,ùõº1,...,ùõºùëõ¬∫. For our purposes it is
sufficient to know that the saddle point of ùêøis where the original constrained optimization
problem is solved optimally.
Penalties
One way of satisfying constrained optimization problems at least approximately is to adapt
theLagrangian ùêø. Ratherthansatisfying ùëêùëñ¬πx¬∫0wesimplyadd ùõºùëñùëêùëñ¬πx¬∫totheobjective
functionùëì¬πùë•¬∫. This ensures that the constraints will not be violated too badly.
In fact, we have been using this trick all along. Consider weight decay in Section 3.7 . In it
weaddùúÜ
2kwk2totheobjectivefunctiontoensurethat wdoesnotgrowtoolarge. Fromthe
constrained optimization point of view we can see that this will ensure that kwk2 ùëü20
for some radius ùëü. Adjusting the value of ùúÜallows us to vary the size of w.
In general, adding penalties is a good way of ensuring approximate constraint satisfaction.
In practice this turns out to be much more robust than exact satisfaction. Furthermore, for
nonconvex problems many of the properties that make the exact approach so appealing in
the convex case (e.g., optimality) no longer hold.
Projections
An alternative strategy for satisfying constraints is projections. Again, we encountered
them before, e.g., when dealing with gradient clipping in Section 9.5 . There we ensured
481 Convexity
that a gradient has length bounded by ùúÉvia
g gmin¬π1,ùúÉ¬ùkgk¬∫. (12.2.18)
Thisturnsout tobe a projection ofgontotheball ofradius ùúÉ. More generally, aprojection
on a convex setXis defined as
ProjX¬πx¬∫=argmin
x02Xkx x0k, (12.2.19)
which is the closest point in Xtox.
tFig. 12.2.4 Convex Projections.
The mathematical definition of projections may sound a bit abstract. Fig. 12.2.4 explains it
somewhatmoreclearly. Initwehavetwoconvexsets,acircleandadiamond. Pointsinside
both sets (yellow) remain unchanged during projections. Points outside both sets (black)
are projected to the points inside the sets (red) that are closet to the original points (black).
While for‚Ñì2balls this leaves the direction unchanged, this need not be the case in general,
as can be seen in the case of the diamond.
One of the uses for convex projections is to compute sparse weight vectors. In this case we
project weight vectors onto an ‚Ñì1ball, which is a generalized version of the diamond case
inFig. 12.2.4 .
12.2.4Summary
In the context of deep learning the main purpose of convex functions is to motivate opti-
mization algorithms and help us understand them in detail. In the following we will see
how gradient descent and stochastic gradient descent can be derived accordingly.
Intersections of convex sets are convex. Unions are not.
Theexpectationofaconvexfunctionisnolessthantheconvexfunctionofanexpectation
(Jensen‚Äôs inequality).
A twice-differentiable function is convex if and only if its Hessian (a matrix of second
derivatives) is positive semidefinite.
Convex constraints can be added via the Lagrangian. In practice we may simply add
them with a penalty to the objective function.
Projections map to points in the convex set closest to the original points.
482 Optimization Algorithms
16712.2.5Exercises
1.Assume that we want to verify convexity of a set by drawing all lines between points
within the set and checking whether the lines are contained.
1.Prove that it is sufficient to check only the points on the boundary.
2.Prove that it is sufficient to check only the vertices of the set.
2.Denote byBùëù¬ªùëü¬ºdef=fxjx2Rùëëandkxkùëùùëügthe ball of radius ùëüusing theùëù-norm.
Prove thatBùëù¬ªùëü¬ºis convex for all ùëù1.
3.Givenconvexfunctions ùëìandùëî,showthat max¬πùëì,ùëî¬∫isconvex,too. Provethat min¬πùëì,ùëî¬∫
is not convex.
4.Prove that the normalization of the softmax function is convex. More specifically prove
the convexity of ùëì¬πùë•¬∫=log√ç
ùëñexp¬πùë•ùëñ¬∫.
5.Prove that linear subspaces, i.e., X=fxjWx=bg, are convex sets.
6.Provethatinthecaseoflinearsubspaceswith b=0theprojectionProjXcanbewritten
asMxfor some matrix M.
7.Show that for twice-differentiable convex functions ùëìwe can write ùëì¬πùë•¬∏ùúñ¬∫=ùëì¬πùë•¬∫¬∏
ùúñùëì0¬πùë•¬∫¬∏1
2ùúñ2ùëì00¬πùë•¬∏ùúâ¬∫for someùúâ2¬ª0,ùúñ¬º.
8.Given a convex set Xand two vectors xandy, prove that projections never increase
distances, i.e.,kx ykkProjX¬πx¬∫ ProjX¬πy¬∫k.
Discussions167.
12.3GradientDescent
In this section we are going to introduce the basic concepts underlying gradient descent .
Although it is rarely used directly in deep learning, an understanding of gradient descent is
keytounderstandingstochasticgradientdescentalgorithms. Forinstance,theoptimization
problem might diverge due to an overly large learning rate. This phenomenon can already
be seen in gradient descent. Likewise, preconditioning is a common technique in gradient
descent and carries over to more advanced algorithms. Let‚Äôs start with a simple special
case.
12.3.1One-DimensionalGradient Descent
Gradient descent in one dimension is an excellent example to explain why the gradient
descent algorithm may reduce the value of the objective function. Consider some con-
tinuously differentiable real-valued function ùëì:R!R. Using a Taylor expansion we
483 Gradient Descent
obtain
ùëì¬πùë•¬∏ùúñ¬∫=ùëì¬πùë•¬∫¬∏ùúñùëì0¬πùë•¬∫¬∏O¬πùúñ2¬∫. (12.3.1)
That is, in first-order approximation ùëì¬πùë•¬∏ùúñ¬∫is given by the function value ùëì¬πùë•¬∫and the
first derivative ùëì0¬πùë•¬∫atùë•. It is not unreasonable to assume that for small ùúñmoving in the
direction of the negative gradient will decrease ùëì. To keep things simple we pick a fixed
step sizeùúÇ> 0and chooseùúñ= ùúÇùëì0¬πùë•¬∫. Plugging this into the Taylor expansion above we
get
ùëì¬πùë• ùúÇùëì0¬πùë•¬∫¬∫=ùëì¬πùë•¬∫ ùúÇùëì02¬πùë•¬∫¬∏O¬πùúÇ2ùëì02¬πùë•¬∫¬∫. (12.3.2)
Ifthederivative ùëì0¬πùë•¬∫‚â†0doesnotvanishwemakeprogresssince ùúÇùëì02¬πùë•¬∫>0. Moreover,
we can always choose ùúÇsmall enough for the higher-order terms to become irrelevant.
Hence we arrive at
ùëì¬πùë• ùúÇùëì0¬πùë•¬∫¬∫‚™Öùëì¬πùë•¬∫. (12.3.3)
This means that, if we use
ùë• ùë• ùúÇùëì0¬πùë•¬∫ (12.3.4)
toiterateùë•,thevalueoffunction ùëì¬πùë•¬∫mightdecline. Therefore,ingradientdescentwefirst
choose an initial value ùë•and a constant ùúÇ > 0and then use them to continuously iterate ùë•
untilthestopconditionisreached,forexample,whenthemagnitudeofthegradient jùëì0¬πùë•¬∫j
is small enough or the number of iterations has reached a certain value.
For simplicity we choose the objective function ùëì¬πùë•¬∫=ùë•2to illustrate how to implement
gradient descent. Although we know that ùë•=0is the solution to minimize ùëì¬πùë•¬∫, we still
use this simple function to observe how ùë•changes.
%matplotlib inline
import numpy asnp
import torch
from d2l import torch asd2l
def f(x): # Objective function
return x**2
def f_grad (x): # Gradient (derivative) of the objective function
return 2*x
Next, we use ùë•=10as the initial value and assume ùúÇ=0.2. Using gradient descent to
iterateùë•for 10 times we can see that, eventually, the value of ùë•approaches the optimal
solution.
def gd(eta, f_grad):
x=10.0
results =[x]
(continues on next page)
484 Optimization Algorithms
(continued from previous page)
for iinrange (10):
x-=eta *f_grad(x)
results .append( float (x))
print (f'epoch 10, x: {x:f}')
return results
results =gd(0.2, f_grad)
epoch 10, x: 0.060466
The progress of optimizing over ùë•can be plotted as follows.
def show_trace (results, f):
n=max(abs(min(results)), abs(max(results)))
f_line =torch .arange( -n, n, 0.01 )
d2l.set_figsize()
d2l.plot([f_line, results], [[f(x) for xinf_line], [
f(x) for xinresults]], 'x','f(x) ', fmts =['-','-o'])
show_trace(results, f)
Learning Rate
Thelearningrate ùúÇcanbesetbythealgorithmdesigner. Ifweusealearningratethatistoo
small, it will cause ùë•to update very slowly, requiring more iterations to get a better solu-
tion. To show what happens in such a case, consider the progress in the same optimization
problem for ùúÇ=0.05. As we can see, even after 10 steps we are still very far from the
optimal solution.
show_trace(gd( 0.05 , f_grad), f)
epoch 10, x: 3.486784
Conversely, if we use an excessively high learning rate, jùúÇùëì0¬πùë•¬∫jmight be too large for
the first-order Taylor expansion formula. That is, the term O¬πùúÇ2ùëì02¬πùë•¬∫¬∫in(12.3.2 )might
485 Gradient Descent
become significant. In this case, we cannot guarantee that the iteration of ùë•will be able to
lowerthevalueof ùëì¬πùë•¬∫. Forexample,whenwesetthelearningrateto ùúÇ=1.1,ùë•overshoots
the optimal solution ùë•=0and gradually diverges.
show_trace(gd( 1.1, f_grad), f)
epoch 10, x: 61.917364
LocalMinima
To illustrate what happens for nonconvex functions consider the case of ùëì¬πùë•¬∫=ùë•cos¬πùëêùë•¬∫
for some constant ùëê. This function has infinitely many local minima. Depending on our
choice of the learning rate and depending on how well conditioned the problem is, we may
end up with one of many solutions. The example below illustrates how an (unrealistically)
high learning rate will lead to a poor local minimum.
c=torch .tensor( 0.15 *np.pi)
def f(x): # Objective function
return x*torch .cos(c *x)
def f_grad (x): # Gradient of the objective function
return torch .cos(c *x)-c*x*torch .sin(c *x)
show_trace(gd( 2, f_grad), f)
486 Optimization Algorithms
epoch 10, x: -1.528166
12.3.2MultivariateGradient Descent
Nowthatwehaveabetterintuitionoftheunivariatecase, let‚Äôsconsiderthesituationwhere
x=¬ªùë•1,ùë•2,...,ùë•ùëë¬º>. That is, the objective function ùëì:Rùëë!Rmaps vectors into
scalars. Correspondingly its gradient is multivariate, too. It is a vector consisting of ùëë
partial derivatives:
rùëì¬πx¬∫=ùúïùëì¬πx¬∫
ùúïùë•1,ùúïùëì¬πx¬∫
ùúïùë•2,...,ùúïùëì¬πx¬∫
ùúïùë•ùëë>
. (12.3.5)
Each partial derivative element ùúïùëì¬πx¬∫¬ùùúïùë•ùëñin the gradient indicates the rate of change of
ùëìatxwith respect to the input ùë•ùëñ. As before in the univariate case we can use the cor-
responding Taylor approximation for multivariate functions to get some idea of what we
should do. In particular, we have that
ùëì¬πx¬∏ùùê¬∫=ùëì¬πx¬∫¬∏ùùê>rùëì¬πx¬∫¬∏O¬πk ùùêk2¬∫. (12.3.6)
Inotherwords,uptosecond-ordertermsin ùùêthedirectionofsteepestdescentisgivenbythe
negative gradient rùëì¬πx¬∫. Choosing a suitable learning rate ùúÇ> 0yields the prototypical
gradient descent algorithm:
x x ùúÇrùëì¬πx¬∫. (12.3.7)
To see how the algorithm behaves in practice let‚Äôs construct an objective function ùëì¬πx¬∫=
ùë•2
1¬∏2ùë•2
2with a two-dimensional vector x=¬ªùë•1,ùë•2¬º>as input and a scalar as output. The
gradient is given by rùëì¬πx¬∫=¬ª2ùë•1,4ùë•2¬º>. We will observe the trajectory of xby gradient
descent from the initial position ¬ª 5, 2¬º.
Tobeginwith,weneedtwomorehelperfunctions. Thefirstusesanupdatefunctionandap-
pliesit20timestotheinitialvalue. Thesecondhelpervisualizesthetrajectoryof x.
def train_2d (trainer, steps =20, f_grad =None ): #@save
"""Optimize a 2D objective function with a customized trainer."""
# `s1` and `s2` are internal state variables that will be used in Momentum,
‚Ü©!adagrad, RMSProp
(continues on next page)
487 Gradient Descent
(continued from previous page)
x1, x2, s1, s2 =-5,-2,0,0
results =[(x1, x2)]
for iinrange (steps):
iff_grad:
x1, x2, s1, s2 =trainer(x1, x2, s1, s2, f_grad)
else :
x1, x2, s1, s2 =trainer(x1, x2, s1, s2)
results .append((x1, x2))
print (f'epoch {i+1}, x1: {float (x1) :f}, x2: {float (x2) :f}')
return results
def show_trace_2d (f, results): #@save
"""Show the trace of 2D variables during optimization."""
d2l.set_figsize()
d2l.plt.plot( *zip(*results), '-o', color ='#ff7f0e ')
x1, x2 =torch .meshgrid(torch .arange( -5.5,1.0,0.1),
torch .arange( -3.0,1.0,0.1), indexing ='ij')
d2l.plt.contour(x1, x2, f(x1, x2), colors ='#1f77b4 ')
d2l.plt.xlabel( 'x1')
d2l.plt.ylabel( 'x2')
Next, we observe the trajectory of the optimization variable xfor learning rate ùúÇ=0.1.
We can see that after 20 steps the value of xapproaches its minimum at ¬ª0,0¬º. Progress is
fairly well-behaved albeit rather slow.
def f_2d (x1, x2): # Objective function
return x1**2+2*x2**2
def f_2d_grad (x1, x2): # Gradient of the objective function
return (2*x1, 4*x2)
def gd_2d (x1, x2, s1, s2, f_grad):
g1, g2 =f_grad(x1, x2)
return (x1 -eta *g1, x2 -eta *g2, 0,0)
eta =0.1
show_trace_2d(f_2d, train_2d(gd_2d, f_grad =f_2d_grad))
epoch 20, x1: -0.057646 , x2: -0.000073

488 Optimization Algorithms
12.3.3AdaptiveMethods
As we could see in Section 12.3.1 , getting the learning rate ùúÇ‚Äújust right‚Äù is tricky. If we
pick it too small, we make little progress. If we pick it too large, the solution oscillates and
in the worst case it might even diverge. What if we could determine ùúÇautomatically or get
ridofhavingtoselectalearningrateatall? Second-ordermethodsthatlooknotonlyatthe
value and gradient of the objective function but also at its curvature can help in this case.
While these methods cannot be applied to deep learning directly due to the computational
cost, they provide useful intuition into how to design advanced optimization algorithms
that mimic many of the desirable properties of the algorithms outlined below.
Newton‚ÄôsMethod
ReviewingtheTaylorexpansionofsomefunction ùëì:Rùëë!Rthereisnoneedtostopafter
the first term. In fact, we can write it as
ùëì¬πx¬∏ùùê¬∫=ùëì¬πx¬∫¬∏ùùê>rùëì¬πx¬∫¬∏1
2ùùê>r2ùëì¬πx¬∫ùùê¬∏O¬πk ùùêk3¬∫. (12.3.8)
To avoid cumbersome notation we define Hdef=r2ùëì¬πx¬∫to be the Hessian of ùëì, which is
aùëëùëëmatrix. For small ùëëand simple problems His easy to compute. For deep neural
networks, ontheotherhand, Hmaybeprohibitivelylarge,duetothecostofstoring O¬πùëë2¬∫
entries. Furthermore it may be too expensive to compute via backpropagation. For now
let‚Äôs ignore such considerations and look at what algorithm we would get.
After all, the minimum of ùëìsatisfiesrùëì=0. Following calculus rules in Section 2.4.3 , by
taking derivatives of (12.3.8 )with regard to ùùêand ignoring higher-order terms we arrive
at
rùëì¬πx¬∫¬∏Hùùê=0and hence ùùê= H 1rùëì¬πx¬∫. (12.3.9)
That is, we need to invert the Hessian Has part of the optimization problem.
As a simple example, for ùëì¬πùë•¬∫=1
2ùë•2we haverùëì¬πùë•¬∫=ùë•andH=1. Hence for any ùë•
we obtainùúñ= ùë•. In other words, a singlestep is sufficient to converge perfectly without
the need for any adjustment! Alas, we got a bit lucky here: the Taylor expansion was exact
sinceùëì¬πùë•¬∏ùúñ¬∫=1
2ùë•2¬∏ùúñùë•¬∏1
2ùúñ2.
Let‚Äôs see what happens in other problems. Given a convex hyperbolic cosine function
ùëì¬πùë•¬∫=cosh¬πùëêùë•¬∫for some constant ùëê, we can see that the global minimum at ùë•=0is
reached after a few iterations.
c=torch .tensor( 0.5)
def f(x): # Objective function
return torch .cosh(c *x)
def f_grad (x): # Gradient of the objective function
return c*torch .sinh(c *x)
(continues on next page)
489 Gradient Descent
(continued from previous page)
def f_hess (x): # Hessian of the objective function
return c**2*torch .cosh(c *x)
def newton (eta =1):
x=10.0
results =[x]
for iinrange (10):
x-=eta *f_grad(x) /f_hess(x)
results .append( float (x))
print ('epoch 10, x: ', x)
return results
show_trace(newton(), f)
epoch 10, x: tensor( 0.)
Now let‚Äôs consider a nonconvex function, such as ùëì¬πùë•¬∫=ùë•cos¬πùëêùë•¬∫for some constant ùëê.
Afterall,notethatinNewton‚ÄôsmethodweendupdividingbytheHessian. Thismeansthat
if the second derivative is negative we may walk into the direction of increasing the value
ofùëì. That is a fatal flaw of the algorithm. Let‚Äôs see what happens in practice.
c=torch .tensor( 0.15 *np.pi)
def f(x): # Objective function
return x*torch .cos(c *x)
def f_grad (x): # Gradient of the objective function
return torch .cos(c *x)-c*x*torch .sin(c *x)
def f_hess (x): # Hessian of the objective function
return -2*c*torch .sin(c *x)-x*c**2*torch .cos(c *x)
show_trace(newton(), f)
epoch 10, x: tensor( 26.8341 )
This went spectacularly wrong. How can we fix it? One way would be to ‚Äúfix‚Äù the Hessian
by taking its absolute value instead. Another strategy is to bring back the learning rate.
490 Optimization Algorithms
This seems to defeat the purpose, but not quite. Having second-order information allows
us to be cautious whenever the curvature is large and to take longer steps whenever the
objective function is flatter. Let‚Äôs see how this works with a slightly smaller learning rate,
sayùúÇ=0.5. As we can see, we have quite an efficient algorithm.
show_trace(newton( 0.5), f)
epoch 10, x: tensor( 7.2699 )
ConvergenceAnalysis
WeonlyanalyzetheconvergencerateofNewton‚Äôsmethodforsomeconvexandthreetimes
differentiable objective function ùëì, where the second derivative is nonzero, i.e., ùëì00>0.
The multivariate proof is a straightforward extension of the one-dimensional argument be-
low and omitted since it does not help us much in terms of intuition.
Denotebyùë•¬πùëò¬∫thevalueofùë•attheùëòthiterationandlet ùëí¬πùëò¬∫def=ùë•¬πùëò¬∫ ùë•bethedistancefrom
optimality at the ùëòthiteration. By Taylor expansion we have that the condition ùëì0¬πùë•¬∫=0
can be written as
0=ùëì0¬πùë•¬πùëò¬∫ ùëí¬πùëò¬∫¬∫=ùëì0¬πùë•¬πùëò¬∫¬∫ ùëí¬πùëò¬∫ùëì00¬πùë•¬πùëò¬∫¬∫¬∏1
2¬πùëí¬πùëò¬∫¬∫2ùëì000¬πùúâ¬πùëò¬∫¬∫, (12.3.10)
whichholdsforsome ùúâ¬πùëò¬∫2¬ªùë•¬πùëò¬∫ ùëí¬πùëò¬∫,ùë•¬πùëò¬∫¬º. Dividingtheaboveexpansionby ùëì00¬πùë•¬πùëò¬∫¬∫
491 Gradient Descent
yields
ùëí¬πùëò¬∫ ùëì0¬πùë•¬πùëò¬∫¬∫
ùëì00¬πùë•¬πùëò¬∫¬∫=1
2¬πùëí¬πùëò¬∫¬∫2ùëì000¬πùúâ¬πùëò¬∫¬∫
ùëì00¬πùë•¬πùëò¬∫¬∫. (12.3.11)
Recall that we have the update ùë•¬πùëò¬∏1¬∫=ùë•¬πùëò¬∫ ùëì0¬πùë•¬πùëò¬∫¬∫¬ùùëì00¬πùë•¬πùëò¬∫¬∫. Plugging in this update
equation and taking the absolute value of both sides, we have
ùëí¬πùëò¬∏1¬∫=1
2¬πùëí¬πùëò¬∫¬∫2ùëì000¬πùúâ¬πùëò¬∫¬∫
ùëì00¬πùë•¬πùëò¬∫¬∫. (12.3.12)
Consequently, whenever we are in a region of boundedùëì000¬πùúâ¬πùëò¬∫¬∫¬ù¬π2ùëì00¬πùë•¬πùëò¬∫¬∫¬∫ùëê, we
have a quadratically decreasing error
ùëí¬πùëò¬∏1¬∫ùëê¬πùëí¬πùëò¬∫¬∫2. (12.3.13)
Asanaside,optimizationresearcherscallthis linearconvergence,whereasaconditionsuch
asùëí¬πùëò¬∏1¬∫ùõºùëí¬πùëò¬∫wouldbecalleda constant rateofconvergence. Notethatthisanalysis
comes with a number of caveats. First, we do not really have muchof a guarantee when we
will reach the region of rapid convergence. Instead, we only know that once we reach it,
convergence will be very quick. Second, this analysis requires that ùëìis well-behaved up to
higher-order derivatives. It comes down to ensuring that ùëìdoes not have any ‚Äúsurprising‚Äù
properties in terms of how it might change its values.
Preconditioning
Quite unsurprisingly computing and storing the full Hessian is very expensive. It is thus
desirable to find alternatives. One way to improve matters is preconditioning . It avoids
computing the Hessian in its entirety but only computes the diagonal entries. This leads to
update algorithms of the form
x x ùúÇdiag¬πH¬∫ 1rùëì¬πx¬∫. (12.3.14)
While this is not quite as good as the full Newton‚Äôs method, it is still much better than not
using it. To see why this might be a good idea consider a situation where one variable
denotes height in millimeters and the other one denotes height in kilometers. Assuming
thatforboththenaturalscaleisinmeters,wehaveaterriblemismatchinparametrizations.
Fortunately, using preconditioning removes this. Effectively preconditioning with gradient
descentamountstoselectingadifferentlearningrateforeachvariable(coordinateofvector
x). Aswewillseelater,preconditioningdrivessomeoftheinnovationinstochasticgradient
descent optimization algorithms.
GradientDescent with Line Search
One of the key problems in gradient descent is that we might overshoot the goal or make
insufficient progress. A simple fix for the problem is to use line search in conjunction with
gradient descent. That is, we use the direction given by rùëì¬πx¬∫and then perform binary
search as to which learning rate ùúÇminimizesùëì¬πx ùúÇrùëì¬πx¬∫¬∫.
492 Optimization Algorithms
168This algorithm converges rapidly (for an analysis and proof see e.g., Boyd and Vanden-
berghe ( 2004)). However, for the purpose of deep learning this is not quite so feasible,
since each step of the line search would require us to evaluate the objective function on the
entire dataset. This is way too costly to accomplish.
12.3.4Summary
Learningratesmatter. Toolargeandwediverge,toosmallandwedonotmakeprogress.
Gradient descent can get stuck in local minima.
In high dimensions adjusting the learning rate is complicated.
Preconditioning can help with scale adjustment.
Newton‚Äôs method is a lot faster once it has started working properly in convex problems.
Beware of using Newton‚Äôs method without any adjustments for nonconvex problems.
12.3.5Exercises
1.Experiment with different learning rates and objective functions for gradient descent.
2.Implement line search to minimize a convex function in the interval ¬ªùëé,ùëè¬º.
1.Do you need derivatives for binary search, i.e., to decide whether to pick ¬ªùëé,¬πùëé¬∏
ùëè¬∫¬ù2¬ºor¬ª¬πùëé¬∏ùëè¬∫¬ù2,ùëè¬º.
2.How rapid is the rate of convergence for the algorithm?
3.Implement the algorithm and apply it to minimizing log¬πexp¬πùë•¬∫¬∏exp¬π 2ùë• 3¬∫¬∫.
3.Design an objective function defined on R2where gradient descent is exceedingly slow.
Hint: scale different coordinates differently.
4.Implement the lightweight version of Newton‚Äôs method using preconditioning:
1.Use diagonal Hessian as preconditioner.
2.Use the absolute values of that rather than the actual (possibly signed) values.
3.Apply this to the problem above.
5.Apply the algorithm above to a number of objective functions (convex or not). What
happens if you rotate coordinates by 45degrees?
Discussions168.
493 Stochastic Gradient Descent
12.4StochasticGradient Descent
Inearlierchapterswekeptusingstochasticgradientdescentinourtrainingprocedure,how-
ever, withoutexplainingwhyitworks. Toshedsomelightonit, wejustdescribedthebasic
principlesofgradientdescentin Section12.3 . Inthissection,wegoontodiscuss stochastic
gradientdescent in greater detail.
%matplotlib inline
import math
import torch
from d2l import torch asd2l
12.4.1StochasticGradient Updates
In deep learning, the objective function is usually the average of the loss functions for each
example in the training dataset. Given a training dataset of ùëõexamples, we assume that
ùëìùëñ¬πx¬∫is the loss function with respect to the training example of index ùëñ, where xis the
parameter vector. Then we arrive at the objective function
ùëì¬πx¬∫=1
ùëõùëõ√ï
ùëñ=1ùëìùëñ¬πx¬∫. (12.4.1)
The gradient of the objective function at xis computed as
rùëì¬πx¬∫=1
ùëõùëõ√ï
ùëñ=1rùëìùëñ¬πx¬∫. (12.4.2)
If gradient descent is used, the computational cost for each independent variable iteration
isO¬πùëõ¬∫, which grows linearly with ùëõ. Therefore, when the training dataset is larger, the
cost of gradient descent for each iteration will be higher.
Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each
iteration of stochastic gradient descent, we uniformly sample an index ùëñ2f1,...,ùëõgfor
data examples at random, and compute the gradient rùëìùëñ¬πx¬∫to update x:
x x ùúÇrùëìùëñ¬πx¬∫, (12.4.3)
whereùúÇisthelearningrate. Wecanseethatthecomputationalcostforeachiterationdrops
fromO¬πùëõ¬∫of the gradient descent to the constant O¬π1¬∫. Moreover, we want to empha-
size that the stochastic gradient rùëìùëñ¬πx¬∫is an unbiased estimate of the full gradient rùëì¬πx¬∫
because
Eùëñrùëìùëñ¬πx¬∫=1
ùëõùëõ√ï
ùëñ=1rùëìùëñ¬πx¬∫=rùëì¬πx¬∫. (12.4.4)
Thismeansthat,onaverage,thestochasticgradientisagoodestimateofthegradient.
Now, we will compare it with gradient descent by adding random noise with a mean of 0
and a variance of 1 to the gradient to simulate a stochastic gradient descent.
494 Optimization Algorithms
def f(x1, x2): # Objective function
return x1**2+2*x2**2
def f_grad (x1, x2): # Gradient of the objective function
return 2*x1, 4*x2
def sgd(x1, x2, s1, s2, f_grad):
g1, g2 =f_grad(x1, x2)
# Simulate noisy gradient
g1+=torch .normal( 0.0,1, (1,)).item()
g2+=torch .normal( 0.0,1, (1,)).item()
eta_t =eta *lr()
return (x1 -eta_t *g1, x2 -eta_t *g2, 0,0)
def constant_lr ():
return 1
eta =0.1
lr=constant_lr # Constant learning rate
d2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))
epoch 50, x1: 0.225517 , x2: -0.076646
As we can see, the trajectory of the variables in the stochastic gradient descent is much
more noisy than the one we observed in gradient descent in Section 12.3 . This is due to
the stochastic nature of the gradient. That is, even when we arrive near the minimum,
we are still subject to the uncertainty injected by the instantaneous gradient via ùúÇrùëìùëñ¬πx¬∫.
Even after 50 steps the quality is still not so good. Even worse, it will not improve after
additional steps (we encourage you to experiment with a larger number of steps to confirm
this). This leaves us with the only alternative: change the learning rate ùúÇ. However, if we
pick this too small, we will not make any meaningful progress initially. On the other hand,
if we pick it too large, we will not get a good solution, as seen above. The only way to
resolve these conflicting goals is to reduce the learning rate dynamically as optimization
progresses.
This is also the reason for adding a learning rate function lrinto the sgdstep function. In
495 Stochastic Gradient Descent
the example above any functionality for learning rate scheduling lies dormant as we set the
associated lrfunction to be constant.
12.4.2DynamicLearning Rate
ReplacingùúÇwith a time-dependent learning rate ùúÇ¬πùë°¬∫adds to the complexity of controlling
convergence of an optimization algorithm. In particular, we need to figure out how rapidly
ùúÇshould decay. If it is too quick, we will stop optimizing prematurely. If we decrease
it too slowly, we waste too much time on optimization. The following are a few basic
strategies that are used in adjusting ùúÇover time (we will discuss more advanced strategies
later):
ùúÇ¬πùë°¬∫=ùúÇùëñifùë°ùëñùë°ùë°ùëñ¬∏1piecewise constant
ùúÇ¬πùë°¬∫=ùúÇ0ùëí ùúÜùë°exponential decay
ùúÇ¬πùë°¬∫=ùúÇ0¬πùõΩùë°¬∏1¬∫ ùõºpolynomial decay(12.4.5)
Inthefirst piecewiseconstant scenariowedecreasethelearningrate,e.g.,wheneverprogress
in optimization stalls. This is a common strategy for training deep networks. Alternatively
we could decrease it much more aggressively by an exponential decay . Unfortunately this
often leads to premature stopping before the algorithm has converged. A popular choice is
polynomial decay withùõº=0.5. In the case of convex optimization there are a number of
proofs that show that this rate is well behaved.
Let‚Äôs see what the exponential decay looks like in practice.
def exponential_lr ():
# Global variable that is defined outside this function and updated inside
global t
t+=1
return math .exp( -0.1 *t)
t=1
lr=exponential_lr
d2l.show_trace_2d(f, d2l .train_2d(sgd, steps =1000 , f_grad =f_grad))
epoch 1000 , x1: -0.758829 , x2: -0.115584
As expected, the variance in the parameters is significantly reduced. However, this comes
496 Optimization Algorithms
169at the expense of failing to converge to the optimal solution x=¬π0,0¬∫. Even after 1000
iterationstepsarewearestillveryfarawayfromtheoptimalsolution. Indeed,thealgorithm
fails to convergeat all. On the other hand, if weuse a polynomialdecaywhere the learning
rate decays with the inverse square root of the number of steps, convergence gets better
after only 50 steps.
def polynomial_lr ():
# Global variable that is defined outside this function and updated inside
global t
t+=1
return (1+0.1 *t)**(-0.5)
t=1
lr=polynomial_lr
d2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))
epoch 50, x1: 0.144834 , x2: 0.041688
Thereexistmanymorechoicesforhowtosetthelearningrate. Forinstance, wecouldstart
with a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We
could even alternate between smaller and larger learning rates. There exists a large variety
of such schedules. For now let‚Äôs focus on learning rate schedules for which a comprehen-
sive theoretical analysis is possible, i.e., on learning rates in a convex setting. For general
nonconvexproblemsitis verydifficulttoobtain meaningfulconvergenceguarantees, since
ingeneralminimizing nonlinearnonconvexproblemsisNP hard. Forasurveyseee.g., the
excellent lecture notes169of Tibshirani 2015.
12.4.3ConvergenceAnalysisforConvexObjectives
The following convergence analysis of stochastic gradient descent for convex objective
functions is optional and primarily serves to convey more intuition about the problem. We
limit ourselves to one of the simplest proofs ( Nesterov and Vial, 2000 ). Significantly more
advanced proof techniques exist, e.g., whenever the objective function is particularly well
behaved.
Suppose that the objective function ùëì¬πùùÉ,x¬∫is convex in xfor all ùùÉ. More concretely, we
497 Stochastic Gradient Descent
consider the stochastic gradient descent update:
xùë°¬∏1=xùë° ùúÇùë°ùúïxùëì¬πùùÉùë°,x¬∫, (12.4.6)
whereùëì¬πùùÉùë°,x¬∫istheobjectivefunctionwithrespecttothetrainingexample ùùÉùë°drawnfrom
some distribution at step ùë°andxis the model parameter. Denote by
ùëÖ¬πx¬∫=ùê∏ùùÉ¬ªùëì¬πùùÉ,x¬∫¬º (12.4.7)
the expected risk and by ùëÖits minimum with regard to x. Last let xbe the minimizer
(we assume that it exists within the domain where xis defined). In this case we can track
the distance between the current parameter xùë°at timeùë°and the risk minimizer xand see
whether it improves over time:
kxùë°¬∏1 xk2
=kxùë° ùúÇùë°ùúïxùëì¬πùùÉùë°,x¬∫ xk2
=kxùë° xk2¬∏ùúÇ2
ùë°kùúïxùëì¬πùùÉùë°,x¬∫k2 2ùúÇùë°
xùë° x,ùúïxùëì¬πùùÉùë°,x¬∫
.(12.4.8)
Weassumethatthe ‚Ñì2normofstochasticgradient ùúïxùëì¬πùùÉùë°,x¬∫isboundedbysomeconstant
ùêø, hence we have that
ùúÇ2
ùë°kùúïxùëì¬πùùÉùë°,x¬∫k2ùúÇ2
ùë°ùêø2. (12.4.9)
We are mostly interested in how the distance between xùë°andxchangesin expectation .
In fact, for any specific sequence of steps the distance might well increase, depending on
whichever ùùÉùë°weencounter. Henceweneedtoboundthedotproduct. Sinceforanyconvex
functionùëìit holds that ùëì¬πy¬∫ùëì¬πx¬∫¬∏hùëì0¬πx¬∫,y xifor all xandy, by convexity we
have
ùëì¬πùùÉùë°,x¬∫ùëì¬πùùÉùë°,xùë°¬∫¬∏
x xùë°,ùúïxùëì¬πùùÉùë°,xùë°¬∫
. (12.4.10)
Plugging both inequalities (12.4.9 )and(12.4.10 )into(12.4.8 )we obtain a bound on the
distance between parameters at time ùë°¬∏1as follows:
kxùë° xk2 kxùë°¬∏1 xk22ùúÇùë°¬πùëì¬πùùÉùë°,xùë°¬∫ ùëì¬πùùÉùë°,x¬∫¬∫ ùúÇ2
ùë°ùêø2. (12.4.11)
This means that we make progress as long as the difference between current loss and the
optimallossoutweighs ùúÇùë°ùêø2¬ù2. Sincethisdifferenceisboundtoconvergetozeroitfollows
that the learning rate ùúÇùë°also needs to vanish.
Next we take expectations over (12.4.11 ). This yields
ùê∏
kxùë° xk2
 ùê∏
kxùë°¬∏1 xk2
2ùúÇùë°¬ªùê∏¬ªùëÖ¬πxùë°¬∫¬º ùëÖ¬º ùúÇ2
ùë°ùêø2. (12.4.12)
The last step involves summing over the inequalities for ùë°2 f1,...,ùëág. Since the sum
telescopes and by dropping the lower term we obtain
kx1 xk22 ùëá√ï
ùë°=1ùúÇùë°!
¬ªùê∏¬ªùëÖ¬πxùë°¬∫¬º ùëÖ¬º ùêø2ùëá√ï
ùë°=1ùúÇ2
ùë°. (12.4.13)
498 Optimization Algorithms
Note that we exploited that x1is given and thus the expectation can be dropped. Last
define
¬Øxdef=√çùëá
ùë°=1ùúÇùë°xùë°√çùëá
ùë°=1ùúÇùë°. (12.4.14)
Since
ùê∏ √çùëá
ùë°=1ùúÇùë°ùëÖ¬πxùë°¬∫
√çùëá
ùë°=1ùúÇùë°!
=√çùëá
ùë°=1ùúÇùë°ùê∏¬ªùëÖ¬πxùë°¬∫¬º
√çùëá
ùë°=1ùúÇùë°=ùê∏¬ªùëÖ¬πxùë°¬∫¬º, (12.4.15)
by Jensen‚Äôs inequality (setting ùëñ=ùë°,ùõºùëñ=ùúÇùë°¬ù√çùëá
ùë°=1ùúÇùë°in(12.2.3 )) and convexity of ùëÖit
follows thatùê∏¬ªùëÖ¬πxùë°¬∫¬ºùê∏¬ªùëÖ¬π¬Øx¬∫¬º, thus
ùëá√ï
ùë°=1ùúÇùë°ùê∏¬ªùëÖ¬πxùë°¬∫¬ºùëá√ï
ùë°=1ùúÇùë°ùê∏¬ªùëÖ¬π¬Øx¬∫¬º. (12.4.16)
Plugging this into the inequality (12.4.13 )yields the bound
¬ªùê∏¬ª¬Øx¬º¬º ùëÖùëü2¬∏ùêø2√çùëá
ùë°=1ùúÇ2
ùë°
2√çùëá
ùë°=1ùúÇùë°, (12.4.17)
whereùëü2def=kx1 xk2is a bound on the distance between the initial choice of parameters
and the final outcome. In short, the speed of convergence depends on how the norm of
stochastic gradient is bounded ( ùêø) and how far away from optimality the initial parameter
value is (ùëü). Note that the bound is in terms of ¬Øxrather than xùëá. This is the case since ¬Øxis
a smoothed version of the optimization path. Whenever ùëü,ùêø, andùëáare known we can pick
the learning rate ùúÇ=ùëü¬ù¬πùêøp
ùëá¬∫. This yields as upper bound ùëüùêø¬ùp
ùëá. That is, we converge
with rateO¬π1¬ùp
ùëá¬∫to the optimal solution.
12.4.4StochasticGradients and Finite Samples
So far we have played a bit fast and loose when it comes to talking about stochastic gra-
dient descent. We posited that we draw instances ùë•ùëñ, typically with labels ùë¶ùëñfrom some
distribution ùëù¬πùë•,ùë¶¬∫and that we use this to update the model parameters in some man-
ner. In particular, for a finite sample size we simply argued that the discrete distribution
ùëù¬πùë•,ùë¶¬∫=1
ùëõ√çùëõ
ùëñ=1ùõøùë•ùëñ¬πùë•¬∫ùõøùë¶ùëñ¬πùë¶¬∫forsomefunctions ùõøùë•ùëñandùõøùë¶ùëñallowsustoperformstochas-
tic gradient descent over it.
However, this is not really what we did. In the toy examples in the current section we
simplyaddednoisetoanotherwisenon-stochasticgradient,i.e.,wepretendedtohavepairs
¬πùë•ùëñ,ùë¶ùëñ¬∫. It turns out that this is justified here (see the exercises for a detailed discussion).
More troubling is that in all previous discussions we clearly did not do this. Instead we
iteratedoverallinstances exactlyonce . Toseewhythisispreferableconsidertheconverse,
namelythatwearesampling ùëõobservationsfromthediscretedistribution withreplacement .
The probability of choosing an element ùëñat random is 1¬ùùëõ. Thus to choose it atleastonce
is
ùëÉ¬πchooseùëñ¬∫=1 ùëÉ¬πomitùëñ¬∫=1 ¬π1 1¬ùùëõ¬∫ùëõ1 ùëí 10.63. (12.4.18)
499 Stochastic Gradient Descent
170Asimilarreasoningshowsthattheprobabilityofpickingsomesample(i.e., trainingexam-
ple)exactly once is given by
ùëõ
11
ùëõ
1 1
ùëõùëõ 1
=ùëõ
ùëõ 1
1 1
ùëõùëõ
ùëí 10.37. (12.4.19)
Sampling with replacement leads to an increased variance and decreased data efficiency
relative to sampling without replacement . Hence, in practice we perform the latter (and
this is the default choice throughout this book). Last note that repeated passes through the
training dataset traverse it in a different random order.
12.4.5Summary
For convex problems we can prove that for a wide choice of learning rates stochastic
gradient descent will converge to the optimal solution.
For deep learning this is generally not the case. However, the analysis of convex prob-
lems gives us useful insight into how to approach optimization, namely to reduce the
learning rate progressively, albeit not too quickly.
Problems occur when the learning rate is too small or too large. In practice a suitable
learning rate is often found only after multiple experiments.
When there are more examples in the training dataset, it costs more to compute each
iterationforgradientdescent,sostochasticgradientdescentispreferredinthesecases.
Optimalityguaranteesforstochasticgradientdescentareingeneralnotavailableinnon-
convex cases since the number of local minima that require checking might well be
exponential.
12.4.6Exercises
1.Experiment with different learning rate schedules for stochastic gradient descent and
with different numbers of iterations. In particular, plot the distance from the optimal
solution¬π0,0¬∫as a function of the number of iterations.
2.Prove that for the function ùëì¬πùë•1,ùë•2¬∫=ùë•2
1¬∏2ùë•2
2adding normal noise to the gradient is
equivalent to minimizing a loss function ùëì¬πx,w¬∫=¬πùë•1 ùë§1¬∫2¬∏2¬πùë•2 ùë§2¬∫2where x
is drawn from a normal distribution.
3.Compareconvergenceofstochasticgradientdescentwhenyousamplefrom f¬πùë•1,ùë¶1¬∫,...,¬πùë•ùëõ,ùë¶ùëõ¬∫g
with replacement and when you sample without replacement.
4.Howwouldyouchangethestochasticgradientdescentsolverifsomegradient(orrather
some coordinate associated with it) was consistently larger than all the other gradients?
5.Assume that ùëì¬πùë•¬∫=ùë•2¬π1¬∏sinùë•¬∫. How many local minima does ùëìhave? Can you
changeùëìin such a way that to minimize it one needs to evaluate all the local minima?
Discussions170.
500 Optimization Algorithms
17112.5MinibatchStochasticGradient Descent
So far we encountered two extremes in the approach to gradient-based learning: Section
12.3usesthefulldatasettocomputegradientsandtoupdateparameters,onepassatatime.
Conversely Section 12.4 processes one training exampleat a time to make progress. Either
ofthemhasitsowndrawbacks. Gradientdescentisnotparticularly dataeÔ¨Äicient whenever
dataisverysimilar. Stochasticgradientdescentisnotparticularly computationallyeÔ¨Äicient
since CPUs and GPUs cannot exploit the full power of vectorization. This suggests that
there might be something in between, and in fact, that is what we have been using so far in
the examples we discussed.
12.5.1Vectorizationand Caches
At the heart of the decision to use minibatches is computational efficiency. This is most
easily understood when considering parallelization to multiple GPUs and multiple servers.
In this case we need to send at least one image to each GPU. With 8 GPUs per server and
16 servers we already arrive at a minibatch size no smaller than 128.
Things are a bit more subtle when it comes to single GPUs or even CPUs. These devices
have multiple types of memory, often multiple types of computational units and different
bandwidth constraints between them. For instance, a CPU has a small number of registers
and then the L1, L2, and in some cases even L3 cache (which is shared among different
processor cores). These caches are of increasing size and latency (and at the same time
they are of decreasing bandwidth). Suffice to say, the processor is capable of performing
many more operations than what the main memory interface is able to provide.
First, a 2GHz CPU with 16 cores and AVX-512 vectorization can process up to 2109
1632=1012bytes per second. The capability of GPUs easily exceeds this number by a
factor of 100. On the other hand, a midrange server processor might not have much more
than 100 GB/s bandwidth, i.e., less than one tenth of what would be required to keep the
processor fed. To make matters worse, not all memory access is created equal: memory
interfaces are typically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a
single byte incurs the cost of a much wider access.
Second, there is significant overhead for the first access whereas sequential access is rela-
tivelycheap(thisisoftencalledaburstread). Therearemanymorethingstokeepinmind,
such as caching when we have multiple sockets, chiplets, and other structures. See this
Wikipedia article171for a more in-depth discussion.
The way to alleviate these constraints is to use a hierarchy of CPU caches that are actu-
ally fast enough to supply the processor with data. This is thedriving force behind batch-
ing in deep learning. To keep matters simple, consider matrix-matrix multiplication, say
A=BC. We have a number of options for calculating A. For instance, we could try the
following:
501 Minibatch Stochastic Gradient Descent
1.We could compute Aùëñùëó=Bùëñ,:C:,ùëó, i.e., we could compute it elementwise by means of
dot products.
2.We could compute A:,ùëó=BC :,ùëó, i.e., we could compute it one column at a time.
Likewise we could compute Aone row Aùëñ,:at a time.
3.We could simply compute A=BC.
4.We could break BandCinto smaller block matrices and compute Aone block at a
time.
If we follow the first option, we will need to copy one row and one column vector into the
CPUeachtimewewanttocomputeanelement Aùëñùëó. Evenworse,duetothefactthatmatrix
elements are aligned sequentially we are thus required to access many disjoint locations
for one of the two vectors as we read them from memory. The second option is much
more favorable. In it, we are able to keep the column vector C:,ùëóin the CPU cache while
we keep on traversing through B. This halves the memory bandwidth requirement with
correspondingly faster access. Of course, option 3 is most desirable. Unfortunately, most
matricesmightnotentirelyfitintocache(thisiswhatwearediscussingafterall). However,
option4offersapracticallyusefulalternative: wecanmoveblocksofthematrixintocache
and multiply them locally. Optimized libraries take care of this for us. Let‚Äôs have a look at
how efficient these operations are in practice.
Beyondcomputationalefficiency,theoverheadintroducedbyPythonandbythedeeplearn-
ing framework itself is considerable. Recall that each time we execute a command the
Python interpreter sends a command to the MXNet engine which needs to insert it into
the computational graph and deal with it during scheduling. Such overhead can be quite
detrimental. In short, it is highly advisable to use vectorization (and matrices) whenever
possible.
%matplotlib inline
import time
import numpy asnp
import torch
from torch import nn
from d2l import torch asd2l
A=torch .zeros( 256,256)
B=torch .randn( 256,256)
C=torch .randn( 256,256)
Since we will benchmark the running time frequently in the rest of the book, let‚Äôs define a
timer.
class Timer :#@save
"""Record multiple running times."""
def __init__ (self ):
self .times =[]
self .start()
(continues on next page)
502 Optimization Algorithms
(continued from previous page)
def start (self ):
"""Start the timer."""
self .tik =time .time()
def stop (self ):
"""Stop the timer and record the time in a list."""
self .times .append(time .time() -self .tik)
return self .times[ -1]
def avg(self ):
"""Return the average time."""
return sum(self .times) /len(self .times)
def sum(self ):
"""Return the sum of time."""
return sum(self .times)
def cumsum (self ):
"""Return the accumulated time."""
return np.array( self .times) .cumsum() .tolist()
timer =Timer()
Element-wiseassignmentsimplyiteratesoverallrowsandcolumnsof BandCrespectively
to assign the value to A.
# Compute A = BC one element at a time
timer .start()
for iinrange (256):
for jinrange (256):
A[i, j] =torch .dot(B[i, :], C[:, j])
timer .stop()
1.7845737934112549
A faster strategy is to perform column-wise assignment.
# Compute A = BC one column at a time
timer .start()
for jinrange (256):
A[:, j] =torch .mv(B, C[:, j])
timer .stop()
0.06541275978088379
Last, the most effective manner is to perform the entire operation in one block. Note that
multiplyinganytwomatrices B2RùëöùëõandC2Rùëõùëùtakesapproximately 2ùëöùëõùëùfloating
pointoperations,whenscalarmultiplicationandadditionarecountedasseparateoperations
503 Minibatch Stochastic Gradient Descent
(fused in practice). Thus, multiplying two 256256matrices takes 0.03billion floating
point operations. Let‚Äôs see what the respective speed of the operations is.
# Compute A = BC in one go
timer .start()
A=torch .mm(B, C)
timer .stop()
gigaflops =[0.03 /ifor iintimer .times]
print (f'performance in Gigaflops: element {gigaflops[ 0]:.3f},'
f'column {gigaflops[ 1]:.3f}, full {gigaflops[ 2]:.3f}')
performance inGigaflops: element 0.017 , column 0.459 , full 51.633
12.5.2Minibatches
In the past we took it for granted that we would read minibatches of data rather than single
observations to update parameters. We now give a brief justification for it. Processing sin-
gle observations requires us to perform many single matrix-vector (or even vector-vector)
multiplications, whichisquiteexpensiveandwhichincursasignificantoverheadonbehalf
oftheunderlyingdeeplearningframework. Thisappliesbothtoevaluatinganetworkwhen
appliedtodata(oftenreferredtoasinference)andwhencomputinggradientstoupdatepa-
rameters. That is, this applies whenever we perform w w ùúÇùë°gùë°where
gùë°=ùúïwùëì¬πxùë°,w¬∫ (12.5.1)
Wecanincreasethe computational efficiencyofthisoperationbyapplyingittoaminibatch
of observations at a time. That is, we replace the gradient gùë°over a single observation by
one over a small batch
gùë°=ùúïw1
jBùë°j√ï
ùëñ2Bùë°ùëì¬πxùëñ,w¬∫ (12.5.2)
Let‚Äôsseewhatthisdoestothestatisticalpropertiesof gùë°: sinceboth xùë°andalsoallelements
oftheminibatchBùë°aredrawnuniformlyatrandomfromthetrainingset,theexpectationof
the gradient remains unchanged. The variance, on the other hand, is reduced significantly.
Since the minibatch gradient is composed of ùëèdef=jBùë°jindependent gradients which are
beingaveraged,itsstandarddeviationisreducedbyafactorof ùëè 1
2. This,byitself,isagood
thing,sinceitmeansthattheupdatesaremorereliablyalignedwiththefullgradient.
Naively this would indicate that choosing a large minibatch Bùë°would be universally desir-
able. Alas,aftersomepoint,theadditionalreductioninstandarddeviationisminimalwhen
compared to the linear increase in computational cost. In practice we pick a minibatch that
islargeenoughtooffergoodcomputationalefficiencywhilestillfittingintothememoryof
a GPU. To illustrate the savings let‚Äôs have a look at some code. In it we perform the same
matrix-matrix multiplication, but this time broken up into ‚Äúminibatches‚Äù of 64 columns at
a time.
504 Optimization Algorithms
172timer .start()
for jinrange (0,256,64):
A[:, j:j +64]=torch .mm(B, C[:, j:j +64])
timer .stop()
print (f'performance in Gigaflops: block {0.03 /timer .times[ 3]:.3f}')
performance inGigaflops: block 37.640
As we can see, the computation on the minibatch is essentially as efficient as on the full
matrix. A word of caution is in order. In Section 8.5 we used a type of regularization that
was heavily dependent on the amount of variance in a minibatch. As we increase the latter,
the variance decreases and with it the benefit of the noise-injection due to batch normal-
ization. See e.g., Ioffe ( 2017) for details on how to rescale and compute the appropriate
terms.
12.5.3Readingthe Dataset
Let‚Äôs have a look at how minibatches are efficiently generated from data. In the following
we use a dataset developed by NASA to test the wing noise from different aircraft172
to compare these optimization algorithms. For convenience we only use the first 1,500
examples. Thedataiswhitenedforpreprocessing,i.e.,weremovethemeanandrescalethe
variance to 1per coordinate.
#@save
d2l.DATA_HUB[ 'airfoil ']=(d2l .DATA_URL +'airfoil_self_noise.dat ',
'76e5be1548fd8222e5074cf0faae75edff8cf93f ')
#@save
def get_data_ch11 (batch_size =10, n=1500 ):
data =np.genfromtxt(d2l .download( 'airfoil '),
dtype =np.float32, delimiter ='\t')
data =torch .from_numpy((data -data .mean(axis =0))/data .std(axis =0))
data_iter =d2l.load_array((data[:n, : -1], data[:n, -1]),
batch_size, is_train =True )
return data_iter, data .shape[ 1]-1
12.5.4Implementation fromScratch
Recall the minibatch stochastic gradient descent implementation from Section 3.4 . In the
following we provide a slightly more general implementation. For convenience it has the
same call signature as the other optimization algorithms introduced later in this chapter.
Specifically, we add the status input states and place the hyperparameter in dictionary
hyperparams . Inaddition, wewillaveragethelossofeachminibatchexampleinthetrain-
ing function, so the gradient in the optimization algorithm does not need to be divided by
the batch size.
505 Minibatch Stochastic Gradient Descent
def sgd(params, states, hyperparams):
for pinparams:
p.data .sub_(hyperparams[ 'lr']*p.grad)
p.grad .data .zero_()
Next,weimplementagenerictrainingfunctiontofacilitatetheuseoftheotheroptimization
algorithms introduced later in this chapter. It initializes a linear regression model and can
be used to train the model with minibatch stochastic gradient descent and other algorithms
introduced subsequently.
#@save
def train_ch11 (trainer_fn, states, hyperparams, data_iter,
feature_dim, num_epochs =2):
# Initialization
w=torch .normal(mean =0.0, std =0.01 , size =(feature_dim, 1),
requires_grad =True )
b=torch .zeros(( 1), requires_grad =True )
net, loss =lambda X: d2l .linreg(X, w, b), d2l .squared_loss
# Train
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[0, num_epochs], ylim =[0.22 ,0.35 ])
n, timer =0, d2l .Timer()
for _inrange (num_epochs):
for X, y indata_iter:
l=loss(net(X), y) .mean()
l.backward()
trainer_fn([w, b], states, hyperparams)
n+=X.shape[ 0]
ifn%200 ==0:
timer .stop()
animator .add(n /X.shape[ 0]/len(data_iter),
(d2l .evaluate_loss(net, data_iter, loss),))
timer .start()
print (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/
‚Ü©!epoch ')
return timer .cumsum(), animator .Y[0]
Let‚Äôs see how optimization proceeds for batch gradient descent. This can be achieved by
setting the minibatch size to 1500 (i.e., to the total number of examples). As a result the
model parameters are updated only once per epoch. There is little progress. In fact, after 6
steps progress stalls.
def train_sgd (lr, batch_size, num_epochs =2):
data_iter, feature_dim =get_data_ch11(batch_size)
return train_ch11(
sgd, None , {'lr': lr}, data_iter, feature_dim, num_epochs)
gd_res =train_sgd( 1,1500 ,10)
loss: 0.247 ,0.020 sec/epoch
506 Optimization Algorithms
When the batch size equals 1, we use stochastic gradient descent for optimization. For
simplicityofimplementationwepickedaconstant(albeitsmall)learningrate. Instochastic
gradient descent, the model parameters are updated whenever an example is processed. In
ourcasethisamountsto1500updatesperepoch. Aswecansee, thedeclineinthevalueof
theobjectivefunctionslowsdownafteroneepoch. Althoughboththeproceduresprocessed
1500 examples within one epoch, stochastic gradient descent consumes more time than
gradient descent in our experiment. This is because stochastic gradient descent updated
the parameters more frequently and since it is less efficient to process single observations
one at a time.
sgd_res =train_sgd( 0.005 ,1)
loss: 0.245 ,0.685 sec/epoch
Finally, when the batch size equals 100, we use minibatch stochastic gradient descent for
optimization. The time required per epoch is shorter than the time needed for stochastic
gradient descent and the time for batch gradient descent.
mini1_res =train_sgd( .4,100)
loss: 0.246 ,0.025 sec/epoch
Reducing the batch size to 10, the time for each epoch increases because the workload for
each batch is less efficient to execute.
507 Minibatch Stochastic Gradient Descent
mini2_res =train_sgd( .05,10)
loss: 0.246 ,0.090 sec/epoch
Now we can compare the time vs. loss for the previous four experiments. As can be seen,
although stochastic gradient descent converges faster than GD in terms of number of ex-
amplesprocessed,itusesmoretimetoreachthesamelossthanGDbecausecomputingthe
gradient example by example is not as efficient. Minibatch stochastic gradient descent is
able to trade-off convergence speed and computation efficiency. A minibatch size of 10 is
more efficient than stochastic gradient descent; a minibatch size of 100 even outperforms
GD in terms of runtime.
d2l.set_figsize([ 6,3])
d2l.plot( *list (map(list ,zip(gd_res, sgd_res, mini1_res, mini2_res))),
'time (sec) ','loss ', xlim =[1e-2 ,10],
legend =['gd','sgd','batch size=100 ','batch size=10 '])
d2l.plt.gca() .set_xscale( 'log')
12.5.5ConciseImplementation
InGluon,wecanusethe Trainer classtocalloptimizationalgorithms. Thisisusedtoim-
plementagenerictrainingfunction. Wewillusethisthroughoutthecurrentchapter.
508 Optimization Algorithms
#@save
def train_concise_ch11 (trainer_fn, hyperparams, data_iter, num_epochs =4):
# Initialization
net =nn.Sequential(nn .Linear( 5,1))
def init_weights (module):
iftype (module) ==nn.Linear:
torch .nn.init .normal_(module .weight, std =0.01 )
net.apply(init_weights)
optimizer =trainer_fn(net .parameters(), **hyperparams)
loss =nn.MSELoss(reduction ='none ')
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[0, num_epochs], ylim =[0.22 ,0.35 ])
n, timer =0, d2l .Timer()
for _inrange (num_epochs):
for X, y indata_iter:
optimizer .zero_grad()
out =net(X)
y=y.reshape(out .shape)
l=loss(out, y)
l.mean() .backward()
optimizer .step()
n+=X.shape[ 0]
ifn%200 ==0:
timer .stop()
# `MSELoss` computes squared error without the 1/2 factor
animator .add(n /X.shape[ 0]/len(data_iter),
(d2l .evaluate_loss(net, data_iter, loss) /2,))
timer .start()
print (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/
‚Ü©!epoch ')
Using Gluon to repeat the last experiment shows identical behavior.
data_iter, _ =get_data_ch11( 10)
trainer =torch .optim .SGD
train_concise_ch11(trainer, { 'lr':0.01 }, data_iter)
509 Minibatch Stochastic Gradient Descent
loss: 0.243 ,0.096 sec/epoch
12.5.6Summary
Vectorization makes code more efficient due to reduced overhead arising from the deep
learning framework and due to better memory locality and caching on CPUs and
GPUs.
Thereisatrade-offbetweenstatisticalefficiencyarisingfromstochasticgradientdescent
and computational efficiency arising from processing large batches of data at a time.
Minibatch stochastic gradient descent offers the best of both worlds: computational and
statistical efficiency.
Inminibatchstochasticgradientdescentweprocessbatchesofdataobtainedbyarandom
permutation of the training data (i.e., each observation is processed only once per
epoch, albeit in random order).
It is advisable to decay the learning rates during training.
Ingeneral,minibatchstochasticgradientdescentisfasterthanstochasticgradientdescent
and gradient descent for convergence to a smaller risk, when measured in terms of
clock time.
12.5.7Exercises
1.Modify the batch size and learning rate and observe the rate of decline for the value of
the objective function and the time consumed in each epoch.
2.Read the MXNet documentation and use the Trainer class set_learning_rate func-
tion to reduce the learning rate of the minibatch stochastic gradient descent to 1/10 of
its previous value after each epoch.
3.Compareminibatchstochasticgradientdescentwithavariantthatactually sampleswith
replacement from the training set. What happens?
4.An evil genie replicates your dataset without telling you (i.e., each observation occurs
twice and your dataset grows to twice its original size, but nobody told you). How does
510 Optimization Algorithms
173the behavior of stochastic gradient descent, minibatch stochastic gradient descent and
that of gradient descent change?
Discussions173.
12.6Momentum
InSection 12.4 we reviewed what happens when performing stochastic gradient descent,
i.e., when performing optimization where only a noisy variant of the gradient is available.
Inparticular,wenoticedthatfornoisygradientsweneedtobeextracautiouswhenitcomes
tochoosingthe learning rate inthe faceof noise. If wedecrease it too rapidly, convergence
stalls. Ifwearetoolenient,wefailtoconvergetoagoodenoughsolutionsincenoisekeeps
on driving us away from optimality.
12.6.1Basics
In this section, we will explore more effective optimization algorithms, especially for cer-
tain types of optimization problems that are common in practice.
Leaky Averages
The previous section saw us discussing minibatch SGD as a means for accelerating com-
putation. It also had the nice side-effect that averaging gradients reduced the amount of
variance. The minibatch stochastic gradient descent can be calculated by:
gùë°,ùë° 1=ùúïw1
jBùë°j√ï
ùëñ2Bùë°ùëì¬πxùëñ,wùë° 1¬∫=1
jBùë°j√ï
ùëñ2Bùë°hùëñ,ùë° 1. (12.6.1)
To keep the notation simple, here we used hùëñ,ùë° 1=ùúïwùëì¬πxùëñ,wùë° 1¬∫as the stochastic gra-
dient descent for sample ùëñusing the weights updated at time ùë° 1. It would be nice if we
could benefit from the effect of variance reduction even beyond averaging gradients on a
minibatch. One option to accomplish this task is to replace the gradient computation by a
‚Äúleaky average‚Äù:
vùë°=ùõΩvùë° 1¬∏gùë°,ùë° 1 (12.6.2)
forsomeùõΩ2¬π0,1¬∫. Thiseffectivelyreplacestheinstantaneousgradientbyonethatisbeen
averaged over multiple pastgradients. vis calledvelocity. It accumulates past gradients
similar to how a heavy ball rolling down the objective function landscape integrates over
pastforces. Toseewhatishappeninginmoredetaillet‚Äôsexpand vùë°recursivelyinto
vùë°=ùõΩ2vùë° 2¬∏ùõΩgùë° 1,ùë° 2¬∏gùë°,ùë° 1=...,=ùë° 1√ï
ùúè=0ùõΩùúègùë° ùúè,ùë° ùúè 1. (12.6.3)
LargeùõΩamounts to a long-range average, whereas small ùõΩamounts to only a slight correc-
tion relative to a gradient method. The new gradient replacement no longer points into the
511 Momentum
174direction of steepest descent on a particular instance any longer but rather in the direction
of a weighted average of past gradients. This allows us to realize most of the benefits of
averaging over a batch without the cost of actually computing the gradients on it. We will
revisit this averaging procedure in more detail later.
Theabovereasoningformedthebasisforwhatisnowknownas accelerated gradientmeth-
ods, such as gradients with momentum. They enjoy the additional benefit of being much
more effective in cases where the optimization problem is ill-conditioned (i.e., where there
are some directions where progress is much slower than in others, resembling a narrow
canyon). Furthermore, they allow us to average over subsequent gradients to obtain more
stable directions of descent. Indeed, the aspect of acceleration even for noise-free convex
problemsisoneofthekeyreasonswhymomentumworksandwhyitworkssowell.
Asonewouldexpect,duetoitsefficacymomentumisawell-studiedsubjectinoptimization
for deep learning and beyond. See e.g., the beautiful expository article174by Goh ( 2017)
for an in-depth analysis and interactive animation. It was proposed by Polyak ( 1964). Nes-
terov (2018) has a detailed theoretical discussion in the context of convex optimization.
Momentum in deep learning has been known to be beneficial for a long time. See e.g., the
discussion by Sutskever etal.(2013) for details.
An Ill-conditioned Problem
To get a better understanding of the geometric properties of the momentum method we
revisit gradient descent, albeit with a significantly less pleasant objective function. Recall
thatinSection12.3 weusedùëì¬πx¬∫=ùë•2
1¬∏2ùë•2
2,i.e.,amoderatelydistortedellipsoidobjective.
We distort this function further by stretching it out in the ùë•1direction via
ùëì¬πx¬∫=0.1ùë•2
1¬∏2ùë•2
2. (12.6.4)
Asbeforeùëìhasitsminimumat ¬π0,0¬∫. Thisfunctionis veryflatinthedirectionof ùë•1. Let‚Äôs
see what happens when we perform gradient descent as before on this new function. We
pick a learning rate of 0.4.
%matplotlib inline
import torch
from d2l import torch asd2l
eta =0.4
def f_2d (x1, x2):
return 0.1 *x1**2+2*x2**2
def gd_2d (x1, x2, s1, s2):
return (x1 -eta *0.2 *x1, x2 -eta *4*x2, 0,0)
d2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))
epoch 20, x1: -0.943467 , x2: -0.000073
By construction, the gradient in the ùë•2direction is muchhigher and changes much more
512 Optimization Algorithms
rapidly than in the horizontal ùë•1direction. Thus we are stuck between two undesirable
choices: if we pick a small learning rate we ensure that the solution does not diverge in
theùë•2direction but we are saddled with slow convergence in the ùë•1direction. Conversely,
with a large learning rate we progress rapidly in the ùë•1direction but diverge in ùë•2. The
example below illustrates what happens even after a slight increase in learning rate from
0.4to0.6. Convergenceinthe ùë•1directionimprovesbuttheoverallsolutionqualityismuch
worse.
eta =0.6
d2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))
epoch 20, x1: -0.387814 , x2: -1673.365109
The Momentum Method
The momentum method allows us to solve the gradient descent problem described above.
Looking at the optimization trace above we might intuit that averaging gradients over the
past would work well. After all, in the ùë•1direction this will aggregate well-aligned gradi-
ents, thus increasing the distance we cover with every step. Conversely, in the ùë•2direction
where gradients oscillate, an aggregate gradient will reduce step size due to oscillations
that cancel each other out. Using vùë°instead of the gradient gùë°yields the following update
equations:
vùë° ùõΩvùë° 1¬∏gùë°,ùë° 1,
xùë° xùë° 1 ùúÇùë°vùë°.(12.6.5)
513 Momentum
Note that for ùõΩ=0we recover regular gradient descent. Before delving deeper into the
mathematical properties let‚Äôs have a quick look at how the algorithm behaves in prac-
tice.
def momentum_2d (x1, x2, v1, v2):
v1=beta *v1+0.2 *x1
v2=beta *v2+4*x2
return x1-eta *v1, x2 -eta *v2, v1, v2
eta, beta =0.6,0.5
d2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))
epoch 20, x1: 0.007188 , x2: 0.002553
As we can see, even with the same learning rate that we used before, momentum still con-
verges well. Let‚Äôs see what happens when we decrease the momentum parameter. Halving
it toùõΩ=0.25leads to a trajectory that barely converges at all. Nonetheless, it is a lot better
than without momentum (when the solution diverges).
eta, beta =0.6,0.25
d2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))
epoch 20, x1: -0.126340 , x2: -0.186632
Note that we can combine momentum with stochastic gradient descent and in particular,
minibatch stochastic gradient descent. The only change is that in that case we replace the
514 Optimization Algorithms
gradients gùë°,ùë° 1withgùë°. Last, for convenience we initialize v0=0at timeùë°=0. Let‚Äôs
look at what leaky averaging actually does to the updates.
EffectiveSampleWeight
Recall that vùë°=√çùë° 1
ùúè=0ùõΩùúègùë° ùúè,ùë° ùúè 1. In the limit the terms add up to√ç1
ùúè=0ùõΩùúè=1
1 ùõΩ. In
other words, rather than taking a step of size ùúÇin gradient descent or stochastic gradient
descent we take a step of sizeùúÇ
1 ùõΩwhile at the same time, dealing with a potentially much
betterbehaveddescentdirection. Thesearetwobenefitsinone. Toillustratehowweighting
behaves for different choices of ùõΩconsider the diagram below.
d2l.set_figsize()
betas =[0.95 ,0.9,0.6,0]
for beta inbetas:
x=torch .arange( 40).detach() .numpy()
d2l.plt.plot(x, beta **x, label =f'beta = {beta :.2f}')
d2l.plt.xlabel( 'time ')
d2l.plt.legend();
12.6.2Practical Experiments
Let‚Äôs see how momentum works in practice, i.e., when used within the context of a proper
optimizer. For this we need a somewhat more scalable implementation.
Implementation fromScratch
Compared with (minibatch) stochastic gradient descent the momentum method needs to
maintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients
(and variables of the optimization problem). In the implementation below we call these
variables states.
def init_momentum_states (feature_dim):
v_w =torch .zeros((feature_dim, 1))
v_b =torch .zeros( 1)
return (v_w, v_b)
515 Momentum
def sgd_momentum (params, states, hyperparams):
for p, v inzip(params, states):
with torch .no_grad():
v[:] =hyperparams[ 'momentum ']*v+p.grad
p[:] -=hyperparams[ 'lr']*v
p.grad .data .zero_()
Let‚Äôs see how this works in practice.
def train_momentum (lr, momentum, num_epochs =2):
d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),
{'lr': lr, 'momentum ': momentum}, data_iter,
feature_dim, num_epochs)
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
train_momentum( 0.02 ,0.5)
loss: 0.245 ,0.153 sec/epoch
When we increase the momentum hyperparameter momentum to 0.9, it amounts to a signif-
icantly larger effective sample size of1
1 0.9=10. We reduce the learning rate slightly to
0.01to keep matters under control.
train_momentum( 0.01 ,0.9)
loss: 0.248 ,0.109 sec/epoch
Reducing the learning rate further addresses any issue of non-smooth optimization prob-
lems. Setting it to 0.005yields good convergence properties.
train_momentum( 0.005 ,0.9)
loss: 0.243 ,0.107 sec/epoch
516 Optimization Algorithms
ConciseImplementation
There is very little to do in Gluon since the standard sgdsolver already had momentum
built in. Setting matching parameters yields a very similar trajectory.
trainer =torch .optim .SGD
d2l.train_concise_ch11(trainer, { 'lr':0.005 ,'momentum ':0.9}, data_iter)
loss: 0.250 ,0.108 sec/epoch
12.6.3TheoreticalAnalysis
Sofarthe2Dexampleof ùëì¬πùë•¬∫=0.1ùë•2
1¬∏2ùë•2
2seemedrathercontrived. Wewillnowseethat
this is actually quite representative of the types of problem one might encounter, at least in
the case of minimizing convex quadratic objective functions.
517 Momentum
Quadratic ConvexFunctions
Consider the function
‚Ñé¬πx¬∫=1
2x>Qx¬∏x>c¬∏ùëè. (12.6.6)
This is a general quadratic function. For positive definite matrices Q0, i.e., for matrices
with positive eigenvalues this has a minimizer at x= Q 1cwith minimum value ùëè 
1
2c>Q 1c. Hence we can rewrite ‚Ñéas
‚Ñé¬πx¬∫=1
2¬πx Q 1c¬∫>Q¬πx Q 1c¬∫¬∏ùëè 1
2c>Q 1c. (12.6.7)
The gradient is given by ùúïx‚Ñé¬πx¬∫=Q¬πx Q 1c¬∫. That is, it is given by the distance
between xand the minimizer, multiplied by Q. Consequently also the velocity is a linear
combination of terms Q¬πxùë° Q 1c¬∫.
SinceQis positive definite it can be decomposed into its eigensystem via Q=O>ùö≤Ofor
an orthogonal (rotation) matrix Oand a diagonal matrix ùö≤of positive eigenvalues. This
allows us to perform a change of variables from xtozdef=O¬πx Q 1c¬∫to obtain a much
simplified expression:
‚Ñé¬πz¬∫=1
2z>ùö≤z¬∏ùëè0. (12.6.8)
Hereùëè0=ùëè 1
2c>Q 1c. Since Ois only an orthogonal matrix this does not perturb the
gradients in a meaningful way. Expressed in terms of zgradient descent becomes
zùë°=zùë° 1 ùö≤zùë° 1=¬πI ùö≤¬∫zùë° 1. (12.6.9)
Theimportantfactinthisexpressionisthatgradientdescent doesnotmix betweendifferent
eigenspaces. That is, when expressed in terms of the eigensystem of Qthe optimization
problem proceeds in a coordinate-wise manner. This also holds for
vùë°=ùõΩvùë° 1¬∏ùö≤zùë° 1
zùë°=zùë° 1 ùúÇ¬πùõΩvùë° 1¬∏ùö≤zùë° 1¬∫
=¬πI ùúÇùö≤¬∫zùë° 1 ùúÇùõΩvùë° 1.(12.6.10)
In doing this we just proved the following theorem: gradient descent with and without
momentum for a convex quadratic function decomposes into coordinate-wise optimization
in the direction of the eigenvectors of the quadratic matrix.
Scalar Functions
Given the above result let‚Äôs see what happens when we minimize the function ùëì¬πùë•¬∫=ùúÜ
2ùë•2.
For gradient descent we have
ùë•ùë°¬∏1=ùë•ùë° ùúÇùúÜùë•ùë°=¬π1 ùúÇùúÜ¬∫ùë•ùë°. (12.6.11)
Wheneverj1 ùúÇùúÜj<1thisoptimizationconvergesatanexponentialratesinceafter ùë°steps
we haveùë•ùë°=¬π1 ùúÇùúÜ¬∫ùë°ùë•0. This shows how the rate of convergence improves initially as
518 Optimization Algorithms
175we increase the learning rate ùúÇuntilùúÇùúÜ=1. Beyond that things diverge and for ùúÇùúÜ> 2the
optimization problem diverges.
lambdas =[0.1,1,10,19]
eta =0.1
d2l.set_figsize(( 6,4))
for lam inlambdas:
t=torch .arange( 20).detach() .numpy()
d2l.plt.plot(t, ( 1-eta *lam) **t, label =f'lambda = {lam:.2f}')
d2l.plt.xlabel( 'time ')
d2l.plt.legend();
To analyze convergence in the case of momentum we begin by rewriting the update equa-
tions in terms of two scalars: one for ùë•and one for velocity ùë£. This yields:
ùë£ùë°¬∏1
ùë•ùë°¬∏1
=ùõΩùúÜ
 ùúÇùõΩ¬π1 ùúÇùúÜ¬∫ ùë£ùë°
ùë•ùë°
=R¬πùõΩ,ùúÇ,ùúÜ¬∫ùë£ùë°
ùë•ùë°
. (12.6.12)
We used Rto denote the 22governing convergence behavior. After ùë°steps the initial
choice¬ªùë£0,ùë•0¬ºbecomes R¬πùõΩ,ùúÇ,ùúÜ¬∫ùë°¬ªùë£0,ùë•0¬º. Hence, it is up to the eigenvalues of Rto
determine the speed of convergence. See the Distill post175of Goh ( 2017) for a great
animation and Flammarion and Bach ( 2015) for a detailed analysis. One can show that
0< ùúÇùúÜ < 2¬∏2ùõΩvelocity converges. This is a larger range of feasible parameters when
compared to 0< ùúÇùúÜ < 2for gradient descent. It also suggests that in general large values
ofùõΩare desirable. Further details require a fair amount of technical detail and we suggest
that the interested reader consult the original publications.
12.6.4Summary
Momentumreplacesgradientswithaleakyaverageoverpastgradients. Thisaccelerates
convergence significantly.
Itisdesirableforbothnoise-freegradientdescentand(noisy)stochasticgradientdescent.
Momentum prevents stalling of the optimization process that is much more likely to
occur for stochastic gradient descent.
519 Adagrad
176The effective number of gradients is given by1
1 ùõΩdue to exponentiated downweighting
of past data.
In the case of convex quadratic problems this can be analyzed explicitly in detail.
Implementation is quite straightforward but it requires us to store an additional state
vector (velocity v).
12.6.5Exercises
1.Use other combinations of momentum hyperparameters and learning rates and observe
and analyze the different experimental results.
2.Tryoutgradientdescentandmomentumforaquadraticproblemwhereyouhavemulti-
pleeigenvalues, i.e., ùëì¬πùë•¬∫=1
2√ç
ùëñùúÜùëñùë•2
ùëñ, e.g.,ùúÜùëñ=2 ùëñ. Plot howthevaluesof ùë•decrease
for the initialization ùë•ùëñ=1.
3.Derive minimum value and minimizer for ‚Ñé¬πx¬∫=1
2x>Qx¬∏x>c¬∏ùëè.
4.What changes when we perform stochastic gradient descent with momentum? What
happens when we use minibatch stochastic gradient descent with momentum? Experi-
ment with the parameters?
Discussions176.
12.7Adagrad
Let‚Äôs begin by considering learning problems with features that occur infrequently.
12.7.1SparseFeatures and Learning Rates
Imagine that we are training a language model. To get good accuracy we typically want
to decrease the learning rate as we keep on training, usually at a rate of O¬πùë° 1
2¬∫or slower.
Nowconsideramodeltrainingonsparsefeatures,i.e.,featuresthatoccuronlyinfrequently.
This is common for natural language, e.g., it is a lot less likely that we will see the word
preconditioning thanlearning . However,itisalsocommoninotherareassuchascomputa-
tional advertising and personalized collaborative filtering. After all, there are many things
that are of interest only for a small number of people.
Parameters associated with infrequent features only receive meaningful updates whenever
these features occur. Given a decreasing learning rate we might end up in a situation
where the parameters for common features converge rather quickly to their optimal values,
whereas for infrequent features we are still short of observing them sufficiently frequently
before their optimal values can be determined. In other words, the learning rate either
decreases too slowly for frequent features or too quickly for infrequent ones.
520 Optimization Algorithms
A possible hack to redress this issue would be to count the number of times we see a par-
ticular feature and to use this as a clock for adjusting learning rates. That is, rather than
choosing a learning rate of the form ùúÇ=ùúÇ0pùë°¬∏ùëêwe could use ùúÇùëñ=ùúÇ0p
ùë†¬πùëñ,ùë°¬∫¬∏ùëê. Hereùë†¬πùëñ,ùë°¬∫
counts the number of nonzeros for feature ùëñthat we have observed up to time ùë°. This is ac-
tually quite easy to implement at no meaningful overhead. However, it fails whenever we
do not quite have sparsity but rather just data where the gradients are often very small and
only rarely large. After all, it is unclear where one would draw the line between something
that qualifies as an observed feature or not.
Adagrad by Duchi et al.(2011) addresses this by replacing the rather crude counter ùë†¬πùëñ,ùë°¬∫
by an aggregate of the squares of previously observed gradients. In particular, it uses
ùë†¬πùëñ,ùë°¬∏1¬∫=ùë†¬πùëñ,ùë°¬∫¬∏¬πùúïùëñùëì¬πx¬∫¬∫2asameanstoadjustthelearningrate. Thishastwobenefits:
first, we no longer need to decide just when a gradient is large enough. Second, it scales
automatically with the magnitude of the gradients. Coordinates that routinely correspond
to large gradients are scaled down significantly, whereas others with small gradients re-
ceive a much more gentle treatment. In practice this leads to a very effective optimization
procedure for computational advertising and related problems. But this hides some of the
additional benefits inherent in Adagrad that are best understood in the context of precondi-
tioning.
12.7.2Preconditioning
Convex optimization problems are good for analyzing the characteristics of algorithms.
After all, for most nonconvex problems it is difficult to derive meaningful theoretical guar-
antees, but intuition andinsightoften carry over. Let‚Äôs look at the problem of minimizing
ùëì¬πx¬∫=1
2x>Qx¬∏c>x¬∏ùëè.
Aswesawin Section12.6 ,itispossibletorewritethisproblemintermsofitseigendecom-
position Q=U>ùö≤Uto arrive at a much simplified problem where each coordinate can be
solved individually:
ùëì¬πx¬∫=¬Øùëì¬π¬Øx¬∫=1
2¬Øx>ùö≤¬Øx¬∏¬Øc>¬Øx¬∏ùëè. (12.7.1)
Here we used ¬Øx=Uxand consequently ¬Øc=Uc. The modified problem has as its min-
imizer ¬Øx= ùö≤ 1¬Øcand minimum value  1
2¬Øc>ùö≤ 1¬Øc¬∏ùëè. This is much easier to compute
sinceùö≤is a diagonal matrix containing the eigenvalues of Q.
If we perturb cslightly we would hope to find only slight changes in the minimizer of ùëì.
Unfortunately this is not the case. While slight changes in clead to equally slight changes
in¬Øc, this is not the case for the minimizer of ùëì(and of ¬Øùëìrespectively). Whenever the
eigenvalues ùö≤ùëñare large we will see only small changes in ¬Øùë•ùëñand in the minimum of ¬Øùëì.
Conversely, for small ùö≤ùëñchanges in ¬Øùë•ùëñcan be dramatic. The ratio between the largest and
the smallest eigenvalue is called the condition number of an optimization problem.
ùúÖ=ùö≤1
ùö≤ùëë. (12.7.2)
Iftheconditionnumber ùúÖislarge,itisdifficulttosolvetheoptimizationproblemaccurately.
We need to ensure that we are careful in getting a large dynamic range of values right. Our
521 Adagrad
analysis leads to an obvious, albeit somewhat naive question: couldn‚Äôt we simply ‚Äúfix‚Äù the
problem by distorting the space such that all eigenvalues are 1. In theory this is quite easy:
we only need the eigenvalues and eigenvectors of Qto rescale the problem from xto one
inzdef=ùö≤1
2Ux. Inthenewcoordinatesystem x>Qxcouldbesimplifiedto kzk2. Alas,this
is a rather impractical suggestion. Computing eigenvalues and eigenvectors is in general
muchmore expensive than solving the actual problem.
While computing eigenvalues exactly might be expensive, guessing them and computing
them even somewhat approximately may already be a lot better than not doing anything at
all. In particular, we could use the diagonal entries of Qand rescale it accordingly. This is
muchcheaper than computing eigenvalues.
ÀúQ=diag 1
2¬πQ¬∫Qdiag 1
2¬πQ¬∫. (12.7.3)
In this case we have ÀúQùëñùëó=Qùëñùëó¬ùp
QùëñùëñQùëóùëóand specifically ÀúQùëñùëñ=1for allùëñ. In most cases
this simplifies the condition number considerably. For instance, the cases we discussed
previously, this would entirely eliminate the problem at hand since the problem is axis
aligned.
Unfortunately we face yet another problem: in deep learning we typically do not even have
access to the second derivative of the objective function: for x2Rùëëthe second derivative
even on a minibatch may require O¬πùëë2¬∫space and work to compute, thus making it practi-
cally infeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal
of the Hessian that is both relatively cheap to compute and effective‚Äîthe magnitude of the
gradient itself.
In order to see why this works, let‚Äôs look at ¬Øùëì¬π¬Øx¬∫. We have that
ùúï¬Øx¬Øùëì¬π¬Øx¬∫=ùö≤¬Øx¬∏¬Øc=ùö≤¬π¬Øx ¬Øx0¬∫, (12.7.4)
where ¬Øx0is the minimizer of ¬Øùëì. Hence the magnitude of the gradient depends both on ùö≤
and the distance from optimality. If ¬Øx ¬Øx0did not change, this would be all that is needed.
After all, in this case the magnitude of the gradient ùúï¬Øx¬Øùëì¬π¬Øx¬∫suffices. Since AdaGrad is a
stochastic gradient descent algorithm, we will see gradients with nonzero variance even at
optimality. As a result we can safely use the variance of the gradients as a cheap proxy for
the scale of the Hessian. A thorough analysis is beyond the scope of this section (it would
be several pages). We refer the reader to ( Duchietal., 2011) for details.
12.7.3The Algorithm
Let‚Äôs formalize the discussion from above. We use the variable sùë°to accumulate past gra-
dient variance as follows.
gùë°=ùúïwùëô¬πùë¶ùë°, ùëì¬πxùë°,w¬∫¬∫,
sùë°=sùë° 1¬∏g2
ùë°,
wùë°=wùë° 1 ùúÇpsùë°¬∏ùúñgùë°.(12.7.5)
522 Optimization Algorithms
Heretheoperationareappliedcoordinatewise. Thatis, v2hasentriesùë£2
ùëñ. Likewise1pùë£has
entries1pùë£ùëñanduvhas entriesùë¢ùëñùë£ùëñ. As beforeùúÇis the learning rate and ùúñis an additive
constant that ensures that we do not divide by 0. Last, we initialize s0=0.
Just like in the case of momentum we need to keep track of an auxiliary variable, in this
case to allow for an individual learning rate per coordinate. This does not increase the cost
ofAdagradsignificantlyrelativetoSGD,simplysincethemaincostistypicallytocompute
ùëô¬πùë¶ùë°, ùëì¬πxùë°,w¬∫¬∫and its derivative.
Notethataccumulatingsquaredgradientsin sùë°meansthat sùë°growsessentiallyatlinearrate
(somewhat slower than linearly in practice, since the gradients initially diminish). This
leads to anO¬πùë° 1
2¬∫learning rate, albeit adjusted on a per coordinate basis. For convex
problems this is perfectly adequate. In deep learning, though, we might want to decrease
the learning rate rather more slowly. This led to a number of Adagrad variants that we will
discuss in the subsequent chapters. For now let‚Äôs see how it behaves in a quadratic convex
problem. We use the same problem as before:
ùëì¬πx¬∫=0.1ùë•2
1¬∏2ùë•2
2. (12.7.6)
We are going to implement Adagrad using the same learning rate previously, i.e., ùúÇ=0.4.
As we can see, the iterative trajectory of the independent variable is smoother. However,
duetothecumulativeeffectof ùíîùë°,thelearningratecontinuouslydecays,sotheindependent
variable does not move as much during later stages of iteration.
%matplotlib inline
import math
import torch
from d2l import torch asd2l
def adagrad_2d (x1, x2, s1, s2):
eps =1e-6
g1, g2 =0.2 *x1, 4*x2
s1+=g1**2
s2+=g2**2
x1-=eta /math .sqrt(s1 +eps) *g1
x2-=eta /math .sqrt(s2 +eps) *g2
return x1, x2, s1, s2
def f_2d (x1, x2):
return 0.1 *x1**2+2*x2**2
eta =0.4
d2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))
epoch 20, x1: -2.382563 , x2: -0.158591
As we increase the learning rate to 2we see much better behavior. This already indicates
thatthedecreaseinlearningratemightberatheraggressive,eveninthenoise-freecaseand
we need to ensure that parameters converge appropriately.
523 Adagrad
eta =2
d2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))
epoch 20, x1: -0.002295 , x2: -0.000000
12.7.4Implementation fromScratch
Just like the momentum method, Adagrad needs to maintain a state variable of the same
shape as the parameters.
def init_adagrad_states (feature_dim):
s_w =torch .zeros((feature_dim, 1))
s_b =torch .zeros( 1)
return (s_w, s_b)
def adagrad (params, states, hyperparams):
eps =1e-6
for p, s inzip(params, states):
with torch .no_grad():
s[:] +=torch .square(p .grad)
p[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)
p.grad .data .zero_()
Compared to the experiment in Section 12.5 we use a larger learning rate to train the
model.
524 Optimization Algorithms
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
d2l.train_ch11(adagrad, init_adagrad_states(feature_dim),
{'lr':0.1}, data_iter, feature_dim);
loss: 0.243 ,0.162 sec/epoch
12.7.5ConciseImplementation
Using the Trainer instance of the algorithm adagrad , we can invoke the Adagrad algo-
rithm in Gluon.
trainer =torch .optim .Adagrad
d2l.train_concise_ch11(trainer, { 'lr':0.1}, data_iter)
loss: 0.242 ,0.129 sec/epoch
12.7.6Summary
Adagrad decreases the learning rate dynamically on a per-coordinate basis.
It uses the magnitude of the gradient as a means of adjusting how quickly progress is
achieved - coordinates with large gradients are compensated with a smaller learning
rate.
525 RMSProp
177
178Computing the exact second derivative is typically infeasible in deep learning problems
due to memory and computational constraints. The gradient can be a useful proxy.
If the optimization problem has a rather uneven structure Adagrad can help mitigate the
distortion.
Adagrad is particularly effective for sparse features where the learning rate needs to de-
crease more slowly for infrequently occurring terms.
OndeeplearningproblemsAdagradcansometimesbetooaggressiveinreducinglearn-
ingrates. Wewilldiscussstrategiesformitigatingthisinthecontextof Section12.10 .
12.7.7Exercises
1.Prove that for an orthogonal matrix Uand a vector cthe following holds: kc Ô¨Ék2=
kUc UÔ¨Ék2. Whydoesthismeanthatthemagnitudeofperturbationsdoesnotchange
after an orthogonal change of variables?
2.Try out Adagrad for ùëì¬πx¬∫=0.1ùë•2
1¬∏2ùë•2
2and also for the objective function was rotated
by 45 degrees, i.e., ùëì¬πx¬∫=0.1¬πùë•1¬∏ùë•2¬∫2¬∏2¬πùë•1 ùë•2¬∫2. Does it behave differently?
3.ProveGerschgorin‚Äôs circle theorem177which states that eigenvalues ùúÜùëñof a matrix M
satisfyjùúÜùëñ Mùëóùëój√ç
ùëò‚â†ùëójMùëóùëòjfor at least one choice of ùëó.
4.What does Gerschgorin‚Äôs theorem tell us about the eigenvalues of the diagonally pre-
conditioned matrix diag 1
2¬πM¬∫Mdiag 1
2¬πM¬∫?
5.TryoutAdagradforaproperdeepnetwork,suchas Section7.6 whenappliedtoFashion-
MNIST.
6.How would you need to modify Adagrad to achieve a less aggressive decay in learning
rate?
Discussions178.
12.8RMSProp
One of the key issues in Section 12.7 is that the learning rate decreases at a predefined
schedule of effectively O¬πùë° 1
2¬∫. While this is generally appropriate for convex problems,
it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet,
the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.
Tieleman and Hinton ( 2012) proposed the RMSProp algorithm as a simple fix to decouple
rate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accu-
mulates the squares of the gradient gùë°into a state vector sùë°=sùë° 1¬∏g2
ùë°. As a result sùë°
keeps on growing without bound due to the lack of normalization, essentially linearly as
the algorithm converges.
526 Optimization Algorithms
One way of fixing this problem would be to use sùë°¬ùùë°. For reasonable distributions of gùë°
this will converge. Unfortunately it might take a very long time until the limit behavior
starts to matter since the procedure remembers the full trajectory of values. An alternative
is to use a leaky average in the same way we used in the momentum method, i.e., sùë° 
ùõæsùë° 1¬∏¬π1 ùõæ¬∫g2
ùë°for some parameter ùõæ > 0. Keeping all other parts unchanged yields
RMSProp.
12.8.1The Algorithm
Let‚Äôs write out the equations in detail.
sùë° ùõæsùë° 1¬∏¬π1 ùõæ¬∫g2
ùë°,
xùë° xùë° 1 ùúÇpsùë°¬∏ùúñgùë°.(12.8.1)
The constant ùúñ >0is typically set to 10 6to ensure that we do not suffer from division by
zerooroverlylargestepsizes. Giventhisexpansionwearenowfreetocontrolthelearning
rateùúÇindependently of the scaling that is applied on a per-coordinate basis. In terms of
leaky averages we can apply the same reasoning as previously applied in the case of the
momentum method. Expanding the definition of sùë°yields
sùë°=¬π1 ùõæ¬∫g2
ùë°¬∏ùõæsùë° 1
=¬π1 ùõæ¬∫
g2
ùë°¬∏ùõæg2
ùë° 1¬∏ùõæ2gùë° 2¬∏...,
.(12.8.2)
As before in Section 12.6 we use 1¬∏ùõæ¬∏ùõæ2¬∏...,=1
1 ùõæ. Hence the sum of weights is
normalized to 1with a half-life time of an observation of ùõæ 1. Let‚Äôs visualize the weights
for the past 40 time steps for various choices of ùõæ.
import math
import torch
from d2l import torch asd2l
d2l.set_figsize()
gammas =[0.95 ,0.9,0.8,0.7]
for gamma ingammas:
x=torch .arange( 40).detach() .numpy()
d2l.plt.plot(x, ( 1-gamma) *gamma **x, label =f'gamma = {gamma :.2f}')
d2l.plt.xlabel( 'time ');
12.8.2Implementation fromScratch
As before we use the quadratic function ùëì¬πx¬∫=0.1ùë•2
1¬∏2ùë•2
2to observe the trajectory of
RMSProp. Recall that in Section 12.7 , when we used Adagrad with a learning rate of
0.4, the variables moved only very slowly in the later stages of the algorithm since the
learning rate decreased too quickly. Since ùúÇis controlled separately this does not happen
with RMSProp.
527 RMSProp
def rmsprop_2d (x1, x2, s1, s2):
g1, g2, eps =0.2 *x1, 4*x2, 1e-6
s1=gamma *s1+(1-gamma) *g1**2
s2=gamma *s2+(1-gamma) *g2**2
x1-=eta /math .sqrt(s1 +eps) *g1
x2-=eta /math .sqrt(s2 +eps) *g2
return x1, x2, s1, s2
def f_2d (x1, x2):
return 0.1 *x1**2+2*x2**2
eta, gamma =0.4,0.9
d2l.show_trace_2d(f_2d, d2l .train_2d(rmsprop_2d))
epoch 20, x1: -0.010599 , x2: 0.000000
Next, we implement RMSProp to be used in a deep network. This is equally straightfor-
ward.
def init_rmsprop_states (feature_dim):
s_w =torch .zeros((feature_dim, 1))
s_b =torch .zeros( 1)
return (s_w, s_b)
def rmsprop (params, states, hyperparams):
gamma, eps =hyperparams[ 'gamma '],1e-6
(continues on next page)
528 Optimization Algorithms
(continued from previous page)
for p, s inzip(params, states):
with torch .no_grad():
s[:] =gamma *s+(1-gamma) *torch .square(p .grad)
p[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)
p.grad .data .zero_()
Wesettheinitiallearningrateto0.01andtheweightingterm ùõæto0.9. Thatis, saggregates
on average over the past 1¬ù¬π1 ùõæ¬∫=10observations of the square gradient.
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
d2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),
{'lr':0.01 ,'gamma ':0.9}, data_iter, feature_dim);
loss: 0.245 ,0.245 sec/epoch
12.8.3ConciseImplementation
Since RMSProp is a rather popular algorithm it is also available in the Trainer instance.
All we need to do is instantiate it using an algorithm named rmsprop , assigningùõæto the
parameter gamma1.
trainer =torch .optim .RMSprop
d2l.train_concise_ch11(trainer, { 'lr':0.01 ,'alpha ':0.9},
data_iter)
loss: 0.246 ,0.129 sec/epoch
12.8.4Summary
RMSProp is very similar to Adagrad insofar as both use the square of the gradient to
scale coefficients.
RMSProp shares with momentum the leaky averaging. However, RMSProp uses the
technique to adjust the coefficient-wise preconditioner.
529 Adadelta
179The learning rate needs to be scheduled by the experimenter in practice.
The coefficient ùõædetermines how long the history is when adjusting the per-coordinate
scale.
12.8.5Exercises
1.What happens experimentally if we set ùõæ=1? Why?
2.Rotate the optimization problem to minimize ùëì¬πx¬∫=0.1¬πùë•1¬∏ùë•2¬∫2¬∏2¬πùë•1 ùë•2¬∫2. What
happens to the convergence?
3.TryoutwhathappenstoRMSProponarealmachinelearningproblem,suchastraining
on Fashion-MNIST. Experiment with different choices for adjusting the learning rate.
4.Would you want to adjust ùõæas optimization progresses? How sensitive is RMSProp to
this?
Discussions179.
12.9Adadelta
Adadelta is yet another variant of AdaGrad ( Section 12.7 ). The main difference lies in
the fact that it decreases the amount by which the learning rate is adaptive to coordinates.
Moreover,traditionallyitreferredtoasnothavingalearningratesinceitusestheamountof
changeitselfascalibrationforfuturechange. ThealgorithmwasproposedinZeiler( 2012).
It is fairly straightforward, given the discussion of previous algorithms so far.
12.9.1The Algorithm
In a nutshell, Adadelta uses two state variables, sùë°to store a leaky average of the second
momentofthegradientand Œîxùë°tostorealeakyaverageofthesecondmomentofthechange
of parameters in the model itself. Note that we use the original notation and naming of the
authors for compatibility with other publications and implementations (there is no other
530 Optimization Algorithms
real reason why one should use different Greek variables to indicate a parameter serving
the same purpose in momentum, Adagrad, RMSProp, and Adadelta).
Here are the technical details of Adadelta. Given the parameter du jour is ùúå, we obtain the
following leaky updates similarly to Section 12.8 :
sùë°=ùúåsùë° 1¬∏¬π1 ùúå¬∫g2
ùë°. (12.9.1)
The difference to Section 12.8 is that we perform updates with the rescaled gradient g0
ùë°,
i.e.,
xùë°=xùë° 1 g0
ùë°. (12.9.2)
So what is the rescaled gradient g0
ùë°? We can calculate it as follows:
g0
ùë°=pŒîxùë° 1¬∏ùúñpsùë°¬∏ùúñgùë°, (12.9.3)
where Œîxùë° 1is the leaky average of the squared rescaled gradients g0
ùë°. We initialize Œîx0
to be 0and update it at each step with g0
ùë°, i.e.,
Œîxùë°=ùúåŒîxùë° 1¬∏¬π1 ùúå¬∫g0
ùë°2, (12.9.4)
andùúñ(a small value such as 10 5) is added to maintain numerical stability.
12.9.2Implementation
Adadelta needs to maintain two state variables for each variable, sùë°andŒîxùë°. This yields
the following implementation.
%matplotlib inline
import torch
from d2l import torch asd2l
def init_adadelta_states (feature_dim):
s_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)
delta_w, delta_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)
return ((s_w, delta_w), (s_b, delta_b))
def adadelta (params, states, hyperparams):
rho, eps =hyperparams[ 'rho'],1e-5
for p, (s, delta) inzip(params, states):
with torch .no_grad():
# In-place updates via [:]
s[:] =rho *s+(1-rho) *torch .square(p .grad)
g=(torch .sqrt(delta +eps) /torch .sqrt(s +eps)) *p.grad
p[:] -=g
delta[:] =rho *delta +(1-rho) *g*g
p.grad .data .zero_()
Choosingùúå=0.9amounts to a half-life time of 10 for each parameter update. This tends
to work quite well. We get the following behavior.
531 Adadelta
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
d2l.train_ch11(adadelta, init_adadelta_states(feature_dim),
{'rho':0.9}, data_iter, feature_dim);
loss: 0.245 ,0.160 sec/epoch
For a concise implementation we simply use the Adadelta algorithm from high-level APIs.
This yields the following one-liner for a much more compact invocation.
trainer =torch .optim .Adadelta
d2l.train_concise_ch11(trainer, { 'rho':0.9}, data_iter)
loss: 0.243 ,0.119 sec/epoch
12.9.3Summary
Adadeltahasnolearningrateparameter. Instead, itusestherateofchangeintheparam-
eters itself to adapt the learning rate.
Adadelta requires two state variables to store the second moments of gradient and the
change in parameters.
Adadelta uses leaky averages to keep a running estimate of the appropriate statistics.
532 Optimization Algorithms
18012.9.4Exercises
1.Adjust the value of ùúå. What happens?
2.Show how to implement the algorithm without the use of g0
ùë°. Why might this be a good
idea?
3.Is Adadelta really learning rate free? Could you find optimization problems that break
Adadelta?
4.Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.
Discussions180.
12.10Adam
In the discussions leading up to this section we encountered a number of techniques for
efficient optimization. Let‚Äôs recap them in detail here:
We saw that Section 12.4 is more effective than Gradient Descent when solving opti-
mization problems, e.g., due to its inherent resilience to redundant data.
We saw that Section 12.5 affords significant additional efficiency arising from vector-
ization, using larger sets of observations in one minibatch. This is the key to efficient
multi-machine, multi-GPU and overall parallel processing.
Section12.6 addedamechanismforaggregatingahistoryofpastgradientstoaccelerate
convergence.
Section12.7 usedper-coordinatescalingtoallowforacomputationallyefficientprecon-
ditioner.
Section 12.8 decoupled per-coordinate scaling from a learning rate adjustment.
Adam (Kingma and Ba, 2014 ) combines all these techniques into one efficient learning
algorithm. As expected, this is an algorithm that has become rather popular as one of the
more robust and effective optimization algorithms to use in deep learning. It is not without
issues,though. Inparticular,( Reddietal.,2019)showthattherearesituationswhereAdam
candivergeduetopoorvariancecontrol. Inafollow-upworkZaheer etal.(2018)proposed
a hotfix to Adam, called Yogi which addresses these issues. More on this later. For now
let‚Äôs review the Adam algorithm.
12.10.1TheAlgorithm
One of the key components of Adam is that it uses exponential weighted moving averages
(also known as leaky averaging) to obtain an estimate of both the momentum and also the
533 Adam
second moment of the gradient. That is, it uses the state variables
vùë° ùõΩ1vùë° 1¬∏¬π1 ùõΩ1¬∫gùë°,
sùë° ùõΩ2sùë° 1¬∏¬π1 ùõΩ2¬∫g2
ùë°.(12.10.1)
HereùõΩ1andùõΩ2are nonnegative weighting parameters. Common choices for them are
ùõΩ1=0.9andùõΩ2=0.999. That is, the variance estimate moves much more slowly than the
momentumterm. Notethatifweinitialize v0=s0=0wehaveasignificantamountofbias
initiallytowardssmallervalues. Thiscanbeaddressedbyusingthefactthat√çùë° 1
ùëñ=0ùõΩùëñ=1 ùõΩùë°
1 ùõΩ
tore-normalizeterms. Correspondinglythenormalizedstatevariablesaregivenby
ÀÜvùë°=vùë°
1 ùõΩùë°
1and ÀÜsùë°=sùë°
1 ùõΩùë°
2. (12.10.2)
Armed with the proper estimates we can now write out the update equations. First, we
rescale the gradient in a manner very much akin to that of RMSProp to obtain
g0
ùë°=ùúÇÀÜvùë°pÀÜsùë°¬∏ùúñ. (12.10.3)
Unlike RMSProp our update uses the momentum ÀÜvùë°rather than the gradient itself. More-
over, there is a slight cosmetic difference as the rescaling happens using1pÀÜsùë°¬∏ùúñinstead of
1pÀÜsùë°¬∏ùúñ. Theformerworksarguablyslightlybetterinpractice,hencethedeviationfromRM-
SProp. Typically we pick ùúñ=10 6for a good trade-off between numerical stability and
fidelity.
Now we have all the pieces in place to compute updates. This is slightly anticlimactic and
we have a simple update of the form
xùë° xùë° 1 g0
ùë°. (12.10.4)
Reviewing the design of Adam its inspiration is clear. Momentum and scale are clearly
visible in the state variables. Their rather peculiar definition forces us to debias terms
(this could be fixed by a slightly different initialization and update condition). Second, the
combination of both terms is pretty straightforward, given RMSProp. Last, the explicit
learningrate ùúÇallowsustocontrolthesteplengthtoaddressissuesofconvergence.
12.10.2Implementation
Implementing Adam from scratch is not very daunting. For convenience we store the time
step counter ùë°in the hyperparams dictionary. Beyond that all is straightforward.
%matplotlib inline
import torch
from d2l import torch asd2l
def init_adam_states (feature_dim):
v_w, v_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)
s_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)
return ((v_w, s_w), (v_b, s_b))
(continues on next page)
534 Optimization Algorithms
(continued from previous page)
def adam (params, states, hyperparams):
beta1, beta2, eps =0.9,0.999 ,1e-6
for p, (v, s) inzip(params, states):
with torch .no_grad():
v[:] =beta1 *v+(1-beta1) *p.grad
s[:] =beta2 *s+(1-beta2) *torch .square(p .grad)
v_bias_corr =v/(1-beta1 **hyperparams[ 't'])
s_bias_corr =s/(1-beta2 **hyperparams[ 't'])
p[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)
+eps)
p.grad .data .zero_()
hyperparams[ 't']+=1
We are ready to use Adam to train the model. We use a learning rate of ùúÇ=0.01.
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
d2l.train_ch11(adam, init_adam_states(feature_dim),
{'lr':0.01 ,'t':1}, data_iter, feature_dim);
loss: 0.243 ,0.193 sec/epoch
A more concise implementation is straightforward since adamis one of the algorithms pro-
vided as part of the Gluon trainer optimization library. Hence we only need to pass
configuration parameters for an implementation in Gluon.
trainer =torch .optim .Adam
d2l.train_concise_ch11(trainer, { 'lr':0.01 }, data_iter)
loss: 0.243 ,0.152 sec/epoch
12.10.3Yogi
OneoftheproblemsofAdamisthatitcanfailtoconvergeeveninconvexsettingswhenthe
second moment estimate in sùë°blows up. As a fix Zaheer et al.(2018) proposed a refined
535 Adam
update (and initialization) for sùë°. To understand what‚Äôs going on, let‚Äôs rewrite the Adam
update as follows:
sùë° sùë° 1¬∏¬π1 ùõΩ2¬∫
g2
ùë° sùë° 1
. (12.10.5)
Whenever g2
ùë°hashighvarianceorupdatesaresparse, sùë°mightforgetpastvaluestooquickly.
Apossiblefixforthisistoreplace g2
ùë° sùë° 1byg2
ùë°sgn¬πg2
ùë° sùë° 1¬∫. Nowthemagnitudeofthe
update no longer depends on the amount of deviation. This yields the Yogi updates
sùë° sùë° 1¬∏¬π1 ùõΩ2¬∫g2
ùë°sgn¬πg2
ùë° sùë° 1¬∫. (12.10.6)
The authors furthermore advise to initialize the momentum on a larger initial batch rather
than just initial pointwise estimate. We omit the details since they are not material to the
discussion and since even without this convergence remains pretty good.
def yogi (params, states, hyperparams):
beta1, beta2, eps =0.9,0.999 ,1e-3
for p, (v, s) inzip(params, states):
with torch .no_grad():
v[:] =beta1 *v+(1-beta1) *p.grad
s[:] =s+(1-beta2) *torch .sign(
torch .square(p .grad) -s)*torch .square(p .grad)
v_bias_corr =v/(1-beta1 **hyperparams[ 't'])
s_bias_corr =s/(1-beta2 **hyperparams[ 't'])
p[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)
+eps)
p.grad .data .zero_()
hyperparams[ 't']+=1
data_iter, feature_dim =d2l.get_data_ch11(batch_size =10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
{'lr':0.01 ,'t':1}, data_iter, feature_dim);
loss: 0.243 ,0.165 sec/epoch
12.10.4Summary
Adam combines features of many optimization algorithms into a fairly robust update
rule.
536 Optimization Algorithms
181Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic
gradient.
Adam uses bias correction to adjust for a slow startup when estimating momentum and
a second moment.
Forgradientswithsignificantvariancewemayencounterissueswithconvergence. They
can be amended by using larger minibatches or by switching to an improved estimate
forsùë°. Yogi offers such an alternative.
12.10.5Exercises
1.Adjust the learning rate and observe and analyze the experimental results.
2.Can you rewrite momentum and second moment updates such that it does not require
bias correction?
3.Why do you need to reduce the learning rate ùúÇas we converge?
4.Try to construct a case for which Adam diverges and Yogi converges?
Discussions181.
12.11LearningRate Scheduling
Sofarweprimarilyfocusedonoptimization algorithms forhowtoupdatetheweightvectors
ratherthanonthe rateatwhichtheyarebeingupdated. Nonetheless,adjustingthelearning
rate is often just as important as the actual algorithm. There are a number of aspects to
consider:
Mostobviouslythe magnitude ofthelearningratematters. Ifitistoolarge,optimization
diverges, if it is too small, it takes too long to train or we end up with a suboptimal
result. We saw previously that the condition number of the problem matters (see e.g.,
Section 12.6 for details). Intuitively it is the ratio of the amount of change in the least
sensitive direction vs. the most sensitive one.
537 Learning Rate Scheduling
Secondly,therateofdecayisjustasimportant. Ifthelearningrateremainslargewemay
simply end up bouncing around the minimum and thus not reach optimality. Section
12.5discussedthisinsomedetailandweanalyzedperformanceguaranteesin Section
12.4. Inshort,wewanttheratetodecay,butprobablymoreslowlythan O¬πùë° 1
2¬∫which
would be a good choice for convex problems.
Another aspect that is equally important is initialization . This pertains both to how the
parameters are set initially (review Section 5.4 for details) and also how they evolve
initially. This goes under the moniker of warmup, i.e., how rapidly we start moving
towards the solution initially. Large steps in the beginning might not be beneficial, in
particular since the initial set of parameters is random. The initial update directions
might be quite meaningless, too.
Lastly, there are a number of optimization variants that perform cyclical learning rate
adjustment. This is beyond the scope of the current chapter. We recommend the
reader to review details in Izmailov et al.(2018), e.g., how to obtain better solutions
by averaging over an entire pathof parameters.
Giventhe factthat thereis a lot ofdetail needed to managelearningrates, mostdeeplearn-
ing frameworks have tools to deal with this automatically. In the current chapter we will
review the effects that different schedules have on accuracy and also show how this can be
managed efficiently via a learning ratescheduler .
12.11.1ToyProblem
We begin with a toy problem that is cheap enough to compute easily, yet sufficiently non-
trivial to illustrate some of the key aspects. For that we pick a slightly modernized version
of LeNet ( reluinstead of sigmoid activation, MaxPooling rather than AveragePooling),
asappliedtoFashion-MNIST.Moreover, wehybridizethenetworkforperformance. Since
mostofthecodeisstandardwejustintroducethebasicswithoutfurtherdetaileddiscussion.
SeeChapter 7 for a refresher as needed.
%matplotlib inline
import math
import torch
from torch import nn
from torch .optim import lr_scheduler
from d2l import torch asd2l
def net_fn ():
model =nn.Sequential(
nn.Conv2d( 1,6, kernel_size =5, padding =2), nn .ReLU(),
nn.MaxPool2d(kernel_size =2, stride =2),
nn.Conv2d( 6,16, kernel_size =5), nn .ReLU(),
nn.MaxPool2d(kernel_size =2, stride =2),
nn.Flatten(),
nn.Linear( 16*5*5,120), nn .ReLU(),
nn.Linear( 120,84), nn .ReLU(),
nn.Linear( 84,10))
(continues on next page)
538 Optimization Algorithms
(continued from previous page)
return model
loss =nn.CrossEntropyLoss()
device =d2l.try_gpu()
batch_size =256
train_iter, test_iter =d2l.load_data_fashion_mnist(batch_size =batch_size)
# The code is almost identical to `d2l.train_ch6` defined in the
# lenet section of chapter convolutional neural networks
def train (net, train_iter, test_iter, num_epochs, loss, trainer, device,
scheduler =None ):
net.to(device)
animator =d2l.Animator(xlabel ='epoch ', xlim =[0, num_epochs],
legend =['train loss ','train acc ','test acc '])
for epoch inrange (num_epochs):
metric =d2l.Accumulator( 3)# train_loss, train_acc, num_examples
for i, (X, y) inenumerate (train_iter):
net.train()
trainer .zero_grad()
X, y =X.to(device), y .to(device)
y_hat =net(X)
l=loss(y_hat, y)
l.backward()
trainer .step()
with torch .no_grad():
metric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])
train_loss =metric[ 0]/metric[ 2]
train_acc =metric[ 1]/metric[ 2]
if(i+1)%50==0:
animator .add(epoch +i/len(train_iter),
(train_loss, train_acc, None ))
test_acc =d2l.evaluate_accuracy_gpu(net, test_iter)
animator .add(epoch +1, (None ,None , test_acc))
ifscheduler:
ifscheduler .__module__ ==lr_scheduler .__name__ :
# Using PyTorch In-Built scheduler
scheduler .step()
else :
# Using custom defined scheduler
for param_group intrainer .param_groups:
param_group[ 'lr']=scheduler(epoch)
print (f'train loss {train_loss :.3f}, train acc {train_acc :.3f},'
f'test acc {test_acc :.3f}')
Let‚Äôs have a look at what happens if we invoke this algorithm with default settings, such as
a learning rate of 0.3and train for 30iterations. Note how the training accuracy keeps on
increasing while progress in terms of test accuracy stalls beyond a point. The gap between
both curves indicates overfitting.
539 Learning Rate Scheduling
lr, num_epochs =0.3,30
net =net_fn()
trainer =torch .optim .SGD(net .parameters(), lr =lr)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.145 , train acc 0.944 , test acc 0.877
12.11.2Schedulers
One way of adjusting the learning rate is to set it explicitly at each step. This is conve-
niently achieved by the set_learning_rate method. We could adjust it downward after
every epoch (or even after every minibatch), e.g., in a dynamic manner in response to how
optimization is progressing.
lr=0.1
trainer .param_groups[ 0]["lr"]=lr
print (f'learning rate is now {trainer .param_groups[ 0]["lr"]:.2f}')
learning rate isnow 0.10
More generally we want to define a scheduler. When invoked with the number of updates
it returns the appropriate value of the learning rate. Let‚Äôs define a simple one that sets the
learning rate to ùúÇ=ùúÇ0¬πùë°¬∏1¬∫ 1
2.
class SquareRootScheduler :
def __init__ (self , lr =0.1):
self .lr=lr
def __call__ (self , num_update):
return self .lr*pow(num_update +1.0,-0.5)
Let‚Äôs plot its behavior over a range of values.
scheduler =SquareRootScheduler(lr =0.1)
d2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])
540 Optimization Algorithms
Now let‚Äôs see how this plays out for training on Fashion-MNIST. We simply provide the
scheduler as an additional argument to the training algorithm.
net =net_fn()
trainer =torch .optim .SGD(net .parameters(), lr)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
scheduler)
train loss 0.273 , train acc 0.900 , test acc 0.886
This worked quite a bit better than previously. Two things stand out: the curve was rather
moresmooththan previously. Secondly, therewaslessoverfitting. Unfortunatelyitis nota
well-resolved question as to why certain strategies lead to less overfitting in theory. There
is some argument that a smaller stepsize will lead to parameters that are closer to zero and
thussimpler. However,thisdoesnotexplainthephenomenonentirelysincewedonotreally
stop early but simply reduce the learning rate gently.
12.11.3Policies
While we cannot possibly cover the entire variety of learning rate schedulers, we attempt
to give a brief overview of popular policies below. Common choices are polynomial decay
and piecewise constant schedules. Beyond that, cosine learning rate schedules have been
foundtoworkwellempiricallyonsomeproblems. Lastly,onsomeproblemsitisbeneficial
to warm up the optimizer prior to using large learning rates.
541 Learning Rate Scheduling
FactorScheduler
One alternative to a polynomial decay would be a multiplicative one, that is ùúÇùë°¬∏1 ùúÇùë°ùõº
forùõº2¬π0,1¬∫. Topreventthelearningratefromdecayingbeyondareasonablelowerbound
the update equation is often modified to ùúÇùë°¬∏1 max¬πùúÇmin,ùúÇùë°ùõº¬∫.
class FactorScheduler :
def __init__ (self , factor =1, stop_factor_lr =1e-7 , base_lr =0.1):
self .factor =factor
self .stop_factor_lr =stop_factor_lr
self .base_lr =base_lr
def __call__ (self , num_update):
self .base_lr =max(self .stop_factor_lr, self .base_lr *self .factor)
return self .base_lr
scheduler =FactorScheduler(factor =0.9, stop_factor_lr =1e-2 , base_lr =2.0)
d2l.plot(torch .arange( 50), [scheduler(t) for tinrange (50)])
This can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler.
FactorScheduler object. Ittakesafewmoreparameters,suchaswarmupperiod,warmup
mode (linear or constant), the maximum number of desired updates, etc.; Going forward
we will use the built-in schedulers as appropriate and only explain their functionality here.
As illustrated, it is fairly straightforward to build your own scheduler if needed.
Multi FactorScheduler
A common strategy for training deep networks is to keep the learning rate piecewise con-
stant and to decrease it by a given amount every so often. That is, given a set of times
when to decrease the rate, such as ùë†=f5,10,20gdecreaseùúÇùë°¬∏1 ùúÇùë°ùõºwheneverùë°2ùë†.
Assuming that the values are halved at each step we can implement this as follows.
net =net_fn()
trainer =torch .optim .SGD(net .parameters(), lr =0.5)
scheduler =lr_scheduler .MultiStepLR(trainer, milestones =[15,30], gamma =0.5)
def get_lr (trainer, scheduler):
lr=scheduler .get_last_lr()[ 0]
trainer .step()
(continues on next page)
542 Optimization Algorithms
(continued from previous page)
scheduler .step()
return lr
d2l.plot(torch .arange(num_epochs), [get_lr(trainer, scheduler)
for tinrange (num_epochs)])
The intuition behind this piecewise constant learning rate schedule is that one lets opti-
mization proceed until a stationary point has been reached in terms of the distribution of
weight vectors. Then (and only then) do we decrease the rate such as to obtain a higher
quality proxy to a good local minimum. The example below shows how this can produce
ever slightly better solutions.
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
scheduler)
train loss 0.194 , train acc 0.927 , test acc 0.869
Cosine Scheduler
A rather perplexing heuristic was proposed by Loshchilov and Hutter ( 2016). It relies on
the observation that we might not want to decrease the learning rate too drastically in the
beginningandmoreover,thatwemightwantto‚Äúrefine‚Äùthesolutionintheendusingavery
small learning rate. This results in a cosine-like schedule with the following functional
543 Learning Rate Scheduling
form for learning rates in the range ùë°2¬ª0,ùëá¬º.
ùúÇùë°=ùúÇùëá¬∏ùúÇ0 ùúÇùëá
2¬π1¬∏cos¬πùúãùë°¬ùùëá¬∫¬∫ (12.11.1)
HereùúÇ0is the initial learning rate, ùúÇùëáis the target rate at time ùëá. Furthermore, for ùë° > ùëá
we simply pin the value to ùúÇùëáwithout increasing it again. In the following example, we set
the max update step ùëá=20.
class CosineScheduler :
def __init__ (self , max_update, base_lr =0.01 , final_lr =0,
warmup_steps =0, warmup_begin_lr =0):
self .base_lr_orig =base_lr
self .max_update =max_update
self .final_lr =final_lr
self .warmup_steps =warmup_steps
self .warmup_begin_lr =warmup_begin_lr
self .max_steps =self .max_update -self .warmup_steps
def get_warmup_lr (self , epoch):
increase =(self .base_lr_orig -self .warmup_begin_lr) \
*float (epoch) /float (self .warmup_steps)
return self .warmup_begin_lr +increase
def __call__ (self , epoch):
ifepoch <self .warmup_steps:
return self .get_warmup_lr(epoch)
ifepoch <=self .max_update:
self .base_lr =self .final_lr +(
self .base_lr_orig -self .final_lr) *(1+math .cos(
math .pi*(epoch -self .warmup_steps) /self .max_steps)) /2
return self .base_lr
scheduler =CosineScheduler(max_update =20, base_lr =0.3, final_lr =0.01 )
d2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])
In the context of computer vision this schedule canlead to improved results. Note, though,
that such improvements are not guaranteed (as can be seen below).
net =net_fn()
trainer =torch .optim .SGD(net .parameters(), lr =0.3)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
scheduler)
544 Optimization Algorithms
train loss 0.159 , train acc 0.942 , test acc 0.904
Warmup
Insomecasesinitializingtheparametersisnotsufficienttoguaranteeagoodsolution. This
is particularly a problem for some advanced network designs that may lead to unstable
optimization problems. We could address this by choosing a sufficiently small learning
rateto preventdivergencein the beginning. Unfortunatelythismeans that progress isslow.
Conversely, a large learning rate initially leads to divergence.
Arathersimplefixforthisdilemmaistouseawarmupperiodduringwhichthelearningrate
increases to its initial maximum and to cool down the rate until the end of the optimization
process. For simplicity one typically uses a linear increase for this purpose. This leads to
a schedule of the form indicated below.
scheduler =CosineScheduler( 20, warmup_steps =5, base_lr =0.3, final_lr =0.01 )
d2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])
Note that the network converges better initially (in particular observe the performance dur-
ing the first 5 epochs).
net =net_fn()
trainer =torch .optim .SGD(net .parameters(), lr =0.3)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
scheduler)
545 Learning Rate Scheduling
182train loss 0.181 , train acc 0.934 , test acc 0.901
Warmup can be applied to any scheduler (not just cosine). For a more detailed discussion
of learning rate schedules and many more experiments see also ( Gotmare et al., 2018). In
particular they find that a warmup phase limits the amount of divergence of parameters
in very deep networks. This makes intuitively sense since we would expect significant
divergence due to random initialization in those parts of the network that take the most
time to make progress in the beginning.
12.11.4Summary
Decreasing the learning rate during training can lead to improved accuracy and (most
perplexingly) reduced overfitting of the model.
A piecewise decrease of the learning rate whenever progress has plateaued is effective
in practice. Essentially this ensures that we converge efficiently to a suitable solution
and only then reduce the inherent variance of the parameters by reducing the learning
rate.
Cosine schedulers are popular for some computer vision problems. See e.g., GluonCV
182for details of such a scheduler.
A warmup period before optimization can prevent divergence.
Optimizationservesmultiplepurposesindeeplearning. Besidesminimizingthetraining
objective, different choices of optimization algorithms and learning rate scheduling
can lead to rather different amounts of generalization and overfitting on the test set
(for the same amount of training error).
12.11.5Exercises
1.Experiment with the optimization behavior for a given fixed learning rate. What is the
best model you can obtain this way?
2.Howdoesconvergencechangeifyouchangetheexponentofthedecreaseinthelearning
rate? Use PolyScheduler for your convenience in the experiments.
546 Optimization Algorithms
1833.Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet.
How does it affect performance relative to other schedulers?
4.How long should warmup last?
5.Can you connect optimization and sampling? Start by using results from Welling and
Teh (2011) on Stochastic Gradient Langevin Dynamics.
Discussions183.
13 Computational Performance
In deep learning, datasets and models are usually large, which involves heavy computa-
tion. Therefore, computational performance matters a lot. This chapter will focus on the
major factors that affect computational performance: imperative programming, symbolic
programming, asynchronous computing, automatic parallelism, and multi-GPU computa-
tion. By studying this chapter, you may further improve computational performance of
thosemodelsimplementedinthepreviouschapters, forexample, byreducingtrainingtime
without affecting accuracy.
13.1Compilersand Interpreters
So far, this book has focused on imperative programming, which makes use of statements
such as print,+, and ifto change a program‚Äôs state. Consider the following example of a
simple imperative program.
def add(a, b):
return a+b
def fancy_func (a, b, c, d):
e=add(a, b)
f=add(c, d)
g=add(e, f)
return g
print (fancy_func( 1,2,3,4))
10
Python is an interpreted language . When evaluating the above fancy_func function it
performstheoperationsmakingupthefunction‚Äôsbody insequence . Thatis,itwillevaluate
e = add(a, b) and store the results as variable e, thereby changing the program‚Äôs state.
The next two statements f = add(c, d) andg = add(e, f) will be executed similarly,
performing additions and storing the results as variables. Fig. 13.1.1 illustrates the flow of
data.
547
548 Computational Performance
tFig. 13.1.1 Data Ô¨Çow in an imperative program.
Although imperative programming is convenient, it may be inefficient. On the one hand,
even if the addfunction is repeatedly called throughout fancy_func , Python will execute
the three function calls individually. If these are executed, say, on a GPU (or even on mul-
tiple GPUs), the overhead arising from the Python interpreter can become overwhelming.
Moreover, it will need to save the variable values of eandfuntil all the statements in
fancy_func have been executed. This is because we do not know whether the variables e
andfwill be used by other parts of the program after the statements e = add(a, b) and
f = add(c, d) are executed.
13.1.1SymbolicProgramming
Consider the alternative, symbolicprogramming , where computation is usually performed
onlyoncetheprocesshasbeenfullydefined. Thisstrategyisusedbymultipledeeplearning
frameworks, including Theano and TensorFlow (the latter has acquired imperative exten-
sions). It usually involves the following steps:
1.Define the operations to be executed.
2.Compile the operations into an executable program.
3.Provide the required inputs and call the compiled program for execution.
This allows for a significant amount of optimization. First, we can skip the Python inter-
preter in many cases, thus removing a performance bottleneck that can become significant
on multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler
might optimize and rewrite the above code into print((1 + 2) + (3 + 4)) or even
print(10) . This is possible since a compiler gets to see the full code before turning it into
machine instructions. For instance, it can release memory (or never allocate it) whenever a
variableisnolongerneeded. Oritcantransformthecodeentirelyintoanequivalentpiece.
To get a better idea, consider the following simulation of imperative programming (it is
Python after all) below.
def add_ ():
return '''
def add(a, b):
return a + b
'''
def fancy_func_ ():
return '''
(continues on next page)
549 Compilers and Interpreters
(continued from previous page)
def fancy_func(a, b, c, d):
e = add(a, b)
f = add(c, d)
g = add(e, f)
return g
'''
def evoke_ ():
return add_() +fancy_func_() +'print(fancy_func(1, 2, 3, 4)) '
prog =evoke_()
print (prog)
y=compile (prog, '','exec ')
exec(y)
def add(a, b):
return a+b
def fancy_func (a, b, c, d):
e=add(a, b)
f=add(c, d)
g=add(e, f)
return g
print (fancy_func( 1,2,3,4))
10
Thedifferencesbetweenimperative(interpreted)programmingandsymbolicprogramming
are as follows:
Imperative programming is easier. When imperative programming is used in Python,
the majority of the code is straightforward and easy to write. It is also easier to de-
bug imperative programming code. This is because it is easier to obtain and print all
relevant intermediate variable values, or use Python‚Äôs built-in debugging tools.
Symbolic programming is more efficient and easier to port. Symbolic programming
makes it easier to optimize the code during compilation, while also having the ability
to port the program into a format independent of Python. This allows the program to
be run in a non-Python environment, thus avoiding any potential performance issues
related to the Python interpreter.
13.1.2Hybrid Programming
Historically most deep learning frameworks choose between an imperative or a symbolic
approach. For example, Theano, TensorFlow (inspired by the former), Keras, and CNTK
formulate models symbolically. Conversely, Chainer and PyTorch take an imperative ap-
proach. AnimperativemodewasaddedtoTensorFlow2.0andKerasinlaterrevisions.
Asmentionedabove,PyTorchisbasedonimperativeprogrammingandusesdynamiccom-
putationgraphs. Inanefforttoleveragetheportabilityandefficiencyofsymbolicprogram-
ming, developers considered whether it would be possible to combine the benefits of both
550 Computational Performance
programming paradigms. This led to a torchscript that lets users develop and debug us-
ing pure imperative programming, while having the ability to convert most programs into
symbolic programs to be run when product-level computing performance and deployment
are required.
13.1.3Hybridizing the Sequential Class
Theeasiestwaytogetafeelforhowhybridizationworksistoconsiderdeepnetworkswith
multiple layers. Conventionally the Python interpreter will need to execute the code for all
layerstogenerateaninstructionthatcanthenbeforwardedtoaCPUoraGPU.Forasingle
(fast) computing device this does not cause any major issues. On the other hand, if we use
anadvanced8-GPUserversuchasanAWSP3dn.24xlargeinstancePythonwillstruggleto
keep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here.
Let‚Äôsseehowwecanaddressthisforsignificantpartsofthecodebyreplacing Sequential
with HybridSequential . We begin by defining a simple MLP.
import torch
from torch import nn
from d2l import torch asd2l
# Factory for networks
def get_net ():
net =nn.Sequential(nn .Linear( 512,256),
nn.ReLU(),
nn.Linear( 256,128),
nn.ReLU(),
nn.Linear( 128,2))
return net
x=torch .randn(size =(1,512))
net =get_net()
net(x)
tensor([[ -0.1602 ,0.0003 ]], grad_fn =<AddmmBackward0 >)
Byconvertingthemodelusing torch.jit.script function,weareabletocompileandop-
timizethecomputationintheMLP.Themodel‚Äôscomputationresultremainsunchanged.
net =torch .jit.script(net)
net(x)
tensor([[ -0.1602 ,0.0003 ]], grad_fn =<AddmmBackward0 >)
This seems almost too good to be true: write the same code as before and simply convert
the model using torch.jit.script . Once this happens the network is optimized (we will
benchmark the performance below).
551 Compilers and Interpreters
AccelerationbyHybridization
To demonstrate the performance improvement gained by compilation we compare the time
needed to evaluate net(x)before and after hybridization. Let‚Äôs define a class to measure
this time first. It will come handy throughout the chapter as we set out to measure (and
improve) performance.
#@save
class Benchmark :
"""For measuring running time."""
def __init__ (self , description ='Done '):
self .description =description
def __enter__ (self ):
self .timer =d2l.Timer()
return self
def __exit__ (self ,*args):
print (f'{self .description }:{self .timer .stop() :.4f}sec')
Now we can invoke the network twice, once with and once without torchscript.
net =get_net()
with Benchmark( 'Without torchscript '):
for iinrange (1000 ): net(x)
net =torch .jit.script(net)
with Benchmark( 'With torchscript '):
for iinrange (1000 ): net(x)
Without torchscript: 2.1447 sec
With torchscript: 4.0545 sec
As is observed in the above results, after an nn.Sequential instance is scripted using
thetorch.jit.script function, computing performance is improved through the use of
symbolic programming.
Serialization
One of the benefits of compiling the models is that we can serialize (save) the model and
its parameters to disk. This allows us to store a model in a manner that is independent of
the front-end language of choice. This allows us to deploy trained models to other devices
and easily use other front-end programming languages. At the same time the code is often
faster than what can be achieved in imperative programming. Let‚Äôs see the savefunction
in action.
net.save('my_mlp')
!ls -lh my_mlp*
552 Computational Performance
184-rw-r--r--1ci ci 651K Aug 1819:32my_mlp
13.1.4Summary
Imperativeprogrammingmakesiteasytodesignnewmodelssinceitispossibletowrite
code with control flow and the ability to use a large amount of the Python software
ecosystem.
Symbolic programming requires that we specify the program and compile it before exe-
cuting it. The benefit is improved performance.
13.1.5Exercises
1.Review the models that interest you in the previous chapters. Can you improve their
computational performance by reimplementing them?
Discussions184.
13.2AsynchronousComputation
Today‚Äôs computers are highly parallel systems, consisting of multiple CPU cores (often
multiplethreadspercore),multipleprocessingelementsperGPU,andoftenmultipleGPUs
perdevice. Inshort, wecanprocessmanydifferentthingsatthesametime, oftenondiffer-
ent devices. Unfortunately Python is not a great way of writing parallel and asynchronous
code, at least not without some extra help. After all, Python is single-threaded and this is
unlikely to change in the future. Deep learning frameworks such as MXNet and Tensor-
Flow adopt an asynchronousprogramming model to improve performance, while PyTorch
uses Python‚Äôs own scheduler leading to a different performance trade-off. For PyTorch, by
default,GPUoperationsareasynchronous. WhenyoucallafunctionthatusestheGPU,the
operations are enqueued to the particular device, but not necessarily executed until later.
This allows us to execute more computations in parallel, including operations on the CPU
or other GPUs.
Hence, understanding how asynchronous programming works helps us to develop more
efficient programs, by proactively reducing computational requirements and mutual de-
pendencies. This allows us to reduce memory overhead and increase processor utiliza-
tion.
import os
import subprocess
import numpy
import torch
from torch import nn
from d2l import torch asd2l
553 Asynchronous Computation
13.2.1Asynchronyvia Backend
For a warmup consider the following toy problem: we want to generate a random matrix
and multiply it. Let‚Äôs do that both in NumPy and in PyTorch tensor to see the difference.
Note that PyTorch tensoris defined on a GPU.
# Warmup for GPU computation
device =d2l.try_gpu()
a=torch .randn(size =(1000 ,1000 ), device =device)
b=torch .mm(a, a)
with d2l.Benchmark( 'numpy '):
for _inrange (10):
a=numpy .random .normal(size =(1000 ,1000 ))
b=numpy .dot(a, a)
with d2l.Benchmark( 'torch '):
for _inrange (10):
a=torch .randn(size =(1000 ,1000 ), device =device)
b=torch .mm(a, a)
numpy: 1.4693 sec
torch: 0.0022 sec
ThebenchmarkoutputviaPyTorchisordersofmagnitudefaster. NumPydotproductisex-
ecuted on the CPU processor while PyTorchmatrix multiplication is executedon GPU and
hence the latter is expected to be much faster. But the huge time difference suggests some-
thing else must be going on. By default, GPU operations are asynchronous in PyTorch.
Forcing PyTorch to finish all computation prior to returning shows what happened previ-
ously: computation is being executed by the backend while the frontend returns control to
Python.
with d2l.Benchmark():
for _inrange (10):
a=torch .randn(size =(1000 ,1000 ), device =device)
b=torch .mm(a, a)
torch .cuda .synchronize(device)
Done: 0.0058 sec
Broadly speaking, PyTorch has a frontend for direct interaction with the users, e.g., via
Python, as well as a backend used by the system to perform the computation. As shown
inFig. 13.2.1 , users can write PyTorch programs in various frontend languages, such as
Python and C++. Regardless of the frontend programming language used, the execution of
PyTorch programs occurs primarily in the backend of C++ implementations. Operations
issued by the frontend language are passed on to the backend for execution. The backend
manages its own threads that continuously collect and execute queued tasks. Note that for
this to work the backend must be able to keep track of the dependencies between various
554 Computational Performance
steps in the computational graph. Hence, it is not possible to parallelize operations that
depend on each other.
tFig. 13.2.1 Programming language frontends and deep learning framework backends.
Let‚Äôs look at another toy example to understand the dependency graph a bit better.
x=torch .ones(( 1,2), device =device)
y=torch .ones(( 1,2), device =device)
z=x*y+2
z
tensor([[ 3.,3.]], device ='cuda:0 ')
tFig. 13.2.2 The backend tracks dependencies between various steps in the computational graph.
The code snippet above is also illustrated in Fig. 13.2.2 . Whenever the Python frontend
thread executes one of the first three statements, it simply returns the task to the backend
queue. When the last statement‚Äôs results need to be printed, the Python frontend thread
will wait for the C++ backend thread to finish computing the result of the variable z. One
benefit of this design is that the Python frontend thread does not need to perform actual
computations. Thus,thereislittleimpactontheprogram‚Äôsoverallperformance,regardless
ofPython‚Äôsperformance. Fig.13.2.3 illustrateshowfrontendandbackendinteract.
13.2.2Barriers and Blockers
13.2.3ImprovingComputation
555 Automatic Parallelism
tFig. 13.2.3 Interactions of the frontend and backend.
18513.2.4Summary
Deep learning frameworks may decouple the Python frontend from an execution back-
end. This allows for fast asynchronous insertion of commands into the backend and
associated parallelism.
Asynchrony leads to a rather responsive frontend. However, use caution not to overfill
thetaskqueuesinceitmayleadtoexcessivememoryconsumption. Itisrecommended
to synchronize for each minibatch to keep frontend and backend approximately syn-
chronized.
Chip vendors offer sophisticated performance analysis tools to obtain a much more fine-
grained insight into the efficiency of deep learning.
13.2.5Exercises
1.On the CPU, benchmark the same matrix multiplication operations in this section. Can
you still observe asynchrony via the backend?
Discussions185.
13.3AutomaticParallelism
Deep learning frameworks (e.g., MXNet and PyTorch) automatically construct computa-
tional graphs at the backend. Using a computational graph, the system is aware of all the
dependencies, and can selectively execute multiple non-interdependent tasks in parallel to
improve speed. For instance, Fig. 13.2.2 inSection 13.2 initializes two variables indepen-
dently. Consequently the system can choose to execute them in parallel.
Typically,asingleoperatorwilluseallthecomputationalresourcesonallCPUsoronasin-
gleGPU.Forexample,the dotoperatorwilluseallcores(andthreads)onallCPUs,evenif
there are multiple CPU processors on a single machine. The same applies to a single GPU.
Hence parallelization is not quite so useful for single-device computers. With multiple de-
vicesthingsmattermore. Whileparallelizationistypicallymostrelevantbetweenmultiple
GPUs,addingthelocalCPUwillincreaseperformanceslightly. Forexample,seeHadjis et
556 Computational Performance
al.(2016) that focuses on training computer vision models combining a GPU and a CPU.
With the convenience of an automatically parallelizing framework we can accomplish the
samegoalinafewlinesofPythoncode. Morebroadly,ourdiscussionofautomaticparallel
computation focuses on parallel computation using both CPUs and GPUs, as well as the
parallelization of computation and communication.
Note that we need at least two GPUs to run the experiments in this section.
import torch
from d2l import torch asd2l
13.3.1ParallelComputation on GPUs
Let‚Äôs start by defining a reference workload to test: the runfunction below performs 10
matrix-matrix multiplications on the device of our choice using data allocated into two
variables: x_gpu1andx_gpu2.
devices =d2l.try_all_gpus()
def run(x):
return [x.mm(x) for _inrange (50)]
x_gpu1 =torch .rand(size =(4000 ,4000 ), device =devices[ 0])
x_gpu2 =torch .rand(size =(4000 ,4000 ), device =devices[ 1])
Now we apply the function to the data. To ensure that caching does not play a role in the
results we warm up the devices by performing a single pass on either of them prior to mea-
suring. torch.cuda.synchronize() waitsforallkernelsinallstreamsonaCUDAdevice
to complete. It takes in a deviceargument, the device for which we need to synchronize.
It uses the current device, given by current_device() , if the device argument is None
(default).
run(x_gpu1)
run(x_gpu2) # Warm-up all devices
torch .cuda .synchronize(devices[ 0])
torch .cuda .synchronize(devices[ 1])
with d2l.Benchmark( 'GPU1 time '):
run(x_gpu1)
torch .cuda .synchronize(devices[ 0])
with d2l.Benchmark( 'GPU2 time '):
run(x_gpu2)
torch .cuda .synchronize(devices[ 1])
GPU1 time: 0.4660 sec
GPU2 time: 0.4510 sec
Ifweremovethe synchronize statementbetweenbothtasksthesystemisfreetoparallelize
computation on both devices automatically.
557 Automatic Parallelism
with d2l.Benchmark( 'GPU1 & GPU2 '):
run(x_gpu1)
run(x_gpu2)
torch .cuda .synchronize()
GPU1 &GPU2: 0.4659 sec
In the above case the total execution time is less than the sum of its parts, since the deep
learningframeworkautomaticallyschedulescomputationonbothGPUdeviceswithoutthe
need for sophisticated code on behalf of the user.
13.3.2ParallelComputation and Communication
In many cases we need to move data between different devices, say between the CPU and
GPU, or between different GPUs. For instance, this occurs when we want to perform dis-
tributed optimization where we need to aggregate the gradients over multiple accelerator
cards. Let‚Äôs simulate this by computing on the GPU and then copying the results back to
the CPU.
def copy_to_cpu (x, non_blocking =False ):
return [y.to('cpu', non_blocking =non_blocking) for yinx]
with d2l.Benchmark( 'Run on GPU1 '):
y=run(x_gpu1)
torch .cuda .synchronize()
with d2l.Benchmark( 'Copy to CPU '):
y_cpu =copy_to_cpu(y)
torch .cuda .synchronize()
Run on GPU1: 0.4656 sec
Copy to CPU: 2.3125 sec
Thisissomewhatinefficient. Notethatwecouldalreadystartcopyingpartsof ytotheCPU
while the remainder of the list is still being computed. This situation occurs, e.g., when we
compute the (backprop) gradient on a minibatch. The gradients of some of the parameters
will be available earlier than that of others. Hence it works to our advantage to start using
PCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions
suchas to()andcopy_() admitanexplicit non_blocking argument, whichletsthecaller
bypass synchronization when it is unnecessary. Setting non_blocking=True allows us to
simulate this scenario.
with d2l.Benchmark( 'Run on GPU1 and copy to CPU '):
y=run(x_gpu1)
y_cpu =copy_to_cpu(y, True )
torch .cuda .synchronize()
558 Computational Performance
Run on GPU1 and copy to CPU: 1.6907 sec
The total time required for both operations is (as expected) less than the sum of their parts.
Note that this task is different from parallel computation as it uses a different resource: the
busbetweentheCPUandGPUs. Infact,wecouldcomputeonbothdevicesandcommuni-
cate, all at the same time. As noted above, there is a dependency between computation and
communication: y[i]must be computed before it can be copied to the CPU. Fortunately,
thesystemcancopy y[i-1]whilecomputing y[i]toreducethetotalrunningtime.
We conclude with an illustration of the computational graph and its dependencies for a
simpletwo-layerMLPwhentrainingonaCPUandtwoGPUs,asdepictedin Fig.13.3.1 . It
wouldbequitepainfultoscheduletheparallelprogramresultingfromthismanually. Thisis
whereitisadvantageoustohaveagraph-basedcomputingbackendforoptimization.
tFig. 13.3.1 The computational graph and its dependencies of a two-layer MLP on a CPU and two
GPUs.
13.3.3Summary
Modern systems have a variety of devices, such as multiple GPUs and CPUs. They can
be used in parallel, asynchronously.
Modern systems also have a variety of resources for communication, such as PCI Ex-
press, storage (typically solid-state drives or via networks), and network bandwidth.
They can be used in parallel for peak efficiency.
Thebackendcanimproveperformancethroughautomaticparallelcomputationandcom-
munication.
559 Hardware
186
18713.3.4Exercises
1.Eight operations were performed in the runfunction defined in this section. There
are no dependencies between them. Design an experiment to see if the deep learning
framework will automatically execute them in parallel.
2.When the workload of an individual operator is sufficiently small, parallelization can
help even on a single CPU or GPU. Design an experiment to verify this.
3.Design an experiment that uses parallel computation on CPUs, GPUs, and communica-
tion between both devices.
4.Use a debugger such as NVIDIA‚Äôs Nsight186to verify that your code is efficient.
5.Designing computation tasks that include more complex data dependencies, and run
experiments to see if you can obtain the correct results while improving performance.
Discussions187.
13.4Hardware
Building systems with great performance requires a good understanding of the algorithms
and models to capture the statistical aspects of the problem. At the same time it is also
indispensable to have at least a modicum of knowledge of the underlying hardware. The
currentsectionisnosubstituteforapropercourseonhardwareandsystemdesign. Instead,
it might serve as a starting point for understanding why some algorithms are more efficient
than others and how to achieve good throughput. A good design can easily make a differ-
enceofanorderofmagnitudeand,inturn,thiscanmakethedifferencebetweenbeingable
to train a network (e.g., in a week) and not at all (in 3 months, thus missing the deadline).
We will start by looking at computers. Then we will zoom in to look more carefully at
CPUs and GPUs. Lastly we zoom out to review how multiple computers are connected in
a server center or in the cloud.
tFig. 13.4.1 Latency Numbers that every programmer should know.
560 Computational Performance
188
189
190Impatientreadersmaybeabletogetbywith Fig.13.4.1 . ItistakenfromColinScott‚Äôs inter-
activepost188thatgivesagoodoverviewoftheprogressoverthepastdecade. Theoriginal
numbersareduetoJeffDean‚Äôs Stanfordtalkfrom2010189. Thediscussionbelowexplains
someoftherationaleforthesenumbersandhowtheycanguideusindesigningalgorithms.
The discussion below is very high level and cursory. It is clearly no substitute for a proper
coursebutratherjustmeanttoprovideenoughinformationforastatisticalmodelertomake
suitable design decisions. For an in-depth overview of computer architecture we refer the
reader to ( Hennessy and Patterson, 2011 ) or a recent course on the subject, such as the one
byArste Asanovic190.
13.4.1Computers
Most deep learning researchers and practitioners have access to a computer with a fair
amount of memory, computation, some form of an accelerator such as a GPU, or multiples
thereof. A computer consists of the following key components:
Aprocessor(alsoreferredtoasaCPU)thatisabletoexecutetheprogramswegiveit(in
addition to running an operating system and many other things), typically consisting
of 8 or more cores.
Memory (RAM) to store and retrieve the results from computation, such as weight vec-
tors and activations, and training data.
AnEthernetnetworkconnection(sometimesmultiple)withspeedsrangingfrom1GB/s
to 100 GB/s. On high end servers more advanced interconnects can be found.
A high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers
have up to 8 accelerators, often connected in an advanced topology, while desktop
systems have 1 or 2, depending on the budget of the user and the size of the power
supply.
Durable storage, such as a magnetic hard disk drive, a solid state drive, in many cases
connected using the PCIe bus. It provides efficient transfer of training data to the
system and storage of intermediate checkpoints as needed.
tFig. 13.4.2 Connectivity of components of a computer.
AsFig. 13.4.2 indicates, most components (network, GPU, and storage) are connected to
the CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the
CPU. For instance AMD‚Äôs Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable
16Gbit/sdatatransferinbothdirections. ThememoryisdirectlyattachedtotheCPUwith
a total bandwidth of up to 100 GB/s.
Whenweruncodeonacomputerweneedtoshuffledatatotheprocessors(CPUsorGPUs),
561 Hardware
191
192
193
194performcomputation,andthenmovetheresultsofftheprocessorbacktoRAManddurable
storage. Hence, in order to get good performance we need to make sure that this works
seamlessly without any one of the systems becoming a major bottleneck. For instance, if
wecannotloadimagesquicklyenoughtheprocessorwillnothaveanyworktodo. Likewise,
if we cannot move matrices quickly enough to the CPU (or GPU), its processing elements
will starve. Finally, if we want to synchronize multiple computers across the network, the
latter should not slow down computation. One option is to interleave communication and
computation. Let‚Äôs have a look at the various components in more detail.
13.4.2Memory
At its most basic memory is used to store data that needs to be readily accessible. At
present CPU RAM is typically of the DDR4191variety, offering 20‚Äì25 GB/s bandwidth
per module. Each module has a 64-bit-wide bus. Typically pairs of memory modules are
used to allow for multiple channels. CPUs have between 2 and 4 memory channels, i.e.,
they have between 4 0GB/s and 100 GB/s peak memory bandwidth. Often there are two
banks per channel. For instance AMD‚Äôs Zen 3 Threadripper has 8 slots.
While these numbers are impressive, indeed, they only tell part of the story. When we
want to read a portion from memory we first need to tell the memory module where the
information can be found. That is, we first need to send the address to RAM. Once this
is accomplished we can choose to read just a single 64 bit record or a long sequence of
records. The latter is called burst read . In a nutshell, sending an address to memory and
setting up the transfer takes approximately 100 ns (details depend on the specific timing
coefficients of the memory chips used), every subsequent transfer takes only 0.2 ns. In
short, the first read is 500 times as expensive as subsequent ones! Note that we could
perform up to 10,000,000 random reads per second. This suggests that we avoid random
memory access as far as possible and use burst reads (and writes) instead.
Matters are a bit more complex when we take into account that we have multiple banks.
Each bank can read memory largely independently. This means two things. On the one
hand, the effective number of random reads is up to 4 times higher, provided that they are
spread evenly across memory. It also means that it is still a bad idea to perform random
readssinceburstreadsare4timesfaster, too. Ontheotherhand, duetomemoryalignment
to 64 bit boundaries it is a good idea to align any data structures with the same boundaries.
Compilersdothisprettymuch automatically192whentheappropriateflagsareset. Curious
readers are encouraged to review a lecture on DRAMs such as the one by Zeshan Chishti
193.
GPUmemoryissubjecttoevenhigherbandwidthrequirementssincetheyhavemanymore
processing elements than CPUs. By and large there are two options to address them. The
first is to make the memory bus significantly wider. For instance, NVIDIA‚Äôs RTX 2080
Ti has a 352-bit-wide bus. This allows for much more information to be transferred at
the same time. Second, GPUs use specific high-performance memory. Consumer-grade
devices, such as NVIDIA‚Äôs RTX and Titan series typically use GDDR6194chips with over
500 GB/s aggregate bandwidth. An alternative is to use HBM (high bandwidth memory)
modules. TheyuseaverydifferentinterfaceandconnectdirectlywithGPUsonadedicated
562 Computational Performance
siliconwafer. Thismakesthemveryexpensiveandtheiruseistypicallylimitedtohigh-end
server chips, such as the NVIDIA Volta V100 series of accelerators. Quite unsurprisingly,
GPU memory is generally muchsmaller than CPU memory due to the higher cost of the
former. For our purposes, by and large their performance characteristics are similar, just a
lot faster. We can safely ignore the details for the purpose of this book. They only matter
when tuning GPU kernels for high throughput.
13.4.3Storage
We saw that some of the key characteristics of RAM are bandwidth andlatency. The same
is true for storage devices, just that the differences can be even more extreme.
HardDisk Drives
Harddiskdrives (HDDs)havebeeninuseforoverhalfacentury. Inanutshelltheycontain
anumberofspinningplatterswithheadsthatcanbepositionedtoreadorwriteatanygiven
track. High-end disks hold up to 16 TB on 9 platters. One of the key benefits of HDDs
is that they are relatively inexpensive. One of their many downsides are their typically
catastrophic failure modes and their relatively high read latency.
Tounderstandthelatter,considerthefactthatHDDsspinataround7,200RPM(revolutions
perminute). Iftheyweremuchfastertheywouldshatterduetothecentrifugalforceexerted
on the platters. This has a major downside when it comes to accessing a specific sector
on the disk: we need to wait until the platter has rotated in position (we can move the
heads but not accelerate the actual disks). Hence it can take over 8 ms until the requested
data is available. A common way this is expressed is to say that HDDs can operate at
approximately100IOPs(input/outputoperationspersecond). Thisnumberhasessentially
remained unchanged for the past two decades. Worse still, it is equally difficult to increase
bandwidth (it is in the order of 100‚Äì200 MB/s). After all, each head reads a track of bits,
hence the bit rate only scales with the square root of the information density. As a result,
HDDs are quickly becoming relegated to archival storage and low-grade storage for very
large datasets.
SolidState Drives
Solid state drives (SSDs) use flash memory to store information persistently. This allows
formuch faster access to stored records. Modern SSDs can operate at 100,000 to 500,000
IOPs, i.e., up to 3 orders of magnitude faster than HDDs. Furthermore, their bandwidth
can reach 1‚Äì3GB/s, i.e., one order of magnitude faster than HDDs. These improvements
sound almost too good to be true. Indeed, they come with the following caveats, due to the
way SSDs are designed.
SSDsstoreinformationinblocks(256KBorlarger). Theycanonlybewrittenasawhole,
which takes significant time. Consequently bit-wise random writes on SSD have very
poor performance. Likewise, writing data in general takes significant time since the
block has to be read, erased and then rewritten with new information. By now SSD
563 Hardware
controllers and firmware have developed algorithms to mitigate this. Nonetheless,
writes can be much slower, in particular for QLC (quad level cell) SSDs. The key
for improved performance is to maintain a queueof operations, to prefer reads and to
write in large blocks if possible.
ThememorycellsinSSDswearoutrelativelyquickly(oftenalreadyafterafewthousand
writes). Wear-levelprotectionalgorithmsareabletospreadthedegradationovermany
cells. That said, it is not recommended to use SSDs for swapping files or for large
aggregations of log-files.
Lastly, the massive increase in bandwidth has forced computer designers to attach SSDs
directly to the PCIe bus. The drives capable of handling this, referred to as NVMe
(Non Volatile Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to
8GB/s on PCIe 4.0.
Cloud Storage
Cloud storage provides a configurable range of performance. That is, the assignment of
storage to virtual machines is dynamic, both in terms of quantity and in terms of speed,
as chosen by users. We recommend that users increase the provisioned number of IOPs
whenever latency is too high, e.g., during training with many small records.
13.4.4CPUs
Central processing units (CPUs) are the centerpiece of any computer. They consist of a
number of key components: processor cores that are able to execute machine code, a bus
connectingthem(thespecifictopologydifferssignificantlybetweenprocessormodels,gen-
erations,andvendors),and cachestoallowforhigherbandwidthandlowerlatencymemory
access than what is possible by reads from main memory. Lastly, almost all modern CPUs
containvector processing units to aid with high performance linear algebra and convolu-
tions, as they are common in media processing and machine learning.
tFig. 13.4.3 Intel Skylake consumer quad-core CPU.
Fig. 13.4.3 depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated
564 Computational Performance
195GPU,caches, andaringbusconnectingthefourcores. Peripherals, suchasEthernet, WiFi,
Bluetooth, SSD controller, and USB, are either part of the chipset or directly attached
(PCIe) to the CPU.
Microarchitecture
Each of the processor cores consists of a rather sophisticated set of components. While
details differ between generations and vendors, the basic functionality is pretty much stan-
dard. The front-end loads instructions and tries to predict which path will be taken (e.g.,
for control flow). Instructions are then decoded from assembly code to microinstructions.
Assembly code is often not the lowest level code that a processor executes. Instead, com-
plex instructions may be decoded into a set of more lower level operations. These are then
processed by the actual execution core. Often the latter is capable of performing many op-
erations simultaneously. For instance, the ARM Cortex A77 core of Fig. 13.4.4 is able to
perform up to 8 operations simultaneously.
tFig. 13.4.4 ARM Cortex A77 Microarchitecture.
This means that efficient programs might be able to perform more than one instruction per
clock cycle, provided that they can be carried out independently. Not all units are created
equal. Some specialize in integer instructions whereas others are optimized for floating
point performance. To increase throughput, the processor might also follow multiple code
pathssimultaneouslyinabranchinginstructionandthendiscardtheresultsofthebranches
not taken. This is why branch prediction units matter (on the front-end) such that only the
most promising paths are pursued.
Vectorization
Deep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine
learning, one needs to perform many operations in one clock cycle. This is achieved via
vector units. They have different names: on ARM they are called NEON, on x86 they (a
recentgeneration)arereferredtoas AVX2195units. Acommonaspectisthattheyareable
to perform SIMD (single instruction multiple data) operations. Fig. 13.4.5 shows how 8
short integers can be added in one clock cycle on ARM.
565 Hardware
tFig. 13.4.5 128 bit NEON vectorization.
196Depending on architecture choices, such registers are up to 512 bits long, allowing for the
combination of up to 64 pairs of numbers. For instance, we might be multiplying two
numbers and adding them to a third, which is also known as a fused multiply-add. Intel‚Äôs
OpenVino196uses these to achieve respectable throughput for deep learning on server-
grade CPUs. Note, though, that this number is entirely dwarfed by what GPUs are capable
of achieving. For instance, NVIDIA‚Äôs RTX 2080 Ti has 4,352 CUDA cores, each of which
is capable of processing such an operation at any time.
Cache
Consider the following situation: we have a modest CPU core with 4 cores as depicted in
Fig.13.4.3 above,runningat2GHzfrequency. Moreover,let‚ÄôsassumethatwehaveanIPC
(instructionsperclock)countof1andthattheunitshaveAVX2with256-bitwidthenabled.
Let‚Äôs furthermore assume that at least one of the registers used for AVX2 operations needs
to be retrieved from memory. This means that the CPU consumes 4256bit=128bytes
of data per clock cycle. Unless we are able to transfer 2109128=256109bytes
to the processor per second the processing elements are going to starve. Unfortunately the
memory interface of such a chip only supports 20‚Äì40 GB/s data transfer, i.e., one order of
magnitude less. The fix is to avoid loading newdata from memory as far as possible and
rather to cache it locally on the CPU. This is where caches come in handy. Commonly the
following names or concepts are used:
Registers are strictly speaking not part of the cache. They help stage instructions. That
said, CPU registers are memory locations that a CPU can access at clock speed with-
out any delay penalty. CPUs have tens of registers. It is up to the compiler (or pro-
grammer) to use registers efficiently. For instance the C programming language has a
register keyword.
L1 caches are the first line of defense against high memory bandwidth requirements.
L1 caches are tiny (typical sizes might be 32‚Äì64 KB) and often split into data and
instructions caches. When data is found in the L1 cache, access is very fast. If they
cannot be found there, the search progresses down the cache hierarchy.
L2 caches are the next stop. Depending on architecture design and processor size they
might be exclusive. They might be accessible only by a given core or shared among
multiple cores. L2 caches are larger (typically 256‚Äì512 KB per core) and slower than
566 Computational Performance
L1. Furthermore, to access something in L2 we first need to check to realize that the
data is not in L1, which adds a small amount of extra latency.
L3caches aresharedamongmultiplecoresandcanbequitelarge. AMD‚ÄôsEpyc3server
CPUshaveawhopping256MBofcachespreadacrossmultiplechiplets. Moretypical
numbers are in the 4‚Äì8 MB range.
Predicting which memory elements will be needed next is one of the key optimization pa-
rametersinchipdesign. Forinstance,itisadvisabletotraversememoryina forward direc-
tionsincemostcachingalgorithmswilltryto readahead ratherthanbackwards. Likewise,
keeping memory access patterns local is a good way of improving performance.
Adding caches is a double-edge sword. On the one hand they ensure that the processor
cores do not starve of data. At the same time they increase chip size, using up area that
otherwise could have been spent on increasing processing power. Moreover, cache misses
canbeexpensive. Considertheworstcasescenario, falsesharing ,asdepictedin Fig.13.4.6 .
A memory location is cached on processor 0 when a thread on processor 1 requests the
data. To obtain it, processor 0 needs to stop what it is doing, write the information back
to main memory and then let processor 1 read it from memory. During this operation both
processorswait. Quitepotentiallysuchcoderuns moreslowly onmultipleprocessorswhen
compared with an efficient single-processor implementation. This is one more reason for
why there is a practical limit to cache sizes (besides their physical size).
tFig. 13.4.6 False sharing (image courtesy of Intel).
13.4.5GPUs and otherAccelerators
Itisnotanexaggerationtoclaimthatdeeplearningwouldnothavebeensuccessfulwithout
GPUs. Bythesametoken, itisquitereasonabletoarguethatGPUmanufacturers‚Äôfortunes
have increased significantly due to deep learning. This co-evolution of hardware and al-
gorithms has led to a situation where for better or worse deep learning is the preferable
statistical modeling paradigm. Hence it pays to understand the specific benefits that GPUs
and related accelerators such as the TPU ( Jouppietal., 2017).
Of note is a distinction that is often made in practice: accelerators are optimized either for
training or inference. For the latter we only need to compute the forward propagation in
a network. No storage of intermediate data is needed for backpropagation. Moreover, we
maynotneedveryprecisecomputation(FP16orINT8typicallysuffice). Ontheotherhand,
567 Hardware
197during training all intermediate results need storage to compute gradients. Moreover, ac-
cumulatinggradientsrequireshigherprecisiontoavoidnumericalunderflow(oroverflow).
This means that FP16 (or mixed precision with FP32) is the minimum requirement. All
of this necessitates faster and larger memory (HBM2 vs. GDDR6) and more processing
power. For instance, NVIDIA‚Äôs Turing197T4 GPUs are optimized for inference whereas
the V100 GPUs are preferable for training.
Recall vectorization as illustrated in Fig. 13.4.5 . Adding vector units to a processor core
allowed us to increase throughput significantly. For example, in the example in Fig. 13.4.5
we were able to perform 16 operations simultaneously. First, what if we added operations
thatoptimizednotjustoperationsbetweenvectorsbutalsobetweenmatrices? Thisstrategy
led to tensor cores (to be covered shortly). Second, what if we added many more cores?
In a nutshell, these two strategies summarize the design decisions in GPUs. Fig. 13.4.7
gives an overview of a basic processing block. It contains 16 integer and 16 floating point
units. In addition to that, two tensor cores accelerate a narrow subset of additional op-
erations relevant for deep learning. Each streaming multiprocessor consists of four such
blocks.
tFig. 13.4.7 NVIDIA Turing processing block (image courtesy of NVIDIA).
Next, 12 streaming multiprocessors are grouped into graphics processing clusters which
make up the high-end TU102 processors. Ample memory channels and an L2 cache com-
plement the setup. Fig. 13.4.8 has the relevant details. One of the reasons for designing
suchadeviceisthatindividualblockscanbeaddedorremovedasneededtoallowformore
compactchipsandtodealwithyieldissues(faultymodulesmightnotbeactivated). Fortu-
nately programming such devices is well hidden from the casual deep learning researcher
beneathlayersofCUDAandframeworkcode. Inparticular,morethanoneoftheprograms
might well be executed simultaneously on the GPU, provided that there are available re-
sources. Nonetheless it pays to be aware of the limitations of the devices to avoid picking
models that do not fit into device memory.
Alastaspectthatisworthmentioninginmoredetailare tensorcores . Theyareanexample
of a recent trend of adding more optimized circuits that are specifically effective for deep
learning. Forinstance,theTPUaddedasystolicarray( Kung,1988 )forfastmatrixmultipli-
568 Computational Performance
tFig. 13.4.8 NVIDIA Turing architecture (image courtesy of NVIDIA)
198cation. Therethedesignwastosupportaverysmallnumber(oneforthefirstgenerationof
TPUs) of large operations. Tensor cores are at the other end. They are optimized for small
operations involving between 44and 1616matrices, depending on their numerical
precision. Fig. 13.4.9 gives an overview of the optimizations.
tFig. 13.4.9 NVIDIA tensor cores in Turing (image courtesy of NVIDIA).
Obviously when optimizing for computation we end up making certain compromises. One
of them is that GPUs are not very good at handling interrupts and sparse data. While there
arenotableexceptions,suchas Gunrock198(Wangetal.,2016),theaccesspatternofsparse
matrices and vectors do not go well with the high bandwidth burst read operations where
569 Hardware
199
200
201
202
203
204GPUs excel. Matching both goals is an area of active research. See e.g., DGL199, a library
tuned for deep learning on graphs.
13.4.6Networksand Buses
Whenever a single device is insufficient for optimization we need to transfer data to and
from it to synchronize processing. This is where networks and buses come in handy. We
haveanumberofdesignparameters: bandwidth, cost, distance, andflexibility. Ononeend
wehaveWiFithathasaprettygoodrange,isveryeasytouse(nowires,afterall),cheapbut
it offers comparatively mediocre bandwidth and latency. No machine learning researcher
within their right mind would use it to build a cluster of servers. In what follows we focus
on interconnects that are suitable for deep learning.
PCIeis a dedicated bus for very high bandwidth point-to-point connections (up to 32
GB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit
microseconds (5 Œºs). PCIe links are precious. Processors only have a limited number
of them: AMD‚Äôs EPYC 3 has 128 lanes, Intel‚Äôs Xeon has up to 48 lanes per chip;
on desktop-grade CPUs the numbers are 20 (Ryzen 9) and 16 (Core i9) respectively.
Since GPUs have typically 16 lanes, this limits the number of GPUs that can connect
to the CPU at full bandwidth. After all, they need to share the links with other high
bandwidth peripherals such as storageand Ethernet. Justlike with RAM access, large
bulk transfers are preferable due to reduced packet overhead.
Ethernet is the most commonly used way of connecting computers. While it is signifi-
cantlyslowerthanPCIe,itisverycheapandresilienttoinstallandcoversmuchlonger
distances. Typical bandwidth for low-grade servers is 1 GBit/s. Higher-end devices
(e.g.,C5 instances200in the cloud) offer between 10 and 100 GBit/s bandwidth. As
in all previous cases data transmission has significant overheads. Note that we al-
most never use raw Ethernet directly but rather a protocol that is executed on top of
the physical interconnect (such as UDP or TCP/IP). This adds further overhead. Like
PCIe, Ethernet is designed to connect two devices, e.g., a computer and a switch.
Switches allow us to connect multiple devices in a manner where any pair of them can
carry out a (typically full bandwidth) point-to-point connection simultaneously. For
instance, Ethernet switches might connect 40 servers at high cross-sectional band-
width. Notethatswitchesarenotuniquetotraditionalcomputernetworks. EvenPCIe
lanes can be switched201. This occurs, e.g., to connect a large number of GPUs to a
host processor, as is the case for the P2 instances202.
NVLink is an alternative to PCIe when it comes to very high bandwidth interconnects.
It offers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have
sixlinkswhereasconsumer-gradeGPUs(RTX2080Ti)haveonlyonelink, operating
at a reduced 100 Gbit/s rate. We recommend to use NCCL203to achieve high data
transfer between GPUs.
13.4.7MoreLatency Numbers
570 Computational Performance
205The summary in Table 13.4.1 andTable 13.4.2 are from Eliot Eshelman204who maintains
an updated version of the numbers as a GitHub gist205.
Table 13.4.1: Common Latency Numbers.
Action Time Notes
L1 cache reference/hit 1.5 ns 4 cycles
Floating-point add/mult/FMA 1.5 ns 4 cycles
L2 cache reference/hit 5 ns 12 ~ 17 cycles
Branch mispredict 6 ns 15 ~ 20 cycles
L3 cache hit (unshared cache) 16 ns 42 cycles
L3 cache hit (shared in another core) 25 ns 65 cycles
Mutex lock/unlock 25 ns
L3 cache hit (modified in another core) 29 ns 75 cycles
L3 cache hit (on a remote CPU socket) 40 ns 100 ~ 300 cycles (40 ~ 116 ns)
QPI hop to a another CPU (per hop) 40 ns
64MB memory ref. (local CPU) 46 ns TinyMemBench on Broadwell E5-2690v4
64MB memory ref. (remote CPU) 70 ns TinyMemBench on Broadwell E5-2690v4
256MB memory ref. (local CPU) 75 ns TinyMemBench on Broadwell E5-2690v4
Intel Optane random write 94 ns UCSD Non-Volatile Systems Lab
256MB memory ref. (remote CPU) 120 ns TinyMemBench on Broadwell E5-2690v4
Intel Optane random read 305 ns UCSD Non-Volatile Systems Lab
Send 4KB over 100 Gbps HPC fabric 1 Œºs MVAPICH2 over Intel Omni-Path
Compress 1KB with Google Snappy 3 Œºs
Send 4KB over 10 Gbps ethernet 10 Œºs
Write 4KB randomly to NVMe SSD 30 Œºs DC P3608 NVMe SSD (QOS 99% is 500Œºs)
Transfer 1MB to/from NVLink GPU 30 Œºs ~33GB/s on NVIDIA 40GB NVLink
Transfer 1MB to/from PCI-E GPU 80 Œºs ~12GB/s on PCIe 3.0 x16 link
Read 4KB randomly from NVMe SSD 120 Œºs DC P3608 NVMe SSD (QOS 99%)
Read 1MB sequentially from NVMe SSD 208 Œºs ~4.8GB/s DC P3608 NVMe SSD
Write 4KB randomly to SATA SSD 500 Œºs DC S3510 SATA SSD (QOS 99.9%)
Read 4KB randomly from SATA SSD 500 Œºs DC S3510 SATA SSD (QOS 99.9%)
Round trip within same data center 500 Œºs One-way ping is ~250Œºs
Read 1MB sequentially from SATA SSD 2 ms ~550MB/s DC S3510 SATA SSD
Read 1MB sequentially from disk 5 ms ~200MB/s server HDD
Random Disk Access (seek+rotation) 10 ms
Send packet CA->Netherlands->CA 150 ms
Table 13.4.2: Latency Numbers for NVIDIA Tesla GPUs.
571 Hardware
Action Time Notes
GPU Shared Memory access 30 ns 30~90 cycles (bank conflicts add la-
tency)
GPU Global Memory access 200
ns200~800 cycles
Launch CUDA kernel on GPU 10 Œºs Host CPU instructs GPU to start kernel
Transfer 1MB to/from NVLink
GPU30 Œºs ~33GB/s on NVIDIA 40GB NVLink
Transfer 1MB to/from PCI-E GPU 80 Œºs ~12GB/s on PCI-Express x16 link
13.4.8Summary
Devices have overheads for operations. Hence it is important to aim for a small number
of large transfers rather than many small ones. This applies to RAM, SSDs, networks
and GPUs.
Vectorization is key for performance. Make sure you are aware of the specific abilities
of your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 op-
erations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA
Turing shines at FP16, INT8, and INT4 operations.
Numerical overflow due to small data types can be a problem during training (and to a
lesser extent during inference).
Aliasing can significantly degrade performance. For instance, memory alignment on 64
bit CPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea
to keep convolution sizes aligned, e.g., to tensor cores.
Match your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great
speedup(ordersofmagnitude)canbeachievedwhenfittingtheparametersintocaches.
Werecommendthatyousketchouttheperformanceofanovelalgorithmonpaperbefore
verifying the experimental results. Discrepancies of an order-of-magnitude or more
are reasons for concern.
Use profilers to debug performance bottlenecks.
Training and inference hardware have different sweet spots in terms of price and perfor-
mance.
13.4.9Exercises
1.WriteCcodetotestwhetherthereisanydifferenceinspeedbetweenaccessingmemory
aligned or misaligned relative to the external memory interface. Hint: be careful of
caching effects.
2.Test the difference in speed between accessing memory in sequence or with a given
stride.
572 Computational Performance
2063.How could you measure the cache sizes on a CPU?
4.Howwouldyoulayoutdataacrossmultiplememorychannelsformaximumbandwidth?
How would you lay it out if you had many small threads?
5.An enterprise-class HDD is spinning at 10,000 rpm. What is the absolutely minimum
time an HDD needs to spend worst case before it can read data (you can assume that
heads move almost instantaneously)? Why are 2.5‚Äù HDDs becoming popular for com-
mercial servers (relative to 3.5‚Äù and 5.25‚Äù drives)?
6.AssumethatanHDDmanufacturerincreasesthestoragedensityfrom1Tbitpersquare
inch to 5 Tbit per square inch. How much information can you store on a ring on a 2.5‚Äù
HDD? Is there a difference between the inner and outer tracks?
7.Going from 8 bit to 16 bit data types increases the amount of silicon approximately by
four times. Why? Why might NVIDIA have added INT4 operations to their Turing
GPUs?
8.How much faster is it to read forward through memory vs. reading backwards? Does
this number differ between different computers and CPU vendors? Why? Write C code
and experiment with it.
9.Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs
need a cache?
10.Measure the packet overhead when sending messages across the Ethernet. Look up the
difference between UDP and TCP/IP connections.
11.Direct memory access allows devices other than the CPU to write (and read) directly to
(from) memory. Why is this a good idea?
12.Look at the performance numbers for the Turing T4 GPU. Why does the performance
‚Äúonly‚Äù double as you go from FP16 to INT8 and INT4?
13.What is the shortest time it should take for a packet on a round trip between San Fran-
cisco and Amsterdam? Hint: you can assume that the distance is 10,000 km.
Discussions206.
13.5Trainingon Multiple GPUs
So far we discussed how to train models efficiently on CPUs and GPUs. We even showed
how deep learning frameworks allow one to parallelize computation and communication
automatically between them in Section 13.3 . We also showed in Section 6.7 how to list
all the available GPUs on a computer using the nvidia-smi command. What we did not
discuss is how to actually parallelize deep learning training. Instead, we implied in pass-
ing that one would somehow split the data across multiple devices and make it work. The
573 Training on Multiple GPUs
presentsectionfillsinthedetailsandshowshowtotrainanetworkinparallelwhenstarting
from scratch. Details on how to take advantage of functionality in high-level APIs is rele-
gated to Section 13.6 . We assume that you are familiar with minibatch stochastic gradient
descent algorithms such as the ones described in Section 12.5 .
13.5.1Splittingthe Problem
Let‚Äôsstartwithasimplecomputervisionproblemandaslightlyarchaicnetwork, e.g., with
multiple layers of convolutions, pooling, and possibly a few fully connected layers in the
end. That is, let‚Äôs start with a network that looks quite similar to LeNet ( LeCunet al.,
1998) or AlexNet ( Krizhevsky et al., 2012). Given multiple GPUs (2 if it is a desktop
server, 4 on an AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge),
we want to partition training in a manner as to achieve good speedup while simultaneously
benefittingfromsimpleandreproducibledesignchoices. MultipleGPUs,afterall,increase
bothmemory andcomputation ability. In a nutshell, we have the following choices, given
a minibatch of training data that we want to classify.
First, we could partition the network across multiple GPUs. That is, each GPU takes as
input the data flowing into a particular layer, processes data across a number of subsequent
layers and then sends the data to the next GPU. This allows us to process data with larger
networkswhencomparedwithwhatasingleGPUcouldhandle. Besides,memoryfootprint
per GPU can be well controlled (it is a fraction of the total network footprint).
However,theinterfacebetweenlayers(andthusGPUs)requirestightsynchronization. This
can be tricky, in particular if the computational workloads are not properly matched be-
tween layers. The problem is exacerbated for large numbers of GPUs. The interface be-
tween layers also requires large amounts of data transfer, such as activations and gradients.
This may overwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet
sequentialoperationsarenontrivialtopartition. Seee.g.,Mirhoseini etal.(2017)forabest
effort in this regard. It remains a difficult problem and it is unclear whether it is possible
to achieve good (linear) scaling on nontrivial problems. We do not recommend it unless
there is excellent framework or operating system support for chaining together multiple
GPUs.
Second,wecouldsplittheworklayerwise. Forinstance,ratherthancomputing64channels
onasingleGPUwecouldsplituptheproblemacross4GPUs,eachofwhichgeneratesdata
for 16 channels. Likewise, for a fully connected layer we could split the number of output
units.Fig. 13.5.1 (taken from Krizhevsky et al.(2012)) illustrates this design, where this
strategy was used to deal with GPUs that had a very small memory footprint (2 GB at the
time). This allows for good scaling in terms of computation, provided that the number of
channels(orunits)isnottoosmall. Besides,multipleGPUscanprocessincreasinglylarger
networks since the available memory scales linearly.
However, we need a verylarge number of synchronization or barrier operations since each
layer depends on the results from all the other layers. Moreover, the amount of data that
needstobetransferredispotentiallyevenlargerthanwhendistributinglayersacrossGPUs.
Thus,wedonotrecommendthisapproachduetoitsbandwidthcostandcomplexity.
574 Computational Performance
tFig. 13.5.1 Model parallelism in the original AlexNet design due to limited GPU memory.
Last, we could partition data across multiple GPUs. This way all GPUs perform the same
type of work, albeit on different observations. Gradients are aggregated across GPUs after
each minibatch of training data. This is the simplest approach and it can be applied in any
situation. Weonlyneedtosynchronizeaftereachminibatch. Thatsaid,itishighlydesirable
to start exchanging gradients parameters already while others are still being computed.
Moreover, larger numbers of GPUs lead to larger minibatch sizes, thus increasing training
efficiency. However, adding more GPUs does not allow us to train larger models.
tFig. 13.5.2 Parallelization on multiple GPUs. From left to right: original problem, network
partitioning, layerwise partitioning, data parallelism.
A comparison of different ways of parallelization on multiple GPUs is depicted in Fig.
13.5.2. By and large, data parallelism is the most convenient way to proceed, provided
that we have access to GPUs with sufficiently large memory. See also ( Liet al., 2014) for
a detailed description of partitioning for distributed training. GPU memory used to be a
problem in the early days of deep learning. By now this issue has been resolved for all but
the most unusual cases. We focus on data parallelism in what follows.
13.5.2Data Parallelism
Assumethatthereare ùëòGPUsonamachine. Giventhemodeltobetrained,eachGPUwill
maintainacompletesetofmodelparametersindependentlythoughparametervaluesacross
575 Training on Multiple GPUs
the GPUs are identical and synchronized. As an example, Fig. 13.5.3 illustrates training
with data parallelism when ùëò=2.
tFig. 13.5.3 Calculation of minibatch stochastic gradient descent using data parallelism on two GPUs.
In general, the training proceeds as follows:
Inanyiterationoftraining, givenarandomminibatch,wesplittheexamplesinthebatch
intoùëòportions and distribute them evenly across the GPUs.
Each GPU calculates loss and gradient of the model parameters based on the minibatch
subset it was assigned.
Thelocalgradientsofeachofthe ùëòGPUsareaggregatedtoobtainthecurrentminibatch
stochastic gradient.
The aggregate gradient is re-distributed to each GPU.
Each GPU uses this minibatch stochastic gradient to update the complete set of model
parameters that it maintains.
Note that in practice we increase the minibatch size ùëò-fold when training on ùëòGPUs such
that each GPU has the same amount of work to do as if we were training on a single GPU
only. On a 16-GPU server this can increase the minibatch size considerably and we may
havetoincreasethelearningrateaccordingly. Alsonotethatbatchnormalizationin Section
8.5needs to be adjusted, e.g., by keeping a separate batch normalization coefficient per
GPU. In what follows we will use a toy network to illustrate multi-GPU training.
%matplotlib inline
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
13.5.3A ToyNetwork
We use LeNet as introduced in Section 7.6 (with slight modifications). We define it from
scratch to illustrate parameter exchange and synchronization in detail.
576 Computational Performance
# Initialize model parameters
scale =0.01
W1=torch .randn(size =(20,1,3,3))*scale
b1=torch .zeros( 20)
W2=torch .randn(size =(50,20,5,5))*scale
b2=torch .zeros( 50)
W3=torch .randn(size =(800,128))*scale
b3=torch .zeros( 128)
W4=torch .randn(size =(128,10))*scale
b4=torch .zeros( 10)
params =[W1, b1, W2, b2, W3, b3, W4, b4]
# Define the model
def lenet (X, params):
h1_conv =F.conv2d( input =X, weight =params[ 0], bias =params[ 1])
h1_activation =F.relu(h1_conv)
h1=F.avg_pool2d( input =h1_activation, kernel_size =(2,2), stride =(2,2))
h2_conv =F.conv2d( input =h1, weight =params[ 2], bias =params[ 3])
h2_activation =F.relu(h2_conv)
h2=F.avg_pool2d( input =h2_activation, kernel_size =(2,2), stride =(2,2))
h2=h2.reshape(h2 .shape[ 0],-1)
h3_linear =torch .mm(h2, params[ 4])+params[ 5]
h3=F.relu(h3_linear)
y_hat =torch .mm(h3, params[ 6])+params[ 7]
return y_hat
# Cross-entropy loss function
loss =nn.CrossEntropyLoss(reduction ='none ')
13.5.4Data Synchronization
For efficient multi-GPU training we need two basic operations. First we need to have
the ability to distribute a list of parameters to multiple devices and to attach gradients
(get_params ). Without parameters it is impossible to evaluate the network on a GPU.
Second, we need the ability to sum parameters across multiple devices, i.e., we need an
allreduce function.
def get_params (params, device):
new_params =[p.to(device) for pinparams]
for pinnew_params:
p.requires_grad_()
return new_params
Let‚Äôs try it out by copying the model parameters to one GPU.
new_params =get_params(params, d2l .try_gpu( 0))
print ('b1 weight: ', new_params[ 1])
print ('b1 grad: ', new_params[ 1].grad)
b1 weight: tensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,‚ê£
(continues on next page)
577 Training on Multiple GPUs
(continued from previous page)
‚Ü©!0.,0.,0.,0.,0.],
device ='cuda:0 ', requires_grad =True )
b1 grad: None
Since we did not perform any computation yet, the gradient with regard to the bias param-
eter is still zero. Now let‚Äôs assume that we have a vector distributed across multiple GPUs.
The following allreduce function adds up all vectors and broadcasts the result back to all
GPUs. Note that for this to work we need to copy the data to the device accumulating the
results.
def allreduce (data):
for iinrange (1,len(data)):
data[ 0][:] +=data[i] .to(data[ 0].device)
for iinrange (1,len(data)):
data[i][:] =data[ 0].to(data[i] .device)
Let‚Äôs test this by creating vectors with different values on different devices and aggregate
them.
data =[torch .ones(( 1,2), device =d2l.try_gpu(i)) *(i+1)for iinrange (2)]
print ('before allreduce: \n', data[ 0],'\n', data[ 1])
allreduce(data)
print ('after allreduce: \n', data[ 0],'\n', data[ 1])
before allreduce:
tensor([[ 1.,1.]], device ='cuda:0 ')
tensor([[ 2.,2.]], device ='cuda:1 ')
after allreduce:
tensor([[ 3.,3.]], device ='cuda:0 ')
tensor([[ 3.,3.]], device ='cuda:1 ')
13.5.5DistributingData
We need a simple utility function to distribute a minibatch evenly across multiple GPUs.
For instance, on two GPUs we would like to have half of the data to be copied to either of
the GPUs. Since it is more convenient and more concise, we use the built-in function from
the deep learning framework to try it out on a 45matrix.
data =torch .arange( 20).reshape( 4,5)
devices =[torch .device( 'cuda:0 '), torch .device( 'cuda:1 ')]
split =nn.parallel .scatter(data, devices)
print ('input : ', data)
print ('load into ', devices)
print ('output: ', split)
578 Computational Performance
input : tensor([[ 0,1,2,3,4],
[5,6,7,8,9],
[10,11,12,13,14],
[15,16,17,18,19]])
load into [device( type ='cuda ', index =0), device( type ='cuda ', index =1)]
output: (tensor([[ 0,1,2,3,4],
[5,6,7,8,9]], device ='cuda:0 '), tensor([[ 10,11,12,13,14],
[15,16,17,18,19]], device ='cuda:1 '))
For later reuse we define a split_batch function that splits both data and labels.
#@save
def split_batch (X, y, devices):
"""Split `X` and `y` into multiple devices."""
assert X.shape[ 0]==y.shape[ 0]
return (nn.parallel .scatter(X, devices),
nn.parallel .scatter(y, devices))
13.5.6Training
Now we can implement multi-GPU training on a single minibatch. Its implementation is
primarily based on the data parallelism approach described in this section. We will use the
auxiliaryfunctionswejustdiscussed, allreduce andsplit_and_load ,tosynchronizethe
data among multiple GPUs. Note that we do not need to write any specific code to achieve
parallelism. Since the computational graph does not haveany dependencies across devices
within a minibatch, it is executed in parallel automatically .
def train_batch (X, y, device_params, devices, lr):
X_shards, y_shards =split_batch(X, y, devices)
# Loss is calculated separately on each GPU
ls=[loss(lenet(X_shard, device_W), y_shard) .sum()
for X_shard, y_shard, device_W inzip(
X_shards, y_shards, device_params)]
for linls: # Backpropagation is performed separately on each GPU
l.backward()
# Sum all gradients from each GPU and broadcast them to all GPUs
with torch .no_grad():
for iinrange (len(device_params[ 0])):
allreduce([device_params[c][i] .grad for cinrange (len(devices))])
# The model parameters are updated separately on each GPU
for param indevice_params:
d2l.sgd(param, lr, X .shape[ 0])# Here, we use a full-size batch
Now, we can define the training function. It is slightly different from the ones used in the
previous chapters: we need to allocate the GPUs and copy all the model parameters to all
the devices. Obviously each batch is processed using the train_batch function to deal
with multiple GPUs. For convenience (and conciseness of code) we compute the accuracy
on a single GPU, though this is ineÔ¨Äicient since the other GPUs are idle.
579 Training on Multiple GPUs
def train (num_gpus, batch_size, lr):
train_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)
devices =[d2l .try_gpu(i) for iinrange (num_gpus)]
# Copy model parameters to `num_gpus` GPUs
device_params =[get_params(params, d) for dindevices]
num_epochs =10
animator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])
timer =d2l.Timer()
for epoch inrange (num_epochs):
timer .start()
for X, y intrain_iter:
# Perform multi-GPU training for a single minibatch
train_batch(X, y, device_params, devices, lr)
torch .cuda .synchronize()
timer .stop()
# Evaluate the model on GPU 0
animator .add(epoch +1, (d2l .evaluate_accuracy_gpu(
lambda x: lenet(x, device_params[ 0]), test_iter, devices[ 0]),))
print (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '
f'on{str(devices) }')
Let‚Äôs see how well this works on a single GPU. We first use a batch size of 256 and a
learning rate of 0.2.
train(num_gpus =1, batch_size =256, lr =0.2)
test acc: 0.83 ,3.0 sec/epoch on [device( type ='cuda ', index =0)]
BykeepingthebatchsizeandlearningrateunchangedandincreasingthenumberofGPUs
to 2, we can see that the test accuracy roughly stays the same compared with the previous
experiment. Intermsoftheoptimizationalgorithms,theyareidentical. Unfortunatelythere
is no meaningful speedup to be gained here: the model is simply too small; moreover we
only have a small dataset, where our slightly unsophisticated approach to implementing
multi-GPU training suffered from significant Python overhead. We will encounter more
complex models and more sophisticated ways of parallelization going forward. Let‚Äôs see
what happens nonetheless for Fashion-MNIST.
580 Computational Performance
207train(num_gpus =2, batch_size =256, lr =0.2)
test acc: 0.84 ,2.8 sec/epoch on [device( type ='cuda ', index =0), device( type =
‚Ü©!'cuda ', index =1)]
13.5.7Summary
There are multiple ways to split deep network training over multiple GPUs. We could
splitthembetweenlayers,acrosslayers,oracrossdata. Theformertworequiretightly
choreographed data transfers. Data parallelism is the simplest strategy.
Data parallel training is straightforward. However, it increases the effective minibatch
size to be efficient.
In data parallelism, data is split across multiple GPUs, where each GPU executes its
own forward and backward operation and subsequently gradients are aggregated and
results are broadcast back to the GPUs.
We may use slightly increased learning rates for larger minibatches.
13.5.8Exercises
1.When training on ùëòGPUs, change the minibatch size from ùëètoùëòùëè, i.e., scale it up by
the number of GPUs.
2.Compare accuracy for different learning rates. How does it scale with the number of
GPUs?
3.Implementamoreefficient allreduce functionthataggregatesdifferent parameterson
different GPUs? Why is it more efficient?
4.Implement multi-GPU test accuracy computation.
Discussions207.
581 Concise Implementation for Multiple GPUs
13.6ConciseImplementation forMultiple GPUs
Implementing parallelism from scratch for every new model is no fun. Moreover, there is
significantbenefitinoptimizingsynchronizationtoolsforhighperformance. Inthefollow-
ing we will show how to do this using high-level APIs of deep learning frameworks. The
mathematics and the algorithms are the same as in Section 13.5 . Quite unsurprisingly you
will need at least two GPUs to run code of this section.
import torch
from torch import nn
from d2l import torch asd2l
13.6.1A ToyNetwork
Let‚Äôs use a slightly more meaningful network than LeNet from Section 13.5 that is still
sufficiently easy and quick to train. We pick a ResNet-18 variant ( Heet al., 2016). Since
theinputimagesaretinywemodifyitslightly. Inparticular,thedifferencefrom Section8.6
isthatweuseasmallerconvolutionkernel,stride,andpaddingatthebeginning. Moreover,
we remove the max-pooling layer.
#@save
def resnet18 (num_classes, in_channels =1):
"""A slightly modified ResNet-18 model."""
def resnet_block (in_channels, out_channels, num_residuals,
first_block =False ):
blk =[]
for iinrange (num_residuals):
ifi==0and not first_block:
blk.append(d2l .Residual(out_channels, use_1x1conv =True ,
strides =2))
else :
blk.append(d2l .Residual(out_channels))
return nn.Sequential( *blk)
# This model uses a smaller convolution kernel, stride, and padding and
# removes the max-pooling layer
net =nn.Sequential(
nn.Conv2d(in_channels, 64, kernel_size =3, stride =1, padding =1),
nn.BatchNorm2d( 64),
nn.ReLU())
net.add_module( "resnet_block1 ", resnet_block( 64,64,2, first_block =True ))
net.add_module( "resnet_block2 ", resnet_block( 64,128,2))
net.add_module( "resnet_block3 ", resnet_block( 128,256,2))
net.add_module( "resnet_block4 ", resnet_block( 256,512,2))
net.add_module( "global_avg_pool ", nn .AdaptiveAvgPool2d(( 1,1)))
net.add_module( "fc", nn .Sequential(nn .Flatten(),
nn.Linear( 512, num_classes)))
return net
582 Computational Performance
13.6.2NetworkInitialization
We will initialize the network inside the training loop. For a refresher on initialization
methods see Section 5.4 .
net =resnet18( 10)
# Get a list of GPUs
devices =d2l.try_all_gpus()
# We will initialize the network inside the training loop
13.6.3Training
As before, the training code needs to perform several basic functions for efficient paral-
lelism:
Network parameters need to be initialized across all devices.
While iterating over the dataset minibatches are to be divided across all devices.
We compute the loss and its gradient in parallel across devices.
Gradients are aggregated and parameters are updated accordingly.
In the end we compute the accuracy (again in parallel) to report the final performance of
the network. The training routine is quite similar to implementations in previous chapters,
except that we need to split and aggregate data.
def train (net, num_gpus, batch_size, lr):
train_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)
devices =[d2l .try_gpu(i) for iinrange (num_gpus)]
def init_weights (module):
iftype (module) in[nn.Linear, nn .Conv2d]:
nn.init .normal_(module .weight, std =0.01 )
net.apply(init_weights)
# Set the model on multiple GPUs
net =nn.DataParallel(net, device_ids =devices)
trainer =torch .optim .SGD(net .parameters(), lr)
loss =nn.CrossEntropyLoss()
timer, num_epochs =d2l.Timer(), 10
animator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])
for epoch inrange (num_epochs):
net.train()
timer .start()
for X, y intrain_iter:
trainer .zero_grad()
X, y =X.to(devices[ 0]), y .to(devices[ 0])
l=loss(net(X), y)
l.backward()
trainer .step()
timer .stop()
animator .add(epoch +1, (d2l .evaluate_accuracy_gpu(net, test_iter),))
print (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '
f'on{str(devices) }')
583 Concise Implementation for Multiple GPUs
Let‚Äôs see how this works in practice. As a warm-up we train the network on a single
GPU.
train(net, num_gpus =1, batch_size =256, lr =0.1)
test acc: 0.91 ,12.2 sec/epoch on [device( type ='cuda ', index =0)]
Next we use 2 GPUs for training. Compared with LeNet evaluated in Section 13.5 , the
model for ResNet-18 is considerably more complex. This is where parallelization shows
its advantage. The time for computation is meaningfully larger than the time for synchro-
nizing parameters. This improves scalability since the overhead for parallelization is less
relevant.
train(net, num_gpus =2, batch_size =512, lr =0.2)
test acc: 0.73 ,7.5 sec/epoch on [device( type ='cuda ', index =0), device( type =
‚Ü©!'cuda ', index =1)]
13.6.4Summary
Data is automatically evaluated on the devices where the data can be found.
Takecaretoinitializethenetworksoneachdevicebeforetryingtoaccesstheparameters
on that device. Otherwise you will encounter an error.
The optimization algorithms automatically aggregate over multiple GPUs.
584 Computational Performance
20813.6.5Exercises
1.This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use
more GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an
AWS p2.16xlarge instance)?
2.Sometimes, different devices provide different computing power. We could use the
GPUs and the CPU at the same time. How should we divide the work? Is it worth the
effort? Why? Why not?
Discussions208.
13.7ParameterServers
As we move from a single GPU to multiple GPUs and then to multiple servers containing
multiple GPUs, possibly all spread out across multiple racks and network switches, our
algorithms for distributed and parallel training need to become much more sophisticated.
Details matter since different interconnects have very different bandwidth (e.g., NVLink
can offer up to 100 GB/s across 6 links in an appropriate setting, PCIe 4.0 (16-lane) offers
32 GB/s, while even high speed 100GbE Ethernet only amounts to 10 GB/s). At the same
time it is unreasonable to expect that a statistical modeler be an expert in networking and
systems.
ThecoreideaoftheparameterserverwasintroducedinSmolaandNarayanamurthy( 2010)
in the context of distributed latent variable models. A description of the push and pull
semantics then followed in Ahmed et al.(2012) and a description of the system and an
open source library followed in Li et al.(2014). In the following we will motivate the
components needed for efficiency.
13.7.1Data-ParallelTraining
Let‚Äôs review the data parallel training approach to distributed training. We will use this
to the exclusion of all others in this section since it is significantly simpler to implement
in practice. There are virtually no use cases (besides deep learning on graphs) where any
other strategy for parallelism is preferred since GPUs have plenty of memory nowadays.
Fig. 13.7.1 describes the variant of data parallelism that we implemented in Section 13.5 .
The key aspect in it is that the aggregation of gradients occurs on one single GPU (GPU 0)
before the updated parameters are rebroadcast to all GPUs.
In retrospect, the decision to aggregate on GPU 0 seems rather ad-hoc. After all, we might
just as well aggregate on the CPU. In fact, we could even decide to aggregate some of
the parameters on one GPU and some others on another. Provided that the optimization
algorithm supports this, there is no real reason for why we could not. For instance, if we
have four parameter vectors with associated gradients g1,...,g4we could aggregate the
gradients on one GPU for each gùëñ(ùëñ=1,..., 4).
585 Parameter Servers
tFig. 13.7.1 Left: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss
and gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens
and the parameters are re-distributed to all GPUs.
209Thisreasoningseemsarbitraryandfrivolous. Afterall,themathematicsisthesamethrough-
out. However,wearedealingwithrealphysicalhardwarewheredifferentbuseshavediffer-
entbandwidthasdiscussedin Section13.4 . Considerareal4-wayGPUserverasdescribed
inFig. 13.7.2 . If it is particularly well connected, it might have a 100 GbE network card.
More typical numbers are in the 1‚Äì10 GbE range with an effective bandwidth of 100 MB/s
to 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g.,
consumer-grade Intel CPUs have24 lanes) we need a multiplexer209. The bandwidth from
the CPU on a 16x Gen3 link is 16 GB/s. This is also the speed at which eachof the GPUs
is connected to the switch. This means that it is more effective to communicate between
the devices.
tFig. 13.7.2 A 4-way GPU server.
586 Computational Performance
210For the sake of the argument let‚Äôs assume that the gradients are of 160 MB. In this case
it takes 30 ms to send the gradients from all 3 remaining GPUs to the fourth one (each
transfer takes 10 ms = 160 MB / 16 GB/s). Adding another 30 ms to transmit the weight
vectorsbackwearriveatatotalof60ms. IfwesendalldatatotheCPUweincurapenalty
of 40 ms since eachof the four GPUs needs to send the data to the CPU, yielding a total
of 80 ms. Lastly assume that we are able to split the gradients into 4 parts of 40 MB each.
Now we can aggregate each of the parts on a different GPU simultaneously since the PCIe
switch offers a full-bandwidth operation between all links. Instead of 30 ms this takes 7.5
ms, yielding a total of 15 ms for a synchronization operation. In short, depending on how
we synchronize parameters the same operation can take anywhere from 15 ms to 80 ms.
Fig. 13.7.3 depicts the different strategies for exchanging parameters.
tFig. 13.7.3 Parameter synchronization strategies.
Notethatwehaveyetanothertoolatourdisposalwhenitcomestoimprovingperformance:
in a deep network it takes some time to compute all gradients from the top to the bottom.
We can begin synchronizing gradients for some parameter groups even while we are still
busycomputingthemforothers. Seee.g., SergeevandDelBalso( 2018)fordetailsonhow
to do this in Horovod210.
13.7.2RingSynchronization
When it comes to synchronization on modern deep learning hardware we often encounter
significantlybespokenetworkconnectivity. Forinstance,theAWSp3.16xlargeandNVIDIA
DGX-2 instances share the connectivity structure of Fig. 13.7.4 . Each GPU connects to a
host CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also
has 6 NVLink connections, each of which is capable of transferring 300 Gbit/s bidirec-
tionally. This amounts to around 18 GB/s per link per direction. In short, the aggregate
NVLink bandwidth is significantly higher than the PCIe bandwidth. The question is how
to use it most efficiently.
It turns out that the optimal synchronization strategy is to decompose the network into two
ringsandtousethemtosynchronizedatadirectly( Wangetal.,2018).Fig.13.7.5 illustrates
thatthenetworkcanbedecomposedintoonering(1-2-3-4-5-6-7-8-1)withdoubleNVLink
bandwidthandintoone(1-4-6-3-5-8-2-7-1)withregularbandwidth. Designinganefficient
synchronization protocol in this case is nontrivial.
Consider the following thought experiment: given a ring of ùëõcomputing nodes (or GPUs)
587 Parameter Servers
tFig. 13.7.4 NVLink connectivity on 8 V100 GPU servers (image courtesy of NVIDIA).
tFig. 13.7.5 Decomposition of the NVLink network into two rings.
we can send gradients from the first to the second node. There it is added to the local
gradient and sent on to the third node, and so on. After ùëõ 1steps the aggregate gradient
canbefoundinthelast-visitednode. Thatis,thetimetoaggregategradientsgrowslinearly
with the number of nodes. But if we do this the algorithm is quite inefficient. After all,
at any time there is only one of the nodes communicating. What if we broke the gradients
intoùëõchunks and started synchronizing chunk ùëñstarting at node ùëñ? Since each chunk is of
size 1¬ùùëõthe total time is now ¬πùëõ 1¬∫¬ùùëõ1. In other words, the time spent to aggregate
588 Computational Performance
gradients does not grow as we increase the size of the ring. This is quite an astonishing
result.Fig. 13.7.6 illustrates the sequence of steps on ùëõ=4nodes.
tFig. 13.7.6 Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients to
its left neighbor until the assembled gradient can be found in its right neighbor.
If we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive
at approximately 2160MB¬ù¬π318GB/s¬∫  6ms. This is better than using the PCIe
bus, even though we are now using 8 GPUs. Note that in practice these numbers are a
bit worse, since deep learning frameworks often fail to assemble communication into large
burst transfers.
Note that there is a common misconception that ring synchronization is fundamentally
different from other synchronization algorithms. The only difference is that the synchro-
nization path is somewhat more elaborate when compared with a simple tree.
13.7.3Multi-MachineTraining
Distributed training on multiple machines adds a further challenge: we need to communi-
catewithserversthatareonlyconnectedacrossacomparativelylowerbandwidthfabricthat
can be over an order of magnitude slower in some cases. Synchronization across devices is
tricky. After all, different machines running training code will have subtly different speed.
Henceweneedto synchronize themifwewanttousesynchronousdistributedoptimization.
Fig. 13.7.7 illustrates how distributed parallel training occurs.
1.A (different) batch of data is read on each machine, split across multiple GPUs and
transferred to GPU memory. There predictions and gradients are computed on each
GPU batch separately.
2.The gradients from all local GPUs are aggregated on one GPU (or parts of it are aggre-
gated over different GPUs).
589 Parameter Servers
3.The gradients are sent to the CPUs.
4.The CPUs send the gradients to a central parameter server which aggregates all the
gradients.
5.The aggregate gradients are then used to update the parameters and the updated param-
eters are broadcast back to the individual CPUs.
6.The information is sent to one (or multiple) GPUs.
7.The updated parameters are spread across all GPUs.
tFig. 13.7.7 Multi-machine multi-GPU distributed parallel training.
Each of these operations seems rather straightforward. And, indeed, they can be carried
out efficiently withina single machine. Once we look at multiple machines, though, we
can see that the central parameter server becomes the bottleneck. After all, the bandwidth
per server is limited, hence for ùëöworkers the time it takes to send all gradients to the
server isO¬πùëö¬∫. We can break through this barrier by increasing the number of servers
toùëõ. At this point each server only needs to store O¬π1¬ùùëõ¬∫of the parameters, hence the
total time for updates and optimization becomes O¬πùëö¬ùùëõ¬∫. Matching both numbers yields
constantscalingregardlessofhowmanyworkerswearedealingwith. Inpracticeweusethe
samemachines both as workers and as servers. Fig. 13.7.8 illustrates the design (see also
(Liet al., 2014) for details). In particular, ensuring that multiple machines work without
unreasonable delays is nontrivial.
13.7.4Key‚ÄìValueStores
Implementingthestepsrequiredfordistributedmulti-GPUtraininginpracticeisnontrivial.
This is why it pays to use a common abstraction, namely that of a key‚Äìvalue store with
redefined update semantics.
590 Computational Performance
tFig. 13.7.8 Top: a single parameter server is a bottleneck since its bandwidth is Ô¨Ånite. Bottom:
multiple parameter servers store parts of the parameters with aggregate bandwidth.
AcrossmanyworkersandmanyGPUsthecomputationforgradient ùëñcanbedefinedas
gùëñ=√ï
ùëò2workers√ï
ùëó2GPUsgùëñùëóùëò,(13.7.1)
wheregùëñùëóùëòispartofgradient ùëñsplitonGPU ùëóofworkerùëò. Thekeyaspectinthisoperation
is that it is a commutativereduction , that is, it turns many vectors into one and the order in
which the operation is applied does not matter. This is great for our purposes since we do
not(needto)havefinegrainedcontroloverwhenwhichgradientisreceived. Besides, note
that this operation is independent among different ùëñ.
This allows us to define the following two operations: push, which accumulates gradients,
andpull, which retrieves aggregate gradients. Since we have many different sets of gra-
dients (after all, we have many layers), we need to index the gradients with a key ùëñ. This
similarity to key‚Äìvalue stores, such as the one introduced in Dynamo ( DeCandia et al.,
2007) is not by coincidence. They, too, satisfy many similar characteristics, in particular
when it comes to distributing the parameters across multiple servers.
The push and pull operations for key-value stores are described as follows:
push(key, value) sends a particular gradient (the value) from a worker to a common
storage. There the value is aggregated, e.g., by summing it up.
pull(key,value) retrievesanaggregatevaluefromcommonstorage,e.g.,aftercombining
the gradients from all workers.
Byhidingallthecomplexityaboutsynchronizationbehindasimplepushandpulloperation
591 Parameter Servers
211we can decouple the concerns of statistical modelers who want to be able to express opti-
mization in simple terms and the system engineers who need to deal with the complexity
inherent in distributed synchronization.
13.7.5Summary
Synchronization needs to be highly adaptive to specific network infrastructure and con-
nectivity within a server. This can make a significant difference to the time it takes to
synchronize.
Ring-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not
so much.
A hierarchical synchronization strategy works well when adding multiple parameter
servers for increased bandwidth.
13.7.6Exercises
1.Can you increase the ring synchronization even further? Hint: you can send messages
in both directions.
2.Is it possible to allow asynchronous communication (while computation is still ongo-
ing)? How does it affect performance?
3.Whatifwelostaserverduringalong-runningcomputation? Howcanwedesigna fault
tolerance mechanism to avoid restarting the computation fully?
Discussions211.
14 Computer Vision
Whether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart filters,
many applications in the field of computer vision are closely related to our current and fu-
ture lives. In recent years, deep learning has been the transformative power for advancing
the performance of computer vision systems. It can be said that the most advanced com-
puter vision applications are almost inseparable from deep learning. In view of this, this
chapterwillfocusonthefieldofcomputervision,andinvestigatemethodsandapplications
that have recently been influential in academia and industry.
InChapter 7 andChapter 8 , we studied various convolutional neural networks that are
commonly used in computer vision, and applied them to simple image classification tasks.
At the beginning of this chapter, we will describe two methods that may improve model
generalization, namely imageaugmentation andfine-tuning , andapplythemtoimageclas-
sification. Since deep neural networks can effectively represent images in multiple lev-
els, such layerwise representations have been successfully used in various computer vision
taskssuchas objectdetection ,semanticsegmentation ,andstyletransfer . Followingthekey
idea of leveraging layerwise representations in computer vision, we will begin with major
components and techniques for object detection. Next, we will show how to use fully con-
volutionalnetworks forsemanticsegmentationofimages. Thenwewillexplainhowtouse
style transfer techniques to generate images like the cover of this book. In the end, we con-
clude this chapter by applying the materials of this chapter and several previous chapters
on two popular computer vision benchmark datasets.
14.1ImageAugmentation
InSection 8.1 , we mentioned that large datasets are a prerequisite for the success of deep
neuralnetworksinvariousapplications. Imageaugmentation generatessimilarbutdistinct
training examples after a series of random changes to the training images, thereby expand-
ing the size of the training set. Alternatively, image augmentation can be motivated by the
factthatrandomtweaksoftrainingexamplesallowmodelstorelylessoncertainattributes,
thereby improving their generalization ability. For example, we can crop an image in dif-
ferent ways to make the object of interest appear in different positions, thereby reducing
the dependence of a model on the position of the object. We can also adjust factors such as
brightness and color to reduce a model‚Äôs sensitivity to color. It is probably true that image
592
593 Image Augmentation
augmentation was indispensable for the success of AlexNet at that time. In this section we
will discuss this widely used technique in computer vision.
%matplotlib inline
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
14.1.1CommonImageAugmentation Methods
In our investigation of common image augmentation methods, we will use the following
400500image an example.
d2l.set_figsize()
img =d2l.Image .open( '../img/cat1.jpg ')
d2l.plt.imshow(img);
Most image augmentation methods have a certain degree of randomness. To make it easier
for us to observe the effect of image augmentation, next we define an auxiliary function
apply. Thisfunctionrunstheimageaugmentationmethod augmultipletimesontheinput
image imgand shows all the results.
def apply (img, aug, num_rows =2, num_cols =4, scale =1.5):
Y=[aug(img) for _inrange (num_rows *num_cols)]
d2l.show_images(Y, num_rows, num_cols, scale =scale)
Flipping and Cropping
Flippingtheimageleftandrightusuallydoesnotchangethecategoryoftheobject. Thisis
one of the earliest and most widely used methods of image augmentation. Next, we use the
transforms module to create the RandomHorizontalFlip instance, which flips an image
left and right with a 50% chance.
apply(img, torchvision .transforms .RandomHorizontalFlip())
Flipping up and down is not as common as flipping left and right. But at least for this
594 Computer Vision
example image, flipping up and down does not hinder recognition. Next, we create a Ran-
domVerticalFlip instance to flip an image up and down with a 50% chance.
apply(img, torchvision .transforms .RandomVerticalFlip())
Inthe exampleimageweused, the cat isin the middle of the image, but thismaynot be the
caseingeneral. In Section7.5 ,weexplainedthatthepoolinglayercanreducethesensitivity
of a convolutional layer to the target position. In addition, we can also randomly crop the
image to make objects appear in different positions in the image at different scales, which
can also reduce the sensitivity of a model to the target position.
In the code below, we randomly crop an area with an area of 10%100%of the original
areaeachtime, andtheratioofwidthtoheightofthisareaisrandomlyselectedfrom 0.5
2. Then, the width and height of the region are both scaled to 200 pixels. Unless otherwise
specified, the random number between ùëéandùëèin this section refers to a continuous value
obtained by random and uniform sampling from the interval ¬ªùëé,ùëè¬º.
shape_aug =torchvision .transforms .RandomResizedCrop(
(200,200), scale =(0.1,1), ratio =(0.5,2))
apply(img, shape_aug)
ChangingColors
Anotheraugmentationmethodischangingcolors. Wecanchangefouraspectsoftheimage
color: brightness,contrast,saturation,andhue. Intheexamplebelow,werandomlychange
the brightness of the image to a value between 50% ( 1 0.5) and 150% ( 1¬∏0.5) of the
original image.
595 Image Augmentation
apply(img, torchvision .transforms .ColorJitter(
brightness =0.5, contrast =0, saturation =0, hue =0))
Similarly, we can randomly change the hue of the image.
apply(img, torchvision .transforms .ColorJitter(
brightness =0, contrast =0, saturation =0, hue =0.5))
We can also create a RandomColorJitter instance and set how to randomly change the
brightness ,contrast ,saturation , and hueof the image at the same time.
color_aug =torchvision .transforms .ColorJitter(
brightness =0.5, contrast =0.5, saturation =0.5, hue =0.5)
apply(img, color_aug)
596 Computer Vision
CombiningMultiple ImageAugmentation Methods
In practice, we will combine multiple image augmentation methods. For example, we can
combine the different image augmentation methods defined above and apply them to each
image via a Compose instance.
augs =torchvision .transforms .Compose([
torchvision .transforms .RandomHorizontalFlip(), color_aug, shape_aug])
apply(img, augs)
14.1.2Trainingwith Image Augmentation
Let‚Äôs train a model with image augmentation. Here we use the CIFAR-10 dataset instead
of the Fashion-MNIST dataset that we used before. This is because the position and size
of the objects in the Fashion-MNIST dataset have been normalized, while the color and
size of the objects in the CIFAR-10 dataset have more significant differences. The first 32
training images in the CIFAR-10 dataset are shown below.
all_images =torchvision .datasets .CIFAR10(train =True , root ="../data ",
download =True )
d2l.show_images([all_images[i][ 0]for iinrange (32)], 4,8, scale =0.8);
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/
‚Ü©!cifar-10-python.tar.gz
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 170498071/170498071 [00:04<00:00, 37716809.52it/s]
Extracting ../data/cifar-10-python.tar.gz to ../data
In order to obtain definitive results during prediction, we usually only apply image aug-
597 Image Augmentation
mentation to training examples, and do not use image augmentation with random opera-
tions during prediction. Here we only use the simplest random left-right flipping method.
In addition, we use a ToTensor instance to convert a minibatch of images into the format
required by the deep learning framework, i.e., 32-bit floating point numbers between 0 and
1 with the shape of (batch size, number of channels, height, width).
train_augs =torchvision .transforms .Compose([
torchvision .transforms .RandomHorizontalFlip(),
torchvision .transforms .ToTensor()])
test_augs =torchvision .transforms .Compose([
torchvision .transforms .ToTensor()])
Next, we define an auxiliary function to facilitate reading the image and applying image
augmentation. The transform argument provided by PyTorch‚Äôs dataset applies augmen-
tation to transform the images. For a detailed introduction to DataLoader , please refer to
Section 4.2 .
def load_cifar10 (is_train, augs, batch_size):
dataset =torchvision .datasets .CIFAR10(root ="../data ", train =is_train,
transform =augs, download =True )
dataloader =torch .utils .data .DataLoader(dataset, batch_size =batch_size,
shuffle =is_train, num_workers =d2l.get_dataloader_workers())
return dataloader
Multi-GPUTraining
We train the ResNet-18 model from Section 8.6 on the CIFAR-10 dataset. Recall the in-
troduction to multi-GPU training in Section 13.6 . In the following, we define a function to
train and evaluate the model using multiple GPUs.
#@save
def train_batch_ch13 (net, X, y, loss, trainer, devices):
"""Train for a minibatch with multiple GPUs (defined in Chapter 13)."""
ifisinstance (X, list ):
# Required for BERT fine-tuning (to be covered later)
X=[x.to(devices[ 0])for xinX]
(continues on next page)
598 Computer Vision
(continued from previous page)
else :
X=X.to(devices[ 0])
y=y.to(devices[ 0])
net.train()
trainer .zero_grad()
pred =net(X)
l=loss(pred, y)
l.sum() .backward()
trainer .step()
train_loss_sum =l.sum()
train_acc_sum =d2l.accuracy(pred, y)
return train_loss_sum, train_acc_sum
#@save
def train_ch13 (net, train_iter, test_iter, loss, trainer, num_epochs,
devices =d2l.try_all_gpus()):
"""Train a model with multiple GPUs (defined in Chapter 13)."""
timer, num_batches =d2l.Timer(), len(train_iter)
animator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs], ylim =[0,1],
legend =['train loss ','train acc ','test acc '])
net =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])
for epoch inrange (num_epochs):
# Sum of training loss, sum of training accuracy, no. of examples,
# no. of predictions
metric =d2l.Accumulator( 4)
for i, (features, labels) inenumerate (train_iter):
timer .start()
l, acc =train_batch_ch13(
net, features, labels, loss, trainer, devices)
metric .add(l, acc, labels .shape[ 0], labels .numel())
timer .stop()
if(i+1)%(num_batches //5)==0ori==num_batches -1:
animator .add(epoch +(i+1)/num_batches,
(metric[ 0]/metric[ 2], metric[ 1]/metric[ 3],
None ))
test_acc =d2l.evaluate_accuracy_gpu(net, test_iter)
animator .add(epoch +1, (None ,None , test_acc))
print (f'loss {metric[ 0]/metric[ 2]:.3f}, train acc '
f'{metric[ 1]/metric[ 3]:.3f}, test acc {test_acc :.3f}')
print (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec on '
f'{str(devices) }')
Now we can define the train_with_data_aug function to train the model with image
augmentation. This function gets all available GPUs, uses Adam as the optimization algo-
rithm, appliesimageaugmentationtothetrainingdataset, andfinallycallsthe train_ch13
function just defined to train and evaluate the model.
batch_size, devices, net =256, d2l .try_all_gpus(), d2l .resnet18( 10,3)
net.apply(d2l .init_cnn)
def train_with_data_aug (train_augs, test_augs, net, lr =0.001 ):
train_iter =load_cifar10( True , train_augs, batch_size)
(continues on next page)
599 Image Augmentation
(continued from previous page)
test_iter =load_cifar10( False , test_augs, batch_size)
loss =nn.CrossEntropyLoss(reduction ="none ")
trainer =torch .optim .Adam(net .parameters(), lr =lr)
net( next (iter (train_iter))[ 0])
train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
Let‚Äôstrainthemodelusingimageaugmentationbasedonrandomleft-rightflipping.
train_with_data_aug(train_augs, test_augs, net)
loss 0.215 , train acc 0.925 , test acc 0.810
4728.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
14.1.3Summary
Imageaugmentationgeneratesrandomimagesbasedonexistingtrainingdatatoimprove
the generalization ability of models.
In order to obtain definitive results during prediction, we usually only apply image aug-
mentation to training examples, and do not use image augmentation with random op-
erations during prediction.
Deep learning frameworks provide many different image augmentation methods, which
can be applied simultaneously.
14.1.4Exercises
1.Trainthemodelwithoutusingimageaugmentation: train_with_data_aug(test_augs,
test_augs) . Compare training and testing accuracy when using and not using image
augmentation. Can this comparative experiment support the argument that image aug-
mentation can mitigate overfitting? Why?
2.CombinemultipledifferentimageaugmentationmethodsinmodeltrainingontheCIFAR-
10 dataset. Does it improve test accuracy?
600 Computer Vision
2123.Refer to the online documentation of the deep learning framework. What other image
augmentation methods does it also provide?
Discussions212.
14.2Fine-Tuning
In earlier chapters, we discussed how to train models on the Fashion-MNIST training
dataset with only 60000 images. We also described ImageNet, the most widely used large-
scale image dataset in academia, which has more than 10 million images and 1000 objects.
However, the size of the dataset that we usually encounter is between those of the two
datasets.
Suppose that we want to recognize different types of chairs from images, and then recom-
mend purchase links to users. One possible method is to first identify 100 common chairs,
take 1000 images of different angles for each chair, and then train a classification model
on the collected image dataset. Although this chair dataset may be larger than the Fashion-
MNIST dataset, the number of examples is still less than one-tenth of that in ImageNet.
This may lead to overfitting of complicated models that are suitable for ImageNet on this
chair dataset. Besides, due to the limited amount of training examples, the accuracy of the
trained model may not meet practical requirements.
In order to address the above problems, an obvious solution is to collect more data. How-
ever, collecting and labeling data can take a lot of time and money. For example, in order
to collect the ImageNet dataset, researchers have spent millions of dollars from research
funding. Although the current data collection cost has been significantly reduced, this cost
still cannot be ignored.
Another solution is to apply transfer learning to transfer the knowledge learned from the
sourcedataset to thetargetdataset . For example, although most of the images in the Ima-
geNet dataset have nothing to do with chairs, the model trained on this dataset may extract
more general image features, which can help identify edges, textures, shapes, and object
composition. These similar features may also be effective for recognizing chairs.
14.2.1Steps
In this section, we will introduce a common technique in transfer learning: fine-tuning . As
shown in Fig. 14.2.1 , fine-tuning consists of the following four steps:
1.Pretrain a neural network model, i.e., the source model , on a source dataset (e.g., the
ImageNet dataset).
2.Create a new neural network model, i.e., the target model . This copies all model de-
signs and their parameters on the source model except the output layer. We assume that
these model parameters contain the knowledge learned from the source dataset and this
601 Fine-Tuning
knowledge will also be applicable to the target dataset. We also assume that the output
layer of the source model is closely related to the labels of the source dataset; thus it is
not used in the target model.
3.Add an output layer to the target model, whose number of outputs is the number of
categories in the target dataset. Then randomly initialize the model parameters of this
layer.
4.Train the target model on the target dataset, such as a chair dataset. The output layer
will be trained from scratch, while the parameters of all the other layers are fine-tuned
based on the parameters of the source model.
tFig. 14.2.1 Fine tuning.
When target datasets are much smaller than source datasets, fine-tuning helps to improve
models‚Äô generalization ability.
14.2.2HotDog Recognition
Let‚Äôs demonstrate fine-tuning via a concrete case: hot dog recognition. We will fine-tune
a ResNet model on a small dataset, which was pretrained on the ImageNet dataset. This
small dataset consists of thousands of images with and without hot dogs. We will use the
fine-tuned model to recognize hot dogs from images.
%matplotlib inline
import os
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
Readingthe Dataset
The hot dog dataset we use was taken from online images. This dataset consists of 1400
positive-class images containing hot dogs, and as many negative-class images containing
other foods. 1000 images of both classes are used for training and the rest are for test-
ing.
602 Computer Vision
Afterunzippingthedownloadeddataset,weobtaintwofolders hotdog/train andhotdog/
test. Both folders have hotdog andnot-hotdog subfolders, either of which contains
images of the corresponding class.
#@save
d2l.DATA_HUB[ 'hotdog ']=(d2l .DATA_URL +'hotdog.zip ',
'fba480ffa8aa7e0febbb511d181409f899b9baa5 ')
data_dir =d2l.download_extract( 'hotdog ')
Downloading ../data /hotdog .zip from http ://d2l-data .s3-accelerate .amazonaws .
‚Ü©!com/hotdog .zip...
We create two instances to read all the image files in the training and testing datasets, re-
spectively.
train_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'train '))
test_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'test '))
The first 8 positive examples and the last 8 negative images are shown below. As you can
see, the images vary in size and aspect ratio.
hotdogs =[train_imgs[i][ 0]for iinrange (8)]
not_hotdogs =[train_imgs[ -i-1][0]for iinrange (8)]
d2l.show_images(hotdogs +not_hotdogs, 2,8, scale =1.4);
During training, we first crop a random area of random size and random aspect ratio from
theimage,andthenscalethisareatoa 224224inputimage. Duringtesting,wescaleboth
the height and width of an image to 256 pixels, and then crop a central 224224area as
input. In addition, for the three RGB (red, green, and blue) color channels we standardize
theirvalueschannelbychannel. Concretely,themeanvalueofachannelissubtractedfrom
each value of that channel and then the result is divided by the standard deviation of that
channel.
# Specify the means and standard deviations of the three RGB channels to
# standardize each channel
normalize =torchvision .transforms .Normalize(
(continues on next page)
603 Fine-Tuning
(continued from previous page)
[0.485 ,0.456 ,0.406 ], [ 0.229 ,0.224 ,0.225 ])
train_augs =torchvision .transforms .Compose([
torchvision .transforms .RandomResizedCrop( 224),
torchvision .transforms .RandomHorizontalFlip(),
torchvision .transforms .ToTensor(),
normalize])
test_augs =torchvision .transforms .Compose([
torchvision .transforms .Resize([ 256,256]),
torchvision .transforms .CenterCrop( 224),
torchvision .transforms .ToTensor(),
normalize])
Definingand Initializing the Model
We use ResNet-18, which was pretrained on the ImageNet dataset, as the source model.
Here, we specify pretrained=True to automatically download the pretrained model pa-
rameters. If this model is used for the first time, Internet connection is required for down-
load.
pretrained_net =torchvision .models .resnet18(pretrained =True )
The pretrained source model instance contains a number of feature layers and an output
layer fc. The main purpose of this division is to facilitate the fine-tuning of model param-
eters of all layers but the output layer. The member variable fcof source model is given
below.
pretrained_net .fc
Linear(in_features =512, out_features =1000 , bias =True )
As a fully connected layer, it transforms ResNet‚Äôs final global average pooling outputs into
1000 class outputs of the ImageNet dataset. We then construct a new neural network as
the target model. It is defined in the same way as the pretrained source model except that
its number of outputs in the final layer is set to the number of classes in the target dataset
(rather than 1000).
In the code below, the model parameters before the output layer of the target model in-
stance finetune_net are initialized to model parameters of the corresponding layers from
the source model. Since these model parameters were obtained via pretraining on Ima-
geNet, they are effective. Therefore, we can only use a small learning rate to fine-tune such
pretrained parameters. In contrast, model parameters in the output layer are randomly ini-
tialized and generally require a larger learning rate to be learned from scratch. Letting the
base learning rate be ùúÇ, a learning rate of 10ùúÇwill be used to iterate the model parameters
in the output layer.
604 Computer Vision
finetune_net =torchvision .models .resnet18(pretrained =True )
finetune_net .fc=nn.Linear(finetune_net .fc.in_features, 2)
nn.init .xavier_uniform_(finetune_net .fc.weight);
Fine-Tuningthe Model
First, we define a training function train_fine_tuning that uses fine-tuning so it can be
called multiple times.
# If `param_group=True`, the model parameters in the output layer will be
# updated using a learning rate ten times greater
def train_fine_tuning (net, learning_rate, batch_size =128, num_epochs =5,
param_group =True ):
train_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'train '), transform =train_augs),
batch_size =batch_size, shuffle =True )
test_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'test '), transform =test_augs),
batch_size =batch_size)
devices =d2l.try_all_gpus()
loss =nn.CrossEntropyLoss(reduction ="none ")
ifparam_group:
params_1x =[param for name, param innet.named_parameters()
ifname not in["fc.weight ","fc.bias "]]
trainer =torch .optim .SGD([{ 'params ': params_1x},
{'params ': net .fc.parameters(),
'lr': learning_rate *10}],
lr=learning_rate, weight_decay =0.001 )
else :
trainer =torch .optim .SGD(net .parameters(), lr =learning_rate,
weight_decay =0.001 )
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices)
We set the base learning rate to a small value in order to fine-tune the model parameters
obtained via pretraining. Based on the previous settings, we will train the output layer
parameters of the target model from scratch using a learning rate ten times greater.
train_fine_tuning(finetune_net, 5e-5 )
loss 0.242 , train acc 0.909 , test acc 0.940
1062.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
For comparison, we define an identical model, but initialize all of its model parameters to
randomvalues. Sincetheentiremodelneedstobetrainedfromscratch,wecanusealarger
learning rate.
605 Fine-Tuning
scratch_net =torchvision .models .resnet18()
scratch_net .fc=nn.Linear(scratch_net .fc.in_features, 2)
train_fine_tuning(scratch_net, 5e-4 , param_group =False )
loss 0.352 , train acc 0.846 , test acc 0.850
1525.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
As we can see, the fine-tuned model tends to perform better for the same epoch because its
initial parameter values are more effective.
14.2.3Summary
Transferlearningtransfersknowledgelearnedfromthesourcedatasettothetargetdataset.
Fine-tuning is a common technique for transfer learning.
The target model copies all model designs with their parameters from the source model
except the output layer, and fine-tunes these parameters based on the target dataset. In
contrast, the output layer of the target model needs to be trained from scratch.
Generally, fine-tuning parameters uses a smaller learning rate, while training the output
layer from scratch can use a larger learning rate.
14.2.4Exercises
606 Computer Vision
2131.Keep increasing the learning rate of finetune_net . How does the accuracy of the
model change?
2.Furtheradjusthyperparametersof finetune_net andscratch_net inthecomparative
experiment. Do they still differ in accuracy?
3.Settheparametersbeforetheoutputlayerof finetune_net tothoseofthesourcemodel
and donotupdate them during training. How does the accuracy of the model change?
You can use the following code.
for param infinetune_net .parameters():
param .requires_grad =False
4.In fact, there is a ‚Äúhotdog‚Äù class in the ImageNet dataset. Its corresponding weight
parameter in the output layer can be obtained via the following code. How can we
leverage this weight parameter?
weight =pretrained_net .fc.weight
hotdog_w =torch .split(weight .data, 1, dim =0)[934]
hotdog_w .shape
torch .Size([ 1,512])
Discussions213.
14.3Object Detectionand Bounding Boxes
In earlier sections (e.g., Section 8.1 ‚ÄìSection 8.4 ), we introduced various models for image
classification. In image classification tasks, we assume that there is only onemajor object
in the image and we only focus on how to recognize its category. However, there are often
multiple objects in the image of interest. We not only want to know their categories, but
also their specific positions in the image. In computer vision, we refer to such tasks as
objectdetection (orobjectrecognition ).
Objectdetectionhasbeenwidelyappliedinmanyfields. Forexample,self-drivingneedsto
plantravelingroutesbydetectingthepositionsofvehicles,pedestrians,roads,andobstacles
in the captured video images. Besides, robots may use this technique to detect and localize
objectsofinterestthroughoutitsnavigationofanenvironment. Moreover,securitysystems
may need to detect abnormal objects, such as intruders or bombs.
In the next few sections, we will introduce several deep learning methods for object detec-
tion. We will begin with an introduction to positions (orlocations ) of objects.
607 Object Detection and Bounding Boxes
%matplotlib inline
import torch
from d2l import torch asd2l
We will load the sample image to be used in this section. We can see that there is a dog
on the left side of the image and a cat on the right. They are the two major objects in this
image.
d2l.set_figsize()
img =d2l.plt.imread( '../img/catdog.jpg ')
d2l.plt.imshow(img);
14.3.1Bounding Boxes
In object detection, we usually use a bounding box to describe the spatial location of an
object. The bounding box is rectangular, which is determined by the ùë•andùë¶coordinates
of the upper-left corner of the rectangle and the such coordinates of the lower-right corner.
Another commonly used bounding box representation is the ¬πùë•,ùë¶¬∫-axis coordinates of the
bounding box center, and the width and height of the box.
Herewedefinefunctionstoconvertbetweenthesetworepresentations: box_corner_to_center
converts from the two-corner representation to the center-width-height presentation, and
box_center_to_corner viceversa. Theinputargument boxesshouldbeatwo-dimensional
tensor of shape ( ùëõ, 4), whereùëõis the number of bounding boxes.
#@save
def box_corner_to_center (boxes):
"""Convert from (upper-left, lower-right) to (center, width, height)."""
x1, y1, x2, y2 =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx=(x1 +x2) /2
cy=(y1 +y2) /2
w=x2-x1
h=y2-y1
boxes =torch .stack((cx, cy, w, h), axis =-1)
return boxes
#@save
def box_center_to_corner (boxes):
(continues on next page)
608 Computer Vision
(continued from previous page)
"""Convert from (center, width, height) to (upper-left, lower-right)."""
cx, cy, w, h =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
x1=cx-0.5 *w
y1=cy-0.5 *h
x2=cx+0.5 *w
y2=cy+0.5 *h
boxes =torch .stack((x1, y1, x2, y2), axis =-1)
return boxes
We will define the bounding boxes of the dog and the cat in the image based on the co-
ordinate information. The origin of the coordinates in the image is the upper-left corner
of the image, and to the right and down are the positive directions of the ùë•andùë¶axes,
respectively.
# Here `bbox` is the abbreviation for bounding box
dog_bbox, cat_bbox =[60.0 ,45.0 ,378.0 ,516.0 ], [ 400.0 ,112.0 ,655.0 ,493.0 ]
We can verify the correctness of the two bounding box conversion functions by converting
twice.
boxes =torch .tensor((dog_bbox, cat_bbox))
box_center_to_corner(box_corner_to_center(boxes)) ==boxes
tensor([[ True ,True ,True ,True ],
[True ,True ,True ,True ]])
Let‚Äôs draw the bounding boxes in the image to check if they are accurate. Before drawing,
we will define a helper function bbox_to_rect . It represents the bounding box in the
bounding box format of the matplotlib package.
#@save
def bbox_to_rect (bbox, color):
"""Convert bounding box to matplotlib format."""
# Convert the bounding box (upper-left x, upper-left y, lower-right x,
# lower-right y) format to the matplotlib format: ((upper-left x,
# upper-left y), width, height)
return d2l.plt.Rectangle(
xy=(bbox[ 0], bbox[ 1]), width =bbox[ 2]-bbox[ 0], height =bbox[ 3]-bbox[ 1],
fill =False , edgecolor =color, linewidth =2)
After adding the bounding boxes on the image, we can see that the main outline of the two
objects are basically inside the two boxes.
fig =d2l.plt.imshow(img)
fig.axes .add_patch(bbox_to_rect(dog_bbox, 'blue '))
fig.axes .add_patch(bbox_to_rect(cat_bbox, 'red'));
609 Anchor Boxes
21414.3.2Summary
Objectdetectionnotonlyrecognizesalltheobjectsofinterestintheimage,butalsotheir
positions. The position is generally represented by a rectangular bounding box.
We can convert between two commonly used bounding box representations.
14.3.3Exercises
1.Find another image and try to label a bounding box that contains the object. Compare
labeling bounding boxes and categories: which usually takes longer?
2.Whyistheinnermostdimensionoftheinputargument boxesofbox_corner_to_center
andbox_center_to_corner always 4?
Discussions214.
14.4AnchorBoxes
Object detection algorithms usually sample a large number of regions in the input image,
determine whether these regions contain objects of interest, and adjust the boundaries of
theregionssoastopredictthe ground-truthboundingboxes oftheobjectsmoreaccurately.
Different models may adopt different region sampling schemes. Here we introduce one of
such methods: it generates multiple bounding boxes with varying scales and aspect ratios
centered on each pixel. These bounding boxes are called anchor boxes . We will design an
object detection model based on anchor boxes in Section 14.7 .
First, let‚Äôs modify the printing accuracy just for more concise outputs.
%matplotlib inline
import torch
from d2l import torch asd2l
torch .set_printoptions( 2)# Simplify printing accuracy
610 Computer Vision
14.4.1Generating Multiple AnchorBoxes
Suppose that the input image has a height of ‚Ñéand width of ùë§. We generate anchor boxes
withdifferentshapescenteredoneachpixeloftheimage. Letthe scalebeùë†2¬π0,1¬ºandthe
aspectratio (ratioofwidthtoheight)is ùëü >0. Thenthewidthandheightoftheanchorbox
areùë§ùë†pùëüand‚Ñéùë†¬ùpùëü, respectively. Note that when the center position is given, an anchor
box with known width and height is determined.
Togeneratemultipleanchorboxeswithdifferentshapes,let‚Äôssetaseriesofscales ùë†1,...,ùë†ùëõ
and a series of aspect ratios ùëü1,...,ùëüùëö. When using all the combinations of these scales
and aspect ratios with each pixel as the center, the input image will have a total of ùë§‚Ñéùëõùëö
anchorboxes. Althoughtheseanchorboxesmaycoveralltheground-truthboundingboxes,
the computational complexity is easily too high. In practice, we can only consider those
combinations containing ùë†1orùëü1:
¬πùë†1,ùëü1¬∫,¬πùë†1,ùëü2¬∫,...,¬πùë†1,ùëüùëö¬∫,¬πùë†2,ùëü1¬∫,¬πùë†3,ùëü1¬∫,...,¬πùë†ùëõ,ùëü1¬∫. (14.4.1)
That is to say, the number of anchor boxes centered on the same pixel is ùëõ¬∏ùëö 1. For the
entire input image, we will generate a total of ùë§‚Ñé¬πùëõ¬∏ùëö 1¬∫anchor boxes.
The above method of generating anchor boxes is implemented in the following multi-
box_prior function. Wespecifythe inputimage, alistofscales, and alistofaspect ratios,
then this function will return all the anchor boxes.
#@save
def multibox_prior (data, sizes, ratios):
"""Generate anchor boxes with different shapes centered on each pixel."""
in_height, in_width =data .shape[ -2:]
device, num_sizes, num_ratios =data .device, len(sizes), len(ratios)
boxes_per_pixel =(num_sizes +num_ratios -1)
size_tensor =torch .tensor(sizes, device =device)
ratio_tensor =torch .tensor(ratios, device =device)
# Offsets are required to move the anchor to the center of a pixel. Since
# a pixel has height=1 and width=1, we choose to offset our centers by 0.5
offset_h, offset_w =0.5,0.5
steps_h =1.0 /in_height # Scaled steps in y axis
steps_w =1.0 /in_width # Scaled steps in x axis
# Generate all center points for the anchor boxes
center_h =(torch .arange(in_height, device =device) +offset_h) *steps_h
center_w =(torch .arange(in_width, device =device) +offset_w) *steps_w
shift_y, shift_x =torch .meshgrid(center_h, center_w, indexing ='ij')
shift_y, shift_x =shift_y .reshape( -1), shift_x .reshape( -1)
# Generate `boxes_per_pixel` number of heights and widths that are later
# used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)
w=torch .cat((size_tensor *torch .sqrt(ratio_tensor[ 0]),
sizes[ 0]*torch .sqrt(ratio_tensor[ 1:])))\
*in_height /in_width # Handle rectangular inputs
h=torch .cat((size_tensor /torch .sqrt(ratio_tensor[ 0]),
sizes[ 0]/torch .sqrt(ratio_tensor[ 1:])))
# Divide by 2 to get half height and half width
anchor_manipulations =torch .stack(( -w,-h, w, h)) .T.repeat(
(continues on next page)
611 Anchor Boxes
(continued from previous page)
in_height *in_width, 1)/2
# Each center point will have `boxes_per_pixel` number of anchor boxes, so
# generate a grid of all anchor box centers with `boxes_per_pixel` repeats
out_grid =torch .stack([shift_x, shift_y, shift_x, shift_y],
dim=1).repeat_interleave(boxes_per_pixel, dim =0)
output =out_grid +anchor_manipulations
return output .unsqueeze( 0)
We can see that the shape of the returned anchor box variable Yis (batch size, number of
anchor boxes, 4).
img =d2l.plt.imread( '../img/catdog.jpg ')
h, w =img.shape[: 2]
print (h, w)
X=torch .rand(size =(1,3, h, w)) # Construct input data
Y=multibox_prior(X, sizes =[0.75 ,0.5,0.25 ], ratios =[1,2,0.5])
Y.shape
561 728
torch .Size([ 1,2042040 ,4])
After changing the shape of the anchor box variable Yto (image height, image width, num-
ber of anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes
centered on a specified pixel position. In the following, we access the first anchor box cen-
teredon(250,250). Ithasfourelements: the ¬πùë•,ùë¶¬∫-axiscoordinatesattheupper-leftcorner
and the¬πùë•,ùë¶¬∫-axis coordinates at the lower-right corner of the anchor box. The coordinate
values of both axes are divided by the width and height of the image, respectively.
boxes =Y.reshape(h, w, 5,4)
boxes[ 250,250,0, :]
tensor([ 0.06 ,0.07 ,0.63 ,0.82 ])
In order to show all the anchor boxes centered on one pixel in the image, we define the
following show_bboxes function to draw multiple bounding boxes on the image.
#@save
def show_bboxes (axes, bboxes, labels =None , colors =None ):
"""Show bounding boxes."""
def make_list (obj, default_values =None ):
ifobj isNone :
obj =default_values
(continues on next page)
612 Computer Vision
(continued from previous page)
elif not isinstance (obj, ( list ,tuple )):
obj =[obj]
return obj
labels =make_list(labels)
colors =make_list(colors, [ 'b','g','r','m','c'])
for i, bbox inenumerate (bboxes):
color =colors[i %len(colors)]
rect =d2l.bbox_to_rect(bbox .detach() .numpy(), color)
axes .add_patch(rect)
iflabels and len(labels) >i:
text_color ='k'ifcolor =='w'else 'w'
axes .text(rect .xy[0], rect .xy[1], labels[i],
va='center ', ha ='center ', fontsize =9, color =text_color,
bbox =dict (facecolor =color, lw =0))
As we just saw, the coordinate values of the ùë•andùë¶axes in the variable boxeshave been
divided by the width and height of the image, respectively. When drawing anchor boxes,
we need to restore their original coordinate values; thus, we define variable bbox_scale
below. Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you
can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds
the dog in the image.
d2l.set_figsize()
bbox_scale =torch .tensor((w, h, w, h))
fig =d2l.plt.imshow(img)
show_bboxes(fig .axes, boxes[ 250,250, :, :] *bbox_scale,
['s=0.75, r=1 ','s=0.5, r=1 ','s=0.25, r=1 ','s=0.75, r=2 ',
's=0.75, r=0.5 '])
14.4.2IntersectionoverUnion(IoU)
Wejustmentionedthatananchorbox‚Äúwell‚Äùsurroundsthedogintheimage. Iftheground-
truth bounding box of the object is known, how can ‚Äúwell‚Äù here be quantified? Intuitively,
we can measure the similarity between the anchor box and the ground-truth bounding box.
We know that the Jaccard index can measure the similarity between two sets. Given sets
AandB, their Jaccard index is the size of their intersection divided by the size of their
613 Anchor Boxes
union:
ùêΩ¬πA,B¬∫=jA\Bj
jA[Bj. (14.4.2)
In fact, we can consider the pixel area of any bounding box as a set of pixels. In this way,
wecanmeasurethesimilarityofthetwoboundingboxesbytheJaccardindexoftheirpixel
sets. For two bounding boxes, we usually refer their Jaccard index as intersection over
union(IoU), which is the ratio of their intersection area to their union area, as shown in
Fig. 14.4.1 . The range of an IoU is between 0 and 1: 0 means that two bounding boxes do
not overlap at all, while 1 indicates that the two bounding boxes are equal.
tFig. 14.4.1 IoU is the ratio of the intersection area to the union area of two bounding boxes.
Fortheremainderofthissection,wewilluseIoUtomeasurethesimilaritybetweenanchor
boxes and ground-truth bounding boxes, and between different anchor boxes. Given two
lists of anchor or bounding boxes, the following box_iou computes their pairwise IoU
across these two lists.
#@save
def box_iou (boxes1, boxes2):
"""Compute pairwise IoU across two lists of anchor or bounding boxes."""
box_area =lambda boxes: ((boxes[:, 2]-boxes[:, 0])*
(boxes[:, 3]-boxes[:, 1]))
# Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),
# (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)
areas1 =box_area(boxes1)
areas2 =box_area(boxes2)
# Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of
# boxes1, no. of boxes2, 2)
inter_upperlefts =torch .max(boxes1[:, None , :2], boxes2[:, : 2])
inter_lowerrights =torch .min(boxes1[:, None ,2:], boxes2[:, 2:])
inters =(inter_lowerrights -inter_upperlefts) .clamp( min=0)
# Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)
inter_areas =inters[:, :, 0]*inters[:, :, 1]
union_areas =areas1[:, None ]+areas2 -inter_areas
return inter_areas /union_areas
14.4.3Labeling AnchorBoxesin TrainingData
In a training dataset, we consider each anchor box as a training example. In order to train
an object detection model, we need classandoffsetlabels for each anchor box, where the
former is the class of the object relevant to the anchor box and the latter is the offset of the
ground-truth bounding box relative to the anchor box. During the prediction, for each im-
age we generate multiple anchor boxes, predict classes and offsets for all the anchor boxes,
614 Computer Vision
adjust their positions according to the predicted offsets to obtain the predicted bounding
boxes, and finally only output those predicted bounding boxes that satisfy certain crite-
ria.
Asweknow,anobjectdetectiontrainingsetcomeswithlabelsforlocationsof ground-truth
boundingboxes andclassesoftheirsurroundedobjects. Tolabelanygenerated anchorbox ,
we refer to the labeled location and class of its assigned ground-truth bounding box that is
closest to the anchor box. In the following, we describe an algorithm for assigning closest
ground-truth bounding boxes to anchor boxes.
AssigningGround-TruthBounding Boxesto AnchorBoxes
Given an image, suppose that the anchor boxes are ùê¥1,ùê¥2,...,ùê¥ùëõùëéand the ground-truth
bounding boxes are ùêµ1,ùêµ2,...,ùêµùëõùëè, whereùëõùëéùëõùëè. Let‚Äôs define a matrix X2Rùëõùëéùëõùëè,
whose element ùë•ùëñùëóin theùëñthrow andùëóthcolumn is the IoU of the anchor box ùê¥ùëñand the
ground-truth bounding box ùêµùëó. The algorithm consists of the following steps:
1.Find the largest element in matrix Xand denote its row and column indices as ùëñ1and
ùëó1, respectively. Then the ground-truth bounding box ùêµùëó1is assigned to the anchor box
ùê¥ùëñ1. This is quite intuitive because ùê¥ùëñ1andùêµùëó1are the closest among all the pairs of
anchor boxes and ground-truth bounding boxes. After the first assignment, discard all
the elements in the ùëñ1throw and the ùëó1thcolumn in matrix X.
2.Find the largest of the remaining elements in matrix Xand denote its row and column
indices asùëñ2andùëó2, respectively. We assign ground-truth bounding box ùêµùëó2to anchor
boxùê¥ùëñ2and discard all the elements in the ùëñ2throw and the ùëó2thcolumn in matrix X.
3.At this point, elements in two rows and two columns in matrix Xhave been discarded.
We proceed until all elements in ùëõùëècolumns in matrix Xare discarded. At this time,
we have assigned a ground-truth bounding box to each of ùëõùëèanchor boxes.
4.Only traverse through the remaining ùëõùëé ùëõùëèanchor boxes. For example, given any
anchor boxùê¥ùëñ, find the ground-truth bounding box ùêµùëówith the largest IoU with ùê¥ùëñ
throughout the ùëñthrow of matrix X, and assign ùêµùëótoùê¥ùëñonly if this IoU is greater than
a predefined threshold.
Let‚Äôsillustratetheabovealgorithmusingaconcreteexample. Asshownin Fig.14.4.2 (left),
assumingthatthemaximumvalueinmatrix Xisùë•23, weassigntheground-truthbounding
boxùêµ3totheanchorbox ùê¥2. Then,wediscardalltheelementsinrow2andcolumn3ofthe
matrix, findthelargest ùë•71intheremainingelements(shadedarea), andassigntheground-
truthboundingbox ùêµ1totheanchorbox ùê¥7. Next,asshownin Fig.14.4.2 (middle),discard
all the elements in row 7 and column 1 of the matrix, find the largest ùë•54in the remaining
elements (shaded area), and assign the ground-truth bounding box ùêµ4to the anchor box
ùê¥5. Finally, as shown in Fig. 14.4.2 (right), discard all the elements in row 5 and column 4
of the matrix, find the largest ùë•92in the remaining elements (shaded area), and assign the
ground-truth bounding box ùêµ2to the anchor box ùê¥9. After that, we only need to traverse
through the remaining anchor boxes ùê¥1,ùê¥3,ùê¥4,ùê¥6,ùê¥8and determine whether to assign
them ground-truth bounding boxes according to the threshold.
615 Anchor Boxes
tFig. 14.4.2 Assigning ground-truth bounding boxes to anchor boxes.
Thisalgorithmisimplementedinthefollowing assign_anchor_to_bbox function.
#@save
def assign_anchor_to_bbox (ground_truth, anchors, device, iou_threshold =0.5):
"""Assign closest ground-truth bounding boxes to anchor boxes."""
num_anchors, num_gt_boxes =anchors .shape[ 0], ground_truth .shape[ 0]
# Element x_ij in the i-th row and j-th column is the IoU of the anchor
# box i and the ground-truth bounding box j
jaccard =box_iou(anchors, ground_truth)
# Initialize the tensor to hold the assigned ground-truth bounding box for
# each anchor
anchors_bbox_map =torch .full((num_anchors,), -1, dtype =torch .long,
device =device)
# Assign ground-truth bounding boxes according to the threshold
max_ious, indices =torch .max(jaccard, dim =1)
anc_i =torch .nonzero(max_ious >=iou_threshold) .reshape( -1)
box_j =indices[max_ious >=iou_threshold]
anchors_bbox_map[anc_i] =box_j
col_discard =torch .full((num_anchors,), -1)
row_discard =torch .full((num_gt_boxes,), -1)
for _inrange (num_gt_boxes):
max_idx =torch .argmax(jaccard) # Find the largest IoU
box_idx =(max_idx %num_gt_boxes) .long()
anc_idx =(max_idx /num_gt_boxes) .long()
anchors_bbox_map[anc_idx] =box_idx
jaccard[:, box_idx] =col_discard
jaccard[anc_idx, :] =row_discard
return anchors_bbox_map
Labeling Classes and Offsets
Now we can label the class and offset for each anchor box. Suppose that an anchor box
ùê¥is assigned a ground-truth bounding box ùêµ. On the one hand, the class of the anchor
boxùê¥will be labeled as that of ùêµ. On the other hand, the offset of the anchor box ùê¥will
be labeled according to the relative position between the central coordinates of ùêµandùê¥
together with the relative size between these two boxes. Given varying positions and sizes
616 Computer Vision
of different boxes in the dataset, we can apply transformations to those relative positions
and sizes that may lead to more uniformly distributed offsets that are easier to fit. Here we
describe a common transformation. Given the central coordinates of ùê¥andùêµas¬πùë•ùëé,ùë¶ùëé¬∫
and¬πùë•ùëè,ùë¶ùëè¬∫, their widths as ùë§ùëéandùë§ùëè, and their heights as ‚Ñéùëéand‚Ñéùëè, respectively. We
may label the offset of ùê¥as
 ùë•ùëè ùë•ùëé
ùë§ùëé ùúáùë•
ùúéùë•,ùë¶ùëè ùë¶ùëé
‚Ñéùëé ùúáùë¶
ùúéùë¶,logùë§ùëè
ùë§ùëé ùúáùë§
ùúéùë§,log‚Ñéùëè
‚Ñéùëé ùúá‚Ñé
ùúé‚Ñé!
, (14.4.3)
where default values of the constants are ùúáùë•=ùúáùë¶=ùúáùë§=ùúá‚Ñé=0,ùúéùë•=ùúéùë¶=0.1,
andùúéùë§=ùúé‚Ñé=0.2. This transformation is implemented below in the offset_boxes
function.
#@save
def offset_boxes (anchors, assigned_bb, eps =1e-6 ):
"""Transform for anchor box offsets."""
c_anc =d2l.box_corner_to_center(anchors)
c_assigned_bb =d2l.box_corner_to_center(assigned_bb)
offset_xy =10*(c_assigned_bb[:, : 2]-c_anc[:, : 2])/c_anc[:, 2:]
offset_wh =5*torch .log(eps +c_assigned_bb[:, 2:]/c_anc[:, 2:])
offset =torch .cat([offset_xy, offset_wh], axis =1)
return offset
If an anchor box is not assigned a ground-truth bounding box, we just label the class of
the anchor box as ‚Äúbackground‚Äù. Anchor boxes whose classes are background are often
referred to as negative anchor boxes, and the rest are called positive anchor boxes. We
implementthefollowing multibox_target functiontolabelclassesandoffsetsforanchor
boxes (the anchors argument) using ground-truth bounding boxes (the labelsargument).
This function sets the background class to zero and increments the integer index of a new
class by one.
#@save
def multibox_target (anchors, labels):
"""Label anchor boxes using ground-truth bounding boxes."""
batch_size, anchors =labels .shape[ 0], anchors .squeeze( 0)
batch_offset, batch_mask, batch_class_labels =[], [], []
device, num_anchors =anchors .device, anchors .shape[ 0]
for iinrange (batch_size):
label =labels[i, :, :]
anchors_bbox_map =assign_anchor_to_bbox(
label[:, 1:], anchors, device)
bbox_mask =((anchors_bbox_map >=0).float() .unsqueeze( -1)).repeat(
1,4)
# Initialize class labels and assigned bounding box coordinates with
# zeros
class_labels =torch .zeros(num_anchors, dtype =torch .long,
device =device)
assigned_bb =torch .zeros((num_anchors, 4), dtype =torch .float32,
device =device)
# Label classes of anchor boxes using their assigned ground-truth
# bounding boxes. If an anchor box is not assigned any, we label its
(continues on next page)
617 Anchor Boxes
(continued from previous page)
# class as background (the value remains zero)
indices_true =torch .nonzero(anchors_bbox_map >=0)
bb_idx =anchors_bbox_map[indices_true]
class_labels[indices_true] =label[bb_idx, 0].long() +1
assigned_bb[indices_true] =label[bb_idx, 1:]
# Offset transformation
offset =offset_boxes(anchors, assigned_bb) *bbox_mask
batch_offset .append(offset .reshape( -1))
batch_mask .append(bbox_mask .reshape( -1))
batch_class_labels .append(class_labels)
bbox_offset =torch .stack(batch_offset)
bbox_mask =torch .stack(batch_mask)
class_labels =torch .stack(batch_class_labels)
return (bbox_offset, bbox_mask, class_labels)
AnExample
Let‚Äôsillustrateanchorboxlabelingviaaconcreteexample. Wedefineground-truthbound-
ing boxes for the dog and cat in the loaded image, where the first element is the class (0
for dog and 1 for cat) and the remaining four elements are the ¬πùë•,ùë¶¬∫-axis coordinates at
the upper-left corner and the lower-right corner (range is between 0 and 1). We also con-
struct five anchor boxes to be labeled using the coordinates of the upper-left corner and the
lower-right corner: ùê¥0,...,ùê¥ 4(the index starts from 0). Then we plot these ground-truth
bounding boxes and anchor boxes in the image.
ground_truth =torch .tensor([[ 0,0.1,0.08 ,0.52 ,0.92 ],
[1,0.55 ,0.2,0.9,0.88 ]])
anchors =torch .tensor([[ 0,0.1,0.2,0.3], [ 0.15 ,0.2,0.4,0.4],
[0.63 ,0.05 ,0.88 ,0.98 ], [ 0.66 ,0.45 ,0.8,0.8],
[0.57 ,0.3,0.92 ,0.9]])
fig =d2l.plt.imshow(img)
show_bboxes(fig .axes, ground_truth[:, 1:]*bbox_scale, [ 'dog','cat'],'k')
show_bboxes(fig .axes, anchors *bbox_scale, [ '0','1','2','3','4']);
Using the multibox_target function defined above, we can label classes and offsets of
these anchor boxes based on the ground-truth bounding boxes for the dog and cat. In this
example, indices of the background, dog, and cat classes are 0, 1, and 2, respectively.
618 Computer Vision
Below we add an dimension for examples of anchor boxes and ground-truth bounding
boxes.
labels =multibox_target(anchors .unsqueeze(dim =0),
ground_truth .unsqueeze(dim =0))
There are three items in the returned result, all of which are in the tensor format. The third
item contains the labeled classes of the input anchor boxes.
Let‚Äôsanalyzethereturnedclasslabelsbelowbasedonanchorboxandground-truthbound-
ing box positions in the image. First, among all the pairs of anchor boxes and ground-truth
bounding boxes, the IoU of the anchor box ùê¥4and the ground-truth bounding box of the
cat is the largest. Thus, the class of ùê¥4is labeled as the cat. Taking out pairs containing
ùê¥4or the ground-truth bounding box of the cat, among the rest the pair of the anchor box
ùê¥1and the ground-truth bounding box of the dog has the largest IoU. So the class of ùê¥1is
labeledasthedog. Next, weneedtotraversethroughtheremainingthreeunlabeledanchor
boxes:ùê¥0,ùê¥2, andùê¥3. Forùê¥0, the class of the ground-truth bounding box with the largest
IoU is the dog, but the IoU is below the predefined threshold (0.5), so the class is labeled
as background; for ùê¥2, the class of the ground-truth bounding box with the largest IoU is
the cat and the IoU exceedsthe threshold, so the class is labeled as the cat; for ùê¥3, the class
of the ground-truth bounding box with the largest IoU is the cat, but the value is below the
threshold, so the class is labeled as background.
labels[ 2]
tensor([[ 0,1,2,0,2]])
Thesecondreturneditemisamaskvariableoftheshape(batchsize,fourtimesthenumber
of anchor boxes). Every four elements in the mask variable correspond to the four offset
valuesofeachanchorbox. Sincewedonotcareaboutbackgrounddetection,offsetsofthis
negative class should not affect the objective function. Through elementwise multiplica-
tions, zeros in the mask variable will filter out negative class offsets before calculating the
objective function.
labels[ 1]
tensor([[ 0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,1.,1.
‚Ü©!,
1.,1.]])
The first returned item contains the four offset values labeled for each anchor box. Note
that the offsets of negative-class anchor boxes are labeled as zeros.
labels[ 0]
619 Anchor Boxes
tensor([[ -0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,1.40e+00 ,1.00e+01 ,
2.59e+00 ,7.18e+00 ,-1.20e+00 ,2.69e-01 ,1.68e+00 ,-1.57e+00 ,
-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-5.71e-01 ,-1.00e+00 ,
4.17e-06 ,6.26e-01 ]])
14.4.4Predicting Bounding Boxeswith Non-MaximumSuppression
Duringprediction,wegeneratemultipleanchorboxesfortheimageandpredictclassesand
offsets for each of them. A predictedboundingbox is thus obtained according to an anchor
boxwithitspredictedoffset. Belowweimplementthe offset_inverse functionthattakes
in anchors and offset predictions as inputs and applies inverse offset transformations to
return the predicted bounding box coordinates.
#@save
def offset_inverse (anchors, offset_preds):
"""Predict bounding boxes based on anchor boxes with predicted offsets."""
anc =d2l.box_corner_to_center(anchors)
pred_bbox_xy =(offset_preds[:, : 2]*anc[:, 2:]/10)+anc[:, : 2]
pred_bbox_wh =torch .exp(offset_preds[:, 2:]/5)*anc[:, 2:]
pred_bbox =torch .cat((pred_bbox_xy, pred_bbox_wh), axis =1)
predicted_bbox =d2l.box_center_to_corner(pred_bbox)
return predicted_bbox
Whentherearemanyanchorboxes,manysimilar(withsignificantoverlap)predictedbound-
ingboxescanbepotentiallyoutputforsurroundingthesameobject. Tosimplifytheoutput,
we can merge similar predicted bounding boxes that belong to the same object by using
non-maximumsuppression (NMS).
Hereishownon-maximumsuppressionworks. Forapredictedboundingbox ùêµ, theobject
detectionmodelcalculatesthepredictedlikelihoodforeachclass. Denotingby ùëùthelargest
predictedlikelihood,theclasscorrespondingtothisprobabilityisthepredictedclassfor ùêµ.
Specifically,wereferto ùëùastheconfidence (score)ofthepredictedboundingbox ùêµ. Onthe
same image, all the predicted non-background bounding boxes are sorted by confidence in
descendingordertogeneratealist ùêø. Thenwemanipulatethesortedlist ùêøinthefollowing
steps:
1.Select the predicted bounding box ùêµ1with the highest confidence from ùêøas a basis and
removeallnon-basispredictedboundingboxeswhoseIoUwith ùêµ1exceedsapredefined
thresholdùúñfromùêø. At this point, ùêøkeeps the predicted bounding box with the highest
confidence but drops others that are too similar to it. In a nutshell, those with non-
maximum confidence scores are suppressed .
2.Select the predicted bounding box ùêµ2with the second highest confidence from ùêøas
another basis and remove all non-basis predicted bounding boxes whose IoU with ùêµ2
exceedsùúñfromùêø.
3.Repeat the above process until all the predicted bounding boxes in ùêøhave been used as
620 Computer Vision
a basis. At this time, the IoU of any pair of predicted bounding boxes in ùêøis below the
thresholdùúñ; thus, no pair is too similar with each other.
4.Output all the predicted bounding boxes in the list ùêø.
The following nmsfunction sorts confidence scores in descending order and returns their
indices.
#@save
def nms(boxes, scores, iou_threshold):
"""Sort confidence scores of predicted bounding boxes."""
B=torch .argsort(scores, dim =-1, descending =True )
keep =[] # Indices of predicted bounding boxes that will be kept
while B.numel() >0:
i=B[0]
keep .append(i)
ifB.numel() ==1:break
iou =box_iou(boxes[i, :] .reshape( -1,4),
boxes[B[ 1:], :] .reshape( -1,4)).reshape( -1)
inds =torch .nonzero(iou <=iou_threshold) .reshape( -1)
B=B[inds +1]
return torch .tensor(keep, device =boxes .device)
Wedefinethefollowing multibox_detection toapplynon-maximumsuppressiontopre-
dicting bounding boxes. Do not worry if you find the implementation a bit complicated:
we will show how it works with a concrete example right after the implementation.
#@save
def multibox_detection (cls_probs, offset_preds, anchors, nms_threshold =0.5,
pos_threshold =0.009999999 ):
"""Predict bounding boxes using non-maximum suppression."""
device, batch_size =cls_probs .device, cls_probs .shape[ 0]
anchors =anchors .squeeze( 0)
num_classes, num_anchors =cls_probs .shape[ 1], cls_probs .shape[ 2]
out =[]
for iinrange (batch_size):
cls_prob, offset_pred =cls_probs[i], offset_preds[i] .reshape( -1,4)
conf, class_id =torch .max(cls_prob[ 1:], 0)
predicted_bb =offset_inverse(anchors, offset_pred)
keep =nms(predicted_bb, conf, nms_threshold)
# Find all non-`keep` indices and set the class to background
all_idx =torch .arange(num_anchors, dtype =torch .long, device =device)
combined =torch .cat((keep, all_idx))
uniques, counts =combined .unique(return_counts =True )
non_keep =uniques[counts ==1]
all_id_sorted =torch .cat((keep, non_keep))
class_id[non_keep] =-1
class_id =class_id[all_id_sorted]
conf, predicted_bb =conf[all_id_sorted], predicted_bb[all_id_sorted]
# Here `pos_threshold` is a threshold for positive (non-background)
# predictions
below_min_idx =(conf <pos_threshold)
class_id[below_min_idx] =-1
conf[below_min_idx] =1-conf[below_min_idx]
(continues on next page)
621 Anchor Boxes
(continued from previous page)
pred_info =torch .cat((class_id .unsqueeze( 1),
conf .unsqueeze( 1),
predicted_bb), dim =1)
out.append(pred_info)
return torch .stack(out)
Now let‚Äôs apply the above implementations to a concrete example with four anchor boxes.
For simplicity, we assume that the predicted offsets are all zeros. This means that the
predicted bounding boxes are anchor boxes. For each class among the background, dog,
and cat, we also define its predicted likelihood.
anchors =torch .tensor([[ 0.1,0.08 ,0.52 ,0.92 ], [ 0.08 ,0.2,0.56 ,0.95 ],
[0.15 ,0.3,0.62 ,0.91 ], [ 0.55 ,0.2,0.9,0.88 ]])
offset_preds =torch .tensor([ 0]*anchors .numel())
cls_probs =torch .tensor([[ 0]*4,# Predicted background likelihood
[0.9,0.8,0.7,0.1], # Predicted dog likelihood
[0.1,0.2,0.3,0.9]]) # Predicted cat likelihood
We can plot these predicted bounding boxes with their confidence on the image.
fig =d2l.plt.imshow(img)
show_bboxes(fig .axes, anchors *bbox_scale,
['dog=0.9 ','dog=0.8 ','dog=0.7 ','cat=0.9 '])
Nowwecaninvokethe multibox_detection functiontoperformnon-maximumsuppres-
sion, where the threshold is set to 0.5. Note that we add a dimension for examples in the
tensor input.
We can see that the shape of the returned result is (batch size, number of anchor boxes,
6). The six elements in the innermost dimension gives the output information for the same
predicted bounding box. The first element is the predicted class index, which starts from
0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum
suppression. The second element is the confidence of the predicted bounding box. The
remaining four elements are the ¬πùë•,ùë¶¬∫-axis coordinates of the upper-left corner and the
lower-right corner of the predicted bounding box, respectively (range is between 0 and
1).
622 Computer Vision
output =multibox_detection(cls_probs .unsqueeze(dim =0),
offset_preds .unsqueeze(dim =0),
anchors .unsqueeze(dim =0),
nms_threshold =0.5)
output
tensor([[[ 0.00 ,0.90 ,0.10 ,0.08 ,0.52 ,0.92 ],
[1.00 ,0.90 ,0.55 ,0.20 ,0.90 ,0.88 ],
[-1.00 ,0.80 ,0.08 ,0.20 ,0.56 ,0.95 ],
[-1.00 ,0.70 ,0.15 ,0.30 ,0.62 ,0.91 ]]])
Afterremovingthosepredictedboundingboxesofclass-1,wecanoutputthefinalpredicted
bounding box kept by non-maximum suppression.
fig =d2l.plt.imshow(img)
for iinoutput[ 0].detach() .numpy():
ifi[0]==-1:
continue
label =('dog= ','cat= ')[int(i[0])] +str(i[1])
show_bboxes(fig .axes, [torch .tensor(i[ 2:]) *bbox_scale], label)
In practice, we can remove predicted bounding boxes with lower confidence even before
performing non-maximum suppression, thereby reducing computation in this algorithm.
We may also post-process the output of non-maximum suppression, for example, by only
keeping results with higher confidence in the final output.
14.4.5Summary
We generate anchor boxes with different shapes centered on each pixel of the image.
Intersection over union (IoU), also known as Jaccard index, measures the similarity of
two bounding boxes. It is the ratio of their intersection area to their union area.
In a training set, we need two types of labels for each anchor box. One is the class of
the object relevant to the anchor box and the other is the offset of the ground-truth
bounding box relative to the anchor box.
Duringprediction,wecanusenon-maximumsuppression(NMS)toremovesimilarpre-
dicted bounding boxes, thereby simplifying the output.
623 Multiscale Object Detection
21514.4.6Exercises
1.Change values of sizesandratiosin the multibox_prior function. What are the
changes to the generated anchor boxes?
2.Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap
with each other?
3.Modify the variable anchors inSection 14.4.3 andSection 14.4.4 . How do the results
change?
4.Non-maximum suppression is a greedy algorithm that suppresses predicted bounding
boxes by removing them. Is it possible that some of these removed ones are actually
useful? How can this algorithm be modified to suppress softly? You may refer to Soft-
NMS (Bodlaetal., 2017).
5.Rather than being hand-crafted, can non-maximum suppression be learned?
Discussions215.
14.5MultiscaleObject Detection
InSection 14.4 , we generated multiple anchor boxes centered on each pixel of an input
image. Essentially these anchor boxes represent samples of different regions of the image.
However, we may end up with too many anchor boxes to compute if they are generated for
everypixel. Think of a 561728input image. If five anchor boxes with varying shapes
are generated for each pixel as their center, over two million anchor boxes ( 5617285)
need to be labeled and predicted on the image.
14.5.1Multiscale AnchorBoxes
Youmayrealizethatitisnotdifficulttoreduceanchorboxesonanimage. Forinstance,we
canjustuniformlysampleasmallportionofpixelsfromtheinputimagetogenerateanchor
boxes centered on them. In addition, at different scales we can generate different numbers
of anchor boxes of different sizes. Intuitively, smaller objects are more likely to appear on
an image than larger ones. As an example, 11,12, and 22objects can appear on a
22imagein4,2,and1possibleways,respectively. Therefore,whenusingsmalleranchor
boxes to detect smaller objects, we can sample more regions, while for larger objects we
can sample fewer regions.
To demonstrate how to generate anchor boxes at multiple scales, let‚Äôs read an image. Its
height and width are 561 and 728 pixels, respectively.
%matplotlib inline
import torch
(continues on next page)
624 Computer Vision
(continued from previous page)
from d2l import torch asd2l
img =d2l.plt.imread( '../img/catdog.jpg ')
h, w =img.shape[: 2]
h, w
(561,728)
Recall that in Section 7.2 we call a two-dimensional array output of a convolutional layer
a feature map. By defining the feature map shape, we can determine centers of uniformly
sampled anchor boxes on any image.
Thedisplay_anchors functionisdefinedbelow. Wegenerateanchorboxes( anchors )on
the feature map ( fmap) with each unit (pixel) as the anchor box center. Since the ¬πùë•,ùë¶¬∫-
axis coordinate values in the anchor boxes ( anchors ) have been divided by the width and
height of the feature map ( fmap), these values are between 0 and 1, which indicate the
relative positions of anchor boxes in the feature map.
Since centers of the anchor boxes ( anchors ) are spread over all units on the feature map
(fmap), these centers must be uniformly distributed on any input image in terms of their
relative spatial positions. More concretely, given the width and height of the feature map
fmap_wandfmap_h, respectively, the following function will uniformly sample pixels in
fmap_hrows and fmap_wcolumns on any input image. Centered on these uniformly sam-
pled pixels, anchor boxes of scale s(assuming the length of the list sis 1) and different
aspect ratios ( ratios) will be generated.
def display_anchors (fmap_w, fmap_h, s):
d2l.set_figsize()
# Values on the first two dimensions do not affect the output
fmap =torch .zeros(( 1,10, fmap_h, fmap_w))
anchors =d2l.multibox_prior(fmap, sizes =s, ratios =[1,2,0.5])
bbox_scale =torch .tensor((w, h, w, h))
d2l.show_bboxes(d2l .plt.imshow(img) .axes,
anchors[ 0]*bbox_scale)
First, let‚Äôs consider detection of small objects. In order to make it easier to distinguish
when displayed, the anchor boxes with different centers here do not overlap: the anchor
box scale is set to 0.15 and the height and width of the feature map are set to 4. We can see
that the centers of the anchor boxes in 4 rows and 4 columns on the image are uniformly
distributed.
display_anchors(fmap_w =4, fmap_h =4, s=[0.15 ])
Wemoveontoreducetheheightandwidthofthefeaturemapbyhalfanduselargeranchor
boxes to detect larger objects. When the scale is set to 0.4, some anchor boxes will overlap
with each other.
625 Multiscale Object Detection
display_anchors(fmap_w =2, fmap_h =2, s=[0.4])
Finally, we further reduce the height and width of the feature map by half and increase the
anchorboxscaleto0.8. Nowthecenteroftheanchorboxisthecenteroftheimage.
display_anchors(fmap_w =1, fmap_h =1, s=[0.8])
14.5.2Multiscale Detection
Since we have generated multiscale anchor boxes, we will use them to detect objects of
various sizes at different scales. In the following we introduce a CNN-based multiscale
object detection method that we will implement in Section 14.7 .
Atsomescale,saythatwehave ùëêfeaturemapsofshape ‚Ñéùë§. Usingthemethodin Section
14.5.1, we generate ‚Ñéùë§sets of anchor boxes, where each set has ùëéanchor boxes with the
same center. For example, at the first scale in the experiments in Section 14.5.1 , given ten
626 Computer Vision
(numberofchannels) 44featuremaps, wegenerated16setsofanchorboxes, whereeach
set contains 3 anchor boxes with the same center. Next, each anchor box is labeled with
the class and offset based on ground-truth bounding boxes. At the current scale, the object
detection model needs to predict the classes and offsets of ‚Ñéùë§sets of anchor boxes on the
input image, where different sets have different centers.
Assume that the ùëêfeaturemaps here are the intermediate outputs obtained bythe CNN for-
ward propagation based on the input image. Since there are ‚Ñéùë§different spatial positions
on each feature map, the same spatial position can be thought of as having ùëêunits. Ac-
cording to the definition of receptive field in Section 7.2 , theseùëêunits at the same spatial
position of the feature maps have the same receptive field on the input image: they repre-
sent the input image information in the same receptive field. Therefore, we can transform
theùëêunitsofthefeaturemapsatthesamespatialpositionintotheclassesandoffsetsofthe
ùëéanchor boxes generated using this spatial position. In essence, we use the information of
the input image in a certain receptive field to predict the classes and offsets of the anchor
boxes that are close to that receptive field on the input image.
When the feature maps at different layers have varying-size receptive fields on the input
image, they can be used to detect objects of different sizes. For example, we can design a
neural network where units of feature maps that are closer to the output layer have wider
receptive fields, so they can detect larger objects from the input image.
Inanutshell,wecanleveragelayerwiserepresentationsofimagesatmultiplelevelsbydeep
neural networks for multiscale object detection. We will show how this works through a
concrete example in Section 14.7 .
14.5.3Summary
At multiple scales, we can generate anchor boxes with different sizes to detect objects
with different sizes.
By defining the shape of feature maps, we can determine centers of uniformly sampled
anchor boxes on any image.
Weusetheinformationoftheinputimageinacertainreceptivefieldtopredicttheclasses
andoffsetsoftheanchorboxesthatareclosetothatreceptivefieldontheinputimage.
Through deep learning, we can leverage its layerwise representations of images at mul-
tiple levels for multiscale object detection.
14.5.4Exercises
1.According to our discussions in Section 8.1 , deep neural networks learn hierarchical
features with increasing levels of abstraction for images. In multiscale object detection,
do featuremaps at different scales correspond to different levels of abstraction? Whyor
why not?
2.At the first scale ( fmap_w=4, fmap_h=4 ) in the experiments in Section 14.5.1 , generate
uniformly distributed anchor boxes that may overlap.
627 The Object Detection Dataset
2163.Given a feature map variable with shape 1ùëê‚Ñéùë§, whereùëê,‚Ñé, andùë§are the
number of channels, height, and width of the feature maps, respectively. How can you
transform this variable into the classes and offsets of anchor boxes? What is the shape
of the output?
Discussions216.
14.6The Object DetectionDataset
There is no small dataset such as MNIST and Fashion-MNIST in the field of object detec-
tion. In order to quickly demonstrate object detection models, we collected and labeled a
small dataset. First, we took photos of free bananas from our office and generated 1000
banana images with different rotations and sizes. Then we placed each banana image at a
random position on some background image. In the end, we labeled bounding boxes for
those bananas on the images.
14.6.1Downloadingthe Dataset
The banana detection dataset with all the image and csv label files can be downloaded
directly from the Internet.
%matplotlib inline
import os
import pandas aspd
import torch
import torchvision
from d2l import torch asd2l
#@save
d2l.DATA_HUB[ 'banana-detection ']=(
d2l.DATA_URL +'banana-detection.zip ',
'5de26c8fce5ccdea9f91267273464dc968d20d72 ')
14.6.2Readingthe Dataset
We are going to read the banana detection dataset in the read_data_bananas function
below. The dataset includes a csv file for object class labels and ground-truth bounding
box coordinates at the upper-left and lower-right corners.
#@save
def read_data_bananas (is_train =True ):
"""Read the banana detection dataset images and labels."""
data_dir =d2l.download_extract( 'banana-detection ')
csv_fname =os.path .join(data_dir, 'bananas_train 'ifis_train
(continues on next page)
628 Computer Vision
(continued from previous page)
else 'bananas_val ','label.csv ')
csv_data =pd.read_csv(csv_fname)
csv_data =csv_data .set_index( 'img_name ')
images, targets =[], []
for img_name, target incsv_data .iterrows():
images .append(torchvision .io.read_image(
os.path .join(data_dir, 'bananas_train 'ifis_train else
'bananas_val ','images ',f'{img_name }')))
# Here `target` contains (class, upper-left x, upper-left y,
# lower-right x, lower-right y), where all the images have the same
# banana class (index 0)
targets .append( list (target))
return images, torch .tensor(targets) .unsqueeze( 1)/256
By using the read_data_bananas function to read images and labels, the following Ba-
nanasDataset class will allow us to create a customized Dataset instance for loading the
banana detection dataset.
#@save
class BananasDataset (torch .utils .data .Dataset):
"""A customized dataset to load the banana detection dataset."""
def __init__ (self , is_train):
self .features, self .labels =read_data_bananas(is_train)
print ('read '+str(len(self .features)) +(f'training examples 'if
is_train else f'validation examples '))
def __getitem__ (self , idx):
return (self .features[idx] .float(), self .labels[idx])
def __len__ (self ):
return len(self .features)
Finally, we define the load_data_bananas function to return two data iterator instances
for both the training and test sets. For the test dataset, there is no need to read it in random
order.
#@save
def load_data_bananas (batch_size):
"""Load the banana detection dataset."""
train_iter =torch .utils .data .DataLoader(BananasDataset(is_train =True ),
batch_size, shuffle =True )
val_iter =torch .utils .data .DataLoader(BananasDataset(is_train =False ),
batch_size)
return train_iter, val_iter
Let‚Äôs read a minibatch and print the shapes of both images and labels in this minibatch.
The shape of the image minibatch, (batch size, number of channels, height, width), looks
familiar: it is the same as in our earlier image classification tasks. The shape of the label
minibatch is (batch size, ùëö, 5), whereùëöis the largest possible number of bounding boxes
that any image has in the dataset.
629 The Object Detection Dataset
Although computation in minibatches is more efficient, it requires that all the image exam-
ples contain the same number of bounding boxes to form a minibatch via concatenation.
In general, images may have a varying number of bounding boxes; thus, images with fewer
thanùëöboundingboxeswillbepaddedwithillegalboundingboxesuntil ùëöisreached. Then
the label of each bounding box is represented by an array of length 5. The first element in
thearrayistheclassoftheobjectintheboundingbox, where-1indicatesanillegalbound-
ing box for padding. The remaining four elements of the array are the ( ùë•,ùë¶)-coordinate
values of the upper-left corner and the lower-right corner of the bounding box (the range
is between 0 and 1). For the banana dataset, since there is only one bounding box on each
image, we have ùëö=1.
batch_size, edge_size =32,256
train_iter, _ =load_data_bananas(batch_size)
batch =next (iter (train_iter))
batch[ 0].shape, batch[ 1].shape
Downloading ../data /banana -detection .zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/banana -detection .zip...
read 1000 training examples
read 100 validation examples
(torch .Size([ 32,3,256,256]), torch .Size([ 32,1,5]))
14.6.3Demonstration
Let‚Äôs demonstrate ten images with their labeled ground-truth bounding boxes. We can see
that the rotations, sizes, and positions of bananas vary across all these images. Of course,
thisisjustasimpleartificialdataset. Inpractice,real-worlddatasetsareusuallymuchmore
complicated.
imgs =(batch[ 0][:10].permute( 0,2,3,1))/255
axes =d2l.show_images(imgs, 2,5, scale =2)
for ax, label inzip(axes, batch[ 1][:10]):
d2l.show_bboxes(ax, [label[ 0][1:5]*edge_size], colors =['w'])
14.6.4Summary
The banana detection dataset we collected can be used to demonstrate object detection
models.
Thedataloadingforobjectdetectionissimilartothatforimageclassification. However,
inobjectdetectionthelabelsalsocontaininformationofground-truthboundingboxes,
which is missing in image classification.
14.6.5Exercises
630 Computer Vision
2171.Demonstrate other images with ground-truth bounding boxes in the banana detection
dataset. How do they differ with respect to bounding boxes and objects?
2.Saythatwewanttoapplydataaugmentation, suchasrandomcropping, toobjectdetec-
tion. How can it be different from that in image classification? Hint: what if a cropped
image only contains a small portion of an object?
Discussions217.
14.7SingleShotMultiboxDetection
InSection 14.3 ‚ÄìSection 14.6 , we introduced bounding boxes, anchor boxes, multiscale
object detection, and the dataset for object detection. Now we are ready to use such back-
ground knowledge to design an object detection model: single shot multibox detection
(SSD) (Liuet al., 2016). This model is simple, fast, and widely used. Although this is
just one of vast amounts of object detection models, some of the design principles and
implementation details in this section are also applicable to other models.
14.7.1Model
Fig. 14.7.1 provides an overview of the design of single-shot multibox detection. This
modelmainlyconsistsofabasenetworkfollowedbyseveralmultiscalefeaturemapblocks.
Thebasenetworkisforextractingfeaturesfromtheinputimage, soitcanuseadeepCNN.
Forexample,theoriginalsingle-shotmultiboxdetectionpaperadoptsaVGGnetworktrun-
catedbeforetheclassificationlayer( Liuetal.,2016),whileResNethasalsobeencommonly
used. Through our design we can make the base network output larger feature maps so as
togeneratemoreanchorboxesfordetectingsmallerobjects. Subsequently,eachmultiscale
feature map block reduces (e.g., by half) the height and width of the feature maps from the
previous block, and enables each unit of the feature maps to increase its receptive field on
the input image.
631 Single Shot Multibox Detection
Recallthedesignofmultiscaleobjectdetectionthroughlayerwiserepresentationsofimages
by deep neural networks in Section 14.5 . Since multiscale feature maps closer to the top of
Fig. 14.7.1 are smaller but have larger receptive fields, they are suitable for detecting fewer
but larger objects.
In a nutshell, via its base network and several multiscale feature map blocks, single-shot
multibox detection generates a varying number of anchor boxes with different sizes, and
detects varying-size objects by predicting classes and offsets of these anchor boxes (thus
the bounding boxes); thus, this is a multiscale object detection model.
tFig. 14.7.1 As a multiscale object detection model, single-shot multibox detection mainly consists of
a base network followed by several multiscale feature map blocks.
In the following, we will describe the implementation details of different blocks in Fig.
14.7.1. To begin with, we discuss how to implement the class and bounding box predic-
tion.
ClassPrediction Layer
Letthenumberofobjectclassesbe ùëû. Thenanchorboxeshave ùëû¬∏1classes,whereclass0is
background. Atsomescale, supposethattheheightandwidthoffeaturemapsare ‚Ñéandùë§,
respectively. When ùëéanchorboxesaregeneratedwitheachspatialpositionofthesefeature
maps as their center, a total of ‚Ñéùë§ùëéanchor boxes need to be classified. This often makes
classification with fully connected layers infeasible due to likely heavy parametrization
costs. Recall how we used channels of convolutional layers to predict classes in Section
8.3. Single-shot multibox detection uses the same technique to reduce model complex-
ity.
Specifically, the class prediction layer uses a convolutional layer without altering width
or height of feature maps. In this way, there can be a one-to-one correspondence between
outputsandinputsatthesamespatialdimensions(widthandheight)offeaturemaps. More
concretely, channelsof the output feature maps at anyspatial position ( ùë•,ùë¶) represent class
predictionsforalltheanchorboxescenteredon( ùë•,ùë¶)oftheinputfeaturemaps. Toproduce
validpredictions,theremustbe ùëé¬πùëû¬∏1¬∫outputchannels,whereforthesamespatialposition
632 Computer Vision
theoutputchannelwithindex ùëñ¬πùëû¬∏1¬∫¬∏ùëórepresentsthepredictionoftheclass ùëó(0ùëóùëû)
for the anchor box ùëñ(0ùëñ <ùëé).
Belowwedefinesuchaclasspredictionlayer,specifying ùëéandùëûviaarguments num_anchors
andnum_classes , respectively. This layer uses a 33convolutional layer with a padding
of 1. The width and height of the input and output of this convolutional layer remain un-
changed.
%matplotlib inline
import torch
import torchvision
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
def cls_predictor (num_inputs, num_anchors, num_classes):
return nn.Conv2d(num_inputs, num_anchors *(num_classes +1),
kernel_size =3, padding =1)
BoundingBoxPredictionLayer
The design of the bounding box prediction layer is similar to that of the class prediction
layer. The only difference lies in the number of outputs for each anchor box: here we need
to predict four offsets rather than ùëû¬∏1classes.
def bbox_predictor (num_inputs, num_anchors):
return nn.Conv2d(num_inputs, num_anchors *4, kernel_size =3, padding =1)
Concatenating PredictionsforMultiple Scales
As we mentioned, single-shot multibox detection uses multiscale feature maps to generate
anchor boxes and predict their classes and offsets. At different scales, the shapes of feature
mapsorthenumbersofanchorboxescenteredonthesameunitmayvary. Therefore,shapes
of the prediction outputs at different scales may vary.
In the following example, we construct feature maps at two different scales, Y1andY2, for
the same minibatch, where the height and width of Y2are half of those of Y1. Let‚Äôs take
class prediction as an example. Suppose that 5 and 3 anchor boxes are generated for every
unit in Y1andY2, respectively. Suppose further that the number of object classes is 10.
For feature maps Y1andY2the numbers of channels in the class prediction outputs are
5¬π10¬∏1¬∫=55and3¬π10¬∏1¬∫=33, respectively, where either output shape is (batch
size, number of channels, height, width).
def forward (x, block):
return block(x)
(continues on next page)
633 Single Shot Multibox Detection
(continued from previous page)
Y1=forward(torch .zeros(( 2,8,20,20)), cls_predictor( 8,5,10))
Y2=forward(torch .zeros(( 2,16,10,10)), cls_predictor( 16,3,10))
Y1.shape, Y2 .shape
(torch .Size([ 2,55,20,20]), torch .Size([ 2,33,10,10]))
As we can see, except for the batch size dimension, the other three dimensions all have
differentsizes. Toconcatenatethesetwopredictionoutputsformoreefficientcomputation,
we will transform these tensors into a more consistent format.
Notethatthechanneldimensionholdsthepredictionsforanchorboxeswiththesamecenter.
We first move this dimension to the innermost. Since the batch size remains the same for
different scales, we can transform the prediction output into a two-dimensional tensor with
shape (batch size, height widthnumber of channels). Then we can concatenate such
outputs at different scales along dimension 1.
def flatten_pred (pred):
return torch .flatten(pred .permute( 0,2,3,1), start_dim =1)
def concat_preds (preds):
return torch .cat([flatten_pred(p) for pinpreds], dim =1)
In this way, even though Y1andY2have different sizes in channels, heights, and widths,
we can still concatenate these two prediction outputs at two different scales for the same
minibatch.
concat_preds([Y1, Y2]) .shape
torch .Size([ 2,25300 ])
DownsamplingBlock
In order to detect objects at multiple scales, we define the following downsampling block
down_sample_blk thathalvestheheightandwidthofinputfeaturemaps. Infact,thisblock
applies the design of VGG blocks in Section 8.2.1 . More concretely, each downsampling
block consists of two 33convolutional layers with padding of 1 followed by a 22max-
pooling layer with stride of 2. As we know, 33convolutional layers with padding of 1 do
not change the shape of feature maps. However, the subsequent 22max-pooling reduces
the height and width of input feature maps by half. For both input and output feature maps
of this downsampling block, because 12¬∏¬π3 1¬∫¬∏¬π 3 1¬∫=6, each unit in the output
has a 66receptive field on the input. Therefore, the downsampling block enlarges the
receptive field of each unit in its output feature maps.
634 Computer Vision
def down_sample_blk (in_channels, out_channels):
blk =[]
for _inrange (2):
blk.append(nn .Conv2d(in_channels, out_channels,
kernel_size =3, padding =1))
blk.append(nn .BatchNorm2d(out_channels))
blk.append(nn .ReLU())
in_channels =out_channels
blk.append(nn .MaxPool2d( 2))
return nn.Sequential( *blk)
Inthefollowingexample,ourconstructeddownsamplingblockchangesthenumberofinput
channels and halves the height and width of the input feature maps.
forward(torch .zeros(( 2,3,20,20)), down_sample_blk( 3,10)).shape
torch .Size([ 2,10,10,10])
BaseNetworkBlock
The base network block is used to extract features from input images. For simplicity, we
construct a small base network consisting of three downsampling blocks that double the
number of channels at each block. Given a 256256input image, this base network block
outputs 3232feature maps ( 256¬ù23=32).
def base_net ():
blk =[]
num_filters =[3,16,32,64]
for iinrange (len(num_filters) -1):
blk.append(down_sample_blk(num_filters[i], num_filters[i +1]))
return nn.Sequential( *blk)
forward(torch .zeros(( 2,3,256,256)), base_net()) .shape
torch .Size([ 2,64,32,32])
TheCompleteModel
The complete single shot multibox detection model consists of five blocks. The feature
mapsproducedbyeachblockareusedforboth(i)generatinganchorboxesand(ii)predict-
ing classes and offsets of these anchor boxes. Among these five blocks, the first one is the
base network block, the second to the fourth are downsampling blocks, and the last block
uses global max-pooling to reduce both the height and width to 1. Technically, the second
to the fifth blocks are all those multiscale feature map blocks in Fig. 14.7.1 .
635 Single Shot Multibox Detection
def get_blk (i):
ifi==0:
blk =base_net()
elif i==1:
blk =down_sample_blk( 64,128)
elif i==4:
blk =nn.AdaptiveMaxPool2d(( 1,1))
else :
blk =down_sample_blk( 128,128)
return blk
Now we define the forward propagation for each block. Different from in image classifica-
tion tasks, outputs here include (i) CNN feature maps Y, (ii) anchor boxes generated using
Yat the current scale, and (iii) classes and offsets predicted (based on Y) for these anchor
boxes.
def blk_forward (X, blk, size, ratio, cls_predictor, bbox_predictor):
Y=blk(X)
anchors =d2l.multibox_prior(Y, sizes =size, ratios =ratio)
cls_preds =cls_predictor(Y)
bbox_preds =bbox_predictor(Y)
return (Y, anchors, cls_preds, bbox_preds)
Recall that in Fig. 14.7.1 a multiscale feature map block that is closer to the top is for
detectinglargerobjects;thus,itneedstogeneratelargeranchorboxes. Intheaboveforward
propagation,ateachmultiscalefeaturemapblockwepassinalistoftwoscalevaluesviathe
sizesargument of the invoked multibox_prior function (described in Section 14.4 ). In
thefollowing,theintervalbetween0.2and1.05issplitevenlyintofivesectionstodetermine
the smaller scale values at the five blocks: 0.2, 0.37, 0.54, 0.71, and 0.88. Then their larger
scale values are given byp
0.20.37=0.272,p
0.370.54=0.447, and so on.
sizes =[[0.2,0.272 ], [ 0.37 ,0.447 ], [ 0.54 ,0.619 ], [ 0.71 ,0.79 ],
[0.88 ,0.961 ]]
ratios =[[1,2,0.5]]*5
num_anchors =len(sizes[ 0])+len(ratios[ 0])-1
Now we can define the complete model TinySSD as follows.
class TinySSD (nn.Module):
def __init__ (self , num_classes, **kwargs):
super (TinySSD, self ).__init__ (**kwargs)
self .num_classes =num_classes
idx_to_in_channels =[64,128,128,128,128]
for iinrange (5):
# Equivalent to the assignment statement `self.blk_i = get_blk(i)`
setattr (self ,f'blk_ {i}', get_blk(i))
setattr (self ,f'cls_ {i}', cls_predictor(idx_to_in_channels[i],
num_anchors, num_classes))
setattr (self ,f'bbox_ {i}', bbox_predictor(idx_to_in_channels[i],
num_anchors))
(continues on next page)
636 Computer Vision
(continued from previous page)
def forward (self , X):
anchors, cls_preds, bbox_preds =[None ]*5, [None ]*5, [None ]*5
for iinrange (5):
# Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`
X, anchors[i], cls_preds[i], bbox_preds[i] =blk_forward(
X,getattr (self ,f'blk_ {i}'), sizes[i], ratios[i],
getattr (self ,f'cls_ {i}'),getattr (self ,f'bbox_ {i}'))
anchors =torch .cat(anchors, dim =1)
cls_preds =concat_preds(cls_preds)
cls_preds =cls_preds .reshape(
cls_preds .shape[ 0],-1,self .num_classes +1)
bbox_preds =concat_preds(bbox_preds)
return anchors, cls_preds, bbox_preds
We create a model instance and use it to perform forward propagation on a minibatch of
256256images X.
Asshownearlierinthissection,thefirstblockoutputs 3232featuremaps. Recallthatthe
second to fourth downsampling blocks halve the height and width and the fifth block uses
global pooling. Since 4 anchor boxes are generated for each unit along spatial dimensions
of feature maps, at all the five scales a total of ¬π322¬∏162¬∏82¬∏42¬∏1¬∫4=5444anchor
boxes are generated for each image.
net =TinySSD(num_classes =1)
X=torch .zeros(( 32,3,256,256))
anchors, cls_preds, bbox_preds =net(X)
print ('output anchors: ', anchors .shape)
print ('output class preds: ', cls_preds .shape)
print ('output bbox preds: ', bbox_preds .shape)
output anchors: torch .Size([ 1,5444 ,4])
output class preds : torch .Size([ 32,5444 ,2])
output bbox preds: torch .Size([ 32,21776 ])
14.7.2Training
Now we will explain how to train the single shot multibox detection model for object de-
tection.
Readingthe Datasetand Initializing the Model
To begin with, let‚Äôs read the banana detection dataset described in Section 14.6 .
batch_size =32
train_iter, _ =d2l.load_data_bananas(batch_size)
637 Single Shot Multibox Detection
read 1000 training examples
read 100 validation examples
There is only one class in the banana detection dataset. After defining the model, we need
to initialize its parameters and define the optimization algorithm.
device, net =d2l.try_gpu(), TinySSD(num_classes =1)
trainer =torch .optim .SGD(net .parameters(), lr =0.2, weight_decay =5e-4 )
DefiningLoss and EvaluationFunctions
Object detection has two types of losses. The first loss concerns classes of anchor boxes:
its computation can simply reuse the cross-entropy loss function that we used for image
classification. Thesecondlossconcernsoffsetsofpositive(non-background)anchorboxes:
this is a regression problem. For this regression problem, however, here we do not use the
squared loss described in Section 3.1.3 . Instead, we use the ‚Ñì1norm loss, the absolute
value of the difference between the prediction and the ground-truth. The mask variable
bbox_masks filters out negative anchor boxes and illegal (padded) anchor boxes in the loss
calculation. In the end, we sum up the anchor box class loss and the anchor box offset loss
to obtain the loss function for the model.
cls_loss =nn.CrossEntropyLoss(reduction ='none ')
bbox_loss =nn.L1Loss(reduction ='none ')
def calc_loss (cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
batch_size, num_classes =cls_preds .shape[ 0], cls_preds .shape[ 2]
cls =cls_loss(cls_preds .reshape( -1, num_classes),
cls_labels .reshape( -1)).reshape(batch_size, -1).mean(dim =1)
bbox =bbox_loss(bbox_preds *bbox_masks,
bbox_labels *bbox_masks) .mean(dim =1)
return cls +bbox
We can use accuracy to evaluate the classification results. Due to the used ‚Ñì1norm loss
for the offsets, we use the mean absolute error to evaluate the predicted bounding boxes.
These prediction results are obtained from the generated anchor boxes and the predicted
offsets for them.
def cls_eval (cls_preds, cls_labels):
# Because the class prediction results are on the final dimension,
# `argmax` needs to specify this dimension
return float ((cls_preds .argmax(dim =-1).type(
cls_labels .dtype) ==cls_labels) .sum())
def bbox_eval (bbox_preds, bbox_labels, bbox_masks):
return float ((torch .abs((bbox_labels -bbox_preds) *bbox_masks)) .sum())
638 Computer Vision
Trainingthe Model
When training the model, we need to generate multiscale anchor boxes ( anchors ) and pre-
dict their classes ( cls_preds ) and offsets ( bbox_preds ) in the forward propagation. Then
we label the classes ( cls_labels ) and offsets ( bbox_labels ) of such generated anchor
boxes based on the label information Y. Finally, we calculate the loss function using the
predicted and labeled values of the classes and offsets. For concise implementations, eval-
uation of the test dataset is omitted here.
num_epochs, timer =20, d2l .Timer()
animator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],
legend =['class error ','bbox mae '])
net =net.to(device)
for epoch inrange (num_epochs):
# Sum of training accuracy, no. of examples in sum of training accuracy,
# Sum of absolute error, no. of examples in sum of absolute error
metric =d2l.Accumulator( 4)
net.train()
for features, target intrain_iter:
timer .start()
trainer .zero_grad()
X, Y =features .to(device), target .to(device)
# Generate multiscale anchor boxes and predict their classes and
# offsets
anchors, cls_preds, bbox_preds =net(X)
# Label the classes and offsets of these anchor boxes
bbox_labels, bbox_masks, cls_labels =d2l.multibox_target(anchors, Y)
# Calculate the loss function using the predicted and labeled values
# of the classes and offsets
l=calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
bbox_masks)
l.mean() .backward()
trainer .step()
metric .add(cls_eval(cls_preds, cls_labels), cls_labels .numel(),
bbox_eval(bbox_preds, bbox_labels, bbox_masks),
bbox_labels .numel())
cls_err, bbox_mae =1-metric[ 0]/metric[ 1], metric[ 2]/metric[ 3]
animator .add(epoch +1, (cls_err, bbox_mae))
print (f'class err {cls_err :.2e}, bbox mae {bbox_mae :.2e}')
print (f'{len(train_iter .dataset) /timer .stop() :.1f}examples/sec on '
f'{str(device) }')
class err 3.27e-03 , bbox mae 3.08e-03
4279.7 examples /sec on cuda: 0
14.7.3Prediction
During prediction, the goal is to detect all the objects of interest on the image. Below we
read and resize a test image, converting it to a four-dimensional tensor that is required by
convolutional layers.
639 Single Shot Multibox Detection
X=torchvision .io.read_image( '../img/banana.jpg ').unsqueeze( 0).float()
img =X.squeeze( 0).permute( 1,2,0).long()
Usingthe multibox_detection functionbelow,thepredictedboundingboxesareobtained
fromtheanchorboxesandtheirpredictedoffsets. Thennon-maximumsuppressionisused
to remove similar predicted bounding boxes.
def predict (X):
net.eval()
anchors, cls_preds, bbox_preds =net(X .to(device))
cls_probs =F.softmax(cls_preds, dim =2).permute( 0,2,1)
output =d2l.multibox_detection(cls_probs, bbox_preds, anchors)
idx =[ifor i, row inenumerate (output[ 0])ifrow[ 0]!=-1]
return output[ 0, idx]
output =predict(X)
Finally, we display all the predicted bounding boxes with confidence 0.9 or above as out-
put.
def display (img, output, threshold):
d2l.set_figsize(( 5,5))
fig =d2l.plt.imshow(img)
for row inoutput:
score =float (row[ 1])
ifscore <threshold:
continue
h, w =img.shape[: 2]
bbox =[row[ 2:6]*torch .tensor((w, h, w, h), device =row.device)]
d2l.show_bboxes(fig .axes, bbox, '%.2f '%score, 'w')
display(img, output .cpu(), threshold =0.9)
14.7.4Summary
Single shot multibox detection is a multiscale object detection model. Via its base net-
work and several multiscale feature map blocks, single-shot multibox detection gen-
erates a varying number of anchor boxes with different sizes, and detects varying-size
640 Computer Vision
objects by predicting classes and offsets of these anchor boxes (thus the bounding
boxes).
When training the single-shot multibox detection model, the loss function is calculated
based on the predicted and labeled values of the anchor box classes and offsets.
14.7.5Exercises
1.Canyouimprovethesingle-shotmultiboxdetectionbyimprovingthelossfunction? For
example, replace ‚Ñì1norm loss with smooth ‚Ñì1norm loss for the predicted offsets. This
loss function uses a square function around zero for smoothness, which is controlled by
the hyperparameter ùúé:
ùëì¬πùë•¬∫=(
¬πùúéùë•¬∫2¬ù2,ifjùë•j<1¬ùùúé2
jùë•j 0.5¬ùùúé2,otherwise(14.7.1)
Whenùúéis very large, this loss is similar to the ‚Ñì1norm loss. When its value is smaller, the
loss function is smoother.
def smooth_l1 (data, scalar):
out =[]
for iindata:
ifabs(i) <1/(scalar **2):
out.append(((scalar *i)**2)/2)
else :
out.append( abs(i) -0.5 /(scalar **2))
return torch .tensor(out)
sigmas =[10,1,0.5]
lines =['-','--','-.']
x=torch .arange( -2,2,0.1)
d2l.set_figsize()
(continues on next page)
641 Single Shot Multibox Detection
(continued from previous page)
for l, s inzip(lines, sigmas):
y=smooth_l1(x, scalar =s)
d2l.plt.plot(x, y, l, label ='sigma= %.1f '%s)
d2l.plt.legend();
Besides, in the experiment we used cross-entropy loss for class prediction: denoting by ùëùùëó
thepredictedprobabilityfortheground-truthclass ùëó,thecross-entropylossis  logùëùùëó. We
can also use the focal loss ( Linet al., 2017): given hyperparameters ùõæ > 0andùõº> 0, this
loss is defined as:
 ùõº¬π1 ùëùùëó¬∫ùõælogùëùùëó. (14.7.2)
As we can see, increasing ùõæcan effectively reduce the relative loss for well-classified ex-
amples (e.g., ùëùùëó>0.5) so the training can focus more on those difficult examples that are
misclassified.
def focal_loss (gamma, x):
return -(1-x)**gamma *torch .log(x)
x=torch .arange( 0.01 ,1,0.01 )
for l, gamma inzip(lines, [ 0,1,5]):
y=d2l.plt.plot(x, focal_loss(gamma, x), l, label ='gamma= %.1f '%gamma)
d2l.plt.legend();
2.Due to space limitations, we have omitted some implementation details of the single
shotmultiboxdetectionmodelinthissection. Canyoufurtherimprovethemodelinthe
following aspects:
642 Computer Vision
2181.When an object is much smaller compared with the image, the model could resize
the input image bigger.
2.There are typically a vast number of negative anchor boxes. To make the class dis-
tribution more balanced, we could downsample negative anchor boxes.
3.Inthelossfunction,assigndifferentweighthyperparameterstotheclasslossandthe
offset loss.
4.Useothermethodstoevaluatetheobjectdetectionmodel, suchasthoseinthesingle
shot multibox detection paper ( Liuetal., 2016).
Discussions218.
14.8Region-basedCNNs (R-CNNs)
Besides single shot multibox detection described in Section 14.7 , region-based CNNs or
regions with CNN features (R-CNNs) are also among many pioneering approaches of ap-
plying deep learning to object detection ( Girshick et al., 2014). In this section, we will
introducetheR-CNNanditsseriesofimprovements: thefastR-CNN( Girshick,2015 ),the
faster R-CNN ( Renet al., 2015), and the mask R-CNN ( Heet al., 2017). Due to limited
space, we will only focus on the design of these models.
14.8.1R-CNNs
TheR-CNNfirstextractsmany(e.g.,2000) regionproposals fromtheinputimage(e.g.,an-
chorboxescanalsobeconsideredasregionproposals),labelingtheirclassesandbounding
boxes (e.g., offsets).
(Girshicketal., 2014)
Then a CNN is used to perform forward propagation on each region proposal to extract
its features. Next, features of each region proposal are used for predicting the class and
bounding box of this region proposal.
tFig. 14.8.1 The R-CNN model.
Fig.14.8.1 showstheR-CNNmodel. Moreconcretely,theR-CNNconsistsofthefollowing
four steps:
643 Region-based CNNs (R-CNNs)
1.Performselective search to extract multiple high-quality region proposals on the input
image (Uijlingset al., 2013). These proposed regions are usually selected at multiple
scales with different shapes and sizes. Each region proposal will be labeled with a class
and a ground-truth bounding box.
2.Choose a pretrained CNN and truncate it before the output layer. Resize each region
proposal to the input size required by the network, and output the extracted features for
the region proposal through forward propagation.
3.Take the extracted features and labeled class of each region proposal as an example.
Train multiple support vector machines to classify objects, where each support vector
machine individually determines whether the example contains a specific class.
4.Take the extracted features and labeled bounding box of each region proposal as an
example. Train a linear regression model to predict the ground-truth bounding box.
Although the R-CNN model uses pretrained CNNs to effectively extract image features, it
is slow. Imagine that we select thousands of region proposals from a single input image:
this requires thousands of CNN forward propagations to perform object detection. This
massive computing load makes it infeasible to widely use R-CNNs in real-world applica-
tions.
14.8.2FastR-CNN
ThemainperformancebottleneckofanR-CNNliesintheindependentCNNforwardprop-
agationforeachregionproposal,withoutsharingcomputation. Sincetheseregionsusually
have overlaps, independent feature extractions lead to much repeated computation. One
of the major improvements of the fast R-CNN from the R-CNN is that the CNN forward
propagation is only performed on the entire image ( Girshick, 2015 ).
tFig. 14.8.2 The fast R-CNN model.
Fig.14.8.2 describesthefastR-CNNmodel. Itsmajorcomputationsareasfollows:
1.Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extrac-
tion is the entire image, rather than individual region proposals. Moreover, this CNN is
trainable. Given an input image, let the shape of the CNN output be 1ùëê‚Ñé1ùë§1.
644 Computer Vision
2.Suppose that selective search generates ùëõregion proposals. These region proposals (of
differentshapes)markregionsofinterest(ofdifferentshapes)ontheCNNoutput. Then
these regions of interest further extract features of the same shape (say height ‚Ñé2and
widthùë§2are specified) in order to be easily concatenated. To achieve this, the fast R-
CNN introduces the region of interest (RoI) pooling layer: the CNN output and region
proposalsareinputintothislayer,outputtingconcatenatedfeaturesofshape ùëõùëê‚Ñé2
ùë§2that are further extracted for all the region proposals.
3.Using a fully connected layer, transform the concatenated features into an output of
shapeùëõùëë, whereùëëdepends on the model design.
4.Predict the class and bounding box for each of the ùëõregion proposals. More concretely,
in class and bounding box prediction, transform the fully connected layer output into
an output of shape ùëõùëû(ùëûis the number of classes) and an output of shape ùëõ4,
respectively. The class prediction uses softmax regression.
TheregionofinterestpoolinglayerproposedinthefastR-CNNisdifferentfromthepooling
layer introduced in Section 7.5 . In the pooling layer, we indirectly control the output shape
byspecifyingsizesofthepoolingwindow, padding, andstride. Incontrast, wecandirectly
specify the output shape in the region of interest pooling layer.
For example, let‚Äôs specify the output height and width for each region as ‚Ñé2andùë§2, re-
spectively. For any region of interest window of shape ‚Ñéùë§, this window is divided
into a‚Ñé2ùë§2grid of subwindows, where the shape of each subwindow is approximately
¬π‚Ñé¬ù‚Ñé2¬∫¬πùë§¬ùùë§2¬∫. Inpractice, theheightandwidthofanysubwindowshallberoundedup,
and the largest element shall be used as output of the subwindow. Therefore, the region of
interest pooling layer can extract features of the same shape even when regions of interest
have different shapes.
As an illustrative example, in Fig. 14.8.3 , the upper-left 33region of interest is selected
ona 44input. Forthisregionofinterest,weusea 22regionofinterestpoolinglayerto
obtain a 22output. Note that each of the four divided subwindows contains elements 0,
1, 4, and 5 (5 is the maximum); 2 and 6 (6 is the maximum); 8 and 9 (9 is the maximum);
and 10.
tFig. 14.8.3 A 22 region of interest pooling layer.
Below we demonstrate the computation of the region of interest pooling layer. Suppose
that the height and width of the CNN-extracted features Xare both 4, and there is only a
single channel.
import torch
import torchvision
(continues on next page)
645 Region-based CNNs (R-CNNs)
(continued from previous page)
X=torch .arange( 16.).reshape( 1,1,4,4)
X
tensor([[[[ 0.,1.,2.,3.],
[4.,5.,6.,7.],
[8.,9.,10.,11.],
[12.,13.,14.,15.]]]])
Let‚Äôs further suppose that the height and width of the input image are both 40 pixels and
that selective search generates two region proposals on this image. Each region proposal is
expressed as five elements: its object class followed by the ¬πùë•,ùë¶¬∫-coordinates of its upper-
left and lower-right corners.
rois =torch .Tensor([[ 0,0,0,20,20], [ 0,0,10,30,30]])
Because the height and width of Xare1¬ù10of the height and width of the input image,
the coordinates of the tworegion proposals are multiplied by0.1 according to the specified
spatial_scale argument. Then the two regions of interest are marked on XasX[:, :,
0:3, 0:3] andX[:, :, 1:4, 0:4] , respectively. Finally in the 22region of interest
pooling, each region of interest is divided into a grid of sub-windows to further extract
features of the same shape 22.
torchvision .ops.roi_pool(X, rois, output_size =(2,2), spatial_scale =0.1)
tensor([[[[ 5.,6.],
[9.,10.]]],
[[[ 9.,11.],
[13.,15.]]]])
14.8.3FasterR-CNN
To be more accurate in object detection, the fast R-CNN model usually has to generate
a lot of region proposals in selective search. To reduce region proposals without loss of
accuracy, the faster R-CNN proposes to replace selective search with a region proposal
network (Renetal., 2015).
Fig. 14.8.4 shows the faster R-CNN model. Compared with the fast R-CNN, the faster R-
CNN only changes the region proposal method from selective search to a region proposal
network. The rest of the model remain unchanged. The region proposal network works in
the following steps:
1.Use a 33convolutional layer with padding of 1 to transform the CNN output to a
646 Computer Vision
tFig. 14.8.4 The faster R-CNN model.
new output with ùëêchannels. In this way, each unit along the spatial dimensions of the
CNN-extracted feature maps gets a new feature vector of length ùëê.
2.Centered on each pixel of the feature maps, generate multiple anchor boxes of different
scales and aspect ratios and label them.
3.Using the length- ùëêfeature vector at the center of each anchor box, predict the binary
class (background or objects) and bounding box for this anchor box.
4.Consider those predicted bounding boxes whose predicted classes are objects. Remove
overlappedresultsusingnon-maximumsuppression. Theremainingpredictedbounding
boxesforobjectsaretheregionproposalsrequiredbytheregionofinterestpoolinglayer.
It is worth noting that, as part of the faster R-CNN model, the region proposal network
is jointly trained with the rest of the model. In other words, the objective function of the
faster R-CNN includes not only the class and bounding box prediction in object detection,
butalsothebinaryclassandboundingboxpredictionofanchorboxesintheregionproposal
network. As a result of the end-to-end training, the region proposal network learns how to
generate high-quality region proposals, so as to stay accurate in object detection with a
reduced number of region proposals that are learned from data.
14.8.4Mask R-CNN
In the training dataset, if pixel-level positions of object are also labeled on images, the
mask R-CNN can effectively leverage such detailed labels to further improve the accuracy
of object detection ( Heetal., 2017).
As shown in Fig. 14.8.5 , the mask R-CNN is modified based on the faster R-CNN. Specif-
ically, the mask R-CNN replaces the region of interest pooling layer with the region of
interest (RoI) alignment layer. This region of interest alignment layer uses bilinear inter-
polation to preserve the spatial information on the feature maps, which is more suitable for
pixel-level prediction. The output of this layer contains feature maps of the same shape for
all the regions of interest. They are used to predict not only the class and bounding box
for each region of interest, but also the pixel-level position of the object through an addi-
tional fully convolutional network. More details on using a fully convolutional network to
647 Region-based CNNs (R-CNNs)
tFig. 14.8.5 The mask R-CNN model.
219predict pixel-level semantics of an image will be provided in subsequent sections of this
chapter.
14.8.5Summary
TheR-CNNextractsmanyregionproposalsfromtheinputimage,usesaCNNtoperform
forward propagation on each region proposal to extract its features, then uses these
features to predict the class and bounding box of this region proposal.
One of the majorimprovements of the fastR-CNN from the R-CNNis that the CNN for-
ward propagation is only performed on the entire image. It also introduces the region
of interest pooling layer, so that features of the same shape can be further extracted
for regions of interest that have different shapes.
The faster R-CNN replaces the selective search used in the fast R-CNN with a jointly
trained region proposal network, so that the former can stay accurate in object detec-
tion with a reduced number of region proposals.
Based on the faster R-CNN, the mask R-CNN additionally introduces a fully convolu-
tional network, so as to leverage pixel-level labels to further improve the accuracy of
object detection.
14.8.6Exercises
1.Canweframeobjectdetectionasasingleregressionproblem,suchaspredictingbound-
ing boxes and class probabilities? You may refer to the design of the YOLO model
(Redmonetal., 2016).
2.Compare single shot multibox detection with the methods introduced in this section.
What are their major differences? You may refer to Figure 2 of Zhao etal.(2019).
Discussions219.
648 Computer Vision
22014.9Semantic Segmentation and the Dataset
When discussing object detection tasks in Section 14.3 ‚ÄìSection 14.8 , rectangular bound-
ing boxes are used to label and predict objects in images. This section will discuss the
problem of semantic segmentation , which focuses on how to divide an image into regions
belonging to different semantic classes. Different from object detection, semantic seg-
mentation recognizes and understands what are in images in pixel level: its labeling and
prediction of semantic regions are in pixel level. Fig. 14.9.1 shows the labels of the dog,
cat, and background of the image in semantic segmentation. Compared with in object de-
tection, the pixel-level borders labeled in semantic segmentation are obviously more fine-
grained.
tFig. 14.9.1 Labels of the dog, cat, and background of the image in semantic segmentation.
14.9.1ImageSegmentation and Instance Segmentation
Therearealsotwoimportanttasksinthefieldofcomputervisionthataresimilartoseman-
tic segmentation, namely image segmentation and instance segmentation. We will briefly
distinguish them from semantic segmentation as follows.
Image segmentation divides an image into several constituent regions. The methods for
this type of problem usually make use of the correlation between pixels in the image.
It does not need label information about image pixels during training, and it cannot
guarantee that the segmented regions will have the semantics that we hope to obtain
during prediction. Taking the image in Fig. 14.9.1 as input, image segmentation may
divide the dog into two regions: one covers the mouth and eyes which are mainly
black, and the other covers the rest of the body which is mainly yellow.
Instancesegmentation isalsocalled simultaneousdetectionandsegmentation . Itstudies
how to recognize the pixel-level regions of each object instance in an image. Differ-
ent from semantic segmentation, instance segmentation needs to distinguish not only
semantics, but also different object instances. For example, if there are two dogs in
the image, instance segmentation needs to distinguish which of the two dogs a pixel
belongs to.
14.9.2The PascalVOC2012Semantic Segmentation Dataset
On of the most important semantic segmentation dataset is Pascal VOC2012220. In the
649 Semantic Segmentation and the Dataset
following, we will take a look at this dataset.
%matplotlib inline
import os
import torch
import torchvision
from d2l import torch asd2l
The tar file of the dataset is about 2 GB, so it may take a while to download the file. The
extracted dataset is located at ../data/VOCdevkit/VOC2012 .
#@save
d2l.DATA_HUB[ 'voc2012 ']=(d2l .DATA_URL +'VOCtrainval_11-May-2012.tar ',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49 ')
voc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')
Downloading ../data /VOCtrainval_11 -May-2012. tar from http ://d2l-data .s3-
‚Ü©!accelerate .amazonaws .com/VOCtrainval_11 -May-2012. tar...
After entering the path ../data/VOCdevkit/VOC2012 , we can see the different compo-
nents of the dataset. The ImageSets/Segmentation path contains text files that specify
training and test samples, while the JPEGImages andSegmentationClass paths store the
input image and label for each example, respectively. The label here is also in the im-
age format, with the same size as its labeled input image. Besides, pixels with the same
color in any label image belong to the same semantic class. The following defines the
read_voc_images functiontoreadalltheinputimagesandlabelsintothememory.
#@save
def read_voc_images (voc_dir, is_train =True ):
"""Read all VOC feature and label images."""
txt_fname =os.path .join(voc_dir, 'ImageSets ','Segmentation ',
'train.txt 'ifis_train else 'val.txt ')
mode =torchvision .io.image .ImageReadMode .RGB
with open (txt_fname, 'r')asf:
images =f.read() .split()
features, labels =[], []
for i, fname inenumerate (images):
features .append(torchvision .io.read_image(os .path .join(
voc_dir, 'JPEGImages ',f'{fname }.jpg ')))
labels .append(torchvision .io.read_image(os .path .join(
voc_dir, 'SegmentationClass ',f'{fname }.png '), mode))
return features, labels
train_features, train_labels =read_voc_images(voc_dir, True )
Wedrawthefirstfiveinputimagesandtheirlabels. Inthelabelimages,whiteandblackrep-
resent borders and background, respectively, while the other colors correspond to different
classes.
650 Computer Vision
n=5
imgs =train_features[:n] +train_labels[:n]
imgs =[img .permute( 1,2,0)for img inimgs]
d2l.show_images(imgs, 2, n);
Next,weenumeratetheRGBcolorvaluesandclassnamesforallthelabelsinthisdataset.
#@save
VOC_COLORMAP =[[0,0,0], [ 128,0,0], [ 0,128,0], [ 128,128,0],
[0,0,128], [ 128,0,128], [ 0,128,128], [ 128,128,128],
[64,0,0], [ 192,0,0], [ 64,128,0], [ 192,128,0],
[64,0,128], [ 192,0,128], [ 64,128,128], [ 192,128,128],
[0,64,0], [ 128,64,0], [ 0,192,0], [ 128,192,0],
[0,64,128]]
#@save
VOC_CLASSES =['background ','aeroplane ','bicycle ','bird ','boat ',
'bottle ','bus','car','cat','chair ','cow',
'diningtable ','dog','horse ','motorbike ','person ',
'potted plant ','sheep ','sofa ','train ','tv/monitor ']
With the two constants defined above, we can conveniently find the class index for each
pixel in a label. We define the voc_colormap2label function to build the mapping from
the above RGB color values to class indices, and the voc_label_indices function to map
any RGB values to their class indices in this Pascal VOC2012 dataset.
#@save
def voc_colormap2label ():
"""Build the mapping from RGB to class indices for VOC labels."""
colormap2label =torch .zeros( 256 **3, dtype =torch .long)
for i, colormap inenumerate (VOC_COLORMAP):
colormap2label[
(colormap[ 0]*256 +colormap[ 1])*256 +colormap[ 2]]=i
return colormap2label
#@save
def voc_label_indices (colormap, colormap2label):
"""Map any RGB values in VOC labels to their class indices."""
colormap =colormap .permute( 1,2,0).numpy() .astype( 'int32 ')
(continues on next page)
651 Semantic Segmentation and the Dataset
(continued from previous page)
idx =((colormap[:, :, 0]*256 +colormap[:, :, 1])*256
+colormap[:, :, 2])
return colormap2label[idx]
For example, in the first example image, the class index for the front part of the airplane is
1, while the background index is 0.
y=voc_label_indices(train_labels[ 0], voc_colormap2label())
y[105:115,130:140], VOC_CLASSES[ 1]
(tensor([[ 0,0,0,0,0,0,0,0,0,1],
[0,0,0,0,0,0,0,1,1,1],
[0,0,0,0,0,0,1,1,1,1],
[0,0,0,0,0,1,1,1,1,1],
[0,0,0,0,0,1,1,1,1,1],
[0,0,0,0,1,1,1,1,1,1],
[0,0,0,0,0,1,1,1,1,1],
[0,0,0,0,0,1,1,1,1,1],
[0,0,0,0,0,0,1,1,1,1],
[0,0,0,0,0,0,0,0,1,1]]),
'aeroplane ')
Data Preprocessing
In previous experiments such as in Section 8.1 ‚ÄìSection 8.4 , images are rescaled to fit the
model‚Äôsrequiredinputshape. However,insemanticsegmentation,doingsorequiresrescal-
ingthepredictedpixelclassesbacktotheoriginalshapeoftheinputimage. Suchrescaling
may be inaccurate, especially for segmented regions with different classes. To avoid this
issue, we crop the image to a fixedshape instead of rescaling. Specifically, using random
cropping from image augmentation, we crop the same area of the input image and the la-
bel.
#@save
def voc_rand_crop (feature, label, height, width):
"""Randomly crop both feature and label images."""
rect =torchvision .transforms .RandomCrop .get_params(
feature, (height, width))
feature =torchvision .transforms .functional .crop(feature, *rect)
label =torchvision .transforms .functional .crop(label, *rect)
return feature, label
imgs =[]
for _inrange (n):
imgs +=voc_rand_crop(train_features[ 0], train_labels[ 0],200,300)
imgs =[img .permute( 1,2,0)for img inimgs]
d2l.show_images(imgs[:: 2]+imgs[ 1::2],2, n);
652 Computer Vision
Custom Semantic Segmentation DatasetClass
We define a custom semantic segmentation dataset class VOCSegDataset by inheriting the
Dataset class provided by high-level APIs. By implementing the __getitem__ function,
we can arbitrarily access the input image indexed as idxin the dataset and the class index
of each pixel in this image. Since some images in the dataset have a smaller size than
the output size of random cropping, these examples are filtered out by a custom filter
function. In addition, we also define the normalize_image function to standardize the
values of the three RGB channels of input images.
#@save
class VOCSegDataset (torch .utils .data .Dataset):
"""A customized dataset to load the VOC dataset."""
def __init__ (self , is_train, crop_size, voc_dir):
self .transform =torchvision .transforms .Normalize(
mean =[0.485 ,0.456 ,0.406 ], std =[0.229 ,0.224 ,0.225 ])
self .crop_size =crop_size
features, labels =read_voc_images(voc_dir, is_train =is_train)
self .features =[self .normalize_image(feature)
for feature inself .filter(features)]
self .labels =self .filter(labels)
self .colormap2label =voc_colormap2label()
print ('read '+str(len(self .features)) +'examples ')
def normalize_image (self , img):
return self .transform(img .float() /255)
def filter (self , imgs):
return [img for img inimgs if(
img.shape[ 1]>=self .crop_size[ 0]and
img.shape[ 2]>=self .crop_size[ 1])]
def __getitem__ (self , idx):
feature, label =voc_rand_crop( self .features[idx], self .labels[idx],
*self .crop_size)
return (feature, voc_label_indices(label, self .colormap2label))
def __len__ (self ):
return len(self .features)
653 Semantic Segmentation and the Dataset
Readingthe Dataset
We use the custom VOCSegDatase t class to create instances of the training set and test set,
respectively. Suppose that we specify that the output shape of randomly cropped images is
320480. Below we can view the number of examples that are retained in the training set
and test set.
crop_size =(320,480)
voc_train =VOCSegDataset( True , crop_size, voc_dir)
voc_test =VOCSegDataset( False , crop_size, voc_dir)
read 1114 examples
read 1078 examples
Setting the batch size to 64, we define the data iterator for the training set. Let‚Äôs print
the shape of the first minibatch. Different from in image classification or object detection,
labels here are three-dimensional tensors.
batch_size =64
train_iter =torch .utils .data .DataLoader(voc_train, batch_size, shuffle =True ,
drop_last =True ,
num_workers =d2l.get_dataloader_workers())
for X, Y intrain_iter:
print (X.shape)
print (Y.shape)
break
torch .Size([ 64,3,320,480])
torch .Size([ 64,320,480])
PuttingIt All Together
Finally, we define the following load_data_voc function to download and read the Pascal
VOC2012 semantic segmentation dataset. It returns data iterators for both the training and
test datasets.
#@save
def load_data_voc (batch_size, crop_size):
"""Load the VOC semantic segmentation dataset."""
voc_dir =d2l.download_extract( 'voc2012 ', os .path .join(
'VOCdevkit ','VOC2012 '))
num_workers =d2l.get_dataloader_workers()
train_iter =torch .utils .data .DataLoader(
VOCSegDataset( True , crop_size, voc_dir), batch_size,
shuffle =True , drop_last =True , num_workers =num_workers)
test_iter =torch .utils .data .DataLoader(
VOCSegDataset( False , crop_size, voc_dir), batch_size,
drop_last =True , num_workers =num_workers)
return train_iter, test_iter
654 Computer Vision
22114.9.3Summary
Semantic segmentation recognizes and understands what are in an image in pixel level
by dividing the image into regions belonging to different semantic classes.
One of the most important semantic segmentation dataset is Pascal VOC2012.
In semantic segmentation, since the input image and label correspond one-to-one on the
pixel, the input image is randomly cropped to a fixed shape rather than rescaled.
14.9.4Exercises
1.How can semantic segmentation be applied in autonomous vehicles and medical image
diagnostics? Can you think of other applications?
2.Recall the descriptions of data augmentation in Section 14.1 . Which of the image aug-
mentation methods used in image classification would be infeasible to be applied in
semantic segmentation?
Discussions221.
14.10TransposedConvolution
The CNN layers we have seen so far, such as convolutional layers ( Section 7.2 ) and pool-
ing layers ( Section 7.5 ), typically reduce (downsample) the spatial dimensions (height and
width) of the input, or keep them unchanged. In semantic segmentation that classifies at
pixel-level, it will be convenient if the spatial dimensions of the input and output are the
same. For example, the channel dimension at one output pixel can hold the classification
results for the input pixel at the same spatial position.
To achieve this, especially after the spatial dimensions are reduced by CNN layers, we
can use another type of CNN layers that can increase (upsample) the spatial dimensions of
intermediatefeaturemaps. Inthissection,wewillintroduce transposedconvolution ,which
is also called fractionally-strided convolution (Dumoulin and Visin, 2016 ), for reversing
downsampling operations by the convolution.
import torch
from torch import nn
from d2l import torch asd2l
14.10.1Basic Operation
Ignoringchannelsfornow,let‚Äôsbeginwiththebasictransposedconvolutionoperationwith
stride of1 andno padding. Supposethat weare givena ùëõ‚Ñéùëõùë§inputtensorand a ùëò‚Ñéùëòùë§
kernel. Sliding the kernel window with stride of 1 for ùëõùë§times in each row and ùëõ‚Ñétimes
655 Transposed Convolution
in each column yields a total of ùëõ‚Ñéùëõùë§intermediate results. Each intermediate result is
a¬πùëõ‚Ñé¬∏ùëò‚Ñé 1¬∫¬πùëõùë§¬∏ùëòùë§ 1¬∫tensor that are initialized as zeros. To compute each
intermediate tensor, each element in the input tensor is multiplied by the kernel so that
the resulting ùëò‚Ñéùëòùë§tensor replaces a portion in each intermediate tensor. Note that the
position of the replaced portion in each intermediate tensor corresponds to the position of
the element in the input tensor used for the computation. In the end, all the intermediate
results are summed over to produce the output.
As an example, Fig. 14.10.1 illustrates how transposed convolution with a 22kernel is
computed for a 22input tensor.
tFig. 14.10.1 Transposed convolution with a 2 2 kernel. The shaded portions are a portion of an
intermediate tensor as well as the input and kernel tensor elements used for the
computation.
We can implement this basic transposed convolution operation trans_conv for a input
matrix Xand a kernel matrix K.
def trans_conv (X, K):
h, w =K.shape
Y=torch .zeros((X .shape[ 0]+h-1, X.shape[ 1]+w-1))
for iinrange (X.shape[ 0]):
for jinrange (X.shape[ 1]):
Y[i: i +h, j: j +w]+=X[i, j] *K
return Y
In contrast to the regular convolution (in Section 7.2 ) thatreduces input elements via the
kernel, the transposed convolution broadcasts input elements via the kernel, thereby pro-
ducing an output that is larger than the input. We can construct the input tensor Xand the
kernel tensor KfromFig. 14.10.1 to validate the output of the above implementation of the
basic two-dimensional transposed convolution operation.
X=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])
K=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])
trans_conv(X, K)
tensor([[ 0.,0.,1.],
[0.,4.,6.],
[4.,12.,9.]])
656 Computer Vision
Alternatively, when the input Xand kernel Kare both four-dimensional tensors, we can use
high-level APIs to obtain the same results.
X, K =X.reshape( 1,1,2,2), K .reshape( 1,1,2,2)
tconv =nn.ConvTranspose2d( 1,1, kernel_size =2, bias =False )
tconv .weight .data =K
tconv(X)
tensor([[[[ 0.,0.,1.],
[0.,4.,6.],
[4.,12.,9.]]]], grad_fn =<ConvolutionBackward0 >)
14.10.2Padding,Strides, and Multiple Channels
Different from in the regular convolution where padding is applied to input, it is applied to
output in the transposed convolution. For example, when specifying the padding number
on either side of the height and width as 1, the first and last rows and columns will be
removed from the transposed convolution output.
tconv =nn.ConvTranspose2d( 1,1, kernel_size =2, padding =1, bias =False )
tconv .weight .data =K
tconv(X)
tensor([[[[ 4.]]]], grad_fn =<ConvolutionBackward0 >)
In the transposed convolution, strides are specified for intermediate results (thus output),
not for input. Using the same input and kernel tensors from Fig. 14.10.1 , changing the
stride from 1 to 2 increases both the height and weight of intermediate tensors, hence the
output tensor in Fig. 14.10.2 .
The following code snippet can validate the transposed convolution output for stride of 2
inFig. 14.10.2 .
tconv =nn.ConvTranspose2d( 1,1, kernel_size =2, stride =2, bias =False )
tconv .weight .data =K
tconv(X)
tensor([[[[ 0.,0.,0.,1.],
[0.,0.,2.,3.],
[0.,2.,0.,3.],
[4.,6.,6.,9.]]]], grad_fn =<ConvolutionBackward0 >)
For multiple input and output channels, the transposed convolution works in the same way
as the regular convolution. Suppose that the input has ùëêùëñchannels, and that the transposed
convolution assigns a ùëò‚Ñéùëòùë§kernel tensor to each input channel. When multiple output
channels are specified, we will have a ùëêùëñùëò‚Ñéùëòùë§kernel for each output channel.
657 Transposed Convolution
tFig. 14.10.2 Transposed convolution with a 2 2 kernel with stride of 2. The shaded portions are a
portion of an intermediate tensor as well as the input and kernel tensor elements used for
the computation.
As in all, if we feed Xinto a convolutional layer ùëìto output Y=ùëì¬πX¬∫and create a trans-
posed convolutional layer ùëîwith the same hyperparameters as ùëìexcept for the number of
output channels being the number of channels in X, thenùëî¬πùëå¬∫will have the same shape as
X. This can be illustrated in the following example.
X=torch .rand(size =(1,10,16,16))
conv =nn.Conv2d( 10,20, kernel_size =5, padding =2, stride =3)
tconv =nn.ConvTranspose2d( 20,10, kernel_size =5, padding =2, stride =3)
tconv(conv(X)) .shape ==X.shape
True
14.10.3Connectionto Matrix Transposition
The transposed convolution is named after the matrix transposition. To explain, let‚Äôs first
seehowtoimplementconvolutionsusingmatrixmultiplications. Intheexamplebelow,we
define a 33input Xand a 22convolution kernel K, and then use the corr2dfunction
to compute the convolution output Y.
X=torch .arange( 9.0).reshape( 3,3)
K=torch .tensor([[ 1.0,2.0], [ 3.0,4.0]])
Y=d2l.corr2d(X, K)
Y
tensor([[ 27.,37.],
[57.,67.]])
Next, we rewrite the convolution kernel Kas a sparse weight matrix Wcontaining a lot of
658 Computer Vision
zeros. The shape of the weight matrix is ( 4,9), where the non-zero elements come from
the convolution kernel K.
def kernel2matrix (K):
k, W =torch .zeros( 5), torch .zeros(( 4,9))
k[:2], k[ 3:5]=K[0, :], K[ 1, :]
W[0, :5], W[ 1,1:6], W[ 2,3:8], W[ 3,4:]=k, k, k, k
return W
W=kernel2matrix(K)
W
tensor([[ 1.,2.,0.,3.,4.,0.,0.,0.,0.],
[0.,1.,2.,0.,3.,4.,0.,0.,0.],
[0.,0.,0.,1.,2.,0.,3.,4.,0.],
[0.,0.,0.,0.,1.,2.,0.,3.,4.]])
Concatenate the input Xrow by row to get a vector of length 9. Then the matrix multiplica-
tion of Wand the vectorized Xgives a vector of length 4. After reshaping it, we can obtain
thesameresult Yfromtheoriginalconvolutionoperationabove: wejustimplementedcon-
volutions using matrix multiplications.
Y==torch .matmul(W, X .reshape( -1)).reshape( 2,2)
tensor([[ True ,True ],
[True ,True ]])
Likewise, we can implement transposed convolutions using matrix multiplications. In the
following example, we take the 22output Yfrom the above regular convolution as input
to the transposed convolution. To implement this operation by multiplying matrices, we
only need to transpose the weight matrix Wwith the new shape ¬π9,4¬∫.
Z=trans_conv(Y, K)
Z==torch .matmul(W .T, Y .reshape( -1)).reshape( 3,3)
tensor([[ True ,True ,True ],
[True ,True ,True ],
[True ,True ,True ]])
Consider implementing the convolution by multiplying matrices. Given an input vector
xand a weight matrix W, the forward propagation function of the convolution can be
implemented by multiplying its input with the weight matrix and outputting a vector y=
Wx. Since backpropagation follows the chain rule and rxy=W>, the backpropagation
functionoftheconvolutioncanbeimplementedbymultiplyingitsinputwiththetransposed
weight matrix W>. Therefore, the transposed convolutional layer can just exchange the
forward propagation function and the backpropagation function of the convolutional layer:
659 Fully Convolutional Networks
222itsforwardpropagationandbackpropagationfunctionsmultiplytheirinputvectorwith W>
andW, respectively.
14.10.4Summary
In contrast to the regular convolution that reduces input elements via the kernel, the
transposed convolution broadcasts input elements via the kernel, thereby producing
an output that is larger than the input.
If we feed Xinto a convolutional layer ùëìto output Y=ùëì¬πX¬∫and create a transposed
convolutional layer ùëîwith the same hyperparameters as ùëìexcept for the number of
outputchannelsbeingthenumberofchannelsin X,thenùëî¬πùëå¬∫willhavethesameshape
asX.
We can implement convolutions using matrix multiplications. The transposed convolu-
tional layer can just exchange the forward propagation function and the backpropaga-
tion function of the convolutional layer.
14.10.5Exercises
1.InSection14.10.3 ,theconvolutioninput Xandthetransposedconvolutionoutput Zhave
the same shape. Do they have the same value? Why?
2.Is it efficient to use matrix multiplications to implement convolutions? Why?
Discussions222.
14.11FullyConvolutional Networks
As discussed in Section 14.9 , semantic segmentation classifies images in pixel level. A
fullyconvolutionalnetwork(FCN)usesaconvolutionalneuralnetworktotransformimage
pixels to pixel classes ( Longet al., 2015). Unlike the CNNs that we encountered earlier
for image classification or object detection, a fully convolutional network transforms the
height and width of intermediate feature maps back to those of the input image: this is
achievedbythetransposedconvolutionallayerintroducedin Section14.10 . Asaresult,the
classification output and the input image have a one-to-one correspondence in pixel level:
the channel dimension at any output pixel holds the classification results for the input pixel
at the same spatial position.
%matplotlib inline
import torch
import torchvision
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
660 Computer Vision
14.11.1The Model
Here we describe the basic design of the fully convolutional network model. As shown
inFig. 14.11.1 , this model first uses a CNN to extract image features, then transforms the
number of channels into the number of classes via a 11convolutional layer, and finally
transforms the height and width of the feature maps to those of the input image via the
transposed convolution introduced in Section 14.10 . As a result, the model output has the
same height and width as the input image, where the output channel contains the predicted
classes for the input pixel at the same spatial position.
tFig. 14.11.1 Fully convolutional network.
Below, we use a ResNet-18 model pretrained on the ImageNet dataset to extract image
features and denote the model instance as pretrained_net . The last few layers of this
model include a global average pooling layer and a fully connected layer: they are not
needed in the fully convolutional network.
pretrained_net =torchvision .models .resnet18(pretrained =True )
list (pretrained_net .children())[ -3:]
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /
‚Ü©!home/ci/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 44.7M/44.7M [00:00<00:00, 56.3MB/s]
[Sequential(
(0): BasicBlock(
(conv1): Conv2d( 256,512, kernel_size =(3,3), stride =(2,2), padding =(1,‚ê£
‚Ü©!1), bias =False )
(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_
‚Ü©!running_stats =True )
(relu): ReLU(inplace =True )
(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,‚ê£
‚Ü©!1), bias =False )
(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_
(continues on next page)
661 Fully Convolutional Networks
(continued from previous page)
‚Ü©!running_stats =True )
(downsample): Sequential(
(0): Conv2d( 256,512, kernel_size =(1,1), stride =(2,2), bias =False )
(1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_
‚Ü©!running_stats =True )
)
)
(1): BasicBlock(
(conv1): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,‚ê£
‚Ü©!1), bias =False )
(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_
‚Ü©!running_stats =True )
(relu): ReLU(inplace =True )
(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,‚ê£
‚Ü©!1), bias =False )
(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_
‚Ü©!running_stats =True )
)
),
AdaptiveAvgPool2d(output_size =(1,1)),
Linear(in_features =512, out_features =1000 , bias =True )]
Next, we create the fully convolutional network instance net. It copies all the pretrained
layers in the ResNet-18 except for the final global average pooling layer and the fully con-
nected layer that are closest to the output.
net =nn.Sequential( *list (pretrained_net .children())[: -2])
Given an input with height and width of 320 and 480 respectively, the forward propagation
ofnetreducestheinputheightandwidthto1/32oftheoriginal,namely10and15.
X=torch .rand(size =(1,3,320,480))
net(X) .shape
torch .Size([ 1,512,10,15])
Next, we use a 11convolutional layer to transform the number of output channels into
thenumberofclasses(21)ofthePascalVOC2012dataset. Finally, weneedtoincreasethe
height and width of the feature maps by 32 times to change them back to the height and
width of the input image. Recall how to calculate the output shape of a convolutional layer
inSection7.3 . Since¬π320 64¬∏162¬∏32¬∫¬ù32=10and¬π480 64¬∏162¬∏32¬∫¬ù32=15,
weconstructatransposedconvolutionallayerwithstrideof 32,settingtheheightandwidth
of the kernel to 64, the padding to 16. In general, we can see that for stride ùë†, paddingùë†¬ù2
(assumingùë†¬ù2is an integer), and the height and width of the kernel 2ùë†, the transposed
convolution will increase the height and width of the input by ùë†times.
662 Computer Vision
num_classes =21
net.add_module( 'final_conv ', nn .Conv2d( 512, num_classes, kernel_size =1))
net.add_module( 'transpose_conv ', nn .ConvTranspose2d(num_classes, num_classes,
kernel_size =64, padding =16, stride =32))
14.11.2InitializingTransposedConvolutionalLayers
We already know that transposed convolutional layers can increase the height and width of
feature maps. In image processing, we may need to scale up an image, i.e., upsampling .
Bilinearinterpolation is one of the commonly used upsampling techniques. It is also often
used for initializing transposed convolutional layers.
To explain bilinear interpolation, say that given an input image we want to calculate each
pixel of the upsampled output image. In order to calculate the pixel of the output image
at coordinate¬πùë•,ùë¶¬∫, first map¬πùë•,ùë¶¬∫to coordinate¬πùë•0,ùë¶0¬∫on the input image, for example,
accordingtotheratiooftheinputsizetotheoutputsize. Notethatthemapped ùë•0andùë¶0are
real numbers. Then, find the four pixels closest to coordinate ¬πùë•0,ùë¶0¬∫on the input image.
Finally, the pixel of the output image at coordinate ¬πùë•,ùë¶¬∫is calculated based on these four
closest pixels on the input image and their relative distance from ¬πùë•0,ùë¶0¬∫.
Upsampling of bilinear interpolation can be implemented by the transposed convolutional
layer with the kernel constructed by the following bilinear_kernel function. Due to
space limitations, we only provide the implementation of the bilinear_kernel function
below without discussions on its algorithm design.
def bilinear_kernel (in_channels, out_channels, kernel_size):
factor =(kernel_size +1)//2
ifkernel_size %2==1:
center =factor -1
else :
center =factor -0.5
og=(torch .arange(kernel_size) .reshape( -1,1),
torch .arange(kernel_size) .reshape( 1,-1))
filt =(1-torch .abs(og[ 0]-center) /factor) *\
(1-torch .abs(og[ 1]-center) /factor)
weight =torch .zeros((in_channels, out_channels,
kernel_size, kernel_size))
weight[ range (in_channels), range (out_channels), :, :] =filt
return weight
Let‚Äôs experiment with upsampling of bilinear interpolation that is implemented by a trans-
posed convolutional layer. We construct a transposed convolutional layer that doubles the
height and weight, and initialize its kernel with the bilinear_kernel function.
conv_trans =nn.ConvTranspose2d( 3,3, kernel_size =4, padding =1, stride =2,
bias =False )
conv_trans .weight .data .copy_(bilinear_kernel( 3,3,4));
663 Fully Convolutional Networks
Read the image Xand assign the upsampling output to Y. In order to print the image, we
need to adjust the position of the channel dimension.
img =torchvision .transforms .ToTensor()(d2l .Image .open( '../img/catdog.jpg '))
X=img.unsqueeze( 0)
Y=conv_trans(X)
out_img =Y[0].permute( 1,2,0).detach()
As we can see, the transposed convolutional layer increases both the height and width of
the image by a factor of two. Except for the different scales in coordinates, the image
scaled up by bilinear interpolation and the original image printed in Section 14.3 look the
same.
d2l.set_figsize()
print ('input image shape: ', img .permute( 1,2,0).shape)
d2l.plt.imshow(img .permute( 1,2,0));
print ('output image shape: ', out_img .shape)
d2l.plt.imshow(out_img);
input image shape: torch .Size([ 561,728,3])
output image shape: torch .Size([ 1122 ,1456 ,3])
In a fully convolutional network, we initialize the transposed convolutional layer with up-
sampling of bilinear interpolation. For the 11convolutional layer, we use Xavier initial-
ization.
W=bilinear_kernel(num_classes, num_classes, 64)
net.transpose_conv .weight .data .copy_(W);
14.11.3Readingthe Dataset
Wereadthesemanticsegmentationdatasetasintroducedin Section14.9 . Theoutputimage
shapeofrandomcroppingisspecifiedas 320480: boththeheightandwidtharedivisible
by32.
batch_size, crop_size =32, (320,480)
train_iter, test_iter =d2l.load_data_voc(batch_size, crop_size)
664 Computer Vision
read 1114 examples
read 1078 examples
14.11.4Training
Now we can train our constructed fully convolutional network. The loss function and ac-
curacy calculation here are not essentially different from those in image classification of
earlier chapters. Because we use the output channel of the transposed convolutional layer
topredicttheclassforeachpixel,thechanneldimensionisspecifiedinthelosscalculation.
Inaddition,theaccuracyiscalculatedbasedoncorrectnessofthepredictedclassforallthe
pixels.
def loss (inputs, targets):
return F.cross_entropy(inputs, targets, reduction ='none ').mean( 1).mean( 1)
num_epochs, lr, wd, devices =5,0.001 ,1e-3 , d2l .try_all_gpus()
trainer =torch .optim .SGD(net .parameters(), lr =lr, weight_decay =wd)
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.449 , train acc 0.861 , test acc 0.852
226.7 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
14.11.5Prediction
Whenpredicting,weneedtostandardizetheinputimageineachchannelandtransformthe
image into the four-dimensional input format required by the CNN.
def predict (img):
X=test_iter .dataset .normalize_image(img) .unsqueeze( 0)
pred =net(X .to(devices[ 0])).argmax(dim =1)
return pred .reshape(pred .shape[ 1], pred .shape[ 2])
To visualize the predicted class of each pixel, we map the predicted class back to its label
color in the dataset.
665 Fully Convolutional Networks
def label2image (pred):
colormap =torch .tensor(d2l .VOC_COLORMAP, device =devices[ 0])
X=pred .long()
return colormap[X, :]
Images in the test dataset vary in size and shape. Since the model uses a transposed con-
volutional layer with stride of 32, when the height or width of an input image is indivisible
by 32, the output height or width of the transposed convolutional layer will deviate from
the shape of the input image. In order to address this issue, we can crop multiple rectangu-
lar areas with height and width that are integer multiples of 32 in the image, and perform
forward propagation on the pixels in these areas separately. Note that the union of these
rectangular areas needs to completely cover the input image. When a pixel is covered by
multiple rectangular areas, the average of the transposed convolution outputs in separate
areas for this same pixel can be input to the softmax operation to predict the class.
Forsimplicity,weonlyreadafewlargertestimages,andcropa 320480areaforprediction
startingfromtheupper-leftcornerofanimage. Forthesetestimages,weprinttheircropped
areas, prediction results, and ground-truth row by row.
voc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')
test_images, test_labels =d2l.read_voc_images(voc_dir, False )
n, imgs =4, []
for iinrange (n):
crop_rect =(0,0,320,480)
X=torchvision .transforms .functional .crop(test_images[i], *crop_rect)
pred =label2image(predict(X))
imgs +=[X.permute( 1,2,0), pred .cpu(),
torchvision .transforms .functional .crop(
test_labels[i], *crop_rect) .permute( 1,2,0)]
d2l.show_images(imgs[:: 3]+imgs[ 1::3]+imgs[ 2::3],3, n, scale =2);

666 Computer Vision
22314.11.6Summary
The fully convolutional network first uses a CNN to extract image features, then trans-
forms the number of channels into the number of classes via a 11convolutional
layer, and finally transforms the height and width of the feature maps to those of the
input image via the transposed convolution.
In a fully convolutional network, we can use upsampling of bilinear interpolation to
initialize the transposed convolutional layer.
14.11.7Exercises
1.If we use Xavier initialization for the transposed convolutional layer in the experiment,
how does the result change?
2.Can you further improve the accuracy of the model by tuning the hyperparameters?
3.Predict the classes of all pixels in test images.
4.The original fully convolutional network paper also uses outputs of some intermediate
CNN layers ( Longetal., 2015). Try to implement this idea.
Discussions223.
14.12NeuralStyleTransfer
If you are a photography enthusiast, you may be familiar with the filter. It can change
the color style of photos so that landscape photos become sharper or portrait photos have
whitenedskins. However, onefilterusuallyonlychangesoneaspectofthephoto. Toapply
an ideal style to a photo, you probably need to try many different filter combinations. This
process is as complex as tuning the hyperparameters of a model.
In this section, we will leverage layerwise representations of a CNN to automatically apply
the style of one image to another image, i.e., style transfer (Gatyset al., 2016). This task
needs two input images: one is the content image and the other is the style image . We will
useneuralnetworkstomodifythecontentimagetomakeitclosetothestyleimageinstyle.
For example, the content image in Fig. 14.12.1 is a landscape photo taken by us in Mount
RainierNationalParkinthesuburbsofSeattle,whilethestyleimageisanoilpaintingwith
the theme of autumn oak trees. In the output synthesized image, the oil brush strokes of
the style image are applied, leading to more vivid colors, while preserving the main shape
of the objects in the content image.
14.12.1Method
Fig. 14.12.2 illustrates the CNN-based style transfer method with a simplified example.
First, we initialize the synthesized image, for example, into the content image. This syn-
667 Neural Style Transfer
tFig. 14.12.1 Given content and style images, style transfer outputs a synthesized image.
thesizedimageistheonlyvariablethatneedstobeupdatedduringthestyletransferprocess,
i.e.,themodelparameterstobeupdatedduringtraining. ThenwechooseapretrainedCNN
to extract image features and freeze its model parameters during training. This deep CNN
uses multiple layers to extract hierarchical features for images. We can choose the output
of some of these layers as content features or style features. Take Fig. 14.12.2 as an exam-
ple. The pretrained neural network here has 3 convolutional layers, where the second layer
outputs the content features, and the first and third layers output the style features.
tFig. 14.12.2 CNN-based style transfer process. Solid lines show the direction of forward propagation
and dotted lines show backward propagation.
Next, we calculate the loss function of style transfer through forward propagation (direc-
tion of solid arrows), and update the model parameters (the synthesized image for output)
through backpropagation (direction of dashed arrows). The loss function commonly used
in style transfer consists of three parts: (i) content loss makes the synthesized image and
the content image close in content features; (ii) styleloss makes the synthesized image and
style image close in style features; and (iii) total variation loss helps to reduce the noise
in the synthesized image. Finally, when the model training is over, we output the model
parameters of the style transfer to generate the final synthesized image.
In the following, we will explain the technical details of style transfer via a concrete exper-
iment.
668 Computer Vision
14.12.2Readingthe Content and StyleImages
First, we read the content and style images. From their printed coordinate axes, we can tell
that these images have different sizes.
%matplotlib inline
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
d2l.set_figsize()
content_img =d2l.Image .open( '../img/rainier.jpg ')
d2l.plt.imshow(content_img);
style_img =d2l.Image .open( '../img/autumn-oak.jpg ')
d2l.plt.imshow(style_img);
14.12.3Preprocessingand Postprocessing
Below, we define two functions for preprocessing and postprocessing images. The pre-
process function standardizes each of the three RGB channels of the input image and
transforms the results into the CNN input format. The postprocess function restores the
pixel values in the output image to their original values before standardization. Since the
image printing function requires that each pixel has a floating point value from 0 to 1, we
replace any value smaller than 0 or greater than 1 with 0 or 1, respectively.
669 Neural Style Transfer
rgb_mean =torch .tensor([ 0.485 ,0.456 ,0.406 ])
rgb_std =torch .tensor([ 0.229 ,0.224 ,0.225 ])
def preprocess (img, image_shape):
transforms =torchvision .transforms .Compose([
torchvision .transforms .Resize(image_shape),
torchvision .transforms .ToTensor(),
torchvision .transforms .Normalize(mean =rgb_mean, std =rgb_std)])
return transforms(img) .unsqueeze( 0)
def postprocess (img):
img =img[ 0].to(rgb_std .device)
img =torch .clamp(img .permute( 1,2,0)*rgb_std +rgb_mean, 0,1)
return torchvision .transforms .ToPILImage()(img .permute( 2,0,1))
14.12.4Extracting Features
We use the VGG-19 model pretrained on the ImageNet dataset to extract image features
(Gatysetal., 2016).
pretrained_net =torchvision .models .vgg19(pretrained =True )
Downloading: "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth" to /home/
‚Ü©!ci/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 548M/548M [00:02<00:00, 213MB/s]
In order to extract the content features and style features of the image, we can select the
output of certain layers in the VGG network. Generally speaking, the closer to the input
layer, the easier to extract details of the image, and vice versa, the easier to extract the
global information of the image. In order to avoid excessively retaining the details of the
content image in the synthesized image, we choose a VGG layer that is closer to the output
as thecontent layer to output the content features of the image. We also select the output
of different VGG layers for extracting local and global style features. These layers are also
calledstyle layers . As mentioned in Section 8.2 , the VGG network uses 5 convolutional
blocks. Intheexperiment,wechoosethelastconvolutionallayerofthefourthconvolutional
block as the content layer, and the first convolutional layer of each convolutional block as
thestylelayer. Theindicesoftheselayerscanbeobtainedbyprintingthe pretrained_net
instance.
style_layers, content_layers =[0,5,10,19,28], [ 25]
When extracting features using VGG layers, we only need to use all those from the input
layer to the content layer or style layer that is closest to the output layer. Let‚Äôs construct
a new network instance net, which only retains all the VGG layers to be used for feature
extraction.
670 Computer Vision
net =nn.Sequential( *[pretrained_net .features[i] for iin
range (max(content_layers +style_layers) +1)])
Given the input X, if we simply invoke the forward propagation net(X), we can only get
the output of the last layer. Since we also need the outputs of intermediate layers, we need
to perform layer-by-layer computation and keep the content and style layer outputs.
def extract_features (X, content_layers, style_layers):
contents =[]
styles =[]
for iinrange (len(net)):
X=net[i](X)
ifiinstyle_layers:
styles .append(X)
ifiincontent_layers:
contents .append(X)
return contents, styles
Two functions are defined below: the get_contents function extracts content features
from the content image, and the get_styles function extracts style features from the style
image. SincethereisnoneedtoupdatethemodelparametersofthepretrainedVGGduring
training, we can extract the content and the style features even before the training starts.
Since the synthesized image is a set of model parameters to be updated for style transfer,
we can only extract the content and style features of the synthesized image by calling the
extract_features function during training.
def get_contents (image_shape, device):
content_X =preprocess(content_img, image_shape) .to(device)
contents_Y, _ =extract_features(content_X, content_layers, style_layers)
return content_X, contents_Y
def get_styles (image_shape, device):
style_X =preprocess(style_img, image_shape) .to(device)
_, styles_Y =extract_features(style_X, content_layers, style_layers)
return style_X, styles_Y
14.12.5Definingthe Loss Function
Now we will describe the loss function for style transfer. The loss function consists of the
content loss, style loss, and total variation loss.
ContentLoss
Similar to the loss function in linear regression, the content loss measures the difference in
content features between the synthesized image and the content image via the squared loss
function. The two inputs of the squared loss function are both outputs of the content layer
computed by the extract_features function.
671 Neural Style Transfer
def content_loss (Y_hat, Y):
# We detach the target content from the tree used to dynamically compute
# the gradient: this is a stated value, not a variable. Otherwise the loss
# will throw an error.
return torch .square(Y_hat -Y.detach()) .mean()
StyleLoss
Style loss, similar to content loss, also uses the squared loss function to measure the dif-
ference in style between the synthesized image and the style image. To express the style
output of any style layer, we first use the extract_features function to compute the style
layer output. Suppose that the output has 1 example, ùëêchannels, height ‚Ñé, and widthùë§, we
can transform this output into matrix Xwithùëêrows and‚Ñéùë§columns. This matrix can be
thought of as the concatenation of ùëêvectors x1,...,xùëê, each of which has a length of ‚Ñéùë§.
Here, vector xùëñrepresents the style feature of channel ùëñ.
In theGram matrix of these vectors XX>2Rùëêùëê, elementùë•ùëñùëóin rowùëñand column ùëóis
the dot product of vectors xùëñandxùëó. It represents the correlation of the style features of
channelsùëñandùëó. We use this Gram matrix to represent the style output of any style layer.
Note that when the value of ‚Ñéùë§is larger, it likely leads to larger values in the Gram matrix.
Note also that the height and width of the Gram matrix are both the number of channels ùëê.
To allow style loss not to be affected by these values, the gramfunction below divides the
Gram matrix by the number of its elements, i.e., ùëê‚Ñéùë§.
def gram (X):
num_channels, n =X.shape[ 1], X .numel() //X.shape[ 1]
X=X.reshape((num_channels, n))
return torch .matmul(X, X .T)/(num_channels *n)
Obviously, the two Gram matrix inputs of the squared loss function for style loss are based
on the style layer outputs for the synthesized image and the style image. It is assumed here
that the Gram matrix gram_Ybased on the style image has been precomputed.
def style_loss (Y_hat, gram_Y):
return torch .square(gram(Y_hat) -gram_Y .detach()) .mean()
Total VariationLoss
Sometimes, the learned synthesized image has a lot of high-frequency noise, i.e., particu-
larly bright or dark pixels. One common noise reduction method is total variation denois-
ing. Denotebyùë•ùëñ,ùëóthepixelvalueatcoordinate ¬πùëñ,ùëó¬∫. Reducingtotalvariationloss
√ï
ùëñ,ùëóùë•ùëñ,ùëó ùë•ùëñ¬∏1,ùëó¬∏ùë•ùëñ,ùëó ùë•ùëñ,ùëó¬∏1(14.12.1)
makes values of neighboring pixels on the synthesized image closer.
672 Computer Vision
def tv_loss (Y_hat):
return 0.5 *(torch .abs(Y_hat[:, :, 1:, :] -Y_hat[:, :, : -1, :]) .mean() +
torch .abs(Y_hat[:, :, :, 1:]-Y_hat[:, :, :, : -1]).mean())
Loss Function
The loss function of style transfer is the weighted sum of content loss, style loss, and total
variation loss. By adjusting these weight hyperparameters, we can balance among content
retention, style transfer, and noise reduction on the synthesized image.
content_weight, style_weight, tv_weight =1,1e4,10
def compute_loss (X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
# Calculate the content, style, and total variance losses respectively
contents_l =[content_loss(Y_hat, Y) *content_weight for Y_hat, Y inzip(
contents_Y_hat, contents_Y)]
styles_l =[style_loss(Y_hat, Y) *style_weight for Y_hat, Y inzip(
styles_Y_hat, styles_Y_gram)]
tv_l =tv_loss(X) *tv_weight
# Add up all the losses
l=sum(styles_l +contents_l +[tv_l])
return contents_l, styles_l, tv_l, l
14.12.6Initializing the Synthesized Image
In style transfer, the synthesized image is the only variable that needs to be updated during
training. Thus, we can define a simple model, SynthesizedImage , and treat the synthe-
sized image as the model parameters. In this model, forward propagation just returns the
model parameters.
class SynthesizedImage (nn.Module):
def __init__ (self , img_shape, **kwargs):
super (SynthesizedImage, self ).__init__ (**kwargs)
self .weight =nn.Parameter(torch .rand( *img_shape))
def forward (self ):
return self .weight
Next, we define the get_inits function. This function creates a synthesized image model
instance and initializes it to the image X. Gram matrices for the style image at various style
layers, styles_Y_gram , are computed prior to training.
def get_inits (X, device, lr, styles_Y):
gen_img =SynthesizedImage(X .shape) .to(device)
gen_img .weight .data .copy_(X .data)
trainer =torch .optim .Adam(gen_img .parameters(), lr =lr)
styles_Y_gram =[gram(Y) for Yinstyles_Y]
return gen_img(), styles_Y_gram, trainer
673 Neural Style Transfer
14.12.7Training
When training the model for style transfer, we continuously extract content features and
style features of the synthesized image, and calculate the loss function. Below defines the
training loop.
def train (X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
X, styles_Y_gram, trainer =get_inits(X, device, lr, styles_Y)
scheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_decay_epoch, 0.8)
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[10, num_epochs],
legend =['content ','style ','TV'],
ncols =2, figsize =(7,2.5))
for epoch inrange (num_epochs):
trainer .zero_grad()
contents_Y_hat, styles_Y_hat =extract_features(
X, content_layers, style_layers)
contents_l, styles_l, tv_l, l =compute_loss(
X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
l.backward()
trainer .step()
scheduler .step()
if(epoch +1)%10==0:
animator .axes[ 1].imshow(postprocess(X))
animator .add(epoch +1, [float (sum(contents_l)),
float (sum(styles_l)), float (tv_l)])
return X
Now we start to train the model. We rescale the height and width of the content and style
images to 300 by 450 pixels. We use the content image to initialize the synthesized im-
age.
device, image_shape =d2l.try_gpu(), ( 300,450)# PIL Image (h, w)
net =net.to(device)
content_X, contents_Y =get_contents(image_shape, device)
_, styles_Y =get_styles(image_shape, device)
output =train(content_X, contents_Y, styles_Y, device, 0.3,500,50)
Wecanseethatthesynthesizedimageretainsthesceneryandobjectsofthecontentimage,
and transfers the color of the style image at the same time. For example, the synthesized
674 Computer Vision
224image has blocks of color like those in the style image. Some of these blocks even have the
subtle texture of brush strokes.
14.12.8Summary
Thelossfunctioncommonlyusedinstyletransferconsistsofthreeparts: (i)contentloss
makesthesynthesizedimageandthecontentimagecloseincontentfeatures; (ii)style
lossmakesthesynthesizedimageandstyleimagecloseinstylefeatures; and(iii)total
variation loss helps to reduce the noise in the synthesized image.
We can use a pretrained CNN to extract image features and minimize the loss function
to continuously update the synthesized image as model parameters during training.
We use Gram matrices to represent the style outputs from the style layers.
14.12.9Exercises
1.How does the output change when you select different content and style layers?
2.Adjust the weight hyperparameters in the loss function. Does the output retain more
content or have less noise?
3.Use different content and style images. Can you create more interesting synthesized
images?
4.Can we apply style transfer for text? Hint: you may refer to the survey paper by Hu et
al.(2022).
Discussions224.
14.13ImageClassification (CIFAR-10)on Kaggle
So far, we have been using high-level APIs of deep learning frameworks to directly obtain
image datasets in tensor format. However, custom image datasets often come in the form
of image files. In this section, we will start from raw image files, and organize, read, then
transform them into tensor format step by step.
WeexperimentedwiththeCIFAR-10datasetin Section14.1 ,whichisanimportantdataset
in computer vision. In this section, we will apply the knowledge we learned in previous
sections to practice the Kaggle competition of CIFAR-10 image classification. The web
address of the competition is https://www.kaggle.com/c/cifar-10
Fig. 14.13.1 shows the information on the competition‚Äôs webpage. In order to submit the
results, you need to register a Kaggle account.
675 Image ClassiÔ¨Åcation (CIFAR-10) on Kaggle
tFig. 14.13.1 CIFAR-10 image classiÔ¨Åcation competition webpage information. The competition
dataset can be obtained by clicking the ‚ÄúData‚Äù tab.
import collections
import math
import os
import shutil
import pandas aspd
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
14.13.1Obtaining and Organizingthe Dataset
The competition dataset is divided into a training set and a test set, which contain 50000
and 300000 images, respectively. In the test set, 10000 images will be used for evaluation,
while the remaining 290000 images will not be evaluated: they are included just to make
it hard to cheat with manually labeled results of the test set. The images in this dataset
are all png color (RGB channels) image files, whose height and width are both 32 pixels.
The images cover a total of 10 categories, namely airplanes, cars, birds, cats, deer, dogs,
frogs, horses, boats, and trucks. The upper-left corner of Fig. 14.13.1 shows some images
of airplanes, cars, and birds in the dataset.
Downloadingthe Dataset
After logging in to Kaggle, we can click the ‚ÄúData‚Äù tab on the CIFAR-10 image classifi-
cation competition webpage shown in Fig. 14.13.1 and download the dataset by clicking
the ‚ÄúDownload All‚Äù button. After unzipping the downloaded file in ../data , and un-
zipping train.7z andtest.7z inside it, you will find the entire dataset in the following
paths:
../data/cifar-10/train/[1-50000].png
../data/cifar-10/test/[1-300000].png
676 Computer Vision
../data/cifar-10/trainLabels.csv
../data/cifar-10/sampleSubmission.csv
wherethe trainandtestdirectories containthe trainingand testingimages, respectively,
trainLabels.csv provides labels for the training images, and sample_submission.csv
is a sample submission file.
Tomakeiteasiertogetstarted,weprovideasmall-scalesampleofthedatasetthatcontains
the first 1000 training images and 5 random testing images. To use the full dataset of the
Kaggle competition, you need to set the following demovariable to False.
#@save
d2l.DATA_HUB[ 'cifar10_tiny ']=(d2l .DATA_URL +'kaggle_cifar10_tiny.zip ',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd ')
# If you use the full dataset downloaded for the Kaggle competition, set
# `demo` to False
demo =True
ifdemo:
data_dir =d2l.download_extract( 'cifar10_tiny ')
else :
data_dir ='../data/cifar-10/ '
Downloading ../data /kaggle_cifar10_tiny .zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/kaggle_cifar10_tiny .zip...
Organizingthe Dataset
We need to organize datasets to facilitate model training and testing. Let‚Äôs first read the
labels from the csv file. The following function returns a dictionary that maps the non-
extension part of the filename to its label.
#@save
def read_csv_labels (fname):
"""Read `fname` to return a filename to label dictionary."""
with open (fname, 'r')asf:
# Skip the file header line (column name)
lines =f.readlines()[ 1:]
tokens =[l.rstrip() .split( ',')for linlines]
return dict (((name, label) for name, label intokens))
labels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))
print ('# training examples: ',len(labels))
print ('# classes: ',len(set(labels .values())))
# training examples: 1000
# classes: 10
677 Image ClassiÔ¨Åcation (CIFAR-10) on Kaggle
Next,wedefinethe reorg_train_valid functiontosplitthevalidationsetoutoftheorig-
inal training set. The argument valid_ratio in this function is the ratio of the number
of examples in the validation set to the number of examples in the original training set.
More concretely, let ùëõbe the number of images of the class with the least examples, and
ùëübe the ratio. The validation set will split out max¬πbùëõùëüc,1¬∫images for each class. Let‚Äôs
usevalid_ratio=0.1 as an example. Since the original training set has 50000 images,
there will be 45000 images used for training in the path train_valid_test/train , while
the other 5000 images will be split out as validation set in the path train_valid_test/
valid. Afterorganizingthedataset,imagesofthesameclasswillbeplacedunderthesame
folder.
#@save
def copyfile (filename, target_dir):
"""Copy a file into a target directory."""
os.makedirs(target_dir, exist_ok =True )
shutil .copy(filename, target_dir)
#@save
def reorg_train_valid (data_dir, labels, valid_ratio):
"""Split the validation set out of the original training set."""
# The number of examples of the class that has the fewest examples in the
# training dataset
n=collections .Counter(labels .values()) .most_common()[ -1][1]
# The number of examples per class for the validation set
n_valid_per_label =max(1, math .floor(n *valid_ratio))
label_count ={}
for train_file inos.listdir(os .path .join(data_dir, 'train ')):
label =labels[train_file .split( '.')[0]]
fname =os.path .join(data_dir, 'train ', train_file)
copyfile(fname, os .path .join(data_dir, 'train_valid_test ',
'train_valid ', label))
iflabel not inlabel_count orlabel_count[label] <n_valid_per_label:
copyfile(fname, os .path .join(data_dir, 'train_valid_test ',
'valid ', label))
label_count[label] =label_count .get(label, 0)+1
else :
copyfile(fname, os .path .join(data_dir, 'train_valid_test ',
'train ', label))
return n_valid_per_label
Thereorg_test function below organizes the testing set for data loading during predic-
tion.
#@save
def reorg_test (data_dir):
"""Organize the testing set for data loading during prediction."""
for test_file inos.listdir(os .path .join(data_dir, 'test ')):
copyfile(os .path .join(data_dir, 'test ', test_file),
os.path .join(data_dir, 'train_valid_test ','test ',
'unknown '))
678 Computer Vision
Finally, we use a function to invoke the read_csv_labels ,reorg_train_valid , and re-
org_test functions defined above.
def reorg_cifar10_data (data_dir, valid_ratio):
labels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))
reorg_train_valid(data_dir, labels, valid_ratio)
reorg_test(data_dir)
Here we only set the batch size to 32 for the small-scale sample of the dataset. When
training and testing the complete dataset of the Kaggle competition, batch_size should
be set to a larger integer, such as 128. We split out 10% of the training examples as the
validation set for tuning hyperparameters.
batch_size =32ifdemo else 128
valid_ratio =0.1
reorg_cifar10_data(data_dir, valid_ratio)
14.13.2ImageAugmentation
Weuseimageaugmentationtoaddressoverfitting. Forexample,imagescanbeflippedhor-
izontallyatrandomduringtraining. WecanalsoperformstandardizationforthethreeRGB
channels of color images. Below lists some of these operations that you can tweak.
transform_train =torchvision .transforms .Compose([
# Scale the image up to a square of 40 pixels in both height and width
torchvision .transforms .Resize( 40),
# Randomly crop a square image of 40 pixels in both height and width to
# produce a small square of 0.64 to 1 times the area of the original
# image, and then scale it to a square of 32 pixels in both height and
# width
torchvision .transforms .RandomResizedCrop( 32, scale =(0.64 ,1.0),
ratio =(1.0,1.0)),
torchvision .transforms .RandomHorizontalFlip(),
torchvision .transforms .ToTensor(),
# Standardize each channel of the image
torchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],
[0.2023 ,0.1994 ,0.2010 ])])
During testing, we only perform standardization on images so as to remove randomness in
the evaluation results.
transform_test =torchvision .transforms .Compose([
torchvision .transforms .ToTensor(),
torchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],
[0.2023 ,0.1994 ,0.2010 ])])
14.13.3Readingthe Dataset
Next, we read the organized dataset consisting of raw image files. Each example includes
an image and a label.
679 Image ClassiÔ¨Åcation (CIFAR-10) on Kaggle
train_ds, train_valid_ds =[torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'train_valid_test ', folder),
transform =transform_train) for folder in['train ','train_valid ']]
valid_ds, test_ds =[torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'train_valid_test ', folder),
transform =transform_test) for folder in['valid ','test ']]
During training, we need to specify all the image augmentation operations defined above.
When the validation set is used for model evaluation during hyperparameter tuning, no
randomness from image augmentation should be introduced. Before final prediction, we
train the model on the combined training set and validation set to make full use of all the
labeled data.
train_iter, train_valid_iter =[torch .utils .data .DataLoader(
dataset, batch_size, shuffle =True , drop_last =True )
for dataset in(train_ds, train_valid_ds)]
valid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,
drop_last =True )
test_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,
drop_last =False )
14.13.4Defining the Model
We define the ResNet-18 model described in Section 8.6 .
def get_net ():
num_classes =10
net =d2l.resnet18(num_classes, 3)
return net
loss =nn.CrossEntropyLoss(reduction ="none ")
14.13.5Defining the TrainingFunction
We will select models and tune hyperparameters according to the model‚Äôs performance on
the validation set. In the following, we define the model training function train.
def train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay):
trainer =torch .optim .SGD(net .parameters(), lr =lr, momentum =0.9,
weight_decay =wd)
scheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)
num_batches, timer =len(train_iter), d2l .Timer()
legend =['train loss ','train acc ']
ifvalid_iter isnot None :
legend .append( 'valid acc ')
(continues on next page)
680 Computer Vision
(continued from previous page)
animator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],
legend =legend)
net =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])
for epoch inrange (num_epochs):
net.train()
metric =d2l.Accumulator( 3)
for i, (features, labels) inenumerate (train_iter):
timer .start()
l, acc =d2l.train_batch_ch13(net, features, labels,
loss, trainer, devices)
metric .add(l, acc, labels .shape[ 0])
timer .stop()
if(i+1)%(num_batches //5)==0ori==num_batches -1:
animator .add(epoch +(i+1)/num_batches,
(metric[ 0]/metric[ 2], metric[ 1]/metric[ 2],
None ))
ifvalid_iter isnot None :
valid_acc =d2l.evaluate_accuracy_gpu(net, valid_iter)
animator .add(epoch +1, (None ,None , valid_acc))
scheduler .step()
measures =(f'train loss {metric[ 0]/metric[ 2]:.3f},'
f'train acc {metric[ 1]/metric[ 2]:.3f}')
ifvalid_iter isnot None :
measures +=f', valid acc {valid_acc :.3f}'
print (measures +f'\n{metric[ 2]*num_epochs /timer .sum() :.1f}'
f'examples/sec on {str(devices) }')
14.13.6Trainingand Validatingthe Model
Now, wecantrainandvalidatethemodel. Allthefollowinghyperparameterscanbetuned.
For example, we can increase the number of epochs. When lr_period andlr_decay
are set to 4 and 0.9, respectively, the learning rate of the optimization algorithm will be
multiplied by 0.9 after every 4 epochs. Just for ease of demonstration, we only train 20
epochs here.
devices, num_epochs, lr, wd =d2l.try_all_gpus(), 20,2e-4 ,5e-4
lr_period, lr_decay, net =4,0.9, get_net()
net( next (iter (train_iter))[ 0])
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
train loss 0.654 , train acc 0.789 , valid acc 0.438
958.1 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
14.13.7Classifying the TestingSetand Submitting Resultson Kaggle
After obtaining a promising model with hyperparameters, we use all the labeled data (in-
cluding the validation set) to retrain the model and classify the testing set.
681 Image ClassiÔ¨Åcation (CIFAR-10) on Kaggle
net, preds =get_net(), []
net( next (iter (train_valid_iter))[ 0])
train(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,
lr_decay)
for X, _ intest_iter:
y_hat =net(X .to(devices[ 0]))
preds .extend(y_hat .argmax(dim =1).type(torch .int32) .cpu() .numpy())
sorted_ids =list (range (1,len(test_ds) +1))
sorted_ids .sort(key =lambda x:str(x))
df=pd.DataFrame({ 'id': sorted_ids, 'label ': preds})
df['label ']=df['label '].apply( lambda x: train_valid_ds .classes[x])
df.to_csv( 'submission.csv ', index =False )
train loss 0.608 , train acc 0.786
1040.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
The abovecode will generatea submission.csv file, whose format meets the requirement
of the Kaggle competition. The method for submitting results to Kaggle is similar to that
inSection 5.7 .
14.13.8Summary
We can read datasets containing raw image files after organizing them into the required
format.
682 Computer Vision
225We can use convolutional neural networks and image augmentation in an image classi-
fication competition.
14.13.9Exercises
1.Use the complete CIFAR-10 dataset for this Kaggle competition. Set hyperparameters
asbatch_size = 128 ,num_epochs = 100 ,lr = 0.1 ,lr_period = 50 ,and lr_decay
= 0.1. See what accuracy and ranking you can achieve in this competition. Can you
further improve them?
2.What accuracy can you get when not using image augmentation?
Discussions225.
14.14Dog BreedIdentification (ImageNetDogs) on
Kaggle
In this section, we will practice the dog breed identification problem on Kaggle. The web
address of this competition is https://www.kaggle.com/c/dog-breed-identification
In this competition, 120 different breeds of dogs will be recognized. In fact, the dataset for
this competition is a subset of the ImageNet dataset. Unlike the images in the CIFAR-10
dataset in Section 14.13 , the images in the ImageNet dataset are both higher and wider in
varying dimensions. Fig. 14.14.1 shows the information on the competition‚Äôs webpage.
You need a Kaggle account to submit your results.
import os
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
14.14.1Obtaining and Organizingthe Dataset
The competition dataset is divided into a training set and a test set, which contain 10222
and 10357 JPEG images of three RGB (color) channels, respectively. Among the training
dataset, there are 120 breeds of dogs such as Labradors, Poodles, Dachshunds, Samoyeds,
Huskies, Chihuahuas, and Yorkshire Terriers.
Downloadingthe Dataset
After logging into Kaggle, you can click on the ‚ÄúData‚Äù tab on the competition webpage
shown in Fig. 14.14.1 and download the dataset by clicking the ‚ÄúDownload All‚Äù button.
After unzipping the downloaded file in ../data , you will find the entire dataset in the
following paths:
683 Dog Breed IdentiÔ¨Åcation (ImageNet Dogs) on Kaggle
tFig. 14.14.1 The dog breed identiÔ¨Åcation competition website. The competition dataset can be
obtained by clicking the ‚ÄúData‚Äù tab.
../data/dog-breed-identification/labels.csv
../data/dog-breed-identification/sample_submission.csv
../data/dog-breed-identification/train
../data/dog-breed-identification/test
YoumayhavenoticedthattheabovestructureissimilartothatoftheCIFAR-10competition
inSection14.13 ,wherefolders train/andtest/containtrainingandtestingdogimages,
respectively, and labels.csv contains the labels for the training images. Similarly, to
make it easier to get started, we provide a small sample of the dataset mentioned above:
train_valid_test_tiny.zip . If you are going to use the full dataset for the Kaggle
competition, you need to change the demovariable below to False.
#@save
d2l.DATA_HUB[ 'dog_tiny ']=(d2l .DATA_URL +'kaggle_dog_tiny.zip ',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d ')
# If you use the full dataset downloaded for the Kaggle competition, change
# the variable below to `False`
demo =True
ifdemo:
data_dir =d2l.download_extract( 'dog_tiny ')
(continues on next page)
684 Computer Vision
(continued from previous page)
else :
data_dir =os.path .join( '..','data ','dog-breed-identification ')
Downloading ../data /kaggle_dog_tiny .zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/kaggle_dog_tiny .zip...
Organizingthe Dataset
We can organize the dataset similarly to what we did in Section 14.13 , namely splitting out
a validation set from the original training set, and moving images into subfolders grouped
by labels.
Thereorg_dog_data functionbelowreadsthetrainingdatalabels,splitsoutthevalidation
set, and organizes the training set.
def reorg_dog_data (data_dir, valid_ratio):
labels =d2l.read_csv_labels(os .path .join(data_dir, 'labels.csv '))
d2l.reorg_train_valid(data_dir, labels, valid_ratio)
d2l.reorg_test(data_dir)
batch_size =32ifdemo else 128
valid_ratio =0.1
reorg_dog_data(data_dir, valid_ratio)
14.14.2ImageAugmentation
Recall that this dog breed dataset is a subset of the ImageNet dataset, whose images are
largerthanthoseoftheCIFAR-10datasetin Section14.13 . Thefollowinglistsafewimage
augmentation operations that might be useful for relatively larger images.
transform_train =torchvision .transforms .Compose([
# Randomly crop the image to obtain an image with an area of 0.08 to 1 of
# the original area and height-to-width ratio between 3/4 and 4/3. Then,
# scale the image to create a new 224 x 224 image
torchvision .transforms .RandomResizedCrop( 224, scale =(0.08 ,1.0),
ratio =(3.0/4.0,4.0/3.0)),
torchvision .transforms .RandomHorizontalFlip(),
# Randomly change the brightness, contrast, and saturation
torchvision .transforms .ColorJitter(brightness =0.4,
contrast =0.4,
saturation =0.4),
# Add random noise
torchvision .transforms .ToTensor(),
# Standardize each channel of the image
torchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],
[0.229 ,0.224 ,0.225 ])])
Duringprediction,weonlyuseimagepreprocessingoperationswithoutrandomness.
685 Dog Breed IdentiÔ¨Åcation (ImageNet Dogs) on Kaggle
transform_test =torchvision .transforms .Compose([
torchvision .transforms .Resize( 256),
# Crop a 224 x 224 square area from the center of the image
torchvision .transforms .CenterCrop( 224),
torchvision .transforms .ToTensor(),
torchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],
[0.229 ,0.224 ,0.225 ])])
14.14.3Readingthe Dataset
AsinSection14.13 ,wecanreadtheorganizeddatasetconsistingofrawimagefiles.
train_ds, train_valid_ds =[torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'train_valid_test ', folder),
transform =transform_train) for folder in['train ','train_valid ']]
valid_ds, test_ds =[torchvision .datasets .ImageFolder(
os.path .join(data_dir, 'train_valid_test ', folder),
transform =transform_test) for folder in['valid ','test ']]
Below we create data iterator instances the same way as in Section 14.13 .
train_iter, train_valid_iter =[torch .utils .data .DataLoader(
dataset, batch_size, shuffle =True , drop_last =True )
for dataset in(train_ds, train_valid_ds)]
valid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,
drop_last =True )
test_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,
drop_last =False )
14.14.4Fine-Tuninga PretrainedModel
Again, the dataset for this competition is a subset of the ImageNet dataset. Therefore, we
can use the approach discussed in Section 14.2 to select a model pretrained on the full
ImageNet dataset and use it to extract image features to be fed into a custom small-scale
output network. High-level APIs of deep learning frameworks provide a wide range of
modelspretrainedontheImageNetdataset. Here,wechooseapretrainedResNet-34model,
where we simply reuse the input of this model‚Äôs output layer (i.e., the extracted features).
Then we can replace the original output layer with a small custom output network that can
be trained, such as stacking two fully connected layers. Different from the experiment in
Section14.2 ,thefollowingdoesnotretrainthepretrainedmodelusedforfeatureextraction.
This reduces training time and memory for storing gradients.
Recall that we standardized images using the means and standard deviations of the three
RGB channels for the full ImageNet dataset. In fact, this is also consistent with the stan-
dardization operation by the pretrained model on ImageNet.
686 Computer Vision
def get_net (devices):
finetune_net =nn.Sequential()
finetune_net .features =torchvision .models .resnet34(pretrained =True )
# Define a new output network (there are 120 output categories)
finetune_net .output_new =nn.Sequential(nn .Linear( 1000 ,256),
nn.ReLU(),
nn.Linear( 256,120))
# Move the model to devices
finetune_net =finetune_net .to(devices[ 0])
# Freeze parameters of feature layers
for param infinetune_net .features .parameters():
param .requires_grad =False
return finetune_net
Before calculating the loss, we first obtain the input of the pretrained model‚Äôs output layer,
i.e., the extracted feature. Then we use this feature as input for our small custom output
network to calculate the loss.
loss =nn.CrossEntropyLoss(reduction ='none ')
def evaluate_loss (data_iter, net, devices):
l_sum, n =0.0,0
for features, labels indata_iter:
features, labels =features .to(devices[ 0]), labels .to(devices[ 0])
outputs =net(features)
l=loss(outputs, labels)
l_sum +=l.sum()
n+=labels .numel()
return l_sum /n
14.14.5Defining the TrainingFunction
We will select the model and tune hyperparameters according to the model‚Äôs performance
on the validation set. The model training function trainonly iterates parameters of the
small custom output network.
def train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay):
# Only train the small custom output network
net =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])
trainer =torch .optim .SGD((param for param innet.parameters()
ifparam .requires_grad), lr =lr,
momentum =0.9, weight_decay =wd)
scheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)
num_batches, timer =len(train_iter), d2l .Timer()
legend =['train loss ']
ifvalid_iter isnot None :
legend .append( 'valid loss ')
animator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],
legend =legend)
for epoch inrange (num_epochs):
metric =d2l.Accumulator( 2)
(continues on next page)
687 Dog Breed IdentiÔ¨Åcation (ImageNet Dogs) on Kaggle
(continued from previous page)
for i, (features, labels) inenumerate (train_iter):
timer .start()
features, labels =features .to(devices[ 0]), labels .to(devices[ 0])
trainer .zero_grad()
output =net(features)
l=loss(output, labels) .sum()
l.backward()
trainer .step()
metric .add(l, labels .shape[ 0])
timer .stop()
if(i+1)%(num_batches //5)==0ori==num_batches -1:
animator .add(epoch +(i+1)/num_batches,
(metric[ 0]/metric[ 1],None ))
measures =f'train loss {metric[ 0]/metric[ 1]:.3f}'
ifvalid_iter isnot None :
valid_loss =evaluate_loss(valid_iter, net, devices)
animator .add(epoch +1, (None , valid_loss .detach() .cpu()))
scheduler .step()
ifvalid_iter isnot None :
measures +=f', valid loss {valid_loss :.3f}'
print (measures +f'\n{metric[ 1]*num_epochs /timer .sum() :.1f}'
f'examples/sec on {str(devices) }')
14.14.6Trainingand Validatingthe Model
Now we can train and validate the model. The following hyperparameters are all tunable.
For example, the number of epochs can be increased. Because lr_period andlr_decay
are set to 2 and 0.9, respectively, the learning rate of the optimization algorithm will be
multiplied by 0.9 after every 2 epochs.
devices, num_epochs, lr, wd =d2l.try_all_gpus(), 10,1e-4 ,1e-4
lr_period, lr_decay, net =2,0.9, get_net(devices)
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
train loss 1.240 , valid loss 1.545
577.5 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]

688 Computer Vision
14.14.7Classifying the TestingSetand Submitting Resultson Kaggle
Similar to the final step in Section 14.13 , in the end all the labeled data (including the
validation set) are used for training the model and classifying the testing set. We will use
the trained custom output network for classification.
net =get_net(devices)
train(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,
lr_decay)
preds =[]
for data, label intest_iter:
output =torch .nn.functional .softmax(net(data .to(devices[ 0])), dim =1)
preds .extend(output .cpu() .detach() .numpy())
ids =sorted (os.listdir(
os.path .join(data_dir, 'train_valid_test ','test ','unknown ')))
with open ('submission.csv ','w')asf:
f.write( 'id,'+','.join(train_valid_ds .classes) +'\n')
for i, output inzip(ids, preds):
f.write(i .split( '.')[0]+','+','.join(
[str(num) for num inoutput]) +'\n')
train loss 1.217
742.7 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
Theabovecodewillgeneratea submission.csv filetobesubmittedtoKaggleinthesame
way described in Section 5.7 .
14.14.8Summary
ImagesintheImageNetdatasetarelarger(withvaryingdimensions)thanCIFAR-10im-
ages. We may modify image augmentation operations for tasks on a different dataset.
To classify a subset of the ImageNet dataset, we can leverage pre-trained models on the
full ImageNet dataset to extract features and only train a custom small-scale output
network. This will lead to less computational time and memory cost.
14.14.9Exercises
689 Dog Breed IdentiÔ¨Åcation (ImageNet Dogs) on Kaggle
2261.WhenusingthefullKagglecompetitiondataset,whatresultscanyouachievewhenyou
increase batch_size (batch size) and num_epochs (number of epochs) while setting
some other hyperparameters as lr = 0.01 ,lr_period = 10 , and lr_decay = 0.1 ?
2.Do you get better results if you use a deeper pretrained model? How do you tune hyper-
parameters? Can you further improve the results?
Discussions226.
15Natural Language Processing: Pretraining
Humansneedtocommunicate. Outofthisbasicneedofthehumancondition,avastamount
of written text has been generated on an everyday basis. Given rich text in social media,
chat apps, emails, product reviews, news articles, research papers, and books, it becomes
vital to enable computers to understand them to offer assistance or make decisions based
on human languages.
Natural language processing studies interactions between computers and humans using
natural languages. In practice, it is very common to use natural language processing tech-
niquestoprocessandanalyzetext(humannaturallanguage)data, suchaslanguagemodels
inSection 9.3 and machine translation models in Section 10.5 .
To understand text, we can begin by learning its representations. Leveraging the existing
text sequences from large corpora, self-supervised learning has been extensively used to
pretrain text representations, such as by predicting some hidden part of the text using some
other part of their surrounding text. In this way, models learn through supervision from
massive text data without expensive labeling efforts!
As we will see in this chapter, when treating each word or subword as an individual token,
the representation of each token can be pretrained using word2vec, GloVe, or subword
embedding models on large corpora. After pretraining, representation of each token can
be a vector, however, it remains the same no matter what the context is. For instance,
the vector representation of ‚Äúbank‚Äù is the same in both ‚Äúgo to the bank to deposit some
money‚Äù and ‚Äúgo to the bank to sit down‚Äù. Thus, many more recent pretraining models
adaptrepresentationofthesametokentodifferentcontexts. AmongthemisBERT,amuch
deeper self-supervised model based on the Transformer encoder. In this chapter, we will
focus on how to pretrain such representations for text, as highlighted in Fig. 15.1 .
For sight of the big picture, Fig. 15.1 shows that the pretrained text representations can be
fed to a variety of deep learning architectures for different downstream natural language
processing applications. We will cover them in Chapter 16 .
690
691 Word Embedding (word2vec)
tFig. 15.1 Pretrained text representations can be fed to various deep learning architectures for
different downstream natural language processing applications. This chapter focuses on
the upstream text representation pretraining.
22715.1WordEmbedding (word2vec)
Natural language is a complex system used to express meanings. In this system, words
are the basic unit of the meaning. As the name implies, word vectors are vectors used to
represent words, and can also be considered as feature vectors or representations of words.
The technique of mapping words to real vectors is called wordembedding . In recent years,
word embedding has gradually become the basic knowledge of natural language process-
ing.
15.1.1One-HotVectorsArea Bad Choice
We used one-hot vectorsto represent words(characters are words)in Section 9.5 . Suppose
that the number of different words in the dictionary (the dictionary size) is ùëÅ, and each
word corresponds to a different integer (index) from 0toùëÅ 1. To obtain the one-hot
vectorrepresentationforanywordwithindex ùëñ, wecreatealength- ùëÅvectorwithall0sand
set the element at position ùëñto 1. In this way, each word is represented as a vector of length
ùëÅ, and it can be used directly by neural networks.
Althoughone-hot wordvectorsare easyto construct, theyare usuallynot a goodchoice. A
main reason is that one-hot word vectors cannot accurately express the similarity between
differentwords,suchasthe cosinesimilarity thatweoftenuse. Forvectors x,y2Rùëë,their
cosine similarity is the cosine of the angle between them:
x>y
kxkkyk2¬ª  1,1¬º. (15.1.1)
Sincethecosinesimilaritybetweenone-hotvectorsofanytwodifferentwordsis0,one-hot
vectors cannot encode similarities among words.
15.1.2Self-Supervisedword2vec
Theword2vec227tool was proposed to address the above issue. It maps each word to a
692 Natural Language Processing: Pretraining
fixed-lengthvector,andthesevectorscanbetterexpressthesimilarityandanalogyrelation-
ship among different words. The word2vec tool contains two models, namely skip-gram
(Mikolovet al., 2013) andcontinuous bag of words (CBOW) ( Mikolovet al., 2013). For
semantically meaningful representations, their training relies on conditional probabilities
thatcanbeviewedaspredictingsomewordsusingsomeoftheirsurroundingwordsincor-
pora. Sincesupervisioncomesfromthedatawithoutlabels,bothskip-gramandcontinuous
bag of words are self-supervised models.
In the following, we will introduce these two models and their training methods.
15.1.3TheSkip-Gram Model
Theskip-gram modelassumesthatawordcanbeusedtogenerateitssurroundingwordsin
a text sequence. Take the text sequence ‚Äúthe‚Äù, ‚Äúman‚Äù, ‚Äúloves‚Äù, ‚Äúhis‚Äù, ‚Äúson‚Äù as an example.
Let‚Äôs choose ‚Äúloves‚Äù as the centerword and set the context window size to 2. As shown in
Fig. 15.1.1 , given the center word ‚Äúloves‚Äù, the skip-gram model considers the conditional
probability for generating the contextwords : ‚Äúthe‚Äù, ‚Äúman‚Äù, ‚Äúhis‚Äù, and ‚Äúson‚Äù, which are no
more than 2 words away from the center word:
ùëÉ¬π‚Äùthe‚Äù,‚Äùman‚Äù,‚Äùhis‚Äù,‚Äùson‚Äùj‚Äùloves‚Äù¬∫. (15.1.2)
Assume that the context words are independently generated given the center word (i.e.,
conditional independence). In this case, the above conditional probability can be rewritten
as
ùëÉ¬π‚Äùthe‚Äùj‚Äùloves‚Äù¬∫ùëÉ¬π‚Äùman‚Äùj‚Äùloves‚Äù¬∫ùëÉ¬π‚Äùhis‚Äùj‚Äùloves‚Äù¬∫ùëÉ¬π‚Äùson‚Äùj‚Äùloves‚Äù¬∫.
(15.1.3)
tFig. 15.1.1 The skip-gram model considers the conditional probability of generating the surrounding
context words given a center word.
In the skip-gram model, each word has two ùëë-dimensional-vector representations for cal-
culating conditional probabilities. More concretely, for any word with index ùëñin the dic-
tionary, denote by vùëñ2Rùëëanduùëñ2Rùëëits two vectors when used as a centerword and a
contextword, respectively. The conditional probability of generating any context word ùë§ùëú
(with indexùëúin the dictionary) given the center word ùë§ùëê(with indexùëêin the dictionary)
can be modeled by a softmax operation on vector dot products:
ùëÉ¬πùë§ùëújùë§ùëê¬∫=exp¬πu>
ùëúvùëê¬∫√ç
ùëñ2Vexp¬πu>
ùëñvùëê¬∫, (15.1.4)
693 Word Embedding (word2vec)
where the vocabulary index set V=f0,1,...,jVj  1g. Given a text sequence of length
ùëá, where the word at time step ùë°is denoted as ùë§¬πùë°¬∫. Assume that context words are in-
dependently generated given any center word. For context window size ùëö, the likelihood
function of the skip-gram model is the probability of generating all context words given
any center word:
ùëá√ñ
ùë°=1√ñ
 ùëöùëóùëö, ùëó‚â†0ùëÉ¬πùë§¬πùë°¬∏ùëó¬∫jùë§¬πùë°¬∫¬∫, (15.1.5)
where any time step that is less than 1or greater than ùëácan be omitted.
Training
The skip-gram model parameters are the center word vector and context word vector for
eachwordinthevocabulary. Intraining,welearnthemodelparametersbymaximizingthe
likelihoodfunction(i.e.,maximumlikelihoodestimation). Thisisequivalenttominimizing
the following loss function:
 ùëá√ï
ùë°=1√ï
 ùëöùëóùëö, ùëó‚â†0logùëÉ¬πùë§¬πùë°¬∏ùëó¬∫jùë§¬πùë°¬∫¬∫. (15.1.6)
When using stochastic gradient descent to minimize the loss, in each iteration we can ran-
domly sample a shorter subsequence to calculate the (stochastic) gradient for this subse-
quence to update the model parameters. To calculate this (stochastic) gradient, we need
to obtain the gradients of the log conditional probability with respect to the center word
vector and the context word vector. In general, according to (15.1.4 )the log conditional
probability involving any pair of the center word ùë§ùëêand the context word ùë§ùëúis
logùëÉ¬πùë§ùëújùë§ùëê¬∫=u>
ùëúvùëê log √ï
ùëñ2Vexp¬πu>
ùëñvùëê¬∫!
. (15.1.7)
Through differentiation, we can obtain its gradient with respect to the center word vector
vùëêas
ùúïlogùëÉ¬πùë§ùëújùë§ùëê¬∫
ùúïvùëê=uùëú √ç
ùëó2Vexp¬πu>
ùëóvùëê¬∫uùëó√ç
ùëñ2Vexp¬πu>
ùëñvùëê¬∫
=uùëú √ï
ùëó2V exp¬πu>
ùëóvùëê¬∫
√ç
ùëñ2Vexp¬πu>
ùëñvùëê¬∫!
uùëó
=uùëú √ï
ùëó2VùëÉ¬πùë§ùëójùë§ùëê¬∫uùëó.(15.1.8)
Note that the calculation in (15.1.8 )requires the conditional probabilities of all words in
the dictionary with ùë§ùëêas the center word. The gradients for the other word vectors can be
obtained in the same way.
After training, for any word with index ùëñin the dictionary, we obtain both word vectors
694 Natural Language Processing: Pretraining
vùëñ(as the center word) and uùëñ(as the context word). In natural language processing ap-
plications, the center word vectors of the skip-gram model are typically used as the word
representations.
15.1.4TheContinuous Bag of Words(CBOW)Model
Thecontinuousbagofwords (CBOW)modelissimilartotheskip-grammodel. Themajor
differencefromtheskip-grammodelisthatthecontinuousbagofwordsmodelassumesthat
acenterwordisgeneratedbasedonitssurroundingcontextwordsinthetextsequence. For
example,inthesametextsequence‚Äúthe‚Äù,‚Äúman‚Äù,‚Äúloves‚Äù,‚Äúhis‚Äù,and‚Äúson‚Äù,with‚Äúloves‚Äùas
the center word and the context window size being 2, the continuous bag of words model
considers the conditional probability of generating the center word ‚Äúloves‚Äù based on the
context words ‚Äúthe‚Äù, ‚Äúman‚Äù, ‚Äúhis‚Äù and ‚Äúson‚Äù (as shown in Fig. 15.1.2 ), which is
ùëÉ¬π‚Äùloves‚Äùj‚Äùthe‚Äù,‚Äùman‚Äù,‚Äùhis‚Äù,‚Äùson‚Äù¬∫. (15.1.9)
tFig. 15.1.2 The continuous bag of words model considers the conditional probability of generating
the center word given its surrounding context words.
Sincetherearemultiplecontextwordsinthecontinuousbagofwordsmodel, thesecontext
word vectorsare averagedin the calculation of the conditional probability. Specifically, for
any word with index ùëñin the dictionary, denote by vùëñ2Rùëëanduùëñ2Rùëëits two vectors
when used as a contextword and a centerword (meanings are switched in the skip-gram
model), respectively. The conditional probability of generating any center word ùë§ùëê(with
indexùëêin the dictionary) given its surrounding context words ùë§ùëú1,...,ùë§ùëú2ùëö(with index
ùëú1,...,ùëú 2ùëöin the dictionary) can be modeled by
ùëÉ¬πùë§ùëêjùë§ùëú1,...,ùë§ùëú2ùëö¬∫=exp
1
2ùëöu>
ùëê¬πvùëú1¬∏...¬∏vùëú2ùëö¬∫
√ç
ùëñ2Vexp
1
2ùëöu>
ùëñ¬πvùëú1¬∏...¬∏vùëú2ùëö¬∫. (15.1.10)
Forbrevity,letWùëú=fùë§ùëú1,...,ùë§ùëú2ùëögand ¬Øvùëú= vùëú1¬∏...¬∏vùëú2ùëö¬ù¬π2ùëö¬∫. Then (15.1.10 )
can be simplified as
ùëÉ¬πùë§ùëêjWùëú¬∫=exp u>
ùëê¬Øvùëú
√ç
ùëñ2Vexp u>
ùëñ¬Øvùëú. (15.1.11)
Given a text sequence of length ùëá, where the word at time step ùë°is denoted as ùë§¬πùë°¬∫. For
695 Word Embedding (word2vec)
context window size ùëö, the likelihood function of the continuous bag of words model is
the probability of generating all center words given their context words:
ùëá√ñ
ùë°=1ùëÉ¬πùë§¬πùë°¬∫jùë§¬πùë° ùëö¬∫,...,ùë§¬πùë° 1¬∫,ùë§¬πùë°¬∏1¬∫,...,ùë§¬πùë°¬∏ùëö¬∫¬∫. (15.1.12)
Training
Trainingcontinuousbagofwordsmodelsisalmostthesameastrainingskip-grammodels.
The maximum likelihood estimation of the continuous bag of words model is equivalent to
minimizing the following loss function:
 ùëá√ï
ùë°=1logùëÉ¬πùë§¬πùë°¬∫jùë§¬πùë° ùëö¬∫,...,ùë§¬πùë° 1¬∫,ùë§¬πùë°¬∏1¬∫,...,ùë§¬πùë°¬∏ùëö¬∫¬∫. (15.1.13)
Notice that
logùëÉ¬πùë§ùëêjWùëú¬∫=u>
ùëê¬Øvùëú log √ï
ùëñ2Vexp u>
ùëñ¬Øvùëú!
. (15.1.14)
Through differentiation, we can obtain its gradient with respect to any context word vector
vùëúùëñ(ùëñ=1,..., 2ùëö) as
ùúïlogùëÉ¬πùë§ùëêjWùëú¬∫
ùúïvùëúùëñ=1
2ùëö¬©¬≠
¬´uùëê √ï
ùëó2Vexp¬πu>
ùëó¬Øvùëú¬∫uùëó√ç
ùëñ2Vexp¬πu>
ùëñ¬Øvùëú¬∫¬™¬Æ
¬¨=1
2ùëö¬©¬≠
¬´uùëê √ï
ùëó2VùëÉ¬πùë§ùëójWùëú¬∫uùëó¬™¬Æ
¬¨.
(15.1.15)
The gradients for the other word vectors can be obtained in the same way. Unlike the skip-
gram model, the continuous bag of words model typically uses context word vectors as the
word representations.
15.1.5Summary
Word vectors are vectors used to represent words, and can also be considered as feature
vectors or representations of words. The technique of mapping words to real vectors
is called word embedding.
The word2vec tool contains both the skip-gram and continuous bag of words models.
Theskip-grammodelassumesthatawordcanbeusedtogenerateitssurroundingwords
in a text sequence; while the continuous bag of words model assumes that a center
word is generated based on its surrounding context words.
15.1.6Exercises
1.Whatis the computational complexityforcalculating eachgradient? What could be the
issue if the dictionary size is huge?
696 Natural Language Processing: Pretraining
2282.Some fixed phrases in English consist of multiple words, such as ‚Äúnew york‚Äù. How to
train their word vectors? Hint: see Section 4 in the word2vec paper ( Mikolovet al.,
2013).
3.Let‚Äôsreflectontheword2vecdesignbytakingtheskip-grammodelasanexample. What
is the relationship between the dot product of two word vectors in the skip-gram model
and the cosine similarity? For a pair of words with similar semantics, why may the
cosine similarity of their word vectors (trained by the skip-gram model) be high?
Discussions228.
15.2ApproximateTraining
Recall our discussions in Section 15.1 . The main idea of the skip-gram model is using
softmax operations to calculate the conditional probability of generating a context word
ùë§ùëúbased on the given center word ùë§ùëêin(15.1.4 ), whose corresponding logarithmic loss
is given by the opposite of (15.1.7 ).
Due to the nature of the softmax operation, since a context word may be anyone in the
dictionaryV, the opposite of (15.1.7 )contains the summation of items as many as the
entire size of the vocabulary. Consequently, the gradient calculation for the skip-gram
modelin (15.1.8 )andthatforthecontinuousbag-of-wordsmodelin (15.1.15 )bothcontain
the summation. Unfortunately, the computational cost for such gradients that sum over a
large dictionary (often with hundreds of thousands or millions of words) is huge!
Inordertoreducetheaforementionedcomputationalcomplexity,thissectionwillintroduce
twoapproximatetrainingmethods: negativesampling andhierarchicalsoftmax . Duetothe
similarity between the skip-gram model and the continuous bag of words model, we will
just take the skip-gram model as an example to describe these two approximate training
methods.
15.2.1NegativeSampling
Negative sampling modifies the original objective function. Given the context window of
a center word ùë§ùëê, the fact that any (context) word ùë§ùëúcomes from this context window is
considered as an event with the probability modeled by
ùëÉ¬πùê∑=1jùë§ùëê,ùë§ùëú¬∫=ùúé¬πu>
ùëúvùëê¬∫, (15.2.1)
whereùúéuses the definition of the sigmoid activation function:
ùúé¬πùë•¬∫=1
1¬∏exp¬π ùë•¬∫. (15.2.2)
Let‚Äôs begin by maximizing the joint probability of all such events in text sequences to train
word embeddings. Specifically, given a text sequence of length ùëá, denote byùë§¬πùë°¬∫the word
697 Approximate Training
at time stepùë°and let the context window size be ùëö, consider maximizing the joint proba-
bility
ùëá√ñ
ùë°=1√ñ
 ùëöùëóùëö, ùëó‚â†0ùëÉ¬πùê∑=1jùë§¬πùë°¬∫,ùë§¬πùë°¬∏ùëó¬∫¬∫. (15.2.3)
However, (15.2.3 )only considers those events that involve positive examples. As a result,
the joint probability in (15.2.3 )is maximized to 1 only if all the word vectors are equal
to infinity. Of course, such results are meaningless. To make the objective function more
meaningful, negative sampling adds negative examples sampled from a predefined distri-
bution.
Denote byùëÜthe event that a context word ùë§ùëúcomes from the context window of a cen-
ter wordùë§ùëê. For this event involving ùë§ùëú, from a predefined distribution ùëÉ¬πùë§¬∫sampleùêæ
noise words that are not from this context window. Denote by ùëÅùëòthe event that a noise
wordùë§ùëò(ùëò=1,...,ùêæ) does not come from the context window of ùë§ùëê. Assume that these
eventsinvolvingboththepositiveexampleandnegativeexamples ùëÜ,ùëÅ 1,...,ùëÅùêæaremutu-
ally independent. Negative sampling rewrites the joint probability (involving only positive
examples) in (15.2.3 )as
ùëá√ñ
ùë°=1√ñ
 ùëöùëóùëö, ùëó‚â†0ùëÉ¬πùë§¬πùë°¬∏ùëó¬∫jùë§¬πùë°¬∫¬∫, (15.2.4)
where the conditional probability is approximated through events ùëÜ,ùëÅ 1,...,ùëÅùêæ:
ùëÉ¬πùë§¬πùë°¬∏ùëó¬∫jùë§¬πùë°¬∫¬∫=ùëÉ¬πùê∑=1jùë§¬πùë°¬∫,ùë§¬πùë°¬∏ùëó¬∫¬∫ùêæ√ñ
ùëò=1, ùë§ùëòùëÉ¬πùë§¬∫ùëÉ¬πùê∑=0jùë§¬πùë°¬∫,ùë§ùëò¬∫.(15.2.5)
Denote byùëñùë°and‚Ñéùëòthe indices of a word ùë§¬πùë°¬∫at time stepùë°of a text sequence and a noise
wordùë§ùëò, respectively. The logarithmic loss with respect to the conditional probabilities in
(15.2.5 )is
 logùëÉ¬πùë§¬πùë°¬∏ùëó¬∫jùë§¬πùë°¬∫¬∫= logùëÉ¬πùê∑=1jùë§¬πùë°¬∫,ùë§¬πùë°¬∏ùëó¬∫¬∫ ùêæ√ï
ùëò=1, ùë§ùëòùëÉ¬πùë§¬∫logùëÉ¬πùê∑=0jùë§¬πùë°¬∫,ùë§ùëò¬∫
= logùúé
u>
ùëñùë°¬∏ùëóvùëñùë°
 ùêæ√ï
ùëò=1, ùë§ùëòùëÉ¬πùë§¬∫log
1 ùúé
u>
‚Ñéùëòvùëñùë°
= logùúé
u>
ùëñùë°¬∏ùëóvùëñùë°
 ùêæ√ï
ùëò=1, ùë§ùëòùëÉ¬πùë§¬∫logùúé
 u>
‚Ñéùëòvùëñùë°
.
(15.2.6)
We can see that now the computational cost for gradients at each training step has nothing
todowiththedictionarysize, butlinearlydependson ùêæ. Whensettingthehyperparameter
ùêætoasmallervalue,thecomputationalcostforgradientsateachtrainingstepwithnegative
sampling is smaller.
698 Natural Language Processing: Pretraining
15.2.2HierarchicalSoftmax
Asanalternativeapproximatetrainingmethod, hierarchicalsoftmax usesthebinarytree, a
data structure illustrated in Fig. 15.2.1 , where each leaf node of the tree represents a word
in dictionaryV.
tFig. 15.2.1 Hierarchical softmax for approximate training, where each leaf node of the tree represents
a word in the dictionary.
Denote byùêø¬πùë§¬∫the number of nodes (including both ends) on the path from the root node
to the leaf node representing word ùë§in the binary tree. Let ùëõ¬πùë§,ùëó¬∫be theùëóthnode on this
path, with its context word vector being uùëõ¬πùë§,ùëó¬∫. For example, ùêø¬πùë§3¬∫=4inFig. 15.2.1 .
Hierarchical softmax approximates the conditional probability in (15.1.4 )as
ùëÉ¬πùë§ùëújùë§ùëê¬∫=ùêø¬πùë§ùëú¬∫ 1√ñ
ùëó=1ùúé
¬ª¬ªùëõ¬πùë§ùëú,ùëó¬∏1¬∫=leftChild¬πùëõ¬πùë§ùëú,ùëó¬∫¬∫¬º¬ºu>
ùëõ¬πùë§ùëú,ùëó¬∫vùëê
,
(15.2.7)
where function ùúéis defined in (15.2.2 ), and leftChild¬πùëõ¬∫is the left child node of node ùëõ:
ifùë•is true,¬ª¬ªùë•¬º¬º=1; otherwise¬ª¬ªùë•¬º¬º= 1.
To illustrate, let‚Äôs calculate the conditional probability of generating word ùë§3given word
ùë§ùëêinFig. 15.2.1 . This requires dot products between the word vector vùëêofùë§ùëêand non-
leaf node vectors on the path (the path in bold in Fig. 15.2.1 ) from the root to ùë§3, which is
traversed left, right, then left:
ùëÉ¬πùë§3jùë§ùëê¬∫=ùúé¬πu>
ùëõ¬πùë§3,1¬∫vùëê¬∫ùúé¬π u>
ùëõ¬πùë§3,2¬∫vùëê¬∫ùúé¬πu>
ùëõ¬πùë§3,3¬∫vùëê¬∫. (15.2.8)
Sinceùúé¬πùë•¬∫¬∏ùúé¬π ùë•¬∫=1, it holds that the conditional probabilities of generating all the
words in dictionary Vbased on any word ùë§ùëêsum up to one:
√ï
ùë§2VùëÉ¬πùë§jùë§ùëê¬∫=1.(15.2.9)
Fortunately, since ùêø¬πùë§ùëú¬∫ 1is on the order ofO¬πlog2jVj¬∫due to the binary tree struc-
ture, when the dictionary size Vis huge, the computational cost for each training step us-
ing hierarchical softmax is significantly reduced compared with that without approximate
training.
699 The Dataset for Pretraining Word Embeddings
229
23015.2.3Summary
Negative sampling constructs the loss function by considering mutually independent
events that involve both positive and negative examples. The computational cost for
training is linearly dependent on the number of noise words at each step.
Hierarchical softmax constructs the loss function using the path from the root node to
the leaf node in the binary tree. The computational cost for training is dependent on
the logarithm of the dictionary size at each step.
15.2.4Exercises
1.How can we sample noise words in negative sampling?
2.Verify that (15.2.9 )holds.
3.Howtotrainthecontinuousbagofwordsmodelusingnegativesamplingandhierarchi-
cal softmax, respectively?
Discussions229.
15.3The DatasetforPretrainingWordEmbeddings
Now that we know the technical details of the word2vec models and approximate training
methods, let‚Äôs walk through their implementations. Specifically, we will take the skip-
gram model in Section 15.1 and negative sampling in Section 15.2 as an example. In this
section, we begin with the dataset for pretraining the word embedding model: the original
format of the data will be transformed into minibatches that can be iterated over during
training.
import collections
import math
import os
import random
import torch
from d2l import torch asd2l
15.3.1Readingthe Dataset
Thedatasetthatweusehereis PennTreeBank(PTB)230.ThiscorpusissampledfromWall
Street Journal articles, split into training, validation, and test sets. In the original format,
each line of the text file represents a sentence of words that are separated by spaces. Here
we treat each word as a token.
700 Natural Language Processing: Pretraining
#@save
d2l.DATA_HUB[ 'ptb']=(d2l .DATA_URL +'ptb.zip ',
'319d85e578af0cdc590547f26231e4e31cdf1e42 ')
#@save
def read_ptb ():
"""Load the PTB dataset into a list of text lines."""
data_dir =d2l.download_extract( 'ptb')
# Read the training set
with open (os.path .join(data_dir, 'ptb.train.txt '))asf:
raw_text =f.read()
return [line .split() for line inraw_text .split( '\n')]
sentences =read_ptb()
f'# sentences: {len(sentences) }'
Downloading ../data /ptb.zip from http ://d2l-data .s3-accelerate .amazonaws .com/
‚Ü©!ptb.zip...
'# sentences: 42069 '
After reading the training set, we build a vocabulary for the corpus, where any word that
appears less than 10 times is replaced by the ‚Äú<unk>‚Äù token. Note that the original dataset
also contains ‚Äú<unk>‚Äù tokens that represent rare (unknown) words.
vocab =d2l.Vocab(sentences, min_freq =10)
f'vocab size: {len(vocab) }'
'vocab size: 6719 '
15.3.2Subsampling
Text data typically have high-frequency words such as ‚Äúthe‚Äù, ‚Äúa‚Äù, and ‚Äúin‚Äù: they may even
occur billions of times in very large corpora. However, these words often co-occur with
many different words in context windows, providing little useful signals. For instance,
consider the word ‚Äúchip‚Äù in a context window: intuitively its co-occurrence with a low-
frequency word ‚Äúintel‚Äù is more useful in training than the co-occurrence with a high-
frequency word ‚Äúa‚Äù. Moreover, training with vast amounts of (high-frequency) words is
slow. Thus, when training word embedding models, high-frequency words can be sub-
sampled (Mikolovet al., 2013). Specifically, each indexed word ùë§ùëñin the dataset will be
discarded with probability
ùëÉ¬πùë§ùëñ¬∫=max
1 rùë°
ùëì¬πùë§ùëñ¬∫,0
, (15.3.1)
whereùëì¬πùë§ùëñ¬∫is the ratio of the number of words ùë§ùëñto the total number of words in the
dataset, and the constant ùë°is a hyperparameter ( 10 4in the experiment). We can see that
701 The Dataset for Pretraining Word Embeddings
onlywhentherelativefrequency ùëì¬πùë§ùëñ¬∫>ùë°canthe(high-frequency)word ùë§ùëñbediscarded,
and the higher the relative frequency of the word, the greater the probability of being dis-
carded.
#@save
def subsample (sentences, vocab):
"""Subsample high-frequency words."""
# Exclude unknown tokens ('<unk>')
sentences =[[token for token inline ifvocab[token] !=vocab .unk]
for line insentences]
counter =collections .Counter([
token for line insentences for token inline])
num_tokens =sum(counter .values())
# Return True if `token` is kept during subsampling
def keep (token):
return (random .uniform( 0,1)<
math .sqrt( 1e-4 /counter[token] *num_tokens))
return ([[token for token inline ifkeep(token)] for line insentences],
counter)
subsampled, counter =subsample(sentences, vocab)
The following code snippet plots the histogram of the number of tokens per sentence be-
fore and after subsampling. As expected, subsampling significantly shortens sentences by
dropping high-frequency words, which will lead to training speedup.
d2l.show_list_len_pair_hist([ 'origin ','subsampled '],'# tokens per sentence ',
'count ', sentences, subsampled);
For individual tokens, the sampling rate of the high-frequency word ‚Äúthe‚Äù is less than
1/20.
def compare_counts (token):
return (f'# of "{token }":'
f'before= {sum([l.count(token) for linsentences]) },'
f'after= {sum([l.count(token) for linsubsampled]) }')
compare_counts( 'the')
702 Natural Language Processing: Pretraining
'# of "the": before=50770, after=2010 '
In contrast, low-frequency words ‚Äújoin‚Äù are completely kept.
compare_counts( 'join ')
'# of "join ": before=45, after=45 '
After subsampling, we map tokens to their indices for the corpus.
corpus =[vocab[line] for line insubsampled]
corpus[: 3]
[[], [ 4127 ,3228 ,1773 ], [ 3922 ,1922 ,4743 ,2696 ]]
15.3.3Extracting Center Wordsand ContextWords
Thefollowing get_centers_and_contexts functionextractsallthecenterwordsandtheir
contextwordsfrom corpus. Ituniformlysamplesanintegerbetween1and max_window_size
at random as the context window size. For any center word, those words whose distance
from it does not exceed the sampled context window size are its context words.
#@save
def get_centers_and_contexts (corpus, max_window_size):
"""Return center words and context words in skip-gram."""
centers, contexts =[], []
for line incorpus:
# To form a "center word--context word" pair, each sentence needs to
# have at least 2 words
iflen(line) <2:
continue
centers +=line
for iinrange (len(line)): # Context window centered at `i`
window_size =random .randint( 1, max_window_size)
indices =list (range (max(0, i -window_size),
min(len(line), i +1+window_size)))
# Exclude the center word from the context words
indices .remove(i)
contexts .append([line[idx] for idx inindices])
return centers, contexts
Next,wecreateanartificialdatasetcontainingtwosentencesof7and3words,respectively.
Let the maximum context window size be 2 and print all the center words and their context
words.
703 The Dataset for Pretraining Word Embeddings
tiny_dataset =[list (range (7)), list (range (7,10))]
print ('dataset ', tiny_dataset)
for center, context inzip(*get_centers_and_contexts(tiny_dataset, 2)):
print ('center ', center, 'has contexts ', context)
dataset [[ 0,1,2,3,4,5,6], [ 7,8,9]]
center 0has contexts [ 1]
center 1has contexts [ 0,2]
center 2has contexts [ 0,1,3,4]
center 3has contexts [ 1,2,4,5]
center 4has contexts [ 2,3,5,6]
center 5has contexts [ 3,4,6]
center 6has contexts [ 5]
center 7has contexts [ 8,9]
center 8has contexts [ 7,9]
center 9has contexts [ 7,8]
When training on the PTB dataset, we set the maximum context window size to 5. The
following extracts all the center words and their context words in the dataset.
all_centers, all_contexts =get_centers_and_contexts(corpus, 5)
f'# center-context pairs: {sum([len(contexts) for contexts inall_contexts]) }'
'# center-context pairs: 1503420 '
15.3.4NegativeSampling
We use negative sampling for approximate training. To sample noise words according to a
predefineddistribution,wedefinethefollowing RandomGenerator class,wherethe(possi-
blyunnormalized)samplingdistributionispassedviatheargument sampling_weights .
#@save
class RandomGenerator :
"""Randomly draw among {1, ..., n} according to n sampling weights."""
def __init__ (self , sampling_weights):
# Exclude
self .population =list (range (1,len(sampling_weights) +1))
self .sampling_weights =sampling_weights
self .candidates =[]
self .i=0
def draw (self ):
ifself .i==len(self .candidates):
# Cache `k` random sampling results
self .candidates =random .choices(
self .population, self .sampling_weights, k =10000 )
self .i=0
self .i+=1
return self .candidates[ self .i-1]
704 Natural Language Processing: Pretraining
For example, we can draw 10 random variables ùëãamong indices 1, 2, and 3 with sampling
probabilities ùëÉ¬πùëã=1¬∫=2¬ù9,ùëÉ¬πùëã=2¬∫=3¬ù9, andùëÉ¬πùëã=3¬∫=4¬ù9as follows.
For a pair of center word and context word, we randomly sample K(5 in the experiment)
noisewords. Accordingtothesuggestionsintheword2vecpaper, thesamplingprobability
ùëÉ¬πùë§¬∫of a noise word ùë§is set to its relative frequency in the dictionary raised to the power
of 0.75 ( Mikolovetal., 2013).
#@save
def get_negatives (all_contexts, vocab, counter, K):
"""Return noise words in negative sampling."""
# Sampling weights for words with indices 1, 2, ... (index 0 is the
# excluded unknown token) in the vocabulary
sampling_weights =[counter[vocab .to_tokens(i)] **0.75
for iinrange (1,len(vocab))]
all_negatives, generator =[], RandomGenerator(sampling_weights)
for contexts inall_contexts:
negatives =[]
while len(negatives) <len(contexts) *K:
neg =generator .draw()
# Noise words cannot be context words
ifneg not incontexts:
negatives .append(neg)
all_negatives .append(negatives)
return all_negatives
all_negatives =get_negatives(all_contexts, vocab, counter, 5)
15.3.5LoadingTrainingExamplesin Minibatches
After all the center words together with their context words and sampled noise words are
extracted, they will be transformed into minibatches of examples that can be iteratively
loaded during training.
Inaminibatch,the ùëñthexampleincludesacenterwordandits ùëõùëñcontextwordsand ùëöùëñnoise
words. Due to varying context window sizes, ùëõùëñ¬∏ùëöùëñvaries for different ùëñ. Thus, for each
example we concatenate its context words and noise words in the contexts_negatives
variable, and pad zeros until the concatenation length reaches maxùëñùëõùëñ¬∏ùëöùëñ(max_len ). To
excludepaddingsinthecalculationoftheloss,wedefineamaskvariable masks. Thereisa
one-to-onecorrespondencebetweenelementsin masksandelementsin contexts_negatives ,
wherezeros(otherwiseones)in maskscorrespondtopaddingsin contexts_negatives .
To distinguish between positive and negative examples, we separate context words from
noisewordsin contexts_negatives viaa labelsvariable. Similarto masks,thereisalso
aone-to-onecorrespondencebetweenelementsin labelsandelementsin contexts_negatives ,
where ones (otherwise zeros) in labelscorrespond to context words (positive examples)
incontexts_negatives .
The above idea is implemented in the following batchify function. Its input datais a
list with length equal to the batch size, where each element is an example consisting of
the center word center, its context words context , and its noise words negative . This
705 The Dataset for Pretraining Word Embeddings
function returns a minibatch that can be loaded for calculations during training, such as
including the mask variable.
#@save
def batchify (data):
"""Return a minibatch of examples for skip-gram with negative sampling."""
max_len =max(len(c) +len(n) for _, c, n indata)
centers, contexts_negatives, masks, labels =[], [], [], []
for center, context, negative indata:
cur_len =len(context) +len(negative)
centers +=[center]
contexts_negatives +=[context +negative +[0]*(max_len -cur_len)]
masks +=[[1]*cur_len +[0]*(max_len -cur_len)]
labels +=[[1]*len(context) +[0]*(max_len -len(context))]
return (torch .tensor(centers) .reshape(( -1,1)), torch .tensor(
contexts_negatives), torch .tensor(masks), torch .tensor(labels))
Let‚Äôs test this function using a minibatch of two examples.
x_1 =(1, [2,2], [ 3,3,3,3])
x_2 =(1, [2,2,2], [ 3,3])
batch =batchify((x_1, x_2))
names =['centers ','contexts_negatives ','masks ','labels ']
for name, data inzip(names, batch):
print (name, '=', data)
centers =tensor([[ 1],
[1]])
contexts_negatives =tensor([[ 2,2,3,3,3,3],
[2,2,2,3,3,0]])
masks =tensor([[ 1,1,1,1,1,1],
[1,1,1,1,1,0]])
labels =tensor([[ 1,1,0,0,0,0],
[1,1,1,0,0,0]])
15.3.6PuttingIt All Together
Last,wedefinethe load_data_ptb functionthatreadsthePTBdatasetandreturnsthedata
iterator and the vocabulary.
#@save
def load_data_ptb (batch_size, max_window_size, num_noise_words):
"""Download the PTB dataset and then load it into memory."""
num_workers =d2l.get_dataloader_workers()
sentences =read_ptb()
vocab =d2l.Vocab(sentences, min_freq =10)
subsampled, counter =subsample(sentences, vocab)
corpus =[vocab[line] for line insubsampled]
all_centers, all_contexts =get_centers_and_contexts(
corpus, max_window_size)
(continues on next page)
706 Natural Language Processing: Pretraining
(continued from previous page)
all_negatives =get_negatives(
all_contexts, vocab, counter, num_noise_words)
class PTBDataset (torch .utils .data .Dataset):
def __init__ (self , centers, contexts, negatives):
assert len(centers) ==len(contexts) ==len(negatives)
self .centers =centers
self .contexts =contexts
self .negatives =negatives
def __getitem__ (self , index):
return (self .centers[index], self .contexts[index],
self .negatives[index])
def __len__ (self ):
return len(self .centers)
dataset =PTBDataset(all_centers, all_contexts, all_negatives)
data_iter =torch .utils .data .DataLoader(dataset, batch_size, shuffle =True ,
collate_fn =batchify,
num_workers =num_workers)
return data_iter, vocab
Let‚Äôs print the first minibatch of the data iterator.
data_iter, vocab =load_data_ptb( 512,5,5)
for batch indata_iter:
for name, data inzip(names, batch):
print (name, 'shape: ', data .shape)
break
centers shape: torch .Size([ 512,1])
contexts_negatives shape: torch .Size([ 512,60])
masks shape: torch .Size([ 512,60])
labels shape: torch .Size([ 512,60])
15.3.7Summary
High-frequency words may not be so useful in training. We can subsample them for
speedup in training.
For computational efficiency, we load examples in minibatches. We can define other
variablestodistinguishpaddingsfromnon-paddings,andpositiveexamplesfromneg-
ative ones.
15.3.8Exercises
1.How does the running time of code in this section changes if not using subsampling?
707 Pretraining word2vec
2312.TheRandomGenerator class caches krandom sampling results. Set kto other values
and see how it affects the data loading speed.
3.What other hyperparameters in the code of this section may affect the data loading
speed?
Discussions231.
15.4Pretrainingword2vec
Wegoontoimplementtheskip-grammodeldefinedin Section15.1 . Thenwewillpretrain
word2vec using negative sampling on the PTB dataset. First of all, let‚Äôs obtain the data
iterator and the vocabulary for this dataset by calling the d2l.load_data_ptb function,
which was described in Section 15.3
import math
import torch
from torch import nn
from d2l import torch asd2l
batch_size, max_window_size, num_noise_words =512,5,5
data_iter, vocab =d2l.load_data_ptb(batch_size, max_window_size,
num_noise_words)
15.4.1TheSkip-Gram Model
We implement the skip-gram model by using embedding layers and batch matrix multipli-
cations. First, let‚Äôs review how embedding layers work.
Embedding Layer
As described in Section 10.7 , an embedding layer maps a token‚Äôs index to its feature vec-
tor. The weight of this layer is a matrix whose number of rows equals to the dictio-
nary size ( input_dim ) and number of columns equals to the vector dimension for each
token ( output_dim ). After a word embedding model is trained, this weight is what we
need.
embed =nn.Embedding(num_embeddings =20, embedding_dim =4)
print (f'Parameter embedding_weight ( {embed .weight .shape },'
f'dtype= {embed .weight .dtype })')
Parameter embedding_weight (torch .Size([ 20,4]), dtype =torch .float32)
The input of an embedding layer is the index of a token (word). For any token index ùëñ, its
708 Natural Language Processing: Pretraining
vectorrepresentationcanbeobtainedfromthe ùëñthrowoftheweightmatrixintheembedding
layer. Since the vector dimension ( output_dim ) was set to 4, the embedding layer returns
vectors with shape (2, 3, 4) for a minibatch of token indices with shape (2, 3).
x=torch .tensor([[ 1,2,3], [ 4,5,6]])
embed(x)
tensor([[[ 0.7606 ,0.3872 ,-0.1864 ,1.1732 ],
[1.5035 ,2.3623 ,-1.7542 ,-1.4990 ],
[-1.2639 ,-1.5313 ,2.1719 ,0.4151 ]],
[[-1.9079 ,0.2434 ,1.5395 ,1.2990 ],
[0.7470 ,1.0129 ,0.4039 ,0.0591 ],
[-0.6293 ,-0.1814 ,-0.4782 ,-0.5289 ]]], grad_fn =<EmbeddingBackward0 >)
Defining the ForwardPropagation
In the forward propagation, the input of the skip-gram model includes the center word
indices centerofshape(batchsize,1)andtheconcatenatedcontextandnoisewordindices
contexts_and_negatives of shape (batch size, max_len ), where max_len is defined in
Section15.3.5 . Thesetwovariablesarefirsttransformedfromthetokenindicesintovectors
viatheembeddinglayer,thentheirbatchmatrixmultiplication(describedin Section11.3.2 )
returns an output of shape (batch size, 1, max_len ). Each element in the output is the dot
product of a center word vector and a context or noise word vector.
def skip_gram (center, contexts_and_negatives, embed_v, embed_u):
v=embed_v(center)
u=embed_u(contexts_and_negatives)
pred =torch .bmm(v, u .permute( 0,2,1))
return pred
Let‚Äôs print the output shape of this skip_gram function for some example inputs.
skip_gram(torch .ones(( 2,1), dtype =torch .long),
torch .ones(( 2,4), dtype =torch .long), embed, embed) .shape
torch .Size([ 2,1,4])
15.4.2Training
Before training the skip-gram model with negative sampling, let‚Äôs first define its loss func-
tion.
BinaryCross-EntropyLoss
According to the definition of the loss function for negative sampling in Section 15.2.1 , we
will use the binary cross-entropy loss.
709 Pretraining word2vec
class SigmoidBCELoss (nn.Module):
# Binary cross-entropy loss with masking
def __init__ (self ):
super ().__init__ ()
def forward (self , inputs, target, mask =None ):
out =nn.functional .binary_cross_entropy_with_logits(
inputs, target, weight =mask, reduction ="none ")
return out.mean(dim =1)
loss =SigmoidBCELoss()
Recall our descriptions of the mask variable and the label variable in Section 15.3.5 . The
following calculates the binary cross-entropy loss for the given variables.
pred =torch .tensor([[ 1.1,-2.2,3.3,-4.4]]*2)
label =torch .tensor([[ 1.0,0.0,0.0,0.0], [ 0.0,1.0,0.0,0.0]])
mask =torch .tensor([[ 1,1,1,1], [ 1,1,0,0]])
loss(pred, label, mask) *mask .shape[ 1]/mask .sum(axis =1)
tensor([ 0.9352 ,1.8462 ])
Belowshowshowtheaboveresultsarecalculated(inalessefficientway)usingthesigmoid
activationfunctioninthebinarycross-entropyloss. Wecanconsiderthetwooutputsastwo
normalized losses that are averaged over non-masked predictions.
def sigmd (x):
return -math .log( 1/(1+math .exp( -x)))
print (f'{(sigmd( 1.1)+sigmd( 2.2)+sigmd( -3.3)+sigmd( 4.4))/4:.4f}')
print (f'{(sigmd( -1.1)+sigmd( -2.2))/2:.4f}')
0.9352
1.8462
InitializingModel Parameters
We define two embedding layers for all the words in the vocabulary when they are used as
center words and context words, respectively. The word vector dimension embed_size is
set to 100.
embed_size =100
net =nn.Sequential(nn .Embedding(num_embeddings =len(vocab),
embedding_dim =embed_size),
nn.Embedding(num_embeddings =len(vocab),
embedding_dim =embed_size))
710 Natural Language Processing: Pretraining
Defining the TrainingLoop
The training loop is defined below. Because of the existence of padding, the calculation of
the loss function is slightly different compared to the previous training functions.
def train (net, data_iter, lr, num_epochs, device =d2l.try_gpu()):
def init_weights (module):
iftype (module) ==nn.Embedding:
nn.init .xavier_uniform_(module .weight)
net.apply(init_weights)
net =net.to(device)
optimizer =torch .optim .Adam(net .parameters(), lr =lr)
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[1, num_epochs])
# Sum of normalized losses, no. of normalized losses
metric =d2l.Accumulator( 2)
for epoch inrange (num_epochs):
timer, num_batches =d2l.Timer(), len(data_iter)
for i, batch inenumerate (data_iter):
optimizer .zero_grad()
center, context_negative, mask, label =[
data .to(device) for data inbatch]
pred =skip_gram(center, context_negative, net[ 0], net[ 1])
l=(loss(pred .reshape(label .shape) .float(), label .float(), mask)
/mask .sum(axis =1)*mask .shape[ 1])
l.sum() .backward()
optimizer .step()
metric .add(l .sum(), l .numel())
if(i+1)%(num_batches //5)==0ori==num_batches -1:
animator .add(epoch +(i+1)/num_batches,
(metric[ 0]/metric[ 1],))
print (f'loss {metric[ 0]/metric[ 1]:.3f},'
f'{metric[ 1]/timer .stop() :.1f}tokens/sec on {str(device) }')
Now we can train a skip-gram model using negative sampling.
lr, num_epochs =0.002 ,5
train(net, data_iter, lr, num_epochs)
loss 0.410 ,223485.0 tokens /sec on cuda: 0

711 Word Embedding with Global Vectors (GloVe)
23215.4.3ApplyingWordEmbeddings
After training the word2vec model, we can use the cosine similarity of word vectors from
the trained model to find words from the dictionary that are most semantically similar to
an input word.
def get_similar_tokens (query_token, k, embed):
W=embed .weight .data
x=W[vocab[query_token]]
# Compute the cosine similarity. Add 1e-9 for numerical stability
cos =torch .mv(W, x) /torch .sqrt(torch .sum(W *W, dim =1)*
torch .sum(x *x)+1e-9 )
topk =torch .topk(cos, k =k+1)[1].cpu() .numpy() .astype( 'int32 ')
for iintopk[ 1:]: # Remove the input words
print (f'cosine sim= {float (cos[i]) :.3f}:{vocab .to_tokens(i) }')
get_similar_tokens( 'chip ',3, net[ 0])
cosine sim =0.702 : microprocessor
cosine sim =0.649 : mips
cosine sim =0.643 : intel
15.4.4Summary
We can train a skip-gram model with negative sampling using embedding layers and the
binary cross-entropy loss.
Applicationsofwordembeddingsincludefindingsemanticallysimilarwordsforagiven
word based on the cosine similarity of word vectors.
15.4.5Exercises
1.Usingthetrainedmodel,findsemanticallysimilarwordsforotherinputwords. Canyou
improve the results by tuning hyperparameters?
2.When a training corpus is huge, we often sample context words and noise words for
the center words in the current minibatch when updating model parameters . In other
words,thesamecenterwordmayhavedifferentcontextwordsornoisewordsindifferent
training epochs. What are the benefits of this method? Try to implement this training
method.
Discussions232.
15.5WordEmbedding with Global Vectors (GloVe)
Word-word co-occurrences within context windows may carry rich semantic information.
For example, in a large corpus word ‚Äúsolid‚Äù is more likely to co-occur with ‚Äúice‚Äù than
712 Natural Language Processing: Pretraining
‚Äústeam‚Äù, but word ‚Äúgas‚Äù probably co-occurs with ‚Äústeam‚Äù more frequently than ‚Äúice‚Äù. Be-
sides, global corpus statistics of such co-occurrences can be precomputed: this can lead
to more efficient training. To leverage statistical information in the entire corpus for word
embedding,let‚Äôsfirstrevisittheskip-grammodelin Section15.1.3 ,butinterpretingitusing
global corpus statistics such as co-occurrence counts.
15.5.1Skip-Gramwith Global Corpus Statistics
Denoting by ùëûùëñùëóthe conditional probability ùëÉ¬πùë§ùëójùë§ùëñ¬∫of wordùë§ùëógiven wordùë§ùëñin the
skip-gram model, we have
ùëûùëñùëó=exp¬πu>
ùëóvùëñ¬∫
√ç
ùëò2Vexp¬πu>
ùëòvùëñ¬∫, (15.5.1)
where for any index ùëñvectors vùëñanduùëñrepresent word ùë§ùëñas the center word and context
word, respectively, and V=f0,1,...,jVj  1gis the index set of the vocabulary.
Consider word ùë§ùëñthat may occur multiple times in the corpus. In the entire corpus, all the
context words wherever ùë§ùëñis taken as their center word form a multisetCùëñof word indices
thatallows for multiple instances of the same element . For any element, its number of in-
stancesiscalledits multiplicity . Toillustratewithanexample,supposethatword ùë§ùëñoccurs
twice in the corpus and indices of the context words that take ùë§ùëñas their center word in the
twocontextwindowsare ùëò,ùëó,ùëö,ùëò andùëò,ùëô,ùëò,ùëó. Thus,multisetCùëñ=fùëó,ùëó,ùëò,ùëò,ùëò,ùëò,ùëô,ùëög,
where multiplicities of elements ùëó,ùëò,ùëô,ùëö are 2, 4, 1, 1, respectively.
Now let‚Äôs denote the multiplicity of element ùëóin multisetCùëñasùë•ùëñùëó. This is the global co-
occurrence count of word ùë§ùëó(as the context word) and word ùë§ùëñ(as the center word) in
the same context window in the entire corpus. Using such global corpus statistics, the loss
function of the skip-gram model is equivalent to
 √ï
ùëñ2V√ï
ùëó2Vùë•ùëñùëólogùëûùëñùëó.(15.5.2)
We further denote by ùë•ùëñthe number of all the context words in the context windows where
ùë§ùëñoccurs as their center word, which is equivalent to jCùëñj. Lettingùëùùëñùëóbe the conditional
probabilityùë•ùëñùëó¬ùùë•ùëñfor generating context word ùë§ùëógiven center word ùë§ùëñ,(15.5.2 )can be
rewritten as
 √ï
ùëñ2Vùë•ùëñ√ï
ùëó2Vùëùùëñùëólogùëûùëñùëó.(15.5.3)
In(15.5.3 ), √ç
ùëó2Vùëùùëñùëólogùëûùëñùëócalculatesthecross-entropyoftheconditionaldistribution
ùëùùëñùëóofglobalcorpusstatisticsandtheconditionaldistribution ùëûùëñùëóofmodelpredictions. This
loss is also weighted by ùë•ùëñas explained above. Minimizing the loss function in (15.5.3 )
will allow the predicted conditional distribution to get close to the conditional distribution
from the global corpus statistics.
Thoughbeingcommonlyusedformeasuringthedistancebetweenprobabilitydistributions,
the cross-entropy loss function may not be a good choice here. On the one hand, as we
mentioned in Section 15.2 , the cost of properly normalizing ùëûùëñùëóresults in the sum over
713 Word Embedding with Global Vectors (GloVe)
the entire vocabulary, which can be computationally expensive. On the other hand, a large
number of rare events from a large corpus are often modeled by the cross-entropy loss to
be assigned with too much weight.
15.5.2The GloVeModel
In view of this, the GloVemodel makes three changes to the skip-gram model based on
squared loss ( Pennington etal., 2014):
1.Use variables ùëù0
ùëñùëó=ùë•ùëñùëóandùëû0
ùëñùëó=exp¬πu>
ùëóvùëñ¬∫that are not probability distributions
and take the logarithm of both, so the squared loss term is
logùëù0
ùëñùëó logùëû0
ùëñùëó2
=

u>
ùëóvùëñ logùë•ùëñùëó2
.
2.Add two scalar model parameters for each word ùë§ùëñ: the center word bias ùëèùëñand the
context word bias ùëêùëñ.
3.Replace the weight of each loss term with the weight function ‚Ñé¬πùë•ùëñùëó¬∫, where‚Ñé¬πùë•¬∫is
increasing in the interval of ¬ª0,1¬º.
Puttingallthingstogether,trainingGloVeistominimizethefollowinglossfunction:
√ï
ùëñ2V√ï
ùëó2V‚Ñé¬πùë•ùëñùëó¬∫
u>
ùëóvùëñ¬∏ùëèùëñ¬∏ùëêùëó logùë•ùëñùëó2
. (15.5.4)
For the weight function, a suggested choice is: ‚Ñé¬πùë•¬∫=¬πùë•¬ùùëê¬∫ùõº(e.gùõº=0.75) ifùë• <ùëê(e.g.,
ùëê=100); otherwise ‚Ñé¬πùë•¬∫=1. In this case, because ‚Ñé¬π0¬∫=0, the squared loss term for any
ùë•ùëñùëó=0can be omitted for computational efficiency. For example, when using minibatch
stochastic gradient descent for training, at each iteration we randomly sample a minibatch
ofnon-zeroùë•ùëñùëóto calculate gradients and update the model parameters. Note that these
non-zeroùë•ùëñùëóare precomputed global corpus statistics; thus, the model is called GloVe for
GlobalVectors .
It should be emphasized that if word ùë§ùëñappears in the context window of word ùë§ùëó, then
vice versa . Therefore, ùë•ùëñùëó=ùë•ùëóùëñ. Unlike word2vec that fits the asymmetric conditional
probabilityùëùùëñùëó, GloVe fits the symmetric logùë•ùëñùëó. Therefore, the center word vector and
the context word vector of any word are mathematically equivalent in the GloVe model.
However in practice, owing to different initialization values, the same word may still get
different values in these two vectors after training: GloVe sums them up as the output
vector.
15.5.3InterpretingGloVefromthe Ratio of Co-occurrence
Probabilities
We can also interpret the GloVe model from another perspective. Using the same notation
inSection 15.5.1 , letùëùùëñùëódef=ùëÉ¬πùë§ùëójùë§ùëñ¬∫be the conditional probability of generating the
context word ùë§ùëógivenùë§ùëñas the center word in the corpus. tab_glove lists several co-
occurrence probabilities given words ‚Äúice‚Äù and ‚Äústeam‚Äù and their ratios based on statistics
from a large corpus.
714 Natural Language Processing: Pretraining
:Word-word co-occurrence probabilities and their ratios from a large corpus (adapted from
Table 1 in Pennington etal.(2014))
Table 15.5.1: label: tab_glove
ùë§ùëò= solid gas water fashion
ùëù1=ùëÉ¬πùë§ùëòjice¬∫0.00019 0.000066 0.003 0.000017
ùëù2=ùëÉ¬πùë§ùëòjsteam¬∫0.000022 0.00078 0.0022 0.000018
ùëù1¬ùùëù2 8.9 0.085 1.36 0.96
We can observe the following from tab_glove :
For a wordùë§ùëòthat is related to ‚Äúice‚Äù but unrelated to ‚Äústeam‚Äù, such as ùë§ùëò=solid, we
expect a larger ratio of co-occurence probabilities, such as 8.9.
For a wordùë§ùëòthat is related to ‚Äústeam‚Äù but unrelated to ‚Äúice‚Äù, such as ùë§ùëò=gas, we
expect a smaller ratio of co-occurence probabilities, such as 0.085.
For a wordùë§ùëòthat is related to both ‚Äúice‚Äù and ‚Äústeam‚Äù, such as ùë§ùëò=water, we expect
a ratio of co-occurence probabilities that is close to 1, such as 1.36.
For a wordùë§ùëòthat is unrelated to both ‚Äúice‚Äù and ‚Äústeam‚Äù, such as ùë§ùëò=fashion, we
expect a ratio of co-occurence probabilities that is close to 1, such as 0.96.
It can be seen that the ratio of co-occurrence probabilities can intuitively express the rela-
tionship between words. Thus, we can design a function of three word vectors to fit this
ratio. Fortheratioofco-occurrenceprobabilities ùëùùëñùëó¬ùùëùùëñùëòwithùë§ùëñbeingthecenterwordand
ùë§ùëóandùë§ùëòbeing the context words, we want to fit this ratio using some function ùëì:
ùëì¬πuùëó,uùëò,vùëñ¬∫ùëùùëñùëó
ùëùùëñùëò. (15.5.5)
Among many possible designs for ùëì, we only pick a reasonable choice in the following.
Since the ratio of co-occurrence probabilities is a scalar, we require that ùëìbe a scalar
function, such as ùëì¬πuùëó,uùëò,vùëñ¬∫=ùëì ¬πuùëó uùëò¬∫>vùëñ. Switching word indices ùëóandùëòin
(15.5.5 ), it must hold that ùëì¬πùë•¬∫ùëì¬π ùë•¬∫=1, so one possibility is ùëì¬πùë•¬∫=exp¬πùë•¬∫, i.e.,
ùëì¬πuùëó,uùëò,vùëñ¬∫=exp
u>
ùëóvùëñ
exp
u>
ùëòvùëñùëùùëñùëó
ùëùùëñùëò. (15.5.6)
Now let‚Äôs pick exp
u>
ùëóvùëñ
ùõºùëùùëñùëó, whereùõºis a constant. Since ùëùùëñùëó=ùë•ùëñùëó¬ùùë•ùëñ, after taking
the logarithm on both sides we get u>
ùëóvùëñlogùõº¬∏logùë•ùëñùëó logùë•ùëñ. We may use additional
bias terms to fit logùõº¬∏logùë•ùëñ, such as the center word bias ùëèùëñand the context word bias
ùëêùëó:
u>
ùëóvùëñ¬∏ùëèùëñ¬∏ùëêùëólogùë•ùëñùëó. (15.5.7)
Measuring the squared error of (15.5.7 )with weights, the GloVe loss function in (15.5.4 )
is obtained.
715 Subword Embedding
23315.5.4Summary
Theskip-grammodelcanbeinterpretedusingglobalcorpusstatisticssuchasword-word
co-occurrence counts.
The cross-entropy loss may not be a good choice for measuring the difference of two
probability distributions, especially for a large corpus. GloVe uses squared loss to fit
precomputed global corpus statistics.
The center word vector and the context word vector are mathematically equivalent for
any word in GloVe.
GloVe can be interpreted from the ratio of word-word co-occurrence probabilities.
15.5.5Exercises
1.Ifwordsùë§ùëñandùë§ùëóco-occurinthesamecontextwindow,howcanweusetheirdistance
in the text sequence to redesign the method for calculating the conditional probability
ùëùùëñùëó? Hint: see Section 4.2 of the GloVe paper ( Pennington etal., 2014).
2.For any word, are its center word bias and context word bias mathematically equivalent
in GloVe? Why?
Discussions233.
15.6SubwordEmbedding
In English, words such as ‚Äúhelps‚Äù, ‚Äúhelped‚Äù, and ‚Äúhelping‚Äù are inflected forms of the same
word‚Äúhelp‚Äù. Therelationshipbetween‚Äúdog‚Äù and‚Äúdogs‚Äùis thesameas thatbetween‚Äúcat‚Äù
and‚Äúcats‚Äù,andtherelationshipbetween‚Äúboy‚Äùand‚Äúboyfriend‚Äùisthesameasthatbetween
‚Äúgirl‚Äù and ‚Äúgirlfriend‚Äù. In other languages such as French and Spanish, many verbs have
over 40 inflected forms, while in Finnish, a noun may have up to 15 cases. In linguistics,
morphologystudieswordformationandwordrelationships. However,theinternalstructure
of words was neither explored in word2vec nor in GloVe.
15.6.1ThefastTextModel
Recall how words are represented in word2vec. In both the skip-gram model and the con-
tinuous bag-of-words model, different inflected forms of the same word are directly repre-
sented by different vectors without shared parameters. To use morphological information,
thefastTextmodel proposed a subwordembedding approach, where a subword is a charac-
terùëõ-gram(Bojanowski etal.,2017). Insteadoflearningword-levelvectorrepresentations,
fastText can be considered as the subword-level skip-gram, where each centerword is rep-
resented by the sum of its subword vectors.
Let‚Äôs illustrate how to obtain subwords for each center word in fastText using the word
716 Natural Language Processing: Pretraining
‚Äúwhere‚Äù. First, add special characters ‚Äú<‚Äù and ‚Äú>‚Äù at the beginning and end of the word
to distinguish prefixes and suffixes from other subwords. Then, extract character ùëõ-grams
from the word. For example, when ùëõ=3, we obtain all subwords of length 3: ‚Äú<wh‚Äù,
‚Äúwhe‚Äù, ‚Äúher‚Äù, ‚Äúere‚Äù, ‚Äúre>‚Äù, and the special subword ‚Äú<where>‚Äù.
In fastText, for any word ùë§, denote byGùë§the union of all its subwords of length between
3 and 6 and its special subword. The vocabulary is the union of the subwords of all words.
Letting zùëîbethevectorofsubword ùëîinthedictionary,thevector vùë§forwordùë§asacenter
word in the skip-gram model is the sum of its subword vectors:
vùë§=√ï
ùëî2Gùë§zùëî.(15.6.1)
The rest of fastText is the same as the skip-gram model. Compared with the skip-gram
model, the vocabulary in fastText is larger, resulting in more model parameters. Besides,
tocalculatetherepresentationofaword,allitssubwordvectorshavetobesummed,leading
to higher computational complexity. However, thanks to shared parameters from subwords
among words with similar structures, rare words and even out-of-vocabulary words may
obtain better vector representations in fastText.
15.6.2BytePairEncoding
In fastText, all the extracted subwords have to be of the specified lengths, such as 3to6,
thus the vocabulary size cannot be predefined. To allow for variable-length subwords in
a fixed-size vocabulary, we can apply a compression algorithm called byte pair encoding
(BPE) to extract subwords ( Sennrich etal., 2015).
Byte pair encoding performs a statistical analysis of the training dataset to discover com-
mon symbols within a word, such as consecutive characters of arbitrary length. Starting
from symbols of length 1, byte pair encoding iteratively merges the most frequent pair of
consecutive symbols to produce new longer symbols. Note that for efficiency, pairs cross-
ing word boundaries are not considered. In the end, we can use such symbols as subwords
to segment words. Byte pair encoding and its variants has been used for input representa-
tions in popular natural language processing pretraining models such as GPT-2 ( Radford
et al., 2019) and RoBERTa ( Liuet al., 2019). In the following, we will illustrate how byte
pair encoding works.
First, we initialize the vocabulary of symbols as all the English lowercase characters, a
special end-of-word symbol '_', and a special unknown symbol '[UNK]' .
import collections
symbols =['a','b','c','d','e','f','g','h','i','j','k','l','m',
'n','o','p','q','r','s','t','u','v','w','x','y','z',
'_','[UNK] ']
Since we do not consider symbol pairs that cross boundaries of words, we only need a
dictionary raw_token_freqs thatmapswordstotheirfrequencies(numberofoccurrences)
in a dataset. Note that the special symbol '_'is appended to each word so that we can
717 Subword Embedding
easily recover a word sequence (e.g., ‚Äúa taller man‚Äù) from a sequence of output symbols
( e.g., ‚Äúa_ tall er_ man‚Äù). Since we start the merging process from a vocabulary of only
single characters and special symbols, space is inserted between every pair of consecutive
characters within each word (keys of the dictionary token_freqs ). In other words, space
is the delimiter between symbols within a word.
raw_token_freqs ={'fast_ ':4,'faster_ ':3,'tall_ ':5,'taller_ ':4}
token_freqs ={}
for token, freq inraw_token_freqs .items():
token_freqs[ ''.join( list (token))] =raw_token_freqs[token]
token_freqs
{'f a s t _ ':4,'f a s t e r _ ':3,'t a l l _ ':5,'t a l l e r _ ':4}
We define the following get_max_freq_pair function that returns the most frequent pair
ofconsecutivesymbolswithinaword,wherewordscomefromkeysoftheinputdictionary
token_freqs .
def get_max_freq_pair (token_freqs):
pairs =collections .defaultdict( int)
for token, freq intoken_freqs .items():
symbols =token .split()
for iinrange (len(symbols) -1):
# Key of `pairs` is a tuple of two consecutive symbols
pairs[symbols[i], symbols[i +1]]+=freq
return max(pairs, key =pairs .get) # Key of `pairs` with the max value
As a greedy approach based on frequency of consecutive symbols, byte pair encoding will
use the following merge_symbols function to merge the most frequent pair of consecutive
symbols to produce new symbols.
def merge_symbols (max_freq_pair, token_freqs, symbols):
symbols .append( ''.join(max_freq_pair))
new_token_freqs =dict ()
for token, freq intoken_freqs .items():
new_token =token .replace( ''.join(max_freq_pair),
''.join(max_freq_pair))
new_token_freqs[new_token] =token_freqs[token]
return new_token_freqs
Nowweiterativelyperformthebytepairencodingalgorithmoverthekeysofthedictionary
token_freqs . In the first iteration, the most frequent pair of consecutive symbols are 't'
and'a', thus byte pair encoding merges them to produce a new symbol 'ta'. In the
second iteration, byte pair encoding continues to merge 'ta'and'l'to result in another
new symbol 'tal'.
num_merges =10
for iinrange (num_merges):
(continues on next page)
718 Natural Language Processing: Pretraining
(continued from previous page)
max_freq_pair =get_max_freq_pair(token_freqs)
token_freqs =merge_symbols(max_freq_pair, token_freqs, symbols)
print (f'merge # {i+1}:', max_freq_pair)
merge #1: ('t', 'a')
merge #2: ('ta', 'l')
merge #3: ('tal', 'l')
merge #4: ('f', 'a')
merge #5: ('fa', 's')
merge #6: ('fas', 't')
merge #7: ('e', 'r')
merge #8: ('er', '_')
merge #9: ('tall', '_')
merge #10: ('fast', '_')
After 10 iterations of byte pair encoding, we can see that list symbols now contains 10
more symbols that are iteratively merged from other symbols.
print (symbols)
['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p
‚Ü©!','q','r','s','t','u','v','w','x','y','z','_','[UNK] ','ta','tal
‚Ü©!','tall ','fa','fas','fast ','er','er_','tall_ ','fast_ ']
For the same dataset specified in the keys of the dictionary raw_token_freqs , each word
in the dataset is now segmented by subwords ‚Äúfast_‚Äù, ‚Äúfast‚Äù, ‚Äúer_‚Äù, ‚Äútall_‚Äù, and ‚Äútall‚Äù as a
result of the byte pair encoding algorithm. For instance, words ‚Äúfaster_‚Äù and ‚Äútaller_‚Äù are
segmented as ‚Äúfast er_‚Äù and ‚Äútall er_‚Äù, respectively.
print (list (token_freqs .keys()))
['fast_ ','fast er_ ','tall_ ','tall er_ ']
Note that the result of byte pair encoding depends on the dataset being used. We can also
use the subwords learned from one dataset to segment words of another dataset. As a
greedyapproach, thefollowing segment_BPE functiontriestobreakwordsintothelongest
possible subwords from the input argument symbols .
def segment_BPE (tokens, symbols):
outputs =[]
for token intokens:
start, end =0,len(token)
cur_output =[]
# Segment token with the longest possible subwords from symbols
while start <len(token) and start <end:
iftoken[start: end] insymbols:
(continues on next page)
719 Subword Embedding
234(continued from previous page)
cur_output .append(token[start: end])
start =end
end =len(token)
else :
end -=1
ifstart <len(token):
cur_output .append( '[UNK] ')
outputs .append( ''.join(cur_output))
return outputs
Inthefollowing, weusethesubwordsinlist symbols , whichislearnedfromtheaforemen-
tioned dataset, to segment tokensthat represent another dataset.
tokens =['tallest_ ','fatter_ ']
print (segment_BPE(tokens, symbols))
['tall e s t _ ','fa t t er_ ']
15.6.3Summary
The fastText model proposes a subword embedding approach. Based on the skip-gram
model in word2vec, it represents a center word as the sum of its subword vectors.
Bytepairencodingperformsastatisticalanalysisofthetrainingdatasettodiscovercom-
mon symbols within a word. As a greedy approach, byte pair encoding iteratively
merges the most frequent pair of consecutive symbols.
Subword embedding may improve the quality of representations of rare words and out-
of-dictionary words.
15.6.4Exercises
1.As an example, there are about 3108possible 6-grams in English. What is the issue
when there are too many subwords? How to address the issue? Hint: refer to the end of
Section 3.2 of the fastText paper ( Bojanowski etal., 2017).
2.How to design a subword embedding model based on the continuous bag-of-words
model?
3.Togetavocabularyofsize ùëö,howmanymergingoperationsareneededwhentheinitial
symbol vocabulary size is ùëõ?
4.How to extend the idea of byte pair encoding to extract phrases?
Discussions234.
720 Natural Language Processing: Pretraining
235
23615.7WordSimilarity and Analogy
InSection 15.4 , we trained a word2vec model on a small dataset, and applied it to find
semantically similar words for an input word. In practice, word vectors that are pretrained
on large corpora can be applied to downstream natural language processing tasks, which
will be covered later in Chapter 16 . To demonstrate semantics of pretrained word vectors
from large corpora in a straightforward way, let‚Äôs apply them in the word similarity and
analogy tasks.
import os
import torch
from torch import nn
from d2l import torch asd2l
15.7.1LoadingPretrainedWordVectors
Below lists pretrained GloVe embeddings of dimension 50, 100, and 300, which can be
downloaded from the GloVe website235. The pretrained fastText embeddings are available
in multiple languages. Here we consider one English version (300-dimensional ‚Äúwiki.en‚Äù)
that can be downloaded from the fastText website236.
#@save
d2l.DATA_HUB[ 'glove.6b.50d ']=(d2l .DATA_URL +'glove.6B.50d.zip ',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d ')
#@save
d2l.DATA_HUB[ 'glove.6b.100d ']=(d2l .DATA_URL +'glove.6B.100d.zip ',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a ')
#@save
d2l.DATA_HUB[ 'glove.42b.300d ']=(d2l .DATA_URL +'glove.42B.300d.zip ',
'b5116e234e9eb9076672cfeabf5469f3eec904fa ')
#@save
d2l.DATA_HUB[ 'wiki.en ']=(d2l .DATA_URL +'wiki.en.zip ',
'c1816da3821ae9f43899be655002f6c723e91b88 ')
To load these pretrained GloVe and fastText embeddings, we define the following Token-
Embedding class.
#@save
class TokenEmbedding :
"""Token Embedding."""
def __init__ (self , embedding_name):
self .idx_to_token, self .idx_to_vec =self ._load_embedding(
embedding_name)
self .unknown_idx =0
(continues on next page)
721 Word Similarity and Analogy
(continued from previous page)
self .token_to_idx ={token: idx for idx, token in
enumerate (self .idx_to_token)}
def _load_embedding (self , embedding_name):
idx_to_token, idx_to_vec =['<unk> '], []
data_dir =d2l.download_extract(embedding_name)
# GloVe website: https://nlp.stanford.edu/projects/glove/
# fastText website: https://fasttext.cc/
with open (os.path .join(data_dir, 'vec.txt '),'r')asf:
for line inf:
elems =line .rstrip() .split( '')
token, elems =elems[ 0], [ float (elem) for elem inelems[ 1:]]
# Skip header information, such as the top row in fastText
iflen(elems) >1:
idx_to_token .append(token)
idx_to_vec .append(elems)
idx_to_vec =[[0]*len(idx_to_vec[ 0])] +idx_to_vec
return idx_to_token, torch .tensor(idx_to_vec)
def __getitem__ (self , tokens):
indices =[self .token_to_idx .get(token, self .unknown_idx)
for token intokens]
vecs =self .idx_to_vec[torch .tensor(indices)]
return vecs
def __len__ (self ):
return len(self .idx_to_token)
Below we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia sub-
set). When creating the TokenEmbedding instance, the specified embedding file has to be
downloaded if it was not yet.
glove_6b50d =TokenEmbedding( 'glove.6b.50d ')
Downloading ../data /glove .6B.50d.zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/glove .6B.50d.zip...
Output the vocabulary size. The vocabulary contains 400000 words (tokens) and a special
unknown token.
len(glove_6b50d)
400001
We can get the index of a word in the vocabulary, and vice versa.
glove_6b50d .token_to_idx[ 'beautiful '], glove_6b50d .idx_to_token[ 3367 ]
722 Natural Language Processing: Pretraining
(3367 ,'beautiful ')
15.7.2ApplyingPretrained WordVectors
Using the loaded GloVe vectors, we will demonstrate their semantics by applying them in
the following word similarity and analogy tasks.
WordSimilarity
Similar to Section 15.4.3 , in order to find semantically similar words for an input word
based on cosine similarities between word vectors, we implement the following knn(ùëò-
nearest neighbors) function.
def knn(W, x, k):
# Add 1e-9 for numerical stability
cos =torch .mv(W, x .reshape( -1,)) /(
torch .sqrt(torch .sum(W *W, axis =1)+1e-9 )*
torch .sqrt((x *x).sum()))
_, topk =torch .topk(cos, k =k)
return topk, [cos[ int(i)] for iintopk]
Then, we search for similar words using the pretrained word vectors from the TokenEm-
bedding instance embed.
def get_similar_tokens (query_token, k, embed):
topk, cos =knn(embed .idx_to_vec, embed[[query_token]], k +1)
for i, c inzip(topk[ 1:], cos[ 1:]): # Exclude the input word
print (f'cosine sim= {float (c):.3f}:{embed .idx_to_token[ int(i)] }')
The vocabulary of the pretrained word vectors in glove_6b50d contains 400000 words
and a special unknown token. Excluding the input word and unknown token, among this
vocabulary let‚Äôs find three most semantically similar words to word ‚Äúchip‚Äù.
get_similar_tokens( 'chip ',3, glove_6b50d)
cosine sim =0.856 : chips
cosine sim =0.749 : intel
cosine sim =0.749 : electronics
Below outputs similar words to ‚Äúbaby‚Äù and ‚Äúbeautiful‚Äù.
get_similar_tokens( 'baby ',3, glove_6b50d)
cosine sim =0.839 : babies
cosine sim =0.800 : boy
cosine sim =0.792 : girl
723 Word Similarity and Analogy
get_similar_tokens( 'beautiful ',3, glove_6b50d)
cosine sim =0.921 : lovely
cosine sim =0.893 : gorgeous
cosine sim =0.830 : wonderful
WordAnalogy
Besides finding similar words, we can also apply word vectors to word analogy tasks. For
example, ‚Äúman‚Äù:‚Äúwoman‚Äù::‚Äúson‚Äù:‚Äúdaughter‚Äù is the form of a word analogy: ‚Äúman‚Äù is to
‚Äúwoman‚Äù as ‚Äúson‚Äù is to ‚Äúdaughter‚Äù. Specifically, the word analogy completion task can be
defined as: for a word analogy ùëé:ùëè::ùëê:ùëë, given the first three words ùëé,ùëèandùëê, findùëë.
Denote the vector of word ùë§by vec¬πùë§¬∫. To complete the analogy, we will find the word
whose vector is most similar to the result of vec ¬πùëê¬∫¬∏vec¬πùëè¬∫ vec¬πùëé¬∫.
def get_analogy (token_a, token_b, token_c, embed):
vecs =embed[[token_a, token_b, token_c]]
x=vecs[ 1]-vecs[ 0]+vecs[ 2]
topk, cos =knn(embed .idx_to_vec, x, 1)
return embed .idx_to_token[ int(topk[ 0])] # Remove unknown words
Let‚Äôs verify the ‚Äúmale-female‚Äù analogy using the loaded word vectors.
get_analogy( 'man','woman ','son', glove_6b50d)
'daughter '
Below completes a ‚Äúcapital-country‚Äù analogy: ‚Äúbeijing‚Äù:‚Äúchina‚Äù::‚Äútokyo‚Äù:‚Äújapan‚Äù. This
demonstrates semantics in the pretrained word vectors.
get_analogy( 'beijing ','china ','tokyo ', glove_6b50d)
'japan '
For the ‚Äúadjective-superlative adjective‚Äù analogy such as ‚Äúbad‚Äù:‚Äúworst‚Äù::‚Äúbig‚Äù:‚Äúbiggest‚Äù,
we can see that the pretrained word vectors may capture the syntactic information.
get_analogy( 'bad','worst ','big', glove_6b50d)
'biggest '
To show the captured notion of past tense in the pretrained word vectors, we can test the
syntax using the ‚Äúpresent tense-past tense‚Äù analogy: ‚Äúdo‚Äù:‚Äúdid‚Äù::‚Äúgo‚Äù:‚Äúwent‚Äù.
724 Natural Language Processing: Pretraining
237get_analogy( 'do','did','go', glove_6b50d)
'went '
15.7.3Summary
In practice, word vectors that are pretrained on large corpora can be applied to down-
stream natural language processing tasks.
Pretrained word vectors can be applied to the word similarity and analogy tasks.
15.7.4Exercises
1.Test the fastText results using TokenEmbedding('wiki.en') .
2.When the vocabulary is extremely large, how can we find similar words or complete a
word analogy faster?
Discussions237.
15.8BidirectionalEncoder Representations from
Transformers(BERT)
We have introduced several word embedding models for natural language understanding.
After pretraining, the output can be thought of as a matrix where each row is a vector that
represents a word of a predefined vocabulary. In fact, these word embedding models are
allcontext-independent . Let‚Äôs begin by illustrating this property.
15.8.1FromContext-Independent to Context-Sensitive
Recalltheexperimentsin Section15.4 andSection15.7 . Forinstance,word2vecandGloVe
both assign the same pretrained vector to the same word regardless of the context of the
word (if any). Formally, a context-independent representation of any token ùë•is a func-
tionùëì¬πùë•¬∫that only takes ùë•as its input. Given the abundance of polysemy and complex
semantics in natural languages, context-independent representations have obvious limita-
tions. For instance, the word ‚Äúcrane‚Äù in contexts ‚Äúa crane is flying‚Äù and ‚Äúa crane driver
came‚Äù has completely different meanings; thus, the same word may be assigned different
representations depending on contexts.
Thismotivatesthedevelopmentof context-sensitive wordrepresentations, whererepresen-
tations of words depend on their contexts. Hence, a context-sensitive representation of
725 Bidirectional Encoder Representations from Transformers (BERT)
tokenùë•is a function ùëì¬πùë•,ùëê¬πùë•¬∫¬∫depending on both ùë•and its context ùëê¬πùë•¬∫. Popular context-
sensitiverepresentationsincludeTagLM(language-model-augmentedsequencetagger)( Pe-
tersetal., 2017), CoVe (Context Vectors) ( McCannetal., 2017), and ELMo (Embeddings
from Language Models) ( Petersetal., 2018).
For example, by taking the entire sequence as input, ELMo is a function that assigns a rep-
resentation to each word from the input sequence. Specifically, ELMo combines all the
intermediate layer representations from pretrained bidirectional LSTM as the output rep-
resentation. Then the ELMo representation will be added to a downstream task‚Äôs existing
supervised model as additional features, such as by concatenating ELMo representation
and the original representation (e.g., GloVe) of tokens in the existing model. On the one
hand, all the weights in the pretrained bidirectional LSTM model are frozen after ELMo
representations are added. On the other hand, the existing supervised model is specifically
customized for a given task. Leveraging different best models for different tasks at that
time, adding ELMo improved the state of the art across six natural language processing
tasks: sentiment analysis, natural language inference, semantic role labeling, coreference
resolution, named entity recognition, and question answering.
15.8.2FromTask-Specificto Task-Agnostic
Although ELMo has significantly improved solutions to a diverse set of natural language
processing tasks, each solution still hinges on a task-specific architecture. However, it is
practically non-trivial to craft a specific architecture for every natural language processing
task. The GPT (Generative Pre-Training) model represents an effort in designing a general
task-agnostic model for context-sensitive representations ( Radfordet al., 2018). Built on
a Transformer decoder, GPT pretrains a language model that will be used to represent text
sequences. When applying GPT to a downstream task, the output of the language model
willbefedintoanaddedlinearoutputlayertopredictthelabelofthetask. Insharpcontrast
to ELMo that freezes parameters of the pretrained model, GPT fine-tunes allthe parame-
ters in the pretrained Transformer decoder during supervised learning of the downstream
task. GPT was evaluated on twelve tasks of natural language inference, question answer-
ing, sentencesimilarity, andclassification, andimprovedthestateoftheartinnineofthem
with minimal changes to the model architecture.
However, due to the autoregressive nature of language models, GPT only looks forward
(left-to-right). In contexts ‚Äúi went to the bank to deposit cash‚Äù and ‚Äúi went to the bank
to sit down‚Äù, as ‚Äúbank‚Äù is sensitive to the context to its left, GPT will return the same
representation for ‚Äúbank‚Äù, though it has different meanings.
15.8.3BERT:Combining the Best of BothWorlds
Aswehaveseen,ELMoencodescontextbidirectionallybutusestask-specificarchitectures;
while GPT is task-agnostic but encodes context left-to-right. Combining the best of both
worlds, BERT (Bidirectional Encoder Representations from Transformers) encodes con-
text bidirectionally and requires minimal architecture changes for a wide range of natural
language processing tasks ( Devlinet al., 2018). Using a pretrained Transformer encoder,
BERT is able to represent any token based on its bidirectional context. During supervised
726 Natural Language Processing: Pretraining
learning of downstream tasks, BERT is similar to GPT in two aspects. First, BERT rep-
resentations will be fed into an added output layer, with minimal changes to the model
architecture depending on nature of tasks, such as predicting for every token vs. predicting
for the entire sequence. Second, all the parameters of the pretrained Transformer encoder
are fine-tuned, while the additional output layer will be trained from scratch. Fig. 15.8.1
depicts the differences among ELMo, GPT, and BERT.
tFig. 15.8.1 A comparison of ELMo, GPT, and BERT.
BERT further improved the state of the art on eleven natural language processing tasks
underbroadcategoriesof(i)singletextclassification(e.g.,sentimentanalysis),(ii)textpair
classification (e.g., natural language inference), (iii) question answering, (iv) text tagging
(e.g., named entity recognition). All proposed in 2018, from context-sensitive ELMo to
task-agnostic GPT and BERT, conceptually simple yet empirically powerful pretraining of
deep representations for natural languages have revolutionized solutions to various natural
language processing tasks.
Intherestofthischapter,wewilldiveintothepretrainingofBERT.Whennaturallanguage
processingapplicationsareexplainedin Chapter16 , wewillillustratefine-tuningofBERT
for downstream applications.
import torch
from torch import nn
from d2l import torch asd2l
15.8.4InputRepresentation
In natural language processing, some tasks (e.g., sentiment analysis) take single text as
input, while in some other tasks (e.g., natural language inference), the input is a pair of
text sequences. The BERT input sequence unambiguously represents both single text and
text pairs. In the former, the BERT input sequence is the concatenation of the special
classification token ‚Äú<cls>‚Äù, tokens of a text sequence, and the special separation token
‚Äú<sep>‚Äù. In the latter, the BERT input sequence is the concatenation of ‚Äú<cls>‚Äù, tokens
of the first text sequence, ‚Äú<sep>‚Äù, tokens of the second text sequence, and ‚Äú<sep>‚Äù. We
will consistently distinguish the terminology ‚ÄúBERT input sequence‚Äù from other types of
727 Bidirectional Encoder Representations from Transformers (BERT)
‚Äúsequences‚Äù. For instance, one BERTinputsequence may include either one textsequence
or twotextsequences .
To distinguish text pairs, the learned segment embeddings eùê¥andeùêµare added to the
token embeddings of the first sequence and the second sequence, respectively. For single
text inputs, only eùê¥is used.
The following get_tokens_and_segments takes either one sentence or two sentences as
input, then returns tokens of the BERT input sequence and their corresponding segment
IDs.
#@save
def get_tokens_and_segments (tokens_a, tokens_b =None ):
"""Get tokens of the BERT input sequence and their segment IDs."""
tokens =['<cls> ']+tokens_a +['<sep> ']
# 0 and 1 are marking segment A and B, respectively
segments =[0]*(len(tokens_a) +2)
iftokens_b isnot None :
tokens +=tokens_b +['<sep> ']
segments +=[1]*(len(tokens_b) +1)
return tokens, segments
BERT chooses the Transformer encoder as its bidirectional architecture. Common in the
Transformerencoder,positionalembeddingsareaddedateverypositionoftheBERTinput
sequence. However,differentfromtheoriginalTransformerencoder,BERTuses learnable
positional embeddings. To sum up, Fig. 15.8.2 shows that the embeddings of the BERT
input sequence are the sum of the token embeddings, segment embeddings, and positional
embeddings.
tFig. 15.8.2 The embeddings of the BERT input sequence are the sum of the token embeddings,
segment embeddings, and positional embeddings.
The following BERTEncoder class is similar to the TransformerEncoder class as imple-
mentedin Section11.7 . Differentfrom TransformerEncoder ,BERTEncoder usessegment
embeddings and learnable positional embeddings.
#@save
class BERTEncoder (nn.Module):
"""BERT encoder."""
def __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_blks, dropout, max_len =1000 ,**kwargs):
(continues on next page)
728 Natural Language Processing: Pretraining
(continued from previous page)
super (BERTEncoder, self ).__init__ (**kwargs)
self .token_embedding =nn.Embedding(vocab_size, num_hiddens)
self .segment_embedding =nn.Embedding( 2, num_hiddens)
self .blks =nn.Sequential()
for iinrange (num_blks):
self .blks .add_module( f"{i}", d2l .TransformerEncoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, True ))
# In BERT, positional embeddings are learnable, thus we create a
# parameter of positional embeddings that are long enough
self .pos_embedding =nn.Parameter(torch .randn( 1, max_len,
num_hiddens))
def forward (self , tokens, segments, valid_lens):
# Shape of `X` remains unchanged in the following code snippet:
# (batch size, max sequence length, `num_hiddens`)
X=self .token_embedding(tokens) +self .segment_embedding(segments)
X=X+self .pos_embedding[:, :X .shape[ 1], :]
for blk inself .blks:
X=blk(X, valid_lens)
return X
Suppose that the vocabulary size is 10000. To demonstrate forward inference of BERTEn-
coder, let‚Äôs create an instance of it and initialize its parameters.
vocab_size, num_hiddens, ffn_num_hiddens, num_heads =10000 ,768,1024 ,4
ffn_num_input, num_blks, dropout =768,2,0.2
encoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_blks, dropout)
We define tokensto be 2 BERT input sequences of length 8, where each token is an index
of the vocabulary. The forward inference of BERTEncoder with the input tokensreturns
the encoded result where each token is represented by a vector whose length is predefined
by the hyperparameter num_hiddens . This hyperparameter is usually referred to as the
hiddensize (number of hidden units) of the Transformer encoder.
tokens =torch .randint( 0, vocab_size, ( 2,8))
segments =torch .tensor([[ 0,0,0,0,1,1,1,1], [ 0,0,0,1,1,1,1,1]])
encoded_X =encoder(tokens, segments, None )
encoded_X .shape
torch .Size([ 2,8,768])
15.8.5PretrainingTasks
The forward inference of BERTEncoder gives the BERT representation of each token of
the input text and the inserted special tokens ‚Äú<cls>‚Äù and ‚Äú<seq>‚Äù. Next, we will use
these representations to compute the loss function for pretraining BERT. The pretraining
is composed of the following two tasks: masked language modeling and next sentence
prediction.
729 Bidirectional Encoder Representations from Transformers (BERT)
MaskedLanguage Modeling
As illustrated in Section 9.3 , a language model predicts a token using the context on its
left. To encode context bidirectionally for representing each token, BERT randomly masks
tokens and uses tokens from the bidirectional context to predict the masked tokens in a
self-supervised fashion. This task is referred to as a masked languagemodel .
In this pretraining task, 15% of tokens will be selected at random as the masked tokens for
prediction. To predict a masked token without cheating by using the label, one straight-
forward approach is to always replace it with a special ‚Äú<mask>‚Äù token in the BERT input
sequence. However, the artificial special token ‚Äú<mask>‚Äù will never appear in fine-tuning.
To avoid such a mismatch between pretraining and fine-tuning, if a token is masked for
prediction (e.g., ‚Äúgreat‚Äù is selected to be masked and predicted in ‚Äúthis movie is great‚Äù), in
the input it will be replaced with:
a special ‚Äú<mask>‚Äù token for 80% of the time (e.g., ‚Äúthis movie is great‚Äù becomes ‚Äúthis
movie is <mask>‚Äù);
a random token for 10% of the time (e.g., ‚Äúthis movie is great‚Äù becomes ‚Äúthis movie is
drink‚Äù);
the unchanged label token for 10% of the time (e.g., ‚Äúthis movie is great‚Äù becomes ‚Äúthis
movie is great‚Äù).
Notethatfor10%of15%timearandomtokenisinserted. Thisoccasionalnoiseencourages
BERTtobelessbiasedtowardsthemaskedtoken(especiallywhenthelabeltokenremains
unchanged) in its bidirectional context encoding.
Weimplementthefollowing MaskLMclasstopredictmaskedtokensinthemaskedlanguage
modeltaskofBERTpretraining. Thepredictionusesaone-hidden-layerMLP( self.mlp ).
In forward inference, it takes two inputs: the encoded result of BERTEncoder and the token
positions for prediction. The output is the prediction results at these positions.
#@save
class MaskLM (nn.Module):
"""The masked language model task of BERT."""
def __init__ (self , vocab_size, num_hiddens, **kwargs):
super (MaskLM, self ).__init__ (**kwargs)
self .mlp =nn.Sequential(nn .LazyLinear(num_hiddens),
nn.ReLU(),
nn.LayerNorm(num_hiddens),
nn.LazyLinear(vocab_size))
def forward (self , X, pred_positions):
num_pred_positions =pred_positions .shape[ 1]
pred_positions =pred_positions .reshape( -1)
batch_size =X.shape[ 0]
batch_idx =torch .arange( 0, batch_size)
# Suppose that `batch_size` = 2, `num_pred_positions` = 3, then
# `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`
batch_idx =torch .repeat_interleave(batch_idx, num_pred_positions)
masked_X =X[batch_idx, pred_positions]
(continues on next page)
730 Natural Language Processing: Pretraining
(continued from previous page)
masked_X =masked_X .reshape((batch_size, num_pred_positions, -1))
mlm_Y_hat =self .mlp(masked_X)
return mlm_Y_hat
To demonstrate the forward inference of MaskLM, we create its instance mlmand initialize
it. Recall that encoded_X from the forward inference of BERTEncoder represents 2 BERT
inputsequences. Wedefine mlm_positions asthe3indicestopredictineitherBERTinput
sequenceof encoded_X .Theforwardinferenceof mlmreturnspredictionresults mlm_Y_hat
at all the masked positions mlm_positions ofencoded_X . For each prediction, the size of
the result is equal to the vocabulary size.
mlm =MaskLM(vocab_size, num_hiddens)
mlm_positions =torch .tensor([[ 1,5,2], [ 6,1,5]])
mlm_Y_hat =mlm(encoded_X, mlm_positions)
mlm_Y_hat .shape
torch .Size([ 2,3,10000 ])
With the ground truth labels mlm_Yof the predicted tokens mlm_Y_hat under masks, we
can calculate the cross-entropy loss of the masked language model task in BERT pretrain-
ing.
mlm_Y =torch .tensor([[ 7,8,9], [ 10,20,30]])
loss =nn.CrossEntropyLoss(reduction ='none ')
mlm_l =loss(mlm_Y_hat .reshape(( -1, vocab_size)), mlm_Y .reshape( -1))
mlm_l .shape
torch .Size([ 6])
NextSentence Prediction
Although masked language modeling is able to encode bidirectional context for represent-
ing words, it does not explicitly model the logical relationship between text pairs. To help
understand the relationship between two text sequences, BERT considers a binary classi-
fication task, next sentence prediction , in its pretraining. When generating sentence pairs
for pretraining, for half of the time they are indeed consecutive sentences with the label
‚ÄúTrue‚Äù; while for the other half of the time the second sentence is randomly sampled from
the corpus with the label ‚ÄúFalse‚Äù.
The following NextSentencePred class uses a one-hidden-layer MLP to predict whether
the second sentence is the next sentence of the first in the BERT input sequence. Due to
self-attention in the Transformer encoder, the BERT representation of the special token
‚Äú<cls>‚Äù encodes both the two sentences from the input. Hence, the output layer ( self.
731 Bidirectional Encoder Representations from Transformers (BERT)
output) of the MLP classifier takes Xas input, where Xis the output of the MLP hidden
layer whose input is the encoded ‚Äú<cls>‚Äù token.
#@save
class NextSentencePred (nn.Module):
"""The next sentence prediction task of BERT."""
def __init__ (self ,**kwargs):
super (NextSentencePred, self ).__init__ (**kwargs)
self .output =nn.LazyLinear( 2)
def forward (self , X):
# `X` shape: (batch size, `num_hiddens`)
return self .output(X)
We can see that the forward inference of an NextSentencePred instance returns binary
predictions for each BERT input sequence.
# PyTorch by default will not flatten the tensor as seen in mxnet where, if
# flatten=True, all but the first axis of input data are collapsed together
encoded_X =torch .flatten(encoded_X, start_dim =1)
# input_shape for NSP: (batch size, `num_hiddens`)
nsp =NextSentencePred()
nsp_Y_hat =nsp(encoded_X)
nsp_Y_hat .shape
torch .Size([ 2,2])
The cross-entropy loss of the 2 binary classifications can also be computed.
nsp_y =torch .tensor([ 0,1])
nsp_l =loss(nsp_Y_hat, nsp_y)
nsp_l .shape
torch .Size([ 2])
It is noteworthy that all the labels in both the aforementioned pretraining tasks can be triv-
ially obtained from the pretraining corpus without manual labeling effort. The original
BERT has been pretrained on the concatenation of BookCorpus ( Zhuetal., 2015) and En-
glish Wikipedia. These two text corpora are huge: they have 800 million words and 2.5
billion words, respectively.
15.8.6PuttingIt All Together
When pretraining BERT, the final loss function is a linear combination of both the loss
functions for masked language modeling and next sentence prediction. Now we can de-
fine the BERTModel class by instantiating the three classes BERTEncoder ,MaskLM, and
NextSentencePred . TheforwardinferencereturnstheencodedBERTrepresentations en-
732 Natural Language Processing: Pretraining
coded_X , predictions of masked language modeling mlm_Y_hat , and next sentence predic-
tions nsp_Y_hat .
#@save
class BERTModel (nn.Module):
"""The BERT model."""
def __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_blks, dropout, max_len =1000 ):
super (BERTModel, self ).__init__ ()
self .encoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_blks, dropout,
max_len =max_len)
self .hidden =nn.Sequential(nn .LazyLinear(num_hiddens),
nn.Tanh())
self .mlm =MaskLM(vocab_size, num_hiddens)
self .nsp =NextSentencePred()
def forward (self , tokens, segments, valid_lens =None , pred_positions =None ):
encoded_X =self .encoder(tokens, segments, valid_lens)
ifpred_positions isnot None :
mlm_Y_hat =self .mlm(encoded_X, pred_positions)
else :
mlm_Y_hat =None
# The hidden layer of the MLP classifier for next sentence prediction.
# 0 is the index of the '<cls>' token
nsp_Y_hat =self .nsp( self .hidden(encoded_X[:, 0, :]))
return encoded_X, mlm_Y_hat, nsp_Y_hat
15.8.7Summary
Word embedding models such as word2vec and GloVe are context-independent. They
assign the same pretrained vector to the same word regardless of the context of the
word (if any). It is hard for them to handle well polysemy or complex semantics in
natural languages.
For context-sensitive word representations such as ELMo and GPT, representations of
words depend on their contexts.
ELMo encodes context bidirectionally but uses task-specific architectures (however, it is
practically non-trivial to craft a specific architecture for every natural language pro-
cessing task); while GPT is task-agnostic but encodes context left-to-right.
BERT combines the best of both worlds: it encodes context bidirectionally and requires
minimal architecture changes for a wide range of natural language processing tasks.
The embeddings of the BERT input sequence are the sum of the token embeddings,
segment embeddings, and positional embeddings.
Pretraining BERT is composed of two tasks: masked language modeling and next sen-
tence prediction. The former is able to encode bidirectional context for representing
words, while the latter explicitly models the logical relationship between text pairs.
733 The Dataset for Pretraining BERT
23815.8.8Exercises
1.All other things being equal, will a masked language model require more or fewer pre-
training steps to converge than a left-to-right language model? Why?
2.IntheoriginalimplementationofBERT,thepositionwisefeed-forwardnetworkin BERTEn-
coder(viad2l.TransformerEncoderBlock ) and the fully connected layer in MaskLM
both use the Gaussian error linear unit (GELU) ( Hendrycks and Gimpel, 2016 ) as the
activation function. Research into the difference between GELU and ReLU.
Discussions238.
15.9TheDatasetforPretrainingBERT
TopretraintheBERTmodelasimplementedin Section15.8 ,weneedtogeneratethedataset
in the ideal format to facilitate the two pretraining tasks: masked language modeling and
next sentence prediction. On the one hand, the original BERT model is pretrained on
the concatenation of two huge corpora BookCorpus and English Wikipedia (see Section
15.8.5), making it hard to run for most readers of this book. On the other hand, the off-
the-shelf pretrained BERT model may not fit for applications from specific domains like
medicine. Thus, it is getting popular to pretrain BERT on a customized dataset. To facil-
itate the demonstration of BERT pretraining, we use a smaller corpus WikiText-2 ( Merity
etal., 2016).
Comparing with the PTB dataset used for pretraining word2vec in Section 15.3 , WikiText-
2 (i) retains the original punctuation, making it suitable for next sentence prediction; (ii)
retains the original case and numbers; (iii) is over twice larger.
import os
import random
import torch
from d2l import torch asd2l
In the WikiText-2 dataset, each line represents a paragraph where space is inserted be-
tween any punctuation and its preceding token. Paragraphs with at least two sentences are
retained. To split sentences, we only use the period as the delimiter for simplicity. We
leave discussions of more complex sentence splitting techniques in the exercises at the end
of this section.
#@save
d2l.DATA_HUB[ 'wikitext-2 ']=(
'https://s3.amazonaws.com/research.metamind.io/wikitext/ '
'wikitext-2-v1.zip ','3c914d17d80b1459be871a5039ac23e752a53cbe ')
#@save
(continues on next page)
734 Natural Language Processing: Pretraining
(continued from previous page)
def _read_wiki (data_dir):
file_name =os.path .join(data_dir, 'wiki.train.tokens ')
with open (file_name, 'r')asf:
lines =f.readlines()
# Uppercase letters are converted to lowercase ones
paragraphs =[line .strip() .lower() .split( '.')
for line inlines iflen(line .split( '.'))>=2]
random .shuffle(paragraphs)
return paragraphs
15.9.1Defining Helper Functions forPretrainingTasks
In the following, we begin by implementing helper functions for the two BERT pretraining
tasks: next sentence prediction and masked language modeling. These helper functions
will be invoked later when transforming the raw text corpus into the dataset of the ideal
format to pretrain BERT.
Generating the NextSentence PredictionTask
According to descriptions of Section 15.8.5 , the _get_next_sentence function generates
a training example for the binary classification task.
#@save
def _get_next_sentence (sentence, next_sentence, paragraphs):
ifrandom .random() <0.5:
is_next =True
else :
# `paragraphs` is a list of lists of lists
next_sentence =random .choice(random .choice(paragraphs))
is_next =False
return sentence, next_sentence, is_next
The following function generates training examples for next sentence prediction from the
input paragraph byinvokingthe _get_next_sentence function. Here paragraph isalist
of sentences, where each sentence is a list of tokens. The argument max_len specifies the
maximum length of a BERT input sequence during pretraining.
#@save
def _get_nsp_data_from_paragraph (paragraph, paragraphs, vocab, max_len):
nsp_data_from_paragraph =[]
for iinrange (len(paragraph) -1):
tokens_a, tokens_b, is_next =_get_next_sentence(
paragraph[i], paragraph[i +1], paragraphs)
# Consider 1 '<cls>' token and 2 '<sep>' tokens
iflen(tokens_a) +len(tokens_b) +3>max_len:
continue
tokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph .append((tokens, segments, is_next))
return nsp_data_from_paragraph
735 The Dataset for Pretraining BERT
Generatingthe MaskedLanguageModeling Task
InordertogeneratetrainingexamplesforthemaskedlanguagemodelingtaskfromaBERT
inputsequence,wedefinethefollowing _replace_mlm_tokens function. Initsinputs, to-
kensisalistoftokensrepresentingaBERTinputsequence, candidate_pred_positions
isalistoftokenindicesoftheBERTinputsequenceexcludingthoseofspecialtokens(spe-
cial tokens are not predicted in the masked language modeling task), and num_mlm_preds
indicates the number of predictions (recall 15% random tokens to predict). Following the
definitionofthemaskedlanguagemodelingtaskin Section15.8.5 , at eachpredictionposi-
tion, the input may be replaced by a special ‚Äú<mask>‚Äù token or a random token, or remain
unchanged. Intheend, thefunctionreturnstheinputtokensafterpossiblereplacement, the
token indices where predictions take place and labels for these predictions.
#@save
def _replace_mlm_tokens (tokens, candidate_pred_positions, num_mlm_preds,
vocab):
# For the input of a masked language model, make a new copy of tokens and
# replace some of them by '<mask>' or random tokens
mlm_input_tokens =[token for token intokens]
pred_positions_and_labels =[]
# Shuffle for getting 15% random tokens for prediction in the masked
# language modeling task
random .shuffle(candidate_pred_positions)
for mlm_pred_position incandidate_pred_positions:
iflen(pred_positions_and_labels) >=num_mlm_preds:
break
masked_token =None
# 80% of the time: replace the word with the '<mask>' token
ifrandom .random() <0.8:
masked_token ='<mask> '
else :
# 10% of the time: keep the word unchanged
ifrandom .random() <0.5:
masked_token =tokens[mlm_pred_position]
# 10% of the time: replace the word with a random word
else :
masked_token =random .choice(vocab .idx_to_token)
mlm_input_tokens[mlm_pred_position] =masked_token
pred_positions_and_labels .append(
(mlm_pred_position, tokens[mlm_pred_position]))
return mlm_input_tokens, pred_positions_and_labels
By invoking the aforementioned _replace_mlm_tokens function, the following function
takes a BERT input sequence ( tokens) as an input and returns indices of the input tokens
(after possible token replacement as described in Section 15.8.5 ), the token indices where
predictions take place, and label indices for these predictions.
#@save
def _get_mlm_data_from_tokens (tokens, vocab):
candidate_pred_positions =[]
# `tokens` is a list of strings
for i, token inenumerate (tokens):
(continues on next page)
736 Natural Language Processing: Pretraining
(continued from previous page)
# Special tokens are not predicted in the masked language modeling
# task
iftoken in['<cls> ','<sep> ']:
continue
candidate_pred_positions .append(i)
# 15% of random tokens are predicted in the masked language modeling task
num_mlm_preds =max(1,round (len(tokens) *0.15 ))
mlm_input_tokens, pred_positions_and_labels =_replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels =sorted (pred_positions_and_labels,
key=lambda x: x[ 0])
pred_positions =[v[0]for vinpred_positions_and_labels]
mlm_pred_labels =[v[1]for vinpred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
15.9.2TransformingTextinto the PretrainingDataset
Now we are almost ready to customize a Dataset class for pretraining BERT. Before that,
westillneedtodefineahelperfunction _pad_bert_inputs toappendthespecial‚Äú<pad>‚Äù
tokens to the inputs. Its argument examples contain the outputs from the helper func-
tions _get_nsp_data_from_paragraph and_get_mlm_data_from_tokens for the two
pretraining tasks.
#@save
def _pad_bert_inputs (examples, max_len, vocab):
max_num_mlm_preds =round (max_len *0.15 )
all_token_ids, all_segments, valid_lens, =[], [], []
all_pred_positions, all_mlm_weights, all_mlm_labels =[], [], []
nsp_labels =[]
for (token_ids, pred_positions, mlm_pred_label_ids, segments,
is_next) inexamples:
all_token_ids .append(torch .tensor(token_ids +[vocab[ '<pad> ']]*(
max_len -len(token_ids)), dtype =torch .long))
all_segments .append(torch .tensor(segments +[0]*(
max_len -len(segments)), dtype =torch .long))
# `valid_lens` excludes count of '<pad>' tokens
valid_lens .append(torch .tensor( len(token_ids), dtype =torch .float32))
all_pred_positions .append(torch .tensor(pred_positions +[0]*(
max_num_mlm_preds -len(pred_positions)), dtype =torch .long))
# Predictions of padded tokens will be filtered out in the loss via
# multiplication of 0 weights
all_mlm_weights .append(
torch .tensor([ 1.0]*len(mlm_pred_label_ids) +[0.0]*(
max_num_mlm_preds -len(pred_positions)),
dtype =torch .float32))
all_mlm_labels .append(torch .tensor(mlm_pred_label_ids +[0]*(
max_num_mlm_preds -len(mlm_pred_label_ids)), dtype =torch .long))
nsp_labels .append(torch .tensor(is_next, dtype =torch .long))
return (all_token_ids, all_segments, valid_lens, all_pred_positions,
all_mlm_weights, all_mlm_labels, nsp_labels)
Putting the helper functions for generating training examples of the two pretraining tasks,
737 The Dataset for Pretraining BERT
and the helper function for padding inputs together, we customize the following _Wiki-
TextDataset class as the WikiText-2 dataset for pretraining BERT. By implementing the
__getitem__ function, we can arbitrarily access the pretraining (masked language model-
ing and next sentence prediction) examples generated from a pair of sentences from the
WikiText-2 corpus.
The original BERT model uses WordPiece embeddings whose vocabulary size is 30000
(Wuet al., 2016). The tokenization method of WordPiece is a slight modification of the
original byte pair encoding algorithm in Section 15.6.2 . For simplicity, we use the d2l.
tokenize function for tokenization. Infrequent tokens that appear less than five times are
filtered out.
#@save
class _WikiTextDataset (torch .utils .data .Dataset):
def __init__ (self , paragraphs, max_len):
# Input `paragraphs[i]` is a list of sentence strings representing a
# paragraph; while output `paragraphs[i]` is a list of sentences
# representing a paragraph, where each sentence is a list of tokens
paragraphs =[d2l .tokenize(
paragraph, token ='word ')for paragraph inparagraphs]
sentences =[sentence for paragraph inparagraphs
for sentence inparagraph]
self .vocab =d2l.Vocab(sentences, min_freq =5, reserved_tokens =[
'<pad> ','<mask> ','<cls> ','<sep> '])
# Get data for the next sentence prediction task
examples =[]
for paragraph inparagraphs:
examples .extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self .vocab, max_len))
# Get data for the masked language model task
examples =[(_get_mlm_data_from_tokens(tokens, self .vocab)
+(segments, is_next))
for tokens, segments, is_next inexamples]
# Pad inputs
(self .all_token_ids, self .all_segments, self .valid_lens,
self .all_pred_positions, self .all_mlm_weights,
self .all_mlm_labels, self .nsp_labels) =_pad_bert_inputs(
examples, max_len, self .vocab)
def __getitem__ (self , idx):
return (self .all_token_ids[idx], self .all_segments[idx],
self .valid_lens[idx], self .all_pred_positions[idx],
self .all_mlm_weights[idx], self .all_mlm_labels[idx],
self .nsp_labels[idx])
def __len__ (self ):
return len(self .all_token_ids)
Byusingthe _read_wiki functionandthe _WikiTextDataset class,wedefinethefollow-
ingload_data_wiki to download and WikiText-2 dataset and generate pretraining exam-
ples from it.
738 Natural Language Processing: Pretraining
#@save
def load_data_wiki (batch_size, max_len):
"""Load the WikiText-2 dataset."""
num_workers =d2l.get_dataloader_workers()
data_dir =d2l.download_extract( 'wikitext-2 ','wikitext-2 ')
paragraphs =_read_wiki(data_dir)
train_set =_WikiTextDataset(paragraphs, max_len)
train_iter =torch .utils .data .DataLoader(train_set, batch_size,
shuffle =True , num_workers =num_workers)
return train_iter, train_set .vocab
Setting the batch size to 512 and the maximum length of a BERT input sequence to be
64, we print out the shapes of a minibatch of BERT pretraining examples. Note that in
each BERT input sequence, 10(640.15) positions are predicted for the masked language
modeling task.
batch_size, max_len =512,64
train_iter, vocab =load_data_wiki(batch_size, max_len)
for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
mlm_Y, nsp_y) intrain_iter:
print (tokens_X .shape, segments_X .shape, valid_lens_x .shape,
pred_positions_X .shape, mlm_weights_X .shape, mlm_Y .shape,
nsp_y .shape)
break
Downloading ../data /wikitext -2-v1.zip from https ://s3.amazonaws .com/research .
‚Ü©!metamind .io/wikitext /wikitext -2-v1.zip...
torch .Size([ 512,64]) torch .Size([ 512,64]) torch .Size([ 512]) torch .Size([ 512,‚ê£
‚Ü©!10]) torch .Size([ 512,10]) torch .Size([ 512,10]) torch .Size([ 512])
Intheend,let‚Äôstakealookatthevocabularysize. Evenafterfilteringoutinfrequenttokens,
it is still over twice larger than that of the PTB dataset.
len(vocab)
20256
15.9.3Summary
ComparingwiththePTBdataset,theWikiText-2datesetretainstheoriginalpunctuation,
case and numbers, and is over twice larger.
We can arbitrarily access the pretraining (masked language modeling and next sentence
prediction) examples generated from a pair of sentences from the WikiText-2 corpus.
15.9.4Exercises
739 Pretraining BERT
2391.For simplicity, the period is used as the only delimiter for splitting sentences. Try other
sentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an exam-
ple. You need to install NLTK first: pip install nltk . In the code, first import
nltk. Then, download the Punkt sentence tokenizer: nltk.download('punkt') . To
split sentences such as sentences = 'This is great ! Why not ?' , invok-
ingnltk.tokenize.sent_tokenize(sentences) will return a list of two sentence
strings: ['This is great !', 'Why not ?'] .
2.What is the vocabulary size if we do not filter out any infrequent token?
Discussions239.
15.10PretrainingBERT
WiththeBERTmodelimplementedin Section15.8 andthepretrainingexamplesgenerated
from the WikiText-2 dataset in Section 15.9 , we will pretrain BERT on the WikiText-2
dataset in this section.
import torch
from torch import nn
from d2l import torch asd2l
Tostart,weloadtheWikiText-2datasetasminibatchesofpretrainingexamplesformasked
language modeling and next sentence prediction. The batch size is 512 and the maximum
lengthofaBERTinputsequenceis64. NotethatintheoriginalBERTmodel,themaximum
length is 512.
batch_size, max_len =512,64
train_iter, vocab =d2l.load_data_wiki(batch_size, max_len)
15.10.1PretrainingBERT
TheoriginalBERThastwoversionsofdifferentmodelsizes( Devlinetal.,2018). Thebase
model (BERT BASE) uses 12 layers (Transformer encoder blocks) with 768 hidden units
(hidden size) and 12 self-attention heads. The large model (BERT LARGE) uses 24 layers
with 1024 hidden units and 16 self-attention heads. Notably, the former has 110 million
parameters while the latter has 340 million parameters. For demonstration with ease, we
define a small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads.
net =d2l.BERTModel( len(vocab), num_hiddens =128,
ffn_num_hiddens =256, num_heads =2, num_blks =2, dropout =0.2)
devices =d2l.try_all_gpus()
loss =nn.CrossEntropyLoss()
740 Natural Language Processing: Pretraining
Before defining the training loop, we define a helper function _get_batch_loss_bert .
Given the shard of training examples, this function computes the loss for both the masked
language modeling and next sentence prediction tasks. Note that the final loss of BERT
pretrainingisjustthesumofboththemaskedlanguagemodelinglossandthenextsentence
prediction loss.
#@save
def _get_batch_loss_bert (net, loss, vocab_size, tokens_X,
segments_X, valid_lens_x,
pred_positions_X, mlm_weights_X,
mlm_Y, nsp_y):
# Forward pass
_, mlm_Y_hat, nsp_Y_hat =net(tokens_X, segments_X,
valid_lens_x .reshape( -1),
pred_positions_X)
# Compute masked language model loss
mlm_l =loss(mlm_Y_hat .reshape( -1, vocab_size), mlm_Y .reshape( -1))*\
mlm_weights_X .reshape( -1,1)
mlm_l =mlm_l .sum() /(mlm_weights_X .sum() +1e-8 )
# Compute next sentence prediction loss
nsp_l =loss(nsp_Y_hat, nsp_y)
l=mlm_l +nsp_l
return mlm_l, nsp_l, l
Invoking the two aforementioned helper functions, the following train_bert function de-
finestheproceduretopretrainBERT( net)ontheWikiText-2( train_iter )dataset. Train-
ing BERT can take very long. Instead of specifying the number of epochs for training as in
thetrain_ch13 function(see Section14.1 ),theinput num_steps ofthefollowingfunction
specifies the number of iteration steps for training.
def train_bert (train_iter, net, loss, vocab_size, devices, num_steps):
net( *next (iter (train_iter))[: 4])
net =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])
trainer =torch .optim .Adam(net .parameters(), lr =0.01 )
step, timer =0, d2l .Timer()
animator =d2l.Animator(xlabel ='step ', ylabel ='loss ',
xlim =[1, num_steps], legend =['mlm','nsp'])
# Sum of masked language modeling losses, sum of next sentence prediction
# losses, no. of sentence pairs, count
metric =d2l.Accumulator( 4)
num_steps_reached =False
while step <num_steps and not num_steps_reached:
for tokens_X, segments_X, valid_lens_x, pred_positions_X,\
mlm_weights_X, mlm_Y, nsp_y intrain_iter:
tokens_X =tokens_X .to(devices[ 0])
segments_X =segments_X .to(devices[ 0])
valid_lens_x =valid_lens_x .to(devices[ 0])
pred_positions_X =pred_positions_X .to(devices[ 0])
mlm_weights_X =mlm_weights_X .to(devices[ 0])
mlm_Y, nsp_y =mlm_Y .to(devices[ 0]), nsp_y .to(devices[ 0])
trainer .zero_grad()
timer .start()
mlm_l, nsp_l, l =_get_batch_loss_bert(
(continues on next page)
741 Pretraining BERT
(continued from previous page)
net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
l.backward()
trainer .step()
metric .add(mlm_l, nsp_l, tokens_X .shape[ 0],1)
timer .stop()
animator .add(step +1,
(metric[ 0]/metric[ 3], metric[ 1]/metric[ 3]))
step +=1
ifstep ==num_steps:
num_steps_reached =True
break
print (f'MLM loss {metric[ 0]/metric[ 3]:.3f},'
f'NSP loss {metric[ 1]/metric[ 3]:.3f}')
print (f'{metric[ 2]/timer .sum() :.1f}sentence pairs/sec on '
f'{str(devices) }')
We can plot both the masked language modeling loss and the next sentence prediction loss
during BERT pretraining.
train_bert(train_iter, net, loss, len(vocab), devices, 50)
MLM loss 5.885 , NSP loss 0.760
4413.2 sentence pairs /sec on [device( type ='cuda ', index =0), device( type ='cuda ',
‚Ü©!index =1)]
15.10.2RepresentingTextwith BERT
After pretraining BERT, we can use it to represent single text, text pairs, or any token in
them. The following function returns the BERT ( net) representations for all tokens in
tokens_a andtokens_b .
def get_bert_encoding (net, tokens_a, tokens_b =None ):
tokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)
token_ids =torch .tensor(vocab[tokens], device =devices[ 0]).unsqueeze( 0)
segments =torch .tensor(segments, device =devices[ 0]).unsqueeze( 0)
(continues on next page)
742 Natural Language Processing: Pretraining
(continued from previous page)
valid_len =torch .tensor( len(tokens), device =devices[ 0]).unsqueeze( 0)
encoded_X, _, _ =net(token_ids, segments, valid_len)
return encoded_X
Consider the sentence ‚Äúa crane is flying‚Äù. Recall the input representation of BERT as dis-
cussed in Section 15.8.4 . After inserting special tokens ‚Äú<cls>‚Äù (used for classification)
and ‚Äú<sep>‚Äù (used for separation), the BERT input sequence has a length of six. Since
zero is the index of the ‚Äú<cls>‚Äù token, encoded_text[:, 0, :] is the BERT represen-
tation of the entire input sentence. To evaluate the polysemy token ‚Äúcrane‚Äù, we also print
out the first three elements of the BERT representation of the token.
tokens_a =['a','crane ','is','flying ']
encoded_text =get_bert_encoding(net, tokens_a)
# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'
encoded_text_cls =encoded_text[:, 0, :]
encoded_text_crane =encoded_text[:, 2, :]
encoded_text .shape, encoded_text_cls .shape, encoded_text_crane[ 0][:3]
(torch .Size([ 1,6,128]),
torch .Size([ 1,128]),
tensor([ 0.8414 ,1.4830 ,0.8226 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >))
Nowconsiderasentencepair‚Äúacranedrivercame‚Äùand‚Äúhejustleft‚Äù. Similarly, encoded_pair[:,
0, :]istheencodedresultoftheentiresentencepairfromthepretrainedBERT.Notethat
the first three elements of the polysemy token ‚Äúcrane‚Äù are different from those when the
context is different. This supports that BERT representations are context-sensitive.
tokens_a, tokens_b =['a','crane ','driver ','came '], [ 'he','just ','left ']
encoded_pair =get_bert_encoding(net, tokens_a, tokens_b)
# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',
# 'left', '<sep>'
encoded_pair_cls =encoded_pair[:, 0, :]
encoded_pair_crane =encoded_pair[:, 2, :]
encoded_pair .shape, encoded_pair_cls .shape, encoded_pair_crane[ 0][:3]
(torch .Size([ 1,10,128]),
torch .Size([ 1,128]),
tensor([ 0.0430 ,1.6132 ,0.0437 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >))
InChapter16 ,wewillfine-tuneapretrainedBERTmodelfordownstreamnaturallanguage
processing applications.
15.10.3Summary
The original BERT has two versions, where the base model has 110 million parameters
and the large model has 340 million parameters.
743 Pretraining BERT
240After pretraining BERT, we can use it to represent single text, text pairs, or any token in
them.
Intheexperiment,thesametokenhasdifferentBERTrepresentationwhentheircontexts
are different. This supports that BERT representations are context-sensitive.
15.10.4Exercises
1.In the experiment, we can see that the masked language modeling loss is significantly
higher than the next sentence prediction loss. Why?
2.SetthemaximumlengthofaBERTinputsequencetobe512(sameastheoriginalBERT
model). Use the configurations of the original BERT model such as BERT LARGE. Do
you encounter any error when running this section? Why?
Discussions240.
16Natural Language Processing:
Applications
We have seen how to represent tokens in text sequences and train their representations in
Chapter 15 . Such pretrained text representations can be fed to various models for different
downstream natural language processing tasks.
In fact, earlier chapters have already discussed some natural language processing applica-
tionswithout pretraining , just for explaining deep learning architectures. For instance, in
Chapter9 ,wehavereliedonRNNstodesignlanguagemodelstogeneratenovella-liketext.
InChapter10 andChapter11 , wehavealsodesignedmodelsbasedonRNNsandattention
mechanisms for machine translation.
However, this book does not intend to cover all such applications in a comprehensive man-
ner. Instead, our focus is on how to apply (deep) representation learning of languages to
addressing natural language processing problems . Given pretrained text representations,
this chapter will explore two popular and representative downstream natural language pro-
cessing tasks: sentiment analysis and natural language inference, which analyze single text
and relationships of text pairs, respectively.
tFig. 16.1 Pretrained text representations can be fed to various deep learning architectures for
different downstream natural language processing applications. This chapter focuses on
how to design models for different downstream natural language processing applications.
Asdepictedin Fig.16.1 ,thischapterfocusesondescribingthebasicideasofdesigningnat-
ural language processing models using different types of deep learning architectures, such
as MLPs, CNNs, RNNs, and attention. Though it is possible to combine any pretrained
textrepresentationswithanyarchitectureforeitherapplicationin Fig.16.1 ,weselectafew
representative combinations. Specifically, we will explore popular architectures based on
RNNs and CNNs for sentiment analysis. For natural language inference, we choose atten-
744
745 Sentiment Analysis and the Dataset
241tion and MLPs to demonstrate how to analyze text pairs. In the end, we introduce how to
fine-tune a pretrained BERT model for a wide range of natural language processing appli-
cations, such as on a sequence level (single text classification and text pair classification)
and a token level (text tagging and question answering). As a concrete empirical case, we
will fine-tune BERT for natural language inference.
As we have introduced in Section 15.8 , BERT requires minimal architecture changes for a
wide range of natural language processing applications. However, this benefit comes at the
cost of fine-tuning a huge number of BERT parameters for the downstream applications.
When space or time is limited, those crafted models based on MLPs, CNNs, RNNs, and
attention are more feasible. In the following, we start by the sentiment analysis application
and illustrate the model design based on RNNs and CNNs, respectively.
16.1SentimentAnalysisand the Dataset
With the proliferation of online social media and review platforms, a plethora of opinion-
ateddatahasbeenlogged,bearinggreatpotentialforsupportingdecisionmakingprocesses.
Sentiment analysis studies people‚Äôs sentiments in their produced text, such as product re-
views, blog comments, and forum discussions. It enjoys wide applications to fields as
diverse as politics (e.g., analysis of public sentiments towards policies), finance (e.g., anal-
ysisofsentimentsofthemarket), andmarketing(e.g., productresearchandbrandmanage-
ment).
Since sentiments can be categorized as discrete polarities or scales (e.g., positive and neg-
ative), we can consider sentiment analysis as a text classification task, which transforms a
varying-length text sequence into a fixed-length text category. In this chapter, we will use
Stanford‚Äôs largemoviereviewdataset241forsentimentanalysis. Itconsistsofatrainingset
and a testing set, either containing 25000 movie reviews downloaded from IMDb. In both
datasets, there are equal number of ‚Äúpositive‚Äù and ‚Äúnegative‚Äù labels, indicating different
sentiment polarities.
import os
import torch
from torch import nn
from d2l import torch asd2l
16.1.1Readingthe Dataset
First,downloadandextractthisIMDbreviewdatasetinthepath ../data/aclImdb .
#@save
d2l.DATA_HUB[ 'aclImdb ']=(d2l .DATA_URL +'aclImdb_v1.tar.gz ',
'01ada507287d82875905620988597833ad4e0903 ')
(continues on next page)
746 Natural Language Processing: Applications
(continued from previous page)
data_dir =d2l.download_extract( 'aclImdb ','aclImdb ')
Downloading ../data /aclImdb_v1 .tar.gzfrom http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/aclImdb_v1 .tar.gz...
Next, read the training and test datasets. Each example is a review and its label: 1 for
‚Äúpositive‚Äù and 0 for ‚Äúnegative‚Äù.
#@save
def read_imdb (data_dir, is_train):
"""Read the IMDb review dataset text sequences and labels."""
data, labels =[], []
for label in('pos','neg'):
folder_name =os.path .join(data_dir, 'train 'ifis_train else 'test ',
label)
for file inos.listdir(folder_name):
with open (os.path .join(folder_name, file), 'rb')asf:
review =f.read() .decode( 'utf-8 ').replace( '\n','')
data .append(review)
labels .append( 1iflabel =='pos'else 0)
return data, labels
train_data =read_imdb(data_dir, is_train =True )
print ('# trainings: ',len(train_data[ 0]))
for x, y inzip(train_data[ 0][:3], train_data[ 1][:3]):
print ('label: ', y, 'review: ', x[: 60])
# trainings: 25000
label: 1review: Zentropa has much incommon with The Third Man, another noir
label: 1review: Zentropa isthe most original movie I 've seen in years. If y
label: 1review: Lars Von Trier isnever backward intrying out new technique
16.1.2Preprocessingthe Dataset
Treating each word as a token and filtering out words that appear less than 5 times, we
create a vocabulary out of the training dataset.
train_tokens =d2l.tokenize(train_data[ 0], token ='word ')
vocab =d2l.Vocab(train_tokens, min_freq =5, reserved_tokens =['<pad> '])
After tokenization, let‚Äôs plot the histogram of review lengths in tokens.
d2l.set_figsize()
d2l.plt.xlabel( '# tokens per review ')
d2l.plt.ylabel( 'count ')
d2l.plt.hist([ len(line) for line intrain_tokens], bins =range (0,1000 ,50));
747 Sentiment Analysis and the Dataset
As we expected, the reviews have varying lengths. To process a minibatch of such reviews
at each time, we set the length of each review to 500 with truncation and padding, which is
similartothepreprocessingstepforthemachinetranslationdatasetin Section10.5 .
num_steps =500 # sequence length
train_features =torch .tensor([d2l .truncate_pad(
vocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])
print (train_features .shape)
torch .Size([ 25000 ,500])
16.1.3CreatingData Iterators
Nowwecancreatedataiterators. Ateachiteration,aminibatchofexamplesarereturned.
train_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])), 64)
for X, y intrain_iter:
print ('X:', X.shape, ', y: ', y.shape)
break
print ('# batches: ',len(train_iter))
X: torch .Size([ 64,500]) , y: torch .Size([ 64])
# batches: 391
16.1.4PuttingIt All Together
Last, we wrap up the above steps into the load_data_imdb function. It returns training
and test data iterators and the vocabulary of the IMDb review dataset.
#@save
def load_data_imdb (batch_size, num_steps =500):
"""Return data iterators and the vocabulary of the IMDb review dataset."""
data_dir =d2l.download_extract( 'aclImdb ','aclImdb ')
train_data =read_imdb(data_dir, True )
test_data =read_imdb(data_dir, False )
(continues on next page)
748 Natural Language Processing: Applications
242
243(continued from previous page)
train_tokens =d2l.tokenize(train_data[ 0], token ='word ')
test_tokens =d2l.tokenize(test_data[ 0], token ='word ')
vocab =d2l.Vocab(train_tokens, min_freq =5)
train_features =torch .tensor([d2l .truncate_pad(
vocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])
test_features =torch .tensor([d2l .truncate_pad(
vocab[line], num_steps, vocab[ '<pad> '])for line intest_tokens])
train_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])),
batch_size)
test_iter =d2l.load_array((test_features, torch .tensor(test_data[ 1])),
batch_size,
is_train =False )
return train_iter, test_iter, vocab
16.1.5Summary
Sentiment analysis studies people‚Äôs sentiments in their produced text, which is consid-
ered as a text classification problem that transforms a varying-length text sequence
into a fixed-length text category.
After preprocessing, we can load Stanford‚Äôs large movie review dataset (IMDb review
dataset) into data iterators with a vocabulary.
16.1.6Exercises
1.What hyperparameters in this section can we modify to accelerate training sentiment
analysis models?
2.Can you implement a function to load the dataset of Amazon reviews242into data
iterators and labels for sentiment analysis?
Discussions243.
16.2Sentiment Analysis: Using RecurrentNeural
Networks
Like word similarity and analogy tasks, we can also apply pretrained word vectors to sen-
timent analysis. Since the IMDb review dataset in Section 16.1 is not very big, using text
representations that were pretrained on large-scale corpora may reduce overfitting of the
model. As a specific example illustrated in Fig. 16.2.1 , we will represent each token using
the pretrained GloVe model, and feed these token representations into a multilayer bidi-
rectional RNN to obtain the text sequence representation, which will be transformed into
sentiment analysis outputs ( Maaset al., 2011). For the same downstream application, we
will consider a different architectural choice later.
749 Sentiment Analysis: Using Recurrent Neural Networks
tFig. 16.2.1 This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.
import torch
from torch import nn
from d2l import torch asd2l
batch_size =64
train_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)
16.2.1RepresentingSingle Textwith RNNs
In text classifications tasks, such as sentiment analysis, a varying-length text sequence will
betransformedintofixed-lengthcategories. Inthefollowing BiRNNclass,whileeachtoken
of a text sequence gets its individual pretrained GloVe representation via the embedding
layer ( self.embedding ), the entire sequence is encoded by a bidirectional RNN ( self.
encoder ). More concretely, the hidden states (at the last layer) of the bidirectional LSTM
at both the initial and final time steps are concatenated as the representation of the text
sequence. This single text representation is then transformed into output categories by a
fullyconnectedlayer( self.decoder )withtwooutputs(‚Äúpositive‚Äùand‚Äúnegative‚Äù).
class BiRNN (nn.Module):
def __init__ (self , vocab_size, embed_size, num_hiddens,
num_layers, **kwargs):
super (BiRNN, self ).__init__ (**kwargs)
self .embedding =nn.Embedding(vocab_size, embed_size)
# Set `bidirectional` to True to get a bidirectional RNN
self .encoder =nn.LSTM(embed_size, num_hiddens, num_layers =num_layers,
bidirectional =True )
self .decoder =nn.Linear( 4*num_hiddens, 2)
def forward (self , inputs):
# The shape of `inputs` is (batch size, no. of time steps). Because
# LSTM requires its input's first dimension to be the temporal
# dimension, the input is transposed before obtaining token
# representations. The output shape is (no. of time steps, batch size,
# word vector dimension)
embeddings =self .embedding(inputs .T)
self .encoder .flatten_parameters()
# Returns hidden states of the last hidden layer at different time
(continues on next page)
750 Natural Language Processing: Applications
(continued from previous page)
# steps. The shape of `outputs` is (no. of time steps, batch size,
# 2 * no. of hidden units)
outputs, _ =self .encoder(embeddings)
# Concatenate the hidden states at the initial and final time steps as
# the input of the fully connected layer. Its shape is (batch size,
# 4 * no. of hidden units)
encoding =torch .cat((outputs[ 0], outputs[ -1]), dim =1)
outs =self .decoder(encoding)
return outs
Let‚Äôs construct a bidirectional RNN with two hidden layers to represent single text for sen-
timent analysis.
embed_size, num_hiddens, num_layers, devices =100,100,2, d2l .try_all_gpus()
net =BiRNN( len(vocab), embed_size, num_hiddens, num_layers)
def init_weights (module):
iftype (module) ==nn.Linear:
nn.init .xavier_uniform_(module .weight)
iftype (module) ==nn.LSTM:
for param inmodule ._flat_weights_names:
if"weight "inparam:
nn.init .xavier_uniform_(module ._parameters[param])
net.apply(init_weights);
16.2.2LoadingPretrainedWordVectors
Below we load the pretrained 100-dimensional (needs to be consistent with embed_size )
GloVe embeddings for tokens in the vocabulary.
glove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')
Print the shape of the vectors for all the tokens in the vocabulary.
embeds =glove_embedding[vocab .idx_to_token]
embeds .shape
torch .Size([ 49346 ,100])
Weusethesepretrainedwordvectorstorepresenttokensinthereviewsandwillnotupdate
these vectors during training.
net.embedding .weight .data .copy_(embeds)
net.embedding .weight .requires_grad =False
16.2.3Trainingand Evaluatingthe Model
751 Sentiment Analysis: Using Recurrent Neural Networks
Now we can train the bidirectional RNN for sentiment analysis.
lr, num_epochs =0.01 ,5
trainer =torch .optim .Adam(net .parameters(), lr =lr)
loss =nn.CrossEntropyLoss(reduction ="none ")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.277 , train acc 0.884 , test acc 0.861
2608.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
Wedefinethefollowingfunctiontopredictthesentimentofatextsequenceusingthetrained
model net.
#@save
def predict_sentiment (net, vocab, sequence):
"""Predict the sentiment of a text sequence."""
sequence =torch .tensor(vocab[sequence .split()], device =d2l.try_gpu())
label =torch .argmax(net(sequence .reshape( 1,-1)), dim =1)
return 'positive 'iflabel ==1else 'negative '
Finally,let‚Äôsusethetrainedmodeltopredictthesentimentfortwosimplesentences.
predict_sentiment(net, vocab, 'this movie is so great ')
'positive '
predict_sentiment(net, vocab, 'this movie is so bad ')
'negative '
16.2.4Summary
Pretrained word vectors can represent individual tokens in a text sequence.
752 Natural Language Processing: Applications
244Bidirectional RNNs can represent a text sequence, such as via the concatenation of its
hidden states at the initial and final time steps. This single text representation can be
transformed into categories using a fully connected layer.
16.2.5Exercises
1.Increase the number of epochs. Can you improve the training and testing accuracies?
How about tuning other hyperparameters?
2.Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does
it improve classification accuracy?
3.Can we improve the classification accuracy by using the spaCy tokenization? You need
to install spaCy ( pip install spacy ) and install the English package ( python -m
spacy download en ). In the code, first, import spaCy ( import spacy ). Then, load
the spaCy English package ( spacy_en = spacy.load('en') ). Finally, define the
function def tokenizer(text): return [tok.text for tok in spacy_en.
tokenizer(text)] and replace the original tokenizer function. Note the different
forms of phrase tokens in GloVe and spaCy. For example, the phrase token ‚Äúnew york‚Äù
takes the form of ‚Äúnew-york‚Äù in GloVe and the form of ‚Äúnew york‚Äù after the spaCy
tokenization.
Discussions244.
16.3SentimentAnalysis: Using Convolutional
NeuralNetworks
InChapter7 , weinvestigatedmechanismsforprocessingtwo-dimensionalimagedatawith
two-dimensionalCNNs,whichwereappliedtolocalfeaturessuchasadjacentpixels. Though
originally designed for computer vision, CNNs are also widely used for natural language
processing. Simplyput, justthinkofanytextsequenceasaone-dimensionalimage. Inthis
way, one-dimensional CNNs can process local features such as ùëõ-grams in text.
In this section, we will use the textCNN model to demonstrate how to design a CNN ar-
chitecture for representing single text ( Kim, 2014 ). Compared with Fig. 16.2.1 that uses
an RNN architecture with GloVe pretraining for sentiment analysis, the only difference in
Fig. 16.3.1 lies in the choice of the architecture.
import torch
from torch import nn
from d2l import torch asd2l
batch_size =64
train_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)
753 Sentiment Analysis: Using Convolutional Neural Networks
tFig. 16.3.1 This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.
16.3.1One-DimensionalConvolutions
Before introducing the model, let‚Äôs see how a one-dimensional convolution works. Bear
in mind that it is just a special case of a two-dimensional convolution based on the cross-
correlation operation.
tFig. 16.3.2 One-dimensional cross-correlation operation. The shaded portions are the Ô¨Årst output
element as well as the input and kernel tensor elements used for the output computation:
01¬∏12=2.
As shown in Fig. 16.3.2 , in the one-dimensional case, the convolution window slides from
left to right across the input tensor. During sliding, the input subtensor (e.g., 0and 1in
Fig.16.3.2 )containedintheconvolutionwindowatacertainpositionandthekerneltensor
(e.g., 1and2inFig. 16.3.2 ) are multiplied elementwise. The sum of these multiplications
gives the single scalar value (e.g., 01¬∏12=2inFig. 16.3.2 ) at the corresponding
position of the output tensor.
Weimplementone-dimensionalcross-correlationinthefollowing corr1dfunction. Given
an input tensor Xand a kernel tensor K, it returns the output tensor Y.
def corr1d (X, K):
w=K.shape[ 0]
Y=torch .zeros((X .shape[ 0]-w+1))
for iinrange (Y.shape[ 0]):
Y[i] =(X[i: i +w]*K).sum()
return Y
We can construct the input tensor Xand the kernel tensor KfromFig. 16.3.2 to validate the
output of the above one-dimensional cross-correlation implementation.
X, K =torch .tensor([ 0,1,2,3,4,5,6]), torch .tensor([ 1,2])
corr1d(X, K)
754 Natural Language Processing: Applications
tensor([ 2.,5.,8.,11.,14.,17.])
Foranyone-dimensionalinputwithmultiplechannels,theconvolutionkernelneedstohave
the same number of input channels. Then for each channel, perform a cross-correlation
operation on the one-dimensional tensor of the input and the one-dimensional tensor of
the convolution kernel, summing the results over all the channels to produce the one-
dimensional output tensor. Fig. 16.3.3 shows a one-dimensional cross-correlation oper-
ation with 3 input channels.
tFig. 16.3.3 One-dimensional cross-correlation operation with 3 input channels. The shaded portions
are the Ô¨Årst output element as well as the input and kernel tensor elements used for the
output computation: 0 1¬∏12¬∏13¬∏24¬∏2¬π 1¬∫¬∏3¬π 3¬∫=2.
Wecanimplementtheone-dimensionalcross-correlationoperationformultipleinputchan-
nels and validate the results in Fig. 16.3.3 .
def corr1d_multi_in (X, K):
# First, iterate through the 0th dimension (channel dimension) of `X` and
# `K`. Then, add them together
return sum(corr1d(x, k) for x, k inzip(X, K))
X=torch .tensor([[ 0,1,2,3,4,5,6],
[1,2,3,4,5,6,7],
[2,3,4,5,6,7,8]])
K=torch .tensor([[ 1,2], [ 3,4], [ -1,-3]])
corr1d_multi_in(X, K)
tensor([ 2.,8.,14.,20.,26.,32.])
Note that multi-input-channel one-dimensional cross-correlations are equivalent to single-
input-channel two-dimensional cross-correlations. To illustrate, an equivalent form of the
multi-input-channel one-dimensional cross-correlation in Fig. 16.3.3 is the single-input-
channel two-dimensional cross-correlation in Fig. 16.3.4 , where the height of the convolu-
tion kernel has to be the same as that of the input tensor.
Both the outputs in Fig. 16.3.2 andFig. 16.3.3 have only one channel. Same as two-
dimensional convolutions with multiple output channels described in Section 7.4.2 , we
can also specify multiple output channels for one-dimensional convolutions.
16.3.2Max-Over-Time Pooling
Similarly, we can use pooling to extract the highest value from sequence representations as
the most important feature across time steps. The max-over-timepooling used in textCNN
755 Sentiment Analysis: Using Convolutional Neural Networks
tFig. 16.3.4 Two-dimensional cross-correlation operation with a single input channel. The shaded
portions are the Ô¨Årst output element as well as the input and kernel tensor elements used
for the output computation: 2 ¬π 1¬∫¬∏3¬π 3¬∫¬∏13¬∏24¬∏01¬∏12=2.
works like the one-dimensional global max-pooling ( Collobert et al., 2011). For a multi-
channel input where each channel stores values at different time steps, the output at each
channelisthemaximumvalueforthatchannel. Notethatthemax-over-timepoolingallows
different numbers of time steps at different channels.
16.3.3The textCNNModel
Using the one-dimensional convolution and max-over-time pooling, the textCNN model
takes individual pretrained token representations as input, then obtains and transforms se-
quence representations for the downstream application.
For a single text sequence with ùëõtokens represented by ùëë-dimensional vectors, the width,
height, and number of channels of the input tensor are ùëõ,1, andùëë, respectively. The
textCNN model transforms the input into the output as follows:
1.Define multiple one-dimensional convolution kernels and perform convolution opera-
tions separately on the inputs. Convolution kernels with different widths may capture
local features among different numbers of adjacent tokens.
2.Perform max-over-time pooling on all the output channels, and then concatenate all the
scalar pooling outputs as a vector.
3.Transform the concatenated vector into the output categories using the fully connected
layer. Dropout can be used for reducing overfitting.
Fig.16.3.5 illustratesthemodelarchitectureoftextCNNwithaconcreteexample. Theinput
is a sentence with 11 tokens, where each token is represented by a 6-dimensional vectors.
So we have a 6-channel input with width 11. Define two one-dimensional convolution
kernelsofwidths2and4,with4and5outputchannels,respectively. Theyproduce4output
channelswithwidth 11 2¬∏1=10and5outputchannelswithwidth 11 4¬∏1=8. Despite
different widths of these 9 channels, the max-over-time pooling gives a concatenated 9-
dimensional vector, which is finally transformed into a 2-dimensional output vector for
binary sentiment predictions.
Definingthe Model
We implement the textCNN model in the following class. Compared with the bidirectional
RNN model in Section 16.2 , besides replacing recurrent layers with convolutional layers,
756 Natural Language Processing: Applications
tFig. 16.3.5 The model architecture of textCNN.
we also use two embedding layers: one with trainable weights and the other with fixed
weights.
class TextCNN (nn.Module):
def __init__ (self , vocab_size, embed_size, kernel_sizes, num_channels,
**kwargs):
super (TextCNN, self ).__init__ (**kwargs)
self .embedding =nn.Embedding(vocab_size, embed_size)
# The embedding layer not to be trained
self .constant_embedding =nn.Embedding(vocab_size, embed_size)
self .dropout =nn.Dropout( 0.5)
self .decoder =nn.Linear( sum(num_channels), 2)
# The max-over-time pooling layer has no parameters, so this instance
# can be shared
self .pool =nn.AdaptiveAvgPool1d( 1)
self .relu =nn.ReLU()
# Create multiple one-dimensional convolutional layers
self .convs =nn.ModuleList()
for c, k inzip(num_channels, kernel_sizes):
self .convs .append(nn .Conv1d( 2*embed_size, c, k))
def forward (self , inputs):
# Concatenate two embedding layer outputs with shape (batch size, no.
# of tokens, token vector dimension) along vectors
embeddings =torch .cat((
self .embedding(inputs), self .constant_embedding(inputs)), dim =2)
# Per the input format of one-dimensional convolutional layers,
# rearrange the tensor so that the second dimension stores channels
embeddings =embeddings .permute( 0,2,1)
# For each one-dimensional convolutional layer, after max-over-time
# pooling, a tensor of shape (batch size, no. of channels, 1) is
# obtained. Remove the last dimension and concatenate along channels
encoding =torch .cat([
(continues on next page)
757 Sentiment Analysis: Using Convolutional Neural Networks
(continued from previous page)
torch .squeeze( self .relu( self .pool(conv(embeddings))), dim =-1)
for conv inself .convs], dim =1)
outputs =self .decoder( self .dropout(encoding))
return outputs
Let‚Äôs create a textCNN instance. It has 3 convolutional layers with kernel widths of 3, 4,
and 5, all with 100 output channels.
embed_size, kernel_sizes, nums_channels =100, [3,4,5], [ 100,100,100]
devices =d2l.try_all_gpus()
net =TextCNN( len(vocab), embed_size, kernel_sizes, nums_channels)
def init_weights (module):
iftype (module) in(nn.Linear, nn .Conv1d):
nn.init .xavier_uniform_(module .weight)
net.apply(init_weights);
LoadingPretrainedWordVectors
Same as Section 16.2 , we load pretrained 100-dimensional GloVe embeddings as the ini-
tialized token representations. These token representations (embedding weights) will be
trained in embedding and fixed in constant_embedding .
glove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')
embeds =glove_embedding[vocab .idx_to_token]
net.embedding .weight .data .copy_(embeds)
net.constant_embedding .weight .data .copy_(embeds)
net.constant_embedding .weight .requires_grad =False
Trainingand Evaluatingthe Model
Now we can train the textCNN model for sentiment analysis.
lr, num_epochs =0.001 ,5
trainer =torch .optim .Adam(net .parameters(), lr =lr)
loss =nn.CrossEntropyLoss(reduction ="none ")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.066 , train acc 0.979 , test acc 0.868
4354.2 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
Below we use the trained model to predict the sentiment for two simple sentences.
d2l.predict_sentiment(net, vocab, 'this movie is so great ')
758 Natural Language Processing: Applications
245'positive '
d2l.predict_sentiment(net, vocab, 'this movie is so bad ')
'negative '
16.3.4Summary
One-dimensional CNNs can process local features such as ùëõ-grams in text.
Multi-input-channel one-dimensional cross-correlations are equivalent to single-input-
channel two-dimensional cross-correlations.
The max-over-time pooling allows different numbers of time steps at different channels.
ThetextCNNmodeltransformsindividualtokenrepresentationsintodownstreamappli-
cationoutputsusingone-dimensionalconvolutionallayersandmax-over-timepooling
layers.
16.3.5Exercises
1.Tune hyperparameters and compare the two architectures for sentiment analysis in Sec-
tion 16.2 and in this section, such as in classification accuracy and computational effi-
ciency.
2.Can you further improve the classification accuracy of the model by using the methods
introduced in the exercises of Section 16.2 ?
3.Add positional encoding in the input representations. Does it improve the classification
accuracy?
Discussions245.
759 Natural Language Inference and the Dataset
16.4Natural LanguageInferenceand the Dataset
InSection 16.1 , we discussed the problem of sentiment analysis. This task aims to clas-
sify a single text sequence into predefined categories, such as a set of sentiment polarities.
However, when there is a need to decide whether one sentence can be inferred form an-
other, or eliminate redundancy by identifying sentences that are semantically equivalent,
knowing how to classify one text sequence is insufficient. Instead, we need to be able to
reason over pairs of text sequences.
16.4.1NaturalLanguageInference
Natural language inference studies whether a hypothesis can be inferred from a premise,
where both are a text sequence. In other words, natural language inference determines the
logical relationship between a pair of text sequences. Such relationships usually fall into
three types:
Entailment : the hypothesis can be inferred from the premise.
Contradiction : the negation of the hypothesis can be inferred from the premise.
Neutral: all the other cases.
Natural language inference is also known as the recognizing textual entailment task. For
example, the following pair will be labeled as entailment because ‚Äúshowing affection‚Äù in
the hypothesis can be inferred from ‚Äúhugging one another‚Äù in the premise.
Premise: Two women are hugging each other.
Hypothesis: Two women are showing affection.
The following is an example of contradiction as ‚Äúrunning the coding example‚Äù indicates
‚Äúnot sleeping‚Äù rather than ‚Äúsleeping‚Äù.
Premise: A man is running the coding example from Dive into Deep Learning.
Hypothesis: The man is sleeping.
The third example shows a neutrality relationship because neither ‚Äúfamous‚Äù nor ‚Äúnot fa-
mous‚Äù can be inferred from the fact that ‚Äúare performing for us‚Äù.
Premise: The musicians are performing for us.
Hypothesis: The musicians are famous.
Natural language inference has been a central topic for understanding natural language.
It enjoys wide applications ranging from information retrieval to open-domain question
answering. Tostudythisproblem,wewillbeginbyinvestigatingapopularnaturallanguage
inference benchmark dataset.
760 Natural Language Processing: Applications
16.4.2The StanfordNaturalLanguageInference(SNLI) Dataset
StanfordNaturalLanguageInference(SNLI)Corpusisacollectionofover500000labeled
English sentence pairs ( Bowman etal., 2015). We download and store the extracted SNLI
dataset in the path ../data/snli_1.0 .
import os
import re
import torch
from torch import nn
from d2l import torch asd2l
#@save
d2l.DATA_HUB[ 'SNLI ']=(
'https://nlp.stanford.edu/projects/snli/snli_1.0.zip ',
'9fcde07509c7e87ec61c640c1b2753d9041758e4 ')
data_dir =d2l.download_extract( 'SNLI ')
Readingthe Dataset
The original SNLI dataset contains much richer information than what we really need in
our experiments. Thus, we define a function read_snli to only extract part of the dataset,
then return lists of premises, hypotheses, and their labels.
#@save
def read_snli (data_dir, is_train):
"""Read the SNLI dataset into premises, hypotheses, and labels."""
def extract_text (s):
# Remove information that will not be used by us
s=re.sub( '\\(','', s)
s=re.sub( '\\)','', s)
# Substitute two or more consecutive whitespace with space
s=re.sub( '\\s{2,}','', s)
return s.strip()
label_set ={'entailment ':0,'contradiction ':1,'neutral ':2}
file_name =os.path .join(data_dir, 'snli_1.0_train.txt '
ifis_train else 'snli_1.0_test.txt ')
with open (file_name, 'r')asf:
rows =[row .split( '\t')for row inf.readlines()[ 1:]]
premises =[extract_text(row[ 1])for row inrows ifrow[ 0]inlabel_set]
hypotheses =[extract_text(row[ 2])for row inrows ifrow[ 0]inlabel_set]
labels =[label_set[row[ 0]]for row inrows ifrow[ 0]inlabel_set]
return premises, hypotheses, labels
Now let‚Äôs print the first 3 pairs of premise and hypothesis, as well as their labels (‚Äú0‚Äù, ‚Äú1‚Äù,
and ‚Äú2‚Äù correspond to ‚Äúentailment‚Äù, ‚Äúcontradiction‚Äù, and ‚Äúneutral‚Äù, respectively ).
train_data =read_snli(data_dir, is_train =True )
for x0, x1, y inzip(train_data[ 0][:3], train_data[ 1][:3], train_data[ 2][:3]):
print ('premise: ', x0)
(continues on next page)
761 Natural Language Inference and the Dataset
(continued from previous page)
print ('hypothesis: ', x1)
print ('label: ', y)
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person istraining his horse for a competition .
label: 2
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person isat a diner , ordering an omelette .
label: 1
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person isoutdoors , on a horse .
label: 0
The training set has about 550000 pairs, and the testing set has about 10000 pairs. The fol-
lowingshowsthatthethreelabels‚Äúentailment‚Äù,‚Äúcontradiction‚Äù,and‚Äúneutral‚Äùarebalanced
in both the training set and the testing set.
test_data =read_snli(data_dir, is_train =False )
for data in[train_data, test_data]:
print ([[row for row indata[ 2]].count(i) for iinrange (3)])
[183416 ,183187 ,182764 ]
[3368 ,3237 ,3219 ]
Defininga Class forLoading the Dataset
Below we define a class for loading the SNLI dataset by inheriting from the Dataset class
in Gluon. The argument num_steps in the class constructor specifies the length of a text
sequence so that each minibatch of sequences will have the same shape. In other words,
tokens after the first num_steps ones in longer sequence are trimmed, while special tokens
‚Äú<pad>‚Äù will be appended to shorter sequences until their length becomes num_steps . By
implementingthe __getitem__ function,wecanarbitrarilyaccessthepremise,hypothesis,
and label with the index idx.
#@save
class SNLIDataset (torch .utils .data .Dataset):
"""A customized dataset to load the SNLI dataset."""
def __init__ (self , dataset, num_steps, vocab =None ):
self .num_steps =num_steps
all_premise_tokens =d2l.tokenize(dataset[ 0])
all_hypothesis_tokens =d2l.tokenize(dataset[ 1])
ifvocab isNone :
self .vocab =d2l.Vocab(all_premise_tokens +all_hypothesis_tokens,
min_freq =5, reserved_tokens =['<pad> '])
else :
self .vocab =vocab
self .premises =self ._pad(all_premise_tokens)
(continues on next page)
762 Natural Language Processing: Applications
(continued from previous page)
self .hypotheses =self ._pad(all_hypothesis_tokens)
self .labels =torch .tensor(dataset[ 2])
print ('read '+str(len(self .premises)) +'examples ')
def _pad (self , lines):
return torch .tensor([d2l .truncate_pad(
self .vocab[line], self .num_steps, self .vocab[ '<pad> '])
for line inlines])
def __getitem__ (self , idx):
return (self .premises[idx], self .hypotheses[idx]), self .labels[idx]
def __len__ (self ):
return len(self .premises)
PuttingIt All Together
Now we can invoke the read_snli function and the SNLIDataset class to download the
SNLI dataset and return DataLoader instances for both training and testing sets, together
with the vocabulary of the training set. It is noteworthy that we must use the vocabulary
constructed from the training set as that of the testing set. As a result, any new token from
the testing set will be unknown to the model trained on the training set.
#@save
def load_data_snli (batch_size, num_steps =50):
"""Download the SNLI dataset and return data iterators and vocabulary."""
num_workers =d2l.get_dataloader_workers()
data_dir =d2l.download_extract( 'SNLI ')
train_data =read_snli(data_dir, True )
test_data =read_snli(data_dir, False )
train_set =SNLIDataset(train_data, num_steps)
test_set =SNLIDataset(test_data, num_steps, train_set .vocab)
train_iter =torch .utils .data .DataLoader(train_set, batch_size,
shuffle =True ,
num_workers =num_workers)
test_iter =torch .utils .data .DataLoader(test_set, batch_size,
shuffle =False ,
num_workers =num_workers)
return train_iter, test_iter, train_set .vocab
Herewesetthebatchsizeto128andsequencelengthto50,andinvokethe load_data_snli
functiontogetthedataiteratorsandvocabulary. Thenweprintthevocabularysize.
train_iter, test_iter, vocab =load_data_snli( 128,50)
len(vocab)
read 549367 examples
read 9824 examples
763 Natural Language Inference: Using Attention
24618678
Now we print the shape of the first minibatch. Contrary to sentiment analysis, we have two
inputs X[0]andX[1]representing pairs of premises and hypotheses.
for X, Y intrain_iter:
print (X[0].shape)
print (X[1].shape)
print (Y.shape)
break
torch .Size([ 128,50])
torch .Size([ 128,50])
torch .Size([ 128])
16.4.3Summary
Natural language inference studies whether a hypothesis can be inferred from a premise,
where both are a text sequence.
In natural language inference, relationships between premises and hypotheses include
entailment, contradiction, and neutral.
Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of
natural language inference.
16.4.4Exercises
1.Machine translation has long been evaluated based on superficial ùëõ-gram matching be-
tween an output translation and a ground-truth translation. Can you design a measure
for evaluating machine translation results by using natural language inference?
2.How can we change hyperparameters to reduce the vocabulary size?
Discussions246.
16.5NaturalLanguageInference: Using Attention
WeintroducedthenaturallanguageinferencetaskandtheSNLIdatasetin Section16.4 . In
viewofmanymodelsthatarebasedoncomplexanddeeparchitectures,Parikh etal.(2016)
proposed to address natural language inference with attention mechanisms and called it a
‚Äúdecomposableattentionmodel‚Äù. Thisresultsinamodelwithoutrecurrentorconvolutional
layers,achievingthebestresultatthetimeontheSNLIdatasetwithmuchfewerparameters.
764 Natural Language Processing: Applications
In this section, we will describe and implement this attention-based method (with MLPs)
for natural language inference, as depicted in Fig. 16.5.1 .
tFig. 16.5.1 This section feeds pretrained GloVe to an architecture based on attention and MLPs for
natural language inference.
16.5.1TheModel
Simpler than preserving the order of tokens in premises and hypotheses, we can just align
tokens in one text sequence to every token in the other, and vice versa, then compare and
aggregate such information to predict the logical relationships between premises and hy-
potheses. Similar to alignment of tokens between source and target sentences in machine
translation,thealignmentoftokensbetweenpremisesandhypothesescanbeneatlyaccom-
plished by attention mechanisms.
tFig. 16.5.2 Natural language inference using attention mechanisms.
Fig.16.5.2 depictsthenaturallanguageinferencemethodusingattentionmechanisms. Ata
high level, it consists of three jointly trained steps: attending, comparing, and aggregating.
We will illustrate them step by step in the following.
import torch
from torch import nn
from torch .nnimport functional asF
from d2l import torch asd2l
765 Natural Language Inference: Using Attention
Attending
The first step is to align tokens in one text sequence to each token in the other sequence.
Suppose that the premise is ‚Äúi do need sleep‚Äù and the hypothesis is ‚Äúi am tired‚Äù. Due to
semantical similarity, we may wish to align ‚Äúi‚Äù in the hypothesis with ‚Äúi‚Äù in the premise,
and align ‚Äútired‚Äù in the hypothesis with ‚Äúsleep‚Äù in the premise. Likewise, we may wish
to align ‚Äúi‚Äù in the premise with ‚Äúi‚Äù in the hypothesis, and align ‚Äúneed‚Äù and ‚Äúsleep‚Äù in the
premise with ‚Äútired‚Äù in the hypothesis. Note that such alignment is softusing weighted
average, where ideally large weights are associated with the tokens to be aligned. For ease
of demonstration, Fig. 16.5.2 shows such alignment in a hardway.
Now we describe the soft alignment using attention mechanisms in more detail. Denote
byA=¬πa1,...,aùëö¬∫andB=¬πb1,...,bùëõ¬∫the premise and hypothesis, whose number
of tokens are ùëöandùëõ, respectively, where aùëñ,bùëó2Rùëë(ùëñ=1,...,ùëö,ùëó =1,...,ùëõ) is a
ùëë-dimensional word vector. For soft alignment, we compute the attention weights ùëíùëñùëó2R
as
ùëíùëñùëó=ùëì¬πaùëñ¬∫>ùëì¬πbùëó¬∫, (16.5.1)
where the function ùëìis an MLP defined in the following mlpfunction. The output dimen-
sion ofùëìis specified by the num_hiddens argument of mlp.
def mlp(num_inputs, num_hiddens, flatten):
net =[]
net.append(nn .Dropout( 0.2))
net.append(nn .Linear(num_inputs, num_hiddens))
net.append(nn .ReLU())
ifflatten:
net.append(nn .Flatten(start_dim =1))
net.append(nn .Dropout( 0.2))
net.append(nn .Linear(num_hiddens, num_hiddens))
net.append(nn .ReLU())
ifflatten:
net.append(nn .Flatten(start_dim =1))
return nn.Sequential( *net)
It should be highlighted that, in (16.5.1 )ùëìtakes inputs aùëñandbùëóseparately rather than
takesapairofthemtogetherasinput. This decomposition trickleadstoonly ùëö¬∏ùëõapplica-
tions (linear complexity) of ùëìrather thanùëöùëõapplications (quadratic complexity).
Normalizing the attention weights in (16.5.1 ), we compute the weighted average of all
the token vectors in the hypothesis to obtain representation of the hypothesis that is softly
aligned with the token indexed by ùëñin the premise:
ùú∑ùëñ=ùëõ√ï
ùëó=1exp¬πùëíùëñùëó¬∫√çùëõ
ùëò=1exp¬πùëíùëñùëò¬∫bùëó. (16.5.2)
Likewise, we compute soft alignment of premise tokens for each token indexed by ùëóin the
766 Natural Language Processing: Applications
hypothesis:
ùú∂ùëó=ùëö√ï
ùëñ=1exp¬πùëíùëñùëó¬∫√çùëö
ùëò=1exp¬πùëíùëòùëó¬∫aùëñ. (16.5.3)
Belowwedefinethe Attendclasstocomputethesoftalignmentofhypotheses( beta)with
input premises Aand soft alignment of premises ( alpha) with input hypotheses B.
class Attend (nn.Module):
def __init__ (self , num_inputs, num_hiddens, **kwargs):
super (Attend, self ).__init__ (**kwargs)
self .f=mlp(num_inputs, num_hiddens, flatten =False )
def forward (self , A, B):
# Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,
# `embed_size`)
# Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,
# `num_hiddens`)
f_A =self .f(A)
f_B =self .f(B)
# Shape of `e`: (`batch_size`, no. of tokens in sequence A,
# no. of tokens in sequence B)
e=torch .bmm(f_A, f_B .permute( 0,2,1))
# Shape of `beta`: (`batch_size`, no. of tokens in sequence A,
# `embed_size`), where sequence B is softly aligned with each token
# (axis 1 of `beta`) in sequence A
beta =torch .bmm(F .softmax(e, dim =-1), B)
# Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,
# `embed_size`), where sequence A is softly aligned with each token
# (axis 1 of `alpha`) in sequence B
alpha =torch .bmm(F .softmax(e .permute( 0,2,1), dim =-1), A)
return beta, alpha
Comparing
In the next step, we compare a token in one sequence with the other sequence that is softly
aligned with that token. Note that in soft alignment, all the tokens from one sequence,
though with probably different attention weights, will be compared with a token in the
other sequence. For easy of demonstration, Fig. 16.5.2 pairs tokens with aligned tokens
in ahardway. For example, suppose that the attending step determines that ‚Äúneed‚Äù and
‚Äúsleep‚Äù in the premise are both aligned with ‚Äútired‚Äù in the hypothesis, the pair ‚Äútired‚Äìneed
sleep‚Äù will be compared.
In the comparing step, we feed the concatenation (operator ¬ª,¬º) of tokens from one se-
quence and aligned tokens from the other sequence into a function ùëî(an MLP):
vùê¥,ùëñ=ùëî¬π¬ªaùëñ,ùú∑ùëñ¬º¬∫,ùëñ=1,...,ùëö
vùêµ,ùëó=ùëî¬π¬ªbùëó,ùú∂ùëó¬º¬∫,ùëó=1,...,ùëõ.(16.5.4)
In(16.5.4 ),vùê¥,ùëñis the comparison between token ùëñin the premise and all the hypothesis
tokensthataresoftlyalignedwithtoken ùëñ; while vùêµ,ùëóisthecomparisonbetweentoken ùëóin
767 Natural Language Inference: Using Attention
thehypothesisandallthepremisetokensthataresoftlyalignedwithtoken ùëó. Thefollowing
Compare class defines such as comparing step.
class Compare (nn.Module):
def __init__ (self , num_inputs, num_hiddens, **kwargs):
super (Compare, self ).__init__ (**kwargs)
self .g=mlp(num_inputs, num_hiddens, flatten =False )
def forward (self , A, B, beta, alpha):
V_A =self .g(torch .cat([A, beta], dim =2))
V_B =self .g(torch .cat([B, alpha], dim =2))
return V_A, V_B
Aggregating
With two sets of comparison vectors vùê¥,ùëñ(ùëñ=1,...,ùëö) andvùêµ,ùëó(ùëó=1,...,ùëõ) on hand,
in the last step we will aggregate such information to infer the logical relationship. We
begin by summing up both sets:
vùê¥=ùëö√ï
ùëñ=1vùê¥,ùëñ,vùêµ=ùëõ√ï
ùëó=1vùêµ,ùëó. (16.5.5)
Next we feed the concatenation of both summarization results into function ‚Ñé(an MLP) to
obtain the classification result of the logical relationship:
ÀÜy=‚Ñé¬π¬ªvùê¥,vùêµ¬º¬∫. (16.5.6)
The aggregation step is defined in the following Aggregate class.
class Aggregate (nn.Module):
def __init__ (self , num_inputs, num_hiddens, num_outputs, **kwargs):
super (Aggregate, self ).__init__ (**kwargs)
self .h=mlp(num_inputs, num_hiddens, flatten =True )
self .linear =nn.Linear(num_hiddens, num_outputs)
def forward (self , V_A, V_B):
# Sum up both sets of comparison vectors
V_A =V_A.sum(dim =1)
V_B =V_B.sum(dim =1)
# Feed the concatenation of both summarization results into an MLP
Y_hat =self .linear( self .h(torch .cat([V_A, V_B], dim =1)))
return Y_hat
PuttingIt All Together
By putting the attending, comparing, and aggregating steps together, we define the decom-
posable attention model to jointly train these three steps.
768 Natural Language Processing: Applications
class DecomposableAttention (nn.Module):
def __init__ (self , vocab, embed_size, num_hiddens, num_inputs_attend =100,
num_inputs_compare =200, num_inputs_agg =400,**kwargs):
super (DecomposableAttention, self ).__init__ (**kwargs)
self .embedding =nn.Embedding( len(vocab), embed_size)
self .attend =Attend(num_inputs_attend, num_hiddens)
self .compare =Compare(num_inputs_compare, num_hiddens)
# There are 3 possible outputs: entailment, contradiction, and neutral
self .aggregate =Aggregate(num_inputs_agg, num_hiddens, num_outputs =3)
def forward (self , X):
premises, hypotheses =X
A=self .embedding(premises)
B=self .embedding(hypotheses)
beta, alpha =self .attend(A, B)
V_A, V_B =self .compare(A, B, beta, alpha)
Y_hat =self .aggregate(V_A, V_B)
return Y_hat
16.5.2Trainingand Evaluatingthe Model
Now we will train and evaluate the defined decomposable attention model on the SNLI
dataset. We begin by reading the dataset.
Readingthe dataset
We download and read the SNLI dataset using the function defined in Section 16.4 . The
batch size and sequence length are set to 256and50, respectively.
batch_size, num_steps =256,50
train_iter, test_iter, vocab =d2l.load_data_snli(batch_size, num_steps)
Downloading ../data /snli_1 .0.zip from https ://nlp.stanford .edu/projects /snli /
‚Ü©!snli_1 .0.zip...
read 549367 examples
read 9824 examples
Creatingthe Model
We use the pretrained 100-dimensional GloVe embedding to represent the input tokens.
Thus, we predefine the dimension of vectors aùëñandbùëóin(16.5.1 )as 100. The output
dimension of functions ùëìin(16.5.1 )andùëîin(16.5.4 )is set to 200. Then we create a
modelinstance,initializeitsparameters,andloadtheGloVeembeddingtoinitializevectors
of input tokens.
embed_size, num_hiddens, devices =100,200, d2l .try_all_gpus()
net =DecomposableAttention(vocab, embed_size, num_hiddens)
(continues on next page)
769 Natural Language Inference: Using Attention
(continued from previous page)
glove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')
embeds =glove_embedding[vocab .idx_to_token]
net.embedding .weight .data .copy_(embeds);
Downloading ../data /glove .6B.100 d.zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/glove .6B.100 d.zip...
Trainingand Evaluatingthe Model
Incontrasttothe split_batch functionin Section13.5 thattakessingleinputssuchastext
sequences(orimages), wedefinea split_batch_multi_inputs functiontotakemultiple
inputs such as premises and hypotheses in minibatches.
Now we can train and evaluate the model on the SNLI dataset.
lr, num_epochs =0.001 ,4
trainer =torch .optim .Adam(net .parameters(), lr =lr)
loss =nn.CrossEntropyLoss(reduction ="none ")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.496 , train acc 0.805 , test acc 0.828
20383.2 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
Using the Model
Finally, define the prediction function to output the logical relationship between a pair of
premise and hypothesis.
#@save
def predict_snli (net, vocab, premise, hypothesis):
"""Predict the logical relationship between the premise and hypothesis."""
net.eval()
premise =torch .tensor(vocab[premise], device =d2l.try_gpu())
(continues on next page)
770 Natural Language Processing: Applications
247(continued from previous page)
hypothesis =torch .tensor(vocab[hypothesis], device =d2l.try_gpu())
label =torch .argmax(net([premise .reshape(( 1,-1)),
hypothesis .reshape(( 1,-1))]), dim =1)
return 'entailment 'iflabel ==0else 'contradiction 'iflabel ==1\
else 'neutral '
We can use the trained model to obtain the natural language inference result for a sample
pair of sentences.
predict_snli(net, vocab, [ 'he','is','good ','.'], [ 'he','is','bad','.'])
'contradiction '
16.5.3Summary
The decomposable attention model consists of three steps for predicting the logical rela-
tionships between premises and hypotheses: attending, comparing, and aggregating.
With attention mechanisms, we can align tokens in one text sequence to every token in
theother,andviceversa. Suchalignmentissoftusingweightedaverage,whereideally
large weights are associated with the tokens to be aligned.
Thedecompositiontrickleadstoamoredesirablelinearcomplexitythanquadraticcom-
plexity when computing attention weights.
We can use pretrained word vectors as the input representation for downstream natural
language processing task such as natural language inference.
16.5.4Exercises
1.Train the model with other combinations of hyperparameters. Can you get better accu-
racy on the test set?
2.What are major drawbacks of the decomposable attention model for natural language
inference?
3.Suppose that we want to get the level of semantical similarity (e.g., a continuous value
between 0 and 1) for any pair of sentences. How shall we collect and label the dataset?
Can you design a model with attention mechanisms?
Discussions247.
771 Fine-Tuning BERT for Sequence-Level and Token-Level Applications
16.6Fine-TuningBERTforSequence-Leveland
Token-LevelApplications
In the previous sections of this chapter, we have designed different models for natural lan-
guageprocessingapplications,suchasbasedonRNNs,CNNs,attention,andMLPs. These
modelsarehelpfulwhenthereisspaceortimeconstraint,however,craftingaspecificmodel
for every natural language processing task is practically infeasible. In Section 15.8 , we in-
troduced a pretraining model, BERT, that requires minimal architecture changes fora wide
range of natural language processing tasks. On the one hand, at the time of its proposal,
BERT improved the state of the art on various natural language processing tasks. On the
other hand, as noted in Section 15.10 , the two versions of the original BERT model come
with110millionand340millionparameters. Thus,whentherearesufficientcomputational
resources,wemayconsiderfine-tuningBERTfordownstreamnaturallanguageprocessing
applications.
In the following, we generalize a subset of natural language processing applications as
sequence-level and token-level. On the sequence level, we introduce how to transform the
BERT representation of the text input to the output label in single text classification and
text pair classification or regression. On the token level, we will briefly introduce new ap-
plications such as text tagging and question answering and shed light on how BERT can
representtheirinputsandgettransformedintooutputlabels. Duringfine-tuning,the‚Äúmini-
malarchitecturechanges‚ÄùrequiredbyBERTacrossdifferentapplicationsaretheextrafully
connected layers. During supervised learning of a downstream application, parameters of
the extra layers are learned from scratch while all the parameters in the pretrained BERT
model are fine-tuned.
16.6.1SingleTextClassification
Single text classification takes a single text sequence as input and outputs its classification
result. Besides sentiment analysis that we have studied in this chapter, the Corpus of Lin-
guisticAcceptability(CoLA)isalsoadatasetforsingletextclassification,judgingwhether
a given sentence is grammatically acceptable or not ( Warstadt et al., 2019). For instance,
‚ÄúI should study.‚Äù is acceptable but ‚ÄúI should studying.‚Äù is not.
Section15.8 describestheinputrepresentationofBERT.TheBERTinputsequenceunam-
biguously represents both single text and text pairs, where the special classification token
‚Äú<cls>‚Äù is used for sequence classification and the special classification token ‚Äú<sep>‚Äù
marks the end of single text or separates a pair of text. As shown in Fig. 16.6.1 , in single
text classification applications, the BERT representation of the special classification token
‚Äú<cls>‚Äù encodes the information of the entire input text sequence. As the representation of
the input single text, it will be fed into a small MLP consisting of fully connected (dense)
layers to output the distribution of all the discrete label values.
16.6.2TextPairClassification or Regression
772 Natural Language Processing: Applications
tFig. 16.6.1 Fine-tuning BERT for single text classiÔ¨Åcation applications, such as sentiment analysis
and testing linguistic acceptability. Suppose that the input single text has six tokens.
We have also examined natural language inference in this chapter. It belongs to text pair
classification , a type of application classifying a pair of text.
Taking a pair of text as input but outputting a continuous value, semantictextualsimilarity
is a popular textpairregression task. This task measures semantic similarity of sentences.
For instance, in the Semantic Textual Similarity Benchmark dataset, the similarity score of
a pair of sentences is an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning
equivalence) ( Ceret al., 2017). The goal is to predict these scores. Examples from the
Semantic Textual Similarity Benchmark dataset include (sentence 1, sentence 2, similarity
score):
‚ÄúA plane is taking off.‚Äù, ‚ÄúAn air plane is taking off.‚Äù, 5.000;
‚ÄúA woman is eating something.‚Äù, ‚ÄúA woman is eating meat.‚Äù, 3.000;
‚ÄúA woman is dancing.‚Äù, ‚ÄúA man is talking.‚Äù, 0.000.
tFig. 16.6.2 Fine-tuning BERT for text pair classiÔ¨Åcation or regression applications, such as natural
language inference and semantic textual similarity. Suppose that the input text pair has
two and three tokens.
Comparing with single text classification in Fig. 16.6.1 , fine-tuning BERT for text pair
classification in Fig. 16.6.2 is different in the input representation. For text pair regression
tasks such as semantic textual similarity, trivial changes can be applied such as outputting
773 Fine-Tuning BERT for Sequence-Level and Token-Level Applications
a continuous label value and using the mean squared loss: they are common for regres-
sion.
16.6.3TextTagging
Now let‚Äôs consider token-level tasks, such as text tagging , where each token is assigned a
label. Amongtexttaggingtasks, part-of-speechtagging assignseachwordapart-of-speech
tag (e.g., adjective and determiner) according to the role of the word in the sentence. For
example, according to the Penn Treebank II tag set, the sentence ‚ÄúJohn Smith ‚Äôs car is
new‚Äù should be tagged as ‚ÄúNNP (noun, proper singular) NNP POS (possessive ending)
NN (noun, singular or mass) VB (verb, base form) JJ (adjective)‚Äù.
tFig. 16.6.3 Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose
that the input single text has six tokens.
Fine-tuning BERT for text tagging applications is illustrated in Fig. 16.6.3 . Comparing
withFig. 16.6.1 , the only distinction lies in that in text tagging, the BERT representation
ofevery token of the input text is fed into the same extra fully connected layers to output
the label of the token, such as a part-of-speech tag.
16.6.4QuestionAnswering
Asanothertoken-levelapplication, questionanswering reflectscapabilitiesofreadingcom-
prehension. Forexample,theStanfordQuestionAnsweringDataset(SQuADv1.1)consists
of reading passages and questions, where the answer to every question is just a segment of
text (text span) from the passage that the question is about ( Rajpurkar et al., 2016). To
explain, consider a passage ‚ÄúSome experts report that a mask‚Äôs efficacy is inconclusive.
However, mask makers insist that their products, such as N95 respirator masks, can guard
against the virus.‚Äù and a question ‚ÄúWho say that N95 respirator masks can guard against
the virus?‚Äù. The answer should be the text span ‚Äúmask makers‚Äù in the passage. Thus, the
goal in SQuAD v1.1 is to predict the start and end of the text span in the passage given a
pair of question and passage.
To fine-tune BERT for question answering, the question and passage are packed as the first
and second text sequence, respectively, in the input of BERT. To predict the position of the
start of the text span, the same additional fully connected layer will transform the BERT
774 Natural Language Processing: Applications
tFig. 16.6.4 Fine-tuning BERT for question answering. Suppose that the input text pair has two and
three tokens.
representationofanytokenfromthepassageofposition ùëñintoascalarscore ùë†ùëñ. Suchscores
ofallthepassagetokensarefurthertransformedbythesoftmaxoperationintoaprobability
distribution,sothateachtokenposition ùëñinthepassageisassignedaprobability ùëùùëñofbeing
thestartofthetextspan. Predictingtheendofthetextspanisthesameasabove,exceptthat
parametersinitsadditionalfullyconnectedlayerareindependentfromthoseforpredicting
the start. When predicting the end, any passage token of position ùëñis transformed by the
same fully connected layer into a scalar score ùëíùëñ.Fig. 16.6.4 depicts fine-tuning BERT for
question answering.
Forquestionanswering,thesupervisedlearning‚Äôstrainingobjectiveisasstraightforwardas
maximizing the log-likelihoods of the ground-truth start and end positions. When predict-
ing the span, we can compute the score ùë†ùëñ¬∏ùëíùëófor a valid span from position ùëñto position
ùëó(ùëñùëó), and output the span with the highest score.
16.6.5Summary
BERTrequiresminimalarchitecturechanges(extrafullyconnectedlayers)forsequence-
levelandtoken-levelnaturallanguageprocessingapplications,suchassingletextclas-
sification(e.g.,sentimentanalysisandtestinglinguisticacceptability),textpairclassi-
ficationorregression(e.g.,naturallanguageinferenceandsemantictextualsimilarity),
text tagging (e.g., part-of-speech tagging), and question answering.
During supervised learning of a downstream application, parameters of the extra layers
are learned from scratch while all the parameters in the pretrained BERT model are
fine-tuned.
16.6.6Exercises
1.Let‚Äôs design a search engine algorithm for news articles. When the system receives an
query (e.g., ‚Äúoil industry during the coronavirus outbreak‚Äù), it should return a ranked
listofnewsarticlesthataremostrelevanttothequery. Supposethatwehaveahugepool
of news articles and a large number of queries. To simplify the problem, suppose that
the most relevant article has been labeled for each query. How can we apply negative
sampling (see Section 15.2.1 ) and BERT in the algorithm design?
775 Natural Language Inference: Fine-Tuning BERT
2482.How can we leverage BERT in training language models?
3.Can we leverage BERT in machine translation?
Discussions248.
16.7NaturalLanguageInference: Fine-Tuning
BERT
In earlier sections of this chapter, we have designed an attention-based architecture (in
Section 16.5 ) for the natural language inference task on the SNLI dataset (as described
inSection 16.4 ). Now we revisit this task by fine-tuning BERT. As discussed in Section
16.6, natural language inference is a sequence-level text pair classification problem, and
fine-tuningBERTonlyrequiresanadditionalMLP-basedarchitecture,asillustratedin Fig.
16.7.1.
tFig. 16.7.1 This section feeds pretrained BERT to an MLP-based architecture for natural language
inference.
In this section, we will download a pretrained small version of BERT, then fine-tune it for
natural language inference on the SNLI dataset.
import json
import multiprocessing
import os
import torch
from torch import nn
from d2l import torch asd2l
16.7.1LoadingPretrainedBERT
We have explained how to pretrain BERT on the WikiText-2 dataset in Section 15.9 and
Section 15.10 (note that the original BERT model is pretrained on much bigger corpora).
Asdiscussedin Section15.10 ,theoriginalBERTmodelhashundredsofmillionsofparam-
eters. In the following, we provide two versions of pretrained BERT: ‚Äúbert.base‚Äù is about
776 Natural Language Processing: Applications
as big as the original BERT base model that requires a lot of computational resources to
fine-tune, while ‚Äúbert.small‚Äù is a small version to facilitate demonstration.
d2l.DATA_HUB[ 'bert.base ']=(d2l .DATA_URL +'bert.base.torch.zip ',
'225d66f04cae318b841a13d32af3acc165f253ac ')
d2l.DATA_HUB[ 'bert.small ']=(d2l .DATA_URL +'bert.small.torch.zip ',
'c72329e68a732bef0452e4b96a1c341c8910f81f ')
Either pretrained BERT model contains a ‚Äúvocab.json‚Äù file that defines the vocabulary set
and a ‚Äúpretrained.params‚Äù file of the pretrained parameters. We implement the following
load_pretrained_model function to load pretrained BERT parameters.
def load_pretrained_model (pretrained_model, num_hiddens, ffn_num_hiddens,
num_heads, num_blks, dropout, max_len, devices):
data_dir =d2l.download_extract(pretrained_model)
# Define an empty vocabulary to load the predefined vocabulary
vocab =d2l.Vocab()
vocab .idx_to_token =json .load( open (os.path .join(data_dir, 'vocab.json ')))
vocab .token_to_idx ={token: idx for idx, token inenumerate (
vocab .idx_to_token)}
bert =d2l.BERTModel(
len(vocab), num_hiddens, ffn_num_hiddens =ffn_num_hiddens, num_heads =4,
num_blks =2, dropout =0.2, max_len =max_len)
# Load pretrained BERT parameters
bert .load_state_dict(torch .load(os .path .join(data_dir,
'pretrained.params ')))
return bert, vocab
To facilitate demonstration on most of machines, we will load and fine-tune the small ver-
sion (‚Äúbert.small‚Äù) of the pretrained BERT in this section. In the exercise, we will show
how to fine-tune the much larger ‚Äúbert.base‚Äù to significantly improve the testing accu-
racy.
devices =d2l.try_all_gpus()
bert, vocab =load_pretrained_model(
'bert.small ', num_hiddens =256, ffn_num_hiddens =512, num_heads =4,
num_blks =2, dropout =0.1, max_len =512, devices =devices)
Downloading ../data /bert .small .torch .zip from http ://d2l-data .s3-accelerate .
‚Ü©!amazonaws .com/bert .small .torch .zip...
16.7.2TheDatasetforFine-TuningBERT
For the downstream task natural language inference on the SNLI dataset, we define a cus-
tomized dataset class SNLIBERTDataset . In each example, the premise and hypothesis
form a pair of text sequence and is packed into one BERT input sequence as depicted in
Fig. 16.6.2 . Recall Section 15.8.4 that segment IDs are used to distinguish the premise
and the hypothesis in a BERT input sequence. With the predefined maximum length of a
BERT input sequence ( max_len ), the last token of the longer of the input text pair keeps
777 Natural Language Inference: Fine-Tuning BERT
getting removed until max_len is met. To accelerate generation of the SNLI dataset for
fine-tuning BERT, we use 4 worker processes to generate training or testing examples in
parallel.
class SNLIBERTDataset (torch .utils .data .Dataset):
def __init__ (self , dataset, max_len, vocab =None ):
all_premise_hypothesis_tokens =[[
p_tokens, h_tokens] for p_tokens, h_tokens inzip(
*[d2l .tokenize([s .lower() for sinsentences])
for sentences indataset[: 2]])]
self .labels =torch .tensor(dataset[ 2])
self .vocab =vocab
self .max_len =max_len
(self .all_token_ids, self .all_segments,
self .valid_lens) =self ._preprocess(all_premise_hypothesis_tokens)
print ('read '+str(len(self .all_token_ids)) +'examples ')
def _preprocess (self , all_premise_hypothesis_tokens):
pool =multiprocessing .Pool( 4)# Use 4 worker processes
out =pool .map( self ._mp_worker, all_premise_hypothesis_tokens)
all_token_ids =[
token_ids for token_ids, segments, valid_len inout]
all_segments =[segments for token_ids, segments, valid_len inout]
valid_lens =[valid_len for token_ids, segments, valid_len inout]
return (torch .tensor(all_token_ids, dtype =torch .long),
torch .tensor(all_segments, dtype =torch .long),
torch .tensor(valid_lens))
def _mp_worker (self , premise_hypothesis_tokens):
p_tokens, h_tokens =premise_hypothesis_tokens
self ._truncate_pair_of_tokens(p_tokens, h_tokens)
tokens, segments =d2l.get_tokens_and_segments(p_tokens, h_tokens)
token_ids =self .vocab[tokens] +[self .vocab[ '<pad> ']] \
*(self .max_len -len(tokens))
segments =segments +[0]*(self .max_len -len(segments))
valid_len =len(tokens)
return token_ids, segments, valid_len
def _truncate_pair_of_tokens (self , p_tokens, h_tokens):
# Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT
# input
while len(p_tokens) +len(h_tokens) >self .max_len -3:
iflen(p_tokens) >len(h_tokens):
p_tokens .pop()
else :
h_tokens .pop()
def __getitem__ (self , idx):
return (self .all_token_ids[idx], self .all_segments[idx],
self .valid_lens[idx]), self .labels[idx]
def __len__ (self ):
return len(self .all_token_ids)
After downloading the SNLI dataset, we generate training and testing examples by instan-
778 Natural Language Processing: Applications
tiating the SNLIBERTDataset class. Such examples will be read in minibatches during
training and testing of natural language inference.
# Reduce `batch_size` if there is an out of memory error. In the original BERT
# model, `max_len` = 512
batch_size, max_len, num_workers =512,128, d2l .get_dataloader_workers()
data_dir =d2l.download_extract( 'SNLI ')
train_set =SNLIBERTDataset(d2l .read_snli(data_dir, True ), max_len, vocab)
test_set =SNLIBERTDataset(d2l .read_snli(data_dir, False ), max_len, vocab)
train_iter =torch .utils .data .DataLoader(train_set, batch_size, shuffle =True ,
num_workers =num_workers)
test_iter =torch .utils .data .DataLoader(test_set, batch_size,
num_workers =num_workers)
read 549367 examples
read 9824 examples
16.7.3Fine-TuningBERT
AsFig. 16.6.2 indicates, fine-tuning BERT for natural language inference requires only an
extra MLP consisting of two fully connected layers (see self.hidden andself.output
in the following BERTClassifier class). This MLP transforms the BERT representation
of the special ‚Äú<cls>‚Äù token, which encodes the information of both the premise and the
hypothesis, intothreeoutputsofnaturallanguageinference: entailment, contradiction, and
neutral.
class BERTClassifier (nn.Module):
def __init__ (self , bert):
super (BERTClassifier, self ).__init__ ()
self .encoder =bert .encoder
self .hidden =bert .hidden
self .output =nn.LazyLinear( 3)
def forward (self , inputs):
tokens_X, segments_X, valid_lens_x =inputs
encoded_X =self .encoder(tokens_X, segments_X, valid_lens_x)
return self .output( self .hidden(encoded_X[:, 0, :]))
Inthefollowing,thepretrainedBERTmodel bertisfedintothe BERTClassifier instance
netfor the downstream application. In common implementations of BERT fine-tuning,
onlytheparametersoftheoutputlayeroftheadditionalMLP( net.output )willbelearned
from scratch. All the parameters of the pretrained BERT encoder ( net.encoder ) and the
hidden layer of the additional MLP ( net.hidden ) will be fine-tuned.
net =BERTClassifier(bert)
Recall that in Section 15.8 both the MaskLMclass and the NextSentencePred class have
parameters in their employed MLPs. These parameters are part of those in the pretrained
BERT model bert, and thus part of parameters in net. However, such parameters are
779 Natural Language Inference: Fine-Tuning BERT
only for computing the masked language modeling loss and the next sentence prediction
loss during pretraining. These two loss functions are irrelevant to fine-tuning downstream
applications,thustheparametersoftheemployedMLPsin MaskLMandNextSentencePred
are not updated (staled) when BERT is fine-tuned.
To allow parameters with stale gradients, the flag ignore_stale_grad=True is set in the
stepfunction of d2l.train_batch_ch13 . We use this function to train and evaluate the
model netusing the training set ( train_iter ) and the testing set ( test_iter ) of SNLI.
Duetothelimitedcomputationalresources,thetrainingandtestingaccuracycanbefurther
improved: we leave its discussions in the exercises.
lr, num_epochs =1e-4 ,5
trainer =torch .optim .Adam(net .parameters(), lr =lr)
loss =nn.CrossEntropyLoss(reduction ='none ')
net( next (iter (train_iter))[ 0])
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.520 , train acc 0.791 , test acc 0.786
10588.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',‚ê£
‚Ü©!index =1)]
16.7.4Summary
We can fine-tune the pretrained BERT model for downstream applications, such as nat-
ural language inference on the SNLI dataset.
During fine-tuning, the BERT model becomes part of the model for the downstream
application. Parameters that are only related to pretraining loss will not be updated
during fine-tuning.
16.7.5Exercises
1.Fine-tuneamuchlargerpretrainedBERTmodelthatisaboutasbigastheoriginalBERT
basemodelifyourcomputationalresourceallows. Setargumentsinthe load_pretrained_model
functionas: replacing‚Äòbert.small‚Äôwith‚Äòbert.base‚Äô,increasingvaluesof num_hiddens=256 ,
ffn_num_hiddens=512 ,num_heads=4 , and num_blks=2 to 768, 3072, 12, and 12, re-
780 Natural Language Processing: Applications
249spectively. By increasing fine-tuning epochs (and possibly tuning other hyperparame-
ters), can you get a testing accuracy higher than 0.86?
2.How to truncate a pair of sequences according to their ratio of length? Compare this
pair truncation method and the one used in the SNLIBERTDataset class. What are their
pros and cons?
Discussions249.
250
17 Reinforcement Learning
Pratik Chaudhari (University of Pennsylvania and Amazon ),Rasool Fakoor (Amazon),
andKavoshAsadi (Amazon)
ReinforcementLearning(RL)isasuiteoftechniquesthatallowsustobuildmachinelearn-
ingsystemsthattakedecisionssequentially. Forexample,apackagecontainingnewclothes
that you purchased from an online retailer arrives at your doorstep after a sequence of de-
cisions, e.g., the retailer finding the clothes in the warehouse closest to your house, putting
the clothes in a box, transporting the box via land or by air, and delivering it to your house
within the city. There are many variables that affect the delivery of the package along the
way, e.g., whether or not the clothes were available in the warehouse, how long it took to
transport the box, whether it arrived in your city before the daily delivery truck left, etc.
The key idea is that at each stage these variables that we do not often control affect the
entire sequence of events in the future, e.g., if there were delays in packing the box in the
warehouse the retailer may need to send the package via air instead of ground to ensure a
timely delivery. Reinforcement Learning methods allow us to take the appropriate action
at each stage of a sequential decision making problem in order to maximize some utility
eventually, e.g., the timely delivery of the package to you.
Such sequential decision making problems are seen in numerous other places, e.g., while
playingGo250yourcurrent move determines the next moves and the opponent‚Äôs movesare
thevariablesthatyoucannotcontrol‚Ä¶asequenceofmoveseventuallydetermineswhether
ornotyouwin; themoviesthatNetflixrecommendstoyounowdeterminewhatyouwatch,
whether you like the movie or not is unknown to Netflix, eventually a sequence of movie
recommendations determines how satisfied you are with Netflix. Reinforcement learning
is being used today to develop effective solutions to these problems ( Mnihet al., 2013,
Silveretal., 2016). The key distinction between reinforcement learning and standard deep
learningisthatinstandarddeeplearningthepredictionofatrainedmodelononetestdatum
does not affect the predictions on a future test datum; in reinforcement learning decisions
at future instants (in RL, decisions are also called actions) are affected by what decisions
were made in the past.
In this chapter, we will develop the fundamentals of reinforcement learning and obtain
hands-on experience in implementing some popular reinforcement learning methods. We
will first develop a concept called a Markov Decision Process (MDP) which allows us to
think of such sequential decision making problems. An algorithm called Value Iteration
will be our first insight into solving reinforcement learning problems under the assumption
that we know how the uncontrolled variables in an MDP (in RL, these controlled variables
781
782 Reinforcement Learning
are called the environment) typically behave. Using the more general version of Value
Iteration, an algorithm called Q-Learning, we will be able to take appropriate actions even
when we do not necessarily have full knowledge of the environment. We will then study
how to use deep networks for reinforcement learning problems by imitating the actions of
an expert. And finally, we will develop a reinforcement learning method that uses a deep
networktotakeactionsinunknownenvironments. Thesetechniquesformthebasisofmore
advanced RL algorithms that are used today in a variety of real-world applications, some
of which we will point to in the chapter.
tFig. 17.1 Reinforcement Learning Structure
17.1MarkovDecision Process(MDP)
In this section, we will discuss how to formulate reinforcement learning problems using
Markov decision processes (MDPs) and describe various components of MDPs in de-
tail.
17.1.1Definition of an MDP
AMarkovdecisionprocess(MDP)( Bellman,1957 )isamodelforhowthestateofasystem
evolves as different actions are applied to the system. A few different quantities come
together to form an MDP.
LetSbe the set of states in the MDP. As a concrete example see Fig. 17.1.1 , for a robot
that is navigating a gridworld. In this case, Scorresponds to the set of locations that
the robot can be at any given timestep.
LetAbe the set of actions that the robot can take at each state, e.g., ‚Äúgo forward‚Äù, ‚Äúturn
783 Markov Decision Process (MDP)
tFig. 17.1.1 A simple gridworld navigation task where the robot not only has to Ô¨Ånd its way to the goal
location (shown as a green house) but also has to avoid trap locations (shown as red cross
signs).
right‚Äù, ‚Äúturn left‚Äù, ‚Äústay at the same location‚Äù, etc. Actions can change the current
state of the robot to some other state within the set S.
It may happen that we do not know how the robot moves exactlybut only know it up to
some approximation. We model this situation in reinforcement learning as follows: if
the robot takes an action ‚Äúgo forward‚Äù, there might be a small probability that it stays
at the current state, another small probability that it ‚Äúturns left‚Äù, etc. Mathematically,
this amounts to defining a ‚Äútransition function‚Äù ùëá:SAS!¬ª 0,1¬ºsuch that
ùëá¬πùë†,ùëé,ùë†0¬∫=ùëÉ¬πùë†0jùë†,ùëé¬∫using the conditional probability of reaching a state ùë†0given
thattherobotwasatstate ùë†andtookanaction ùëé. Thetransitionfunctionisaprobability
distribution and we therefore have√ç
ùë†02Sùëá¬πùë†,ùëé,ùë†0¬∫=1for allùë†2Sandùëé2A, i.e.,
the robot has to go to some state if it takes an action.
We now construct a notion of which actions are useful and which ones are not using the
concept of a ‚Äúreward‚Äù ùëü:SA! R. We say that the robot gets a reward ùëü¬πùë†,ùëé¬∫
if the robot takes an action ùëéat stateùë†. If the reward ùëü¬πùë†,ùëé¬∫is large, this indicates
that taking the action ùëéat stateùë†is more useful to achieving the goal of the robot, i.e.,
going to the green house. If the reward ùëü¬πùë†,ùëé¬∫is small, then action ùëéis less useful to
achieving this goal. It is important to note that the reward is designed by the user (the
person who creates the reinforcement learning algorithm) with the goal in mind.
17.1.2Returnand Discount Factor
The different components above together form a Markov decision process (MDP)
MDP :¬πS,A,ùëá,ùëü¬∫. (17.1.1)
784 Reinforcement Learning
Let‚Äôs now consider the situation when the robot starts at a particular state ùë†02 Sand
continues taking actions to result in a trajectory
ùúè=¬πùë†0,ùëé0,ùëü0,ùë†1,ùëé1,ùëü1,ùë†2,ùëé2,ùëü2,...¬∫. (17.1.2)
At each time step ùë°the robot is at a state ùë†ùë°and takes an action ùëéùë°which results in a reward
ùëüùë°=ùëü¬πùë†ùë°,ùëéùë°¬∫. Thereturnof a trajectory is the total reward obtained by the robot along
such a trajectory
ùëÖ¬πùúè¬∫=ùëü0¬∏ùëü1¬∏ùëü2¬∏. (17.1.3)
The goal in reinforcement learning is to find a trajectory that has the largest return.
Think of the situation when the robot continues to travel in the gridworld without ever
reaching the goal location. The sequence of states and actions in a trajectory can be in-
finitelylonginthiscaseandthe returnofanysuchinfinitelylongtrajectorywillbeinfinite.
In order to keep the reinforcement learning formulation meaningful even for such trajecto-
ries, we introduce the notion of a discount factor ùõæ < 1. We write the discounted return
as
ùëÖ¬πùúè¬∫=ùëü0¬∏ùõæùëü1¬∏ùõæ2ùëü2¬∏=1√ï
ùë°=0ùõæùë°ùëüùë°. (17.1.4)
Note that if ùõæis very small, the rewards earned by the robot in the far future, say ùë°=
1000, are heavily discounted by the factor ùõæ1000. This encourages the robot to select short
trajectories that achieve its goal, namely that of going to the green house in the gridwold
example (see Fig. 17.1.1 ). For large values of the discount factor, say ùõæ=0.99, the robot
is encouraged to exploreand then find the best trajectory to go to the goal location.
17.1.3Discussionof the MarkovAssumption
Let us think of a new robot where the state ùë†ùë°is the location as above but the action ùëéùë°is
theaccelerationthattherobotappliestoitswheelsinsteadofanabstractcommandlike‚Äúgo
forward‚Äù. If this robot has some non-zero velocity at state ùë†ùë°, then the next location ùë†ùë°¬∏1is
a function of the past location ùë†ùë°, the acceleration ùëéùë°, also the velocity of the robot at time
ùë°which is proportional to ùë†ùë° ùë†ùë° 1. This indicates that we should have
ùë†ùë°¬∏1=some function¬πùë†ùë°,ùëéùë°,ùë†ùë° 1¬∫; (17.1.5)
the ‚Äúsome function‚Äù in our case would be Newton‚Äôs law of motion. This is quite different
from our transition function that simply depends upon ùë†ùë°andùëéùë°.
Markov systems are all systems where the next state ùë†ùë°¬∏1is only a function of the current
stateùë†ùë°and the action ùëéùë°taken at the current state. In Markov systems, the next state does
not depend on which actions were taken in the past or the states that the robot was at in the
past. Forexample, thenewrobotthathasaccelerationastheactionaboveisnotMarkovian
because the next location ùë†ùë°¬∏1depends upon the previous state ùë†ùë° 1through the velocity.
It may seem that Markovian nature of a system is a restrictive assumption, but it is not so.
Markov Decision Processes are still capable of modeling a very large class of real systems.
For example, for our new robot, if we chose our state ùë†ùë°to the tuple¬πlocation,velocity¬∫
785 Value Iteration
251
252
253then the system is Markovian because its next state ¬πlocationùë°¬∏1,velocityùë°¬∏1¬∫depends only
upon the current state ¬πlocationùë°,velocityùë°¬∫and the action at the current state ùëéùë°.
17.1.4Summary
The reinforcement learning problem is typically modeled using Markov Decision Pro-
cesses. AMarkovdecisionprocess(MDP)isdefinedbyatupleoffourentities ¬πS,A,ùëá,ùëü¬∫
whereSis the state space,Ais the action space, ùëáis the transition function that encodes
the transition probabilities of the MDP and ùëüis the immediate reward obtained by taking
action at a particular state.
17.1.5Exercises
1.Suppose that we want to design an MDP to model MountainCar251problem.
1.What would be the set of states?
2.What would be the set of actions?
3.What would be the possible reward functions?
2.How would you design an MDP for an Atari game like Pong game252?
Discussions253.
17.2ValueIteration
In this section we will discuss how to pick the best action for the robot at each state to
maximize the returnof the trajectory. We will describe an algorithm called Value Iteration
and implement it for a simulated robot that travels over a frozen lake.
17.2.1StochasticPolicy
A stochastic policy denoted as ùúã¬πùëéjùë†¬∫(policy for short) is a conditional distribution over
the actionsùëé2Agiven the state ùë†2S,ùúã¬πùëéjùë†¬∫ùëÉ¬πùëéjùë†¬∫. As an example, if the
robot has four actions A={go left, go down, go right, go up}. The policy at a state
ùë†2Sfor such a set of actions Ais a categorical distribution where the probabilities of
the four actions could be ¬ª0.4,0.2,0.1,0.3¬º; at some other state ùë†02Sthe probabilities
ùúã¬πùëéjùë†0¬∫of the same four actions could be ¬ª0.1,0.1,0.2,0.6¬º. Note that we should have√ç
ùëéùúã¬πùëéjùë†¬∫=1for any state ùë†. A deterministic policy is a special case of a stochastic
policy in that the distribution ùúã¬πùëéjùë†¬∫only gives non-zero probability to one particular
action, e.g.,¬ª1,0,0,0¬ºfor our example with four actions.
To make the notation less cumbersome, we will often write ùúã¬πùë†¬∫as the conditional distri-
bution instead of ùúã¬πùëéjùë†¬∫.
786 Reinforcement Learning
17.2.2ValueFunction
Imagine now that the robot starts at a state ùë†0and at each time instant, it first samples
an action from the policy ùëéùë°ùúã¬πùë†ùë°¬∫and takes this action to result in the next state
ùë†ùë°¬∏1. The trajectory ùúè=¬πùë†0,ùëé0,ùëü0,ùë†1,ùëé1,ùëü1,...¬∫, can be different depending upon which
particular action ùëéùë°is sampled at intermediate instants. We define the average return
ùëÖ¬πùúè¬∫=√ç1
ùë°=0ùõæùë°ùëü¬πùë†ùë°,ùëéùë°¬∫of all such trajectories
ùëâùúã¬πùë†0¬∫=ùê∏ùëéùë°ùúã¬πùë†ùë°¬∫h
ùëÖ¬πùúè¬∫i
=ùê∏ùëéùë°ùúã¬πùë†ùë°¬∫h1√ï
ùë°=0ùõæùë°ùëü¬πùë†ùë°,ùëéùë°¬∫i
, (17.2.1)
whereùë†ùë°¬∏1ùëÉ¬πùë†ùë°¬∏1jùë†ùë°,ùëéùë°¬∫is the next state of the robot and ùëü¬πùë†ùë°,ùëéùë°¬∫is the instantaneous
reward obtained by taking action ùëéùë°in stateùë†ùë°at timeùë°. This is called the ‚Äúvalue function‚Äù
for the policy ùúã. In simple words, the value of a state ùë†0for a policyùúã, denoted by ùëâùúã¬πùë†0¬∫,
is the expected ùõæ-discounted returnobtained by the robot if it begins at state ùë†0and takes
actions from the policy ùúãat each time instant.
We next break down the trajectory into two stages (i) the first stage which corresponds to
ùë†0!ùë†1upon taking the action ùëé0, and (ii) a second stage which is the trajectory ùúè0=
¬πùë†1,ùëé1,ùëü1,...¬∫thereafter. The key idea behind all algorithms in reinforcement learning is
that the value of state ùë†0can be written as the average reward obtained in the first stage
and the value function averaged over all possible next states ùë†1. This is quite intuitive and
arises from our Markov assumption: the average return from the current state is the sum
of the average return from the next state and the average reward of going to the next state.
Mathematically, we write the two stages as
ùëâùúã¬πùë†0¬∫=ùëü¬πùë†0,ùëé0¬∫¬∏ùõæùê∏ùëé0ùúã¬πùë†0¬∫h
ùê∏ùë†1ùëÉ¬πùë†1jùë†0,ùëé0¬∫h
ùëâùúã¬πùë†1¬∫ii
. (17.2.2)
This decomposition is very powerful: it is the foundation of the principle of dynamic pro-
gramming upon which all reinforcement learning algorithms are based. Notice that the
second stage gets two expectations, one over the choices of the action ùëé0taken in the first
stage using the stochastic policy and another over the possible states ùë†1obtained from the
chosen action. We can write (17.2.2 )using the transition probabilities in the Markov de-
cision process (MDP) as
ùëâùúã¬πùë†¬∫=√ï
ùëé2Aùúã¬πùëéjùë†¬∫h
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâùúã¬πùë†0¬∫i
;for allùë†2S.(17.2.3)
Animportantthingtonoticehereisthattheaboveidentityholdsforallstates ùë†2Sbecause
we can think of any trajectory that begins at that state and break down the trajectory into
two stages.
17.2.3Action-ValueFunction
In implementations, it is often useful to maintain a quantity called the ‚Äúaction value‚Äù func-
tion which is a closely related quantity to the value function. This is defined to be the
averagereturnof a trajectory that begins at ùë†0but when the action of the first stage is fixed
787 Value Iteration
to beùëé0
ùëÑùúã¬πùë†0,ùëé0¬∫=ùëü¬πùë†0,ùëé0¬∫¬∏ùê∏ùëéùë°ùúã¬πùë†ùë°¬∫h1√ï
ùë°=1ùõæùë°ùëü¬πùë†ùë°,ùëéùë°¬∫i
, (17.2.4)
note that the summation inside the expectation is from ùë°=1,...,1because the reward of
the first stage is fixed in this case. We can again break down the trajectory into two parts
and write
ùëÑùúã¬πùë†,ùëé¬∫=ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫√ï
ùëé02Aùúã¬πùëé0jùë†0¬∫ùëÑùúã¬πùë†0,ùëé0¬∫;for allùë†2S,ùëé2A.
(17.2.5)
This version is the analog of (17.2.3 )for the action value function.
17.2.4OptimalStochasticPolicy
Boththevaluefunctionandtheaction-valuefunctiondependuponthepolicythattherobot
chooses. We will next think of the ‚Äúoptimal policy‚Äù that achieves the maximal average
return
ùúã=argmax
ùúãùëâùúã¬πùë†0¬∫. (17.2.6)
Of all possible stochastic policies that the robot could have taken, the optimal policy ùúã
achieves the largest average discounted returnfor trajectories starting from state ùë†0. Let us
denote the value function and the action-value function of the optimal policy as ùëâùëâùúã
andùëÑùëÑùúã.
Let us observe that for a deterministic policy where there is only one action that is possible
under the policy at any given state. This gives us
ùúã¬πùë†¬∫=argmax
ùëé2Ah
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâ¬πùë†0¬∫i
.(17.2.7)
A good mnemonic to remember this is that the optimal action at state ùë†(for a deterministic
policy) is the one that maximizes the sum of reward ùëü¬πùë†,ùëé¬∫from the first stage and the
averagereturnof the trajectories starting from the next sate ùë†0, averaged over all possible
next statesùë†0from the second stage.
17.2.5Principle of Dynamic Programming
Our developement in the previous section in (17.2.2 )or(17.2.5 )can be turned into an
algorithm to compute the optimal value function ùëâor the action-value function ùëÑ, re-
spectively. Observe that
ùëâ¬πùë†¬∫=√ï
ùëé2Aùúã¬πùëéjùë†¬∫h
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâ¬πùë†0¬∫i
;for allùë†2S.(17.2.8)
For a deterministic optimal policy ùúã, since there is only one action that can be taken at
stateùë†, we can also write
ùëâ¬πùë†¬∫=argmaxùëé2An
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâ¬πùë†0¬∫o
(17.2.9)
788 Reinforcement Learning
for all states ùë†2S. This identity is called the ‚Äúprinciple of dynamic programming‚Äù ( Bell-
man, 1952 ,Bellman, 1957 ). It was formulated by Richard Bellman in 1950s and we can
remember it as ‚Äúthe remainder of an optimal trajectory is also optimal‚Äù.
17.2.6ValueIteration
Wecanturntheprincipleofdynamicprogrammingintoanalgorithmforfindingtheoptimal
value function called value iteration. The key idea behind value iteration is to think of this
identity as a set of constraints that tie together ùëâ¬πùë†¬∫at different states ùë†2S. We initialize
the value function to some arbitrary values ùëâ0¬πùë†¬∫for all states ùë†2S. At theùëòthiteration,
the Value Iteration algorithm updates the value function as
ùëâùëò¬∏1¬πùë†¬∫=max
ùëé2An
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâùëò¬πùë†0¬∫o
;for allùë†2S.(17.2.10)
It turns out that as ùëò!1the value function estimated by the Value Iteration algorithm
converges to the optimal value function irrespective of the initialization ùëâ0,
ùëâ¬πùë†¬∫=lim
ùëò!1ùëâùëò¬πùë†¬∫;for all states ùë†2S. (17.2.11)
ThesameValueIterationalgorithmcanbeequivalentlywrittenusingtheaction-valuefunc-
tion as
ùëÑùëò¬∏1¬πùë†,ùëé¬∫=ùëü¬πùë†,ùëé¬∫¬∏ùõæmax
ùëé02A√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëÑùëò¬πùë†0,ùëé0¬∫;for allùë†2S,ùëé2A.
(17.2.12)
In this case we initialize ùëÑ0¬πùë†,ùëé¬∫to some arbitrary values for all ùë†2Sandùëé2A. Again
we haveùëÑ¬πùë†,ùëé¬∫=limùëò!1ùëÑùëò¬πùë†,ùëé¬∫for allùë†2Sandùëé2A.
17.2.7Policy Evaluation
Value Iteration enables us to compute the optimal value function, i.e., ùëâùúãof the optimal
deterministic policy ùúã. We can also use similar iterative updates to compute the value
function associated with any other, potentially stochastic, policy ùúã. We again initialize
ùëâùúã
0¬πùë†¬∫to some arbitrary values for all states ùë†2Sand at theùëòthiteration, perform the
updates
ùëâùúã
ùëò¬∏1¬πùë†¬∫=√ï
ùëé2Aùúã¬πùëéjùë†¬∫h
ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫ùëâùúã
ùëò¬πùë†0¬∫i
;for allùë†2S.(17.2.13)
This algorithm is known as policy evaluation and is useful to compute the value function
given the policy. Again, it turns out that as ùëò!1these updates converge to the correct
value function irrespective of the initialization ùëâ0,
ùëâùúã¬πùë†¬∫=lim
ùëò!1ùëâùúã
ùëò¬πùë†¬∫;for all states ùë†2S. (17.2.14)
The algorithm for computing the action-value function ùëÑùúã¬πùë†,ùëé¬∫of a policyùúãis analo-
gous.
789 Value Iteration
25417.2.8Implementation of ValueIteration
WenextshowhowtoimplementValueIterationforanavigationproblemcalledFrozenLake
fromOpen AI Gym254. We first need to setup the enviroment as shown in the following
code.
%matplotlib inline
import random
import numpy asnp
from d2l import torch asd2l
seed =0# Random number generator seed
gamma =0.95 # Discount factor
num_iters =10 # Number of iterations
random .seed(seed) # Set the random seed to ensure results can be reproduced
np.random .seed(seed)
# Now set up the environment
env_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)
In the FrozenLake environment, the robot moves on a 44grid (these are the states) with
actions that are ‚Äúup‚Äù ( "), ‚Äúdown‚Äù (!), ‚Äúleft‚Äù ( ), and ‚Äúright‚Äù (!). The environment
contains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of
which are unknown to the robot. To keep the problem simple, we assume the robot has
reliable actions, i.e. ùëÉ¬πùë†0jùë†,ùëé¬∫=1for allùë†2S,ùëé2A. If the robot reaches the goal, the
trial ends and the robot receives a reward of 1irrespective of the action; the reward at any
otherstateis 0forallactions. Theobjectiveoftherobotistolearnapolicythatreachesthe
goal location (G) from a given start location (S) (this is ùë†0) to maximize the return.
The following function implements Value Iteration, where env_info contains MDP and
environment related information and gammais the discount factor:
def value_iteration (env_info, gamma, num_iters):
env_desc =env_info[ 'desc ']# 2D array shows what each item means
prob_idx =env_info[ 'trans_prob_idx ']
nextstate_idx =env_info[ 'nextstate_idx ']
reward_idx =env_info[ 'reward_idx ']
num_states =env_info[ 'num_states ']
num_actions =env_info[ 'num_actions ']
mdp =env_info[ 'mdp']
V=np.zeros((num_iters +1, num_states))
Q=np.zeros((num_iters +1, num_states, num_actions))
pi=np.zeros((num_iters +1, num_states))
for kinrange (1, num_iters +1):
for sinrange (num_states):
for ainrange (num_actions):
# Calculate \sum_{s'} p(s'\mid s,a) [r + \gamma v_k(s')]
for pxrds inmdp[(s,a)]:
# mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]
pr=pxrds[prob_idx] # p(s'\mid s,a)
(continues on next page)
790 Reinforcement Learning
(continued from previous page)
nextstate =pxrds[nextstate_idx] # Next state
reward =pxrds[reward_idx] # Reward
Q[k,s,a] +=pr*(reward +gamma *V[k -1, nextstate])
# Record max value and max action
V[k,s] =np.max(Q[k,s,:])
pi[k,s] =np.argmax(Q[k,s,:])
d2l.show_value_function_progress(env_desc, V[: -1], pi[: -1])
value_iteration(env_info =env_info, gamma =gamma, num_iters =num_iters)
The above pictures show the policy (the arrow indicates the action) and value function (the
changeincolorshowshowthevaluefunctionchangesovertimefromtheinitialvalueshown
by dark color to the optimal value shown by light colors.). As we see, Value Iteration finds
the optimal value function after 10 iterations and the goal state (G) can be reached starting
fromanystateaslongasitisnotanHcell. Anotherinterestingaspectoftheimplementation
is that in addition to finding the optimal value function, we also automatically found the
optimal policy ùúãcorresponding to this value function.
17.2.9Summary
The main idea behind the Value Iteration algorithm is to use the principle of dynamic pro-
gramming to find the optimal average return obtained from a given state. Note that imple-
791 Q-Learning
255menting the Value Iteration algorithm requires that we know the Markov decision process
(MDP), e.g., the transition and reward functions, completely.
17.2.10Exercises
1.Try increasing the grid size to 88. Compared with 44grid, how many iterations
does it take to find the optimal value function?
2.What is the computational complexity of the Value Iteration algorithm?
3.Run the Value Iteration algorithm again with ùõæ(i.e. ‚Äúgamma‚Äù in the above code) when
it equals to 0,0.5, and 1and analyze its results.
4.How does the value of ùõæaffect the number of iterations taken by Value Iteration to
converge? What happens when ùõæ=1?
Discussions255.
17.3Q-Learning
Intheprevioussection,wediscussedtheValueIterationalgorithmwhichrequiresaccessing
the complete Markov decision process (MDP), e.g., the transition and reward functions. In
this section, we will look at Q-Learning ( Watkins and Dayan, 1992 ) which is an algorithm
tolearnthevaluefunctionwithoutnecessarilyknowingtheMDP.Thisalgorithmembodies
the central idea behind reinforcement learning: it will enable the robot to obtain its own
data.
17.3.1The Q-Learning Algorithm
Recall that value iteration for the action-value function in Value Iteration (page 785) cor-
responds to the update
ùëÑùëò¬∏1¬πùë†,ùëé¬∫=ùëü¬πùë†,ùëé¬∫¬∏ùõæ√ï
ùë†02SùëÉ¬πùë†0jùë†,ùëé¬∫max
ùëé02AùëÑùëò¬πùë†0,ùëé0¬∫;for allùë†2Sandùëé2A.
(17.3.1)
As we discussed, implementing this algorithm requires knowing the MDP, specifically the
transition function ùëÉ¬πùë†0jùë†,ùëé¬∫. The key idea behind Q-Learning is to replace the summa-
tion over all ùë†02Sin the above expression by a summation over the states visited by the
robot. This allows us to subvert the need to know the transition function.
17.3.2AnOptimizationProblemUnderlyingQ-Learning
Let us imagine that the robot uses a policy ùúãùëí¬πùëéjùë†¬∫to take actions. Just like the previous
chapter,itcollectsadatasetof ùëõtrajectoriesof ùëátimestepseachf¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ùë°=0,...,ùëá 1gùëñ=1,...,ùëõ.
Recall that value iteration is really a set of constraints that ties together the action-value
792 Reinforcement Learning
ùëÑ¬πùë†,ùëé¬∫of different states and actions to each other. We can implement an approximate
version of value iteration using the data that the robot has collected using ùúãùëías
ÀÜùëÑ=min
ùëÑ1
ùëõùëáùëõ√ï
ùëñ=1ùëá 1√ï
ùë°=0¬πùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùëü¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùõæmax
ùëé0ùëÑ¬πùë†ùëñ
ùë°¬∏1,ùëé0¬∫¬∫2
|                                                                   {z                                                                   }
def=‚Ñì¬πùëÑ¬∫.
(17.3.2)
Let us first observe the similarities and differences between this expression and value iter-
ation above. If the robot‚Äôs policy ùúãùëíwere equal to the optimal policy ùúã, and if it collected
an infinite amount of data, then this optimization problem would be identical to the opti-
mization problem underlying value iteration. But while value iteration requires us to know
ùëÉ¬πùë†0jùë†,ùëé¬∫, the optimization objective does not have this term. We have not cheated: as
the robot uses the policy ùúãùëíto take an action ùëéùëñ
ùë°at stateùë†ùëñ
ùë°, the next state ùë†ùëñ
ùë°¬∏1is a sample
drawn from the transition function. So the optimization objective also has access to the
transition function, but implicitly in terms of the data collected by the robot.
The variables of our optimization problem are ùëÑ¬πùë†,ùëé¬∫for allùë†2Sandùëé2A. We can
minimize the objective using gradient descent. For every pair ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫in our dataset, we
can write
ùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùõºrùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫‚Ñì¬πùëÑ¬∫
=¬π1 ùõº¬∫ùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùõº
ùëü¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫¬∏ùõæmax
ùëé0ùëÑ¬πùë†ùëñ
ùë°¬∏1,ùëé0¬∫
,(17.3.3)
whereùõºis the learning rate. Typically in real problems, when the robot reaches the goal
location, the trajectories end. The value of such a terminal state is zero because the robot
does not take any further actions beyond this state. We should modify our update to handle
such states as
ùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫=¬π1 ùõº¬∫ùëÑ¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫ ùõº
ùëü¬πùë†ùëñ
ùë°,ùëéùëñ
ùë°¬∫¬∏ùõæ¬π1 ‚äÆùë†ùëñ
ùë°¬∏1is terminal¬∫max
ùëé0ùëÑ¬πùë†ùëñ
ùë°¬∏1,ùëé0¬∫
.
(17.3.4)
where ‚äÆùë†ùëñ
ùë°¬∏1is terminal is an indicator variable that is one if ùë†ùëñ
ùë°¬∏1is a terminal state and zero
otherwise. The value of state-action tuples ¬πùë†,ùëé¬∫that are not a part of the dataset is set to
 1. This algorithm is known as Q-Learning.
Given the solution of these updates ÀÜùëÑ, which is an approximation of the optimal value
functionùëÑ, we can obtain the optimal deterministic policy corresponding to this value
function easily using
ÀÜùúã¬πùë†¬∫=argmaxùëéÀÜùëÑ¬πùë†,ùëé¬∫. (17.3.5)
There can be situations when there are multiple deterministic policies that correspond to
the same optimal value function; such ties can be broken arbitrarily because they have the
same value function.
17.3.3Exploration in Q-Learning
793 Q-Learning
The policy used by the robot to collect data ùúãùëíis critical to ensure that Q-Learning works
well. Afterall, we have replaced the expectation over ùë†0using the transition function ùëÉ¬πùë†0j
ùë†,ùëé¬∫using the data collected by the robot. If the policy ùúãùëídoes not reach diverse parts of
thestate-actionspace,thenitiseasytoimagineourestimate ÀÜùëÑwillbeapoorapproximation
of the optimal ùëÑ. It is also important to note that in such a situation, the estimate of ùëÑat
allstatesùë†2Swill be bad, not just the ones visited by ùúãùëí. This is because the Q-Learning
objective (or value iteration) is a constraint that ties together the value of all state-action
pairs. It is therefore critical to pick the correct policy ùúãùëíto collect data.
Wecanmitigatethisconcernbypickingacompletelyrandompolicy ùúãùëíthatsamplesactions
uniformly randomly from A. Such a policy would visit all states, but it will take a large
number of trajectories before it does so.
We thus arrive at the second key idea in Q-Learning, namely exploration. Typical im-
plementations of Q-Learning tie together the current estimate of ùëÑand the policy ùúãùëíto
set
ùúãùëí¬πùëéjùë†¬∫=(
argmaxùëé0ÀÜùëÑ¬πùë†,ùëé0¬∫with prob. 1 ùúñ
uniform¬πA¬∫ with prob.ùúñ,(17.3.6)
whereùúñis called the ‚Äúexploration parameter‚Äù and is chosen by the user. The policy ùúãùëíis
called an exploration policy. This particular ùúãùëíis called anùúñ-greedy exploration policy
because it chooses the optimal action (under the current estimate ÀÜùëÑ) with probability 1 ùúñ
but explores randomly with the remainder probability ùúñ. We can also use the so-called
softmax exploration policy
ùúãùëí¬πùëéjùë†¬∫=ùëíÀÜùëÑ¬πùë†,ùëé¬∫¬ùùëá
√ç
ùëé0ùëíÀÜùëÑ¬πùë†,ùëé0¬∫¬ùùëá; (17.3.7)
where the hyper-parameter ùëáis called temperature. A large value of ùúñinùúñ-greedy policy
functions similarly to a large value of temperature ùëáfor the softmax policy.
It is important to note that when we pick an exploration that depends upon the current
estimate of the action-value function ÀÜùëÑ, we need to resolve the optimization problem peri-
odically. Typical implementations of Q-Learning make one mini-batch update using a few
state-action pairs in the collected dataset (typically the ones collected from the previous
timestep of the robot) after taking every action using ùúãùëí.
17.3.4The ‚ÄúSelf-correcting‚ÄùPropertyof Q-Learning
ThedatasetcollectedbytherobotduringQ-Learninggrowswithtime. Boththeexploration
policyùúãùëíand the estimate ÀÜùëÑevolve as the robot collects more data. This gives us a key
insight into why Q-Learning works well. Consider a state ùë†: if a particular action ùëéhas
a large value under the current estimate ÀÜùëÑ¬πùë†,ùëé¬∫, then both the ùúñ-greedy and the softmax
explorationpolicieshavealargerprobabilityofpickingthisaction. Ifthisactionactuallyis
nottheidealaction,thenthefuturestatesthatarisefromthisactionwillhavepoorrewards.
ThenextupdateoftheQ-Learningobjectivewillthereforereducethevalue ÀÜùëÑ¬πùë†,ùëé¬∫, which
will reduce the probability of picking this action the next time the robot visits state ùë†. Bad
794 Reinforcement Learning
256actions, e.g., ones whose value is overestimated in ÀÜùëÑ¬πùë†,ùëé¬∫, are explored by the robot but
their value is correct in the next update of the Q-Learning objective. Good actions, e.g.,
whose value ÀÜùëÑ¬πùë†,ùëé¬∫is large, are explored more often by the robot and thereby reinforced.
ThispropertycanbeusedtoshowthatQ-Learningcanconvergetotheoptimalpolicyeven
if it begins with a random policy ùúãùëí(Watkins and Dayan, 1992 ).
This ability to not only collect new data but also collect the right kind of data is the cen-
tral feature of reinforcement learning algorithms, and this is what distinguishes them from
supervised learning. Q-Learning, using deep neural networks (which we will see in the
DQN chapeter later), is responsible for the resurgence of reinforcement learning ( Mnihet
al., 2013).
17.3.5Implementation of Q-Learning
WenowshowhowtoimplementQ-LearningonFrozenLakefrom OpenAIGym256. Note
this is the same setup as we consider in ValueIteration (page 785) experiment.
%matplotlib inline
import random
import numpy asnp
from d2l import torch asd2l
seed =0# Random number generator seed
gamma =0.95 # Discount factor
num_iters =256 # Number of iterations
alpha =0.9 # Learing rate
epsilon =0.9 # Epsilon in epsilion gready algorithm
random .seed(seed) # Set the random seed
np.random .seed(seed)
# Now set up the environment
env_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)
In the FrozenLake environment, the robot moves on a 44grid (these are the states) with
actions that are ‚Äúup‚Äù ( "), ‚Äúdown‚Äù (!), ‚Äúleft‚Äù ( ), and ‚Äúright‚Äù (!). The environment
contains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of
which are unknown to the robot. To keep the problem simple, we assume the robot has
reliable actions, i.e. ùëÉ¬πùë†0jùë†,ùëé¬∫=1for allùë†2S,ùëé2A. If the robot reaches the goal, the
trial ends and the robot receives a reward of 1irrespective of the action; the reward at any
otherstateis 0forallactions. Theobjectiveoftherobotistolearnapolicythatreachesthe
goal location (G) from a given start location (S) (this is ùë†0) to maximize the return.
We first implement ùúñ-greedy method as follows:
def e_greedy (env, Q, s, epsilon):
ifrandom .random() <epsilon:
return env.action_space .sample()
else :
return np.argmax(Q[s,:])
795 Q-Learning
We are now ready to implement Q-learning:
def q_learning (env_info, gamma, num_iters, alpha, epsilon):
env_desc =env_info[ 'desc ']# 2D array specifying what each grid item ‚ê£
‚Ü©!means
env =env_info[ 'env']# 2D array specifying what each grid item means
num_states =env_info[ 'num_states ']
num_actions =env_info[ 'num_actions ']
Q=np.zeros((num_states, num_actions))
V=np.zeros((num_iters +1, num_states))
pi=np.zeros((num_iters +1, num_states))
for kinrange (1, num_iters +1):
# Reset environment
state, done =env.reset(), False
while not done:
# Select an action for a given state and acts in env based on ‚ê£
‚Ü©!selected action
action =e_greedy(env, Q, state, epsilon)
next_state, reward, done, _ =env.step(action)
# Q-update:
y=reward +gamma *np.max(Q[next_state,:])
Q[state, action] =Q[state, action] +alpha *(y-Q[state, ‚ê£
‚Ü©!action])
# Move to the next state
state =next_state
# Record max value and max action for visualization purpose only
for sinrange (num_states):
V[k,s] =np.max(Q[s,:])
pi[k,s] =np.argmax(Q[s,:])
d2l.show_Q_function_progress(env_desc, V[: -1], pi[: -1])
q_learning(env_info =env_info, gamma =gamma, num_iters =num_iters, alpha =alpha, ‚ê£
‚Ü©!epsilon =epsilon)
ThisresultshowsthatQ-learningcanfindtheoptimalsolutionforthisproblemroughlyafter
250 iterations. However, when we compare this result with the Value Iteration algorithm‚Äôs
result(see ImplementationofValueIteration (page789)),wecanseethattheValueIteration
algorithm needs way fewer iterations to find the optimal solution for this problem. This
happens because the Value Iteration algorithm has access to the full MDP whereas Q-
learning does not.
17.3.6Summary
Q-learning is one of the most fundamental reinforcement-learning algorithms. It has been
at the epicenter of the recent success of reinforcement learning, most notably in learning
to play video games ( Mnihet al., 2013). Implementing Q-learning does not require that
we know the Markov decision process (MDP), e.g., the transition and reward functions,
completely.
796 Reinforcement Learning
25717.3.7Exercises
1.Try increasing the grid size to 88. Compared with 44grid, how many iterations
does it take to find the optimal value function?
2.Run the Q-learning algorithm again with ùõæ(i.e. ‚Äúgamma‚Äù in the above code) when it
equals to 0,0.5, and 1and analyze its results.
3.Run the Q-learning algorithm again with ùúñ(i.e. ‚Äúepsilon‚Äù in the above code) when it
equals to 0,0.5, and 1and analyze its results.
Discussions257.
258
18 Gaussian Processes
AndrewGordonWilson (NewYorkUniversityand Amazon )
Gaussianprocesses(GPs)areubitiquous. Youhavealreadyencounteredmanyexamplesof
GPs without realizing it. Any model that is linear in its parameters with a Gaussian distri-
butionovertheparametersisaGaussianprocess. Thisclassspansdiscretemodels, includ-
ing random walks, and autoregressive processes, as well as continuous models, including
Bayesian linear regression models, polynomials, Fourier series, radial basis functions, and
even neural networks with an infinite number of hidden units. There is a running joke that
‚Äúeverything is a special case of a Gaussian process‚Äù.
Learning about Gaussian processes is important for three reasons: (1) they provide a func-
tionspace perspectiveofmodelling,whichmakesunderstandingavarietyofmodelclasses,
including deep neural networks, much more approachable; (2) they have an extraordinary
range of applications where they are state-of-the-art, including active learning, hyperpa-
rameter learning, auto-ML, and spatiotemporal regression; (3) over the last few years,
algorithmic advances have made Gaussian processes increasingly scalable and relevant,
harmonizing with deep learning through frameworks such as GPyTorch258(Gardneret
al., 2018). Indeed, GPs and and deep neural networks are not competing approaches, but
highly complementary, and can be combined to great effect. These algorithmic advances
are not just relevant to Gaussian processes, but provide a foundation in numerical methods
that is broadly useful in deep learning.
In this chapter, we introduce Gaussian processes. In the introductory notebook, we start
by reasoning intuitively about what Gaussian processes are and how they directly model
functions. In the priors notebook, we focus on how to specify Gaussian process priors.
Wedirectlyconnectthetradiationalweight-spaceapproachtomodellingtofunctionspace,
which will help us reason about constructing and understanding machine learning mod-
els, including deep neural networks. We then introduce popular covariance functions, also
known as kernels, which control the generalization properties of a Gaussian process. A
GP with a given kernel defines a prior over functions. In the inference notebook, we will
show how to use data to infer a posterior , in order to make predictions. This notebook
contains from-scratch code for making predictions with a Gaussian process, as well as an
introduction to GPyTorch. In upcoming notebooks, we will introduce the numerics behind
Gaussianprocesses,whichisusefulforscalingGaussianprocessesbutalsoapowerfulgen-
eralfoundationfordeeplearning,andadvanceduse-casessuchashyperparametertuningin
deeplearning. OurexampleswillmakeuseofGPyTorch,whichmakesGaussianprocesses
scale, and is closely integrated with deep learning functionality and PyTorch.
797
798 Gaussian Processes
18.1Introductionto Gaussian Processes
In many cases, machine learning amounts to estimating parameters from data. These pa-
rametersareoftennumerousandrelativelyuninterpretable‚Äîsuchastheweightsofaneu-
ral network. Gaussian processes, by contrast, provide a mechanism for directly reasoning
about the high-level properties of functions that could fit our data. For example, we may
have a sense of whether these functions are quickly varying, periodic, involve conditional
independencies, or translation invariance. Gaussian processes enable us to easily incorpo-
ratethesepropertiesintoourmodel, bydirectlyspecifyingaGaussiandistributionoverthe
function values that could fit our data.
Let‚ÄôsgetafeelforhowGaussianprocessesoperate,bystartingwithsomeexamples.
Suppose we observe the following dataset, of regression targets (outputs), ùë¶, indexed by
inputs,ùë•. As an example, the targets could be changes in carbon dioxide concentrations,
and the inputs could be the times at which these targets have been recorded. What are
some features of the data? How quickly does it seem to varying? Do we have data points
collected at regular intervals, or are there missing inputs? How would you imagine filling
in the missing regions, or forecasting up until ùë•=25?
tFig. 18.1.1 Observed data.
In order to fit the data with a Gaussian process, we start by specifying a prior distribution
over what types of functions we might believe to be reasonable. Here we show several
sample functions from a Gaussian process. Does this prior look reasonable? Note here
we are not looking for functions that fit our dataset, but instead for specifying reasonable
high-level properties of the solutions, such as how quickly they vary with inputs. Note that
we will see code for reproducing all of the plots in this notebook, in the next notebooks on
priors and inference.
Once we condition on data, we can use this prior to infer a posterior distribution over func-
tions that could fit the data. Here we show sample posterior functions.
We see that each of these functions are entirely consistent with our data, perfectly running
through each observation. In order to use these posterior samples to make predictions, we
can average the values of every possible sample function from the posterior, to create the
799 Introduction to Gaussian Processes
tFig. 18.1.2 Sample prior functions that we may want to represent with our model.
tFig. 18.1.3 Sample posterior functions, once we have observed the data.
curve below, in thick blue. Note that we do not actually have to take an infinite number of
samples to compute this expectation; as we will see later, we can compute the expectation
in closed form.
tFig. 18.1.4 Posterior samples, alongside posterior mean, which can be used for point predictions, in
blue.
We may also want a representation of uncertainty, so we know how confident we should
be in our predictions. Intuitively, we should have more uncertainty where there is more
variability in the sample posterior functions, as this tells us there are many more possible
valuesthetruefunctioncouldtake. Thistypeofuncertaintyiscalled epistemicuncertainty ,
whichisthe reducibleuncertainty associatedwithlackofinformation. Asweacquiremore
data, this type of uncertainty disappears, as there will be increasingly fewer solutions con-
sistent with what we observe. Like with the posterior mean, we can compute the posterior
variance (the variability of these functions in the posterior) in closed form. With shade,
we show two times the posterior standard deviation on either side of the mean, creating a
800 Gaussian Processes
credibleinterval that has a 95% probability of containing the true value of the function for
any inputùë•.
tFig. 18.1.5 Posterior samples, including 95% credible set.
The plot looks somewhat cleaner if we remove the posterior samples, simply visualizing
the data, posterior mean, and 95% credible set. Notice how the uncertainty grows away
from the data, a property of epistemic uncertainty.
tFig. 18.1.6 Point predictions, and credible set.
The properties of the Gaussian process that we used to fit the data are strongly controlled
by what‚Äôs called a covariance function , also known as a kernel. The covariance function
we used is called the RBF (Radial Basis Function)kernel , which has the form
ùëòRBF¬πùë•,ùë•0¬∫=Cov¬πùëì¬πùë•¬∫, ùëì¬πùë•0¬∫¬∫=ùëé2exp
 1
2‚Ñì2jjùë• ùë•0jj2
(18.1.1)
Thehyperparameters of this kernel are interpretable. The amplitude parameterùëécontrols
the vertical scale over which the function is varying, and the length-scale parameter‚Ñìcon-
trols the rate of variation (the wiggliness) of the function. Larger ùëémeans larger function
values, and larger ‚Ñìmeans more slowly varying functions. Let‚Äôs see what happens to our
sample prior and posterior functions as we vary ùëéand‚Ñì.
Thelength-scale has a particularly pronounced effect on the predictions and uncertainty of
a GP. Atjjùë• ùë•0jj=‚Ñì, the covariance between a pair of function values is ùëé2exp¬π 0.5¬∫.
At larger distances than ‚Ñì, the values of the function values becomes nearly uncorrelated.
This means that if we want to make a prediction at a point ùë•, then function values with
inputsùë•such thatjjùë• ùë•0jj>‚Ñìwill not have a strong effect on our predictions.
Let‚Äôs see how changing the lengthscale affects sample prior and posterior functions, and
801 Introduction to Gaussian Processes
crediblesets. Theabovefitsusealength-scaleof 2. Let‚Äôsnowconsider ‚Ñì=0.1,0.5,2,5,10
. Alength-scaleof 0.1isverysmallrelativetotherangeoftheinputdomainweareconsid-
ering, 25. For example, the values of the function at ùë•=5andùë•=10will have essentially
no correlation at such a length-scale. On the other hand, for a length-scale of 10, the func-
tion values at these inputs will be highly correlated. Note that the vertical scale changes in
the following figures.

802 Gaussian Processes

803 Introduction to Gaussian Processes
Notice as the length-scale increases the ‚Äòwiggliness‚Äô of the functions decrease, and our
uncertainty decreases. If the length-scale is small, the uncertainty will quickly increase as
we move away from the data, as the datapoints become less informative about the function
values.
Now, let‚Äôs vary the amplitude parameter, holding the length-scale fixed at 2. Note the ver-
tical scale is held fixed for the prior samples, and varies for the posterior samples, so you
can clearly see both the increasing scale of the function, and the fits to the data.

804 Gaussian Processes
Weseetheamplitudeparameteraffectsthescaleofthefunction,butnottherateofvariation.
At this point, we also have the sense that the generalization performance of our procedure
willdependonhavingreasonablevaluesforthesehyperparameters. Valuesof ‚Ñì=2andùëé=
1appeared to provide reasonable fits, while some of the other values did not. Fortunately,
there is a robust and automatic way to specify these hyperparameters, using what is called
themarginallikelihood , which we will return to in the notebook on inference.
805 Introduction to Gaussian Processes
So what is a GP, really? As we started, a GP simply says that any collection of function
valuesùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫, indexed by any collection of inputs ùë•1,...,ùë•ùëõhas a joint multi-
variate Gaussian distribution. The mean vector ùúáof this distribution is given by a mean
function, which is typically taken to be a constant or zero. The covariance matrix of this
distribution is given by the kernelevaluated at all pairs of the inputs ùë•.
266666664ùëì¬πùë•¬∫
ùëì¬πùë•1¬∫
...
ùëì¬πùë•ùëõ¬∫377777775N¬©¬≠¬≠¬≠¬≠¬≠
¬´ùúá,266666664ùëò¬πùë•,ùë•¬∫ùëò¬πùë•,ùë• 1¬∫... ùëò¬πùë•,ùë•ùëõ¬∫
ùëò¬πùë•1,ùë•¬∫ùëò¬πùë•1,ùë•1¬∫... ùëò¬πùë•1,ùë•ùëõ¬∫
............
ùëò¬πùë•ùëõ,ùë•¬∫ùëò¬πùë•ùëõ,ùë•1¬∫... ùëò¬πùë•ùëõ,ùë•ùëõ¬∫377777775¬™¬Æ¬Æ¬Æ¬Æ¬Æ
¬¨(18.1.2)
Equation (18.1.2 )specifiesaGPprior. Wecancomputetheconditionaldistributionof ùëì¬πùë•¬∫
for anyùë•givenùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫, the function values we have observed. This conditional
distribution is called the posterior , and it is what we use to make predictions.
In particular,
ùëì¬πùë•¬∫jùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫N¬πùëö,ùë†2¬∫ (18.1.3)
where
ùëö=ùëò¬πùë•,ùë• 1:ùëõ¬∫ùëò¬πùë•1:ùëõ,ùë•1:ùëõ¬∫ 1ùëì¬πùë•1:ùëõ¬∫ (18.1.4)
ùë†2=ùëò¬πùë•,ùë•¬∫ ùëò¬πùë•,ùë• 1:ùëõ¬∫ùëò¬πùë•1:ùëõ,ùë•1:ùëõ¬∫ 1ùëò¬πùë•,ùë• 1:ùëõ¬∫ (18.1.5)
whereùëò¬πùë•,ùë• 1:ùëõ¬∫is a 1ùëõvector formed by evaluating ùëò¬πùë•,ùë•ùëñ¬∫forùëñ=1,...,ùëõand
ùëò¬πùë•1:ùëõ,ùë•1:ùëõ¬∫is anùëõùëõmatrix formed by evaluating ùëò¬πùë•ùëñ,ùë•ùëó¬∫forùëñ,ùëó=1,...,ùëõ.ùëöis
what we can use as a point predictor for any ùë•, andùë†2is what we use for uncertainty: if we
want to create an interval with a 95% probability that ùëì¬πùë•¬∫is in the interval, we would use
ùëö2ùë†. The predictivemeans and uncertainties forall the above figures werecreated using
theseequations. Theobserveddatapointsweregivenby ùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫andchoseafine
grained set of ùë•points to make predictions.
Let‚Äôs suppose we observe a single datapoint, ùëì¬πùë•1¬∫, and we want to determine the value
ofùëì¬πùë•¬∫at someùë•. Becauseùëì¬πùë•¬∫is described by a Gaussian process, we know the joint
distribution over¬πùëì¬πùë•¬∫, ùëì¬πùë•1¬∫¬∫is Gaussian:
ùëì¬πùë•¬∫
ùëì¬πùë•1¬∫
N
ùúá,ùëò¬πùë•,ùë•¬∫ùëò¬πùë•,ùë• 1¬∫
ùëò¬πùë•1,ùë•¬∫ùëò¬πùë•1,ùë•1¬∫
(18.1.6)
The off-diagonal expression ùëò¬πùë•,ùë• 1¬∫=ùëò¬πùë•1,ùë•¬∫tells us how correlated the function values
will be ‚Äî how strongly determined ùëì¬πùë•¬∫will be from ùëì¬πùë•1¬∫. We have seen already that if
we use a large length-scale, relative to the distance between ùë•andùë•1,jjùë• ùë•1jj, then the
functionvalueswillbehighlycorrelated. Wecanvisualizetheprocessofdetermining ùëì¬πùë•¬∫
fromùëì¬πùë•1¬∫both in the space of functions, and in the joint distribution over ùëì¬πùë•1¬∫, ùëì¬πùë•¬∫.
Let‚Äôs initially consider an ùë•such thatùëò¬πùë•,ùë• 1¬∫=0.9, andùëò¬πùë•,ùë•¬∫=1, meaning that the
valueofùëì¬πùë•¬∫ismoderatelycorrelatedwiththevalueof ùëì¬πùë•1¬∫. Inthejointdistribution,the
contours of constant probability will be relatively narrow ellipses.
Suppose we observe ùëì¬πùë•1¬∫=1.2. To condition on this value of ùëì¬πùë•1¬∫, we can draw a
806 Gaussian Processes
horizontal line at 1.2on our plot of the density, and see that the value of ùëì¬πùë•¬∫is mostly
constrained to¬ª0.64,1.52¬º. We have also drawn this plot in function space, showing the
observedpoint ùëì¬πùë•1¬∫inorange,and1standarddeviationoftheGaussianprocesspredictive
distribution for ùëì¬πùë•¬∫in blue, about the mean value of 1.08.
Now suppose we have a stronger correlation, ùëò¬πùë•,ùë• 1¬∫=0.95. Now the ellipses have nar-
rowed further, and the value of ùëì¬πùë•¬∫is even more strongly determined by ùëì¬πùë•1¬∫. Draw-
ing a horizontal line at 1.2, we see the contours for ùëì¬πùë•¬∫support values mostly within
¬ª0.83,1.45¬º. Again, we also show the plot in function space, with one standard deviation
about the mean predictive value of 1.14.

807 Introduction to Gaussian Processes
We see that the posterior mean predictor of our Gaussian process is closer to 1.2, be-
cause there is now a stronger correlation. We also see that our uncertainty (the error bars)
have somewhat decreased. Despite the strong correlation between these function values,
our uncertainty is still righly quite large, because we have only observed a single data
point!
This procedure can give us a posterior on ùëì¬πùë•¬∫for anyùë•, for any number of points we have
observed. Suppose we observe ùëì¬πùë•1¬∫, ùëì¬πùë•2¬∫. We now visualize the posterior for ùëì¬πùë•¬∫at a
particularùë•=ùë•0in function space. The exact distribution for ùëì¬πùë•¬∫is given by the above
equations.ùëì¬πùë•¬∫is Gaussian distributed, with mean
ùëö=ùëò¬πùë•,ùë• 1:3¬∫ùëò¬πùë•1:3,ùë•1:3¬∫ 1ùëì¬πùë•1:3¬∫ (18.1.7)
and variance
ùë†2=ùëò¬πùë•,ùë•¬∫ ùëò¬πùë•,ùë• 1:3¬∫ùëò¬πùë•1:3,ùë•1:3¬∫ 1ùëò¬πùë•,ùë• 1:3¬∫ (18.1.8)
Inthisintroductorynotebook,wehavebeenconsidering noisefree observations. Aswewill
see, it is easy to include observation noise. If we assume that the data are generated from a
latent noise free function ùëì¬πùë•¬∫plus iid Gaussian noise ùúñ¬πùë•¬∫N¬π 0,ùúé2¬∫with variance ùúé2,
thenourcovariancefunctionsimplybecomes ùëò¬πùë•ùëñ,ùë•ùëó¬∫!ùëò¬πùë•ùëñ,ùë•ùëó¬∫¬∏ùõøùëñùëóùúé2, whereùõøùëñùëó=1
ifùëñ=ùëóand0otherwise.
We have already started getting some intuition about how we can use a Gaussian process
to specify a prior and posterior over solutions, and how the kernel function affects the
properties of these solutions. In the following notebooks, we will precisely show how to
specify a Gaussian process prior, introduce and derive various kernel functions, and then
go through the mechanics of how to automatically learn kernel hyperparameters, and form
a Gaussian process posterior to make predictions. While it takes time and practice to get
used to concepts such as a ‚Äúdistributions over functions‚Äù, the actual mechanics of finding
the GP predictive equations is actually quite simple ‚Äî making it easy to get practice to
form an intuitive understanding of these concepts.
18.1.1Summary
In typical machine learning, we specify a function with some free parameters (such as
a neural network and its weights), and we focus on estimating those parameters, which
808 Gaussian Processes
may not be interpretable. With a Gaussian process, we instead reason about distributions
over functions directly, which enables us to reason about the high-level properties of the
solutions. These properties are controlled by a covariance function (kernel), which often
hasafewhighlyinterpretablehyperparameters. Thesehyperparametersincludethe length-
scale,whichcontrolshowrapidly(howwiggily)thefunctionsare. Anotherhyperparameter
is the amplitude, which controls the vertical scale over which our functions are varying.
Representingmanydifferentfunctionsthatcanfitthedata,andcombiningthemalltogether
into a predictive distribution, is a distinctive feature of Bayesian methods. Because there
is a greater amount of variability between possible solutions far away from the data, our
uncertainty intuitively grows as we move from the data.
A Gaussian process represents a distribution over functions by specifying a multivariate
normal (Gaussian) distribution over all possible function values. It is possible to easily
manipulate Gaussian distributions to find the distribution of one function value based on
the values of any set of other values. In other words, if we observe a set of points, then we
can condition on these points and infer a distribution over what the value of the function
might look like at any other input. How we model the correlations between these points is
determined by the covariance function and is what defines the generalization properties of
the Gaussian process. While it takes time to get used to Gaussian processes, they are easy
to work with, have many applications, and help us understand and develop other model
classes, like neural networks.
18.1.2Exercises
1.What is the difference between epistemic uncertainty versus observation uncertainty?
2.Besides rate of variation and amplitude, what other properties of functions might we
want to consider, and what would be real-world examples of functions that have those
properties?
3.The RBF covariance function we considered says that covariances (and correlations)
between observations decrease with their distance in the input space (times, spatial lo-
cations, etc.). Is this a reasonable assumption? Why or why not?
4.Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables
Gaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a
Gaussian?
5.Repeat the exercise where we observe a data point at ùëì¬πùë•1¬∫=1.2, but now suppose we
additionally observe ùëì¬πùë•2¬∫=1.4. Letùëò¬πùë•,ùë• 1¬∫=0.9, andùëò¬πùë•,ùë• 2¬∫=0.8. Will we be
more or less certain about the value of ùëì¬πùë•¬∫, than when we had only observed ùëì¬πùë•1¬∫?
What is the mean and 95% credible set for our value of ùëì¬πùë•¬∫now?
6.Do you think increasing our estimate of observation noise would increase or decrease
our estimate of the length-scale of the ground truth function?
7.As we move away from the data, suppose the uncertainty in our predictive distribution
increases to a point, then stops increasing. Why might that happen?
809 Gaussian Process Priors
259Discussions259.
18.2GaussianProcess Priors
UnderstandingGaussianprocesses(GPs)isimportantforreasoningaboutmodelconstruc-
tionandgeneralization,andforachievingstate-of-the-artperformanceinavarietyofappli-
cations,includingactivelearning,andhyperparametertuningindeeplearning. GPsareev-
erywhere, anditisinourintereststoknowwhattheyareandhowwecanusethem.
In this section, we introduce Gaussian process priorsover functions. In the next notebook,
we show how to use these priors to do posterior inference and make predictions. The
next section can be viewed as ‚ÄúGPs in a nutshell‚Äù, quickly giving what you need to apply
Gaussian processes in practice.
import numpy asnp
from scipy .spatial import distance_matrix
from d2l import torch asd2l
d2l.set_figsize()
18.2.1Definition
A Gaussian process is defined as a collection of random variables, any finite number of
which have a joint Gaussian distribution . If a function ùëì¬πùë•¬∫is a Gaussian process, with
meanfunction ùëö¬πùë•¬∫andcovariancefunction orkernelùëò¬πùë•,ùë•0¬∫,ùëì¬πùë•¬∫GP¬πùëö,ùëò¬∫,thenany
collection of function values queried at any collection of input points ùë•(times, spatial lo-
cations, imagepixels, etc.), hasajointmultivariateGaussiandistributionwithmeanvector
ùúáand covariance matrix ùêæ:ùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫N¬πùúá,ùêæ¬∫, whereùúáùëñ=ùê∏¬ªùëì¬πùë•ùëñ¬∫¬º=ùëö¬πùë•ùëñ¬∫
andùêæùëñùëó=Cov¬πùëì¬πùë•ùëñ¬∫, ùëì¬πùë•ùëó¬∫¬∫=ùëò¬πùë•ùëñ,ùë•ùëó¬∫.
This definition may seem abstract and inaccessible, but Gaussian processes are in fact very
simple objects. Any function
ùëì¬πùë•¬∫=ùë§>ùúô¬πùë•¬∫=hùë§,ùúô¬πùë•¬∫i, (18.2.1)
withùë§drawn from a Gaussian (normal) distribution, and ùúôbeing any vector of basis func-
tions, for example ùúô¬πùë•¬∫=¬π1,ùë•,ùë•2,...,ùë•ùëë¬∫>, is a Gaussian process. Moreover, any Gaus-
sian process f(x) can be expressed in the form of equation (18.2.1 ). Let‚Äôs consider a few
concrete examples, to begin getting acquainted with Gaussian processes, after which we
can appreciate how simple and useful they really are.
18.2.2ASimpleGaussian Process
810 Gaussian Processes
Supposeùëì¬πùë•¬∫=ùë§0¬∏ùë§1ùë•, andùë§0,ùë§1N¬π 0,1¬∫, withùë§0,ùë§1,ùë•all in one dimension.
We can equivalently write this function as the inner product ùëì¬πùë•¬∫=¬πùë§0,ùë§1¬∫¬π1,ùë•¬∫>. In
(18.2.1 )above,ùë§=¬πùë§0,ùë§1¬∫>andùúô¬πùë•¬∫=¬π1,ùë•¬∫>.
For anyùë•,ùëì¬πùë•¬∫is a sum of two Gaussian random variables. Since Gaussians are closed
under addition, ùëì¬πùë•¬∫is also a Gaussian random variable for any ùë•. In fact, we can compute
for any particular ùë•thatùëì¬πùë•¬∫isN¬π0,1¬∏ùë•2¬∫. Similarly, the joint distribution for any col-
lection of function values, ¬πùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫¬∫, for any collection of inputs ùë•1,...,ùë•ùëõ, is a
multivariate Gaussian distribution. Therefore ùëì¬πùë•¬∫is a Gaussian process.
In short,ùëì¬πùë•¬∫is arandom function , or adistribution over functions . We can gain some
insights into this distribution by repeatedly sampling values for ùë§0,ùë§1, and visualizing the
corresponding functions ùëì¬πùë•¬∫, which are straight lines with slopes and different intercepts,
as follows:
def lin_func (x, n_sample):
preds =np.zeros((n_sample, x .shape[ 0]))
for iiinrange (n_sample):
w=np.random .normal( 0,1,2)
y=w[0]+w[1]*x
preds[ii, :] =y
return preds
x_points =np.linspace( -5,5,50)
outs =lin_func(x_points, 10)
lw_bd =-2*np.sqrt(( 1+x_points **2))
up_bd =2*np.sqrt(( 1+x_points **2))
d2l.plt.fill_between(x_points, lw_bd, up_bd, alpha =0.25 )
d2l.plt.plot(x_points, np .zeros( len(x_points)), linewidth =4, color ='black ')
d2l.plt.plot(x_points, outs .T)
d2l.plt.xlabel( "x", fontsize =20)
d2l.plt.ylabel( "f(x) ", fontsize =20)
d2l.plt.show()
Ifùë§0andùë§1are instead drawn from N¬π0,ùõº2¬∫, how do you imagine varying ùõºaffects the
distribution over functions?
18.2.3FromWeightSpace to Function Space
811 Gaussian Process Priors
In the plot above, we saw how a distribution over parameters in a model induces a distri-
bution over functions. While we often have ideas about the functions we want to model ‚Äî
whether they‚Äôre smooth, periodic, quickly varying, etc. ‚Äî it is relatively tedious to reason
about the parameters, which are largely uninterpretable. Fortunately, Gaussian processes
provide an easy mechanism to reason directlyabout functions. Since a Gaussian distribu-
tionisentirelydefinedbyitsfirsttwomoments,itsmeanandcovariancematrix,aGaussian
process by extension is defined by its mean function and covariance function.
In the above example, the mean function
ùëö¬πùë•¬∫=ùê∏¬ªùëì¬πùë•¬∫¬º=ùê∏¬ªùë§0¬∏ùë§1ùë•¬º=ùê∏¬ªùë§0¬º¬∏ùê∏¬ªùë§1¬ºùë•=0¬∏0=0. (18.2.2)
Similarly, the covariance function is
ùëò¬πùë•,ùë•0¬∫=Cov¬πùëì¬πùë•¬∫, ùëì¬πùë•0¬∫¬∫=ùê∏¬ªùëì¬πùë•¬∫ùëì¬πùë•0¬∫¬º ùê∏¬ªùëì¬πùë•¬∫¬ºùê∏¬ªùëì¬πùë•0¬∫¬º=ùê∏¬ªùë§2
0¬∏ùë§0ùë§1ùë•0¬∏ùë§1ùë§0ùë•¬∏ùë§2
1ùë•ùë•0¬º=1¬∏ùë•ùë•0.
(18.2.3)
Our distribution over functions can now be directly specified and sampled from, without
needing to sample from the distribution over parameters. For example, to draw from ùëì¬πùë•¬∫,
wecansimplyformourmultivariateGaussiandistributionassociatedwithanycollectionof
ùë•wewanttoquery,andsamplefromitdirectly. Wewillbegintoseejusthowadvantageous
this formulation will be.
First, we note that essentially the same derivation for the simple straight line model above
can be applied to find the mean and covariance function for anymodel of the form ùëì¬πùë•¬∫=
ùë§>ùúô¬πùë•¬∫, withùë§N¬πùë¢,ùëÜ¬∫. In this case, the mean function ùëö¬πùë•¬∫=ùë¢>ùúô¬πùë•¬∫, and the
covariancefunction ùëò¬πùë•,ùë•0¬∫=ùúô¬πùë•¬∫>ùëÜùúô¬πùë•0¬∫. Sinceùúô¬πùë•¬∫canrepresentavectorofanynon-
linear basis functions, we are considering a very general model class, including models
with an even an infinitenumber of parameters.
18.2.4TheRadial Basis Function (RBF) Kernel
Theradial basis function (RBF) kernel is the most popular covariance function for Gaus-
sian processes, and kernel machines in general. This kernel has the form ùëòRBF¬πùë•,ùë•0¬∫=
ùëé2exp
 1
2‚Ñì2jjùë• ùë•0jj2
, whereùëéis an amplitude parameter, and ‚Ñìis alengthscale hyper-
parameter.
Let‚Äôs derive this kernel starting from weight space. Consider the function
ùëì¬πùë•¬∫=ùêΩ√ï
ùëñ=1ùë§ùëñùúôùëñ¬πùë•¬∫,ùë§ùëñN
0,ùúé2
ùêΩ
,ùúôùëñ¬πùë•¬∫=exp
 ¬πùë• ùëêùëñ¬∫2
2‚Ñì2
. (18.2.4)
ùëì¬πùë•¬∫is a sum of radial basis functions, with width ‚Ñì, centred at the points ùëêùëñ, as shown in
the following figure.
We can recognize ùëì¬πùë•¬∫as having the form ùë§>ùúô¬πùë•¬∫, whereùë§=¬πùë§1,...,ùë§ùêΩ¬∫>andùúô¬πùë•¬∫
is a vector containing each of the radial basis functions. The covariance function of this
812 Gaussian Processes
Gaussian process is then
ùëò¬πùë•,ùë•0¬∫=ùúé2
ùêΩùêΩ√ï
ùëñ=1ùúôùëñ¬πùë•¬∫ùúôùëñ¬πùë•0¬∫. (18.2.5)
Nowlet‚Äôsconsiderwhathappensaswetakethenumberofparameters(andbasisfunctions)
to infinity. Let ùëêùêΩ=logùêΩ,ùëê1= logùêΩ, andùëêùëñ¬∏1 ùëêùëñ= Œîùëê=2logùêΩ
ùêΩ, andùêΩ!1. The
covariance function becomes the Riemann sum:
ùëò¬πùë•,ùë•0¬∫=lim
ùêΩ!1ùúé2
ùêΩùêΩ√ï
ùëñ=1ùúôùëñ¬πùë•¬∫ùúôùëñ¬πùë•0¬∫=¬πùëê1
ùëê0ùúôùëê¬πùë•¬∫ùúôùëê¬πùë•0¬∫ùëëùëê. (18.2.6)
By settingùëê0= 1andùëê1=1, we spread the infinitely many basis functions across the
whole real line, each a distance Œîùëê!0apart:
ùëò¬πùë•,ùë•0¬∫=¬π1
 1exp¬π ¬πùë• ùëê¬∫2
2‚Ñì2¬∫exp¬π ¬πùë•0 ùëê¬∫2
2‚Ñì2¬∫ùëëùëê=pùúã‚Ñìùúé2exp¬π ¬πùë• ùë•0¬∫2
2¬πp
2‚Ñì¬∫2¬∫/ùëòRBF¬πùë•,ùë•0¬∫.
(18.2.7)
Itisworthtakingamomenttoabsorbwhatwehavedonehere. Bymovingintothefunction
space representation, we have derived how to represent a model with an infinitenumber of
parameters, using a finite amount of computation. A Gaussian process with an RBF kernel
is auniversal approximator , capable of representing any continuous function to arbitrary
precision. We can intuitively see why from the above derivation. We can collapse each
radial basis function to a point mass taking ‚Ñì!0, and give each point mass any height we
wish.
So a Gaussian process with an RBF kernel is a model with an infinite number of param-
eters and much more flexibility than any finite neural network. Perhaps all the fuss about
overparametrized neural networks is misplaced. As we will see, GPs with RBF kernels
do not overfit, and in fact provide especially compelling generalization performance on
small datasets. Moreover, the examples in ( Zhanget al., 2021), such as the ability to fit
images with random labels perfectly, but still generalize well on structured problems, (can
be perfectly reproduced using Gaussian processes) ( Wilson and Izmailov, 2020 ). Neural
networks are not as distinct as we make them out to be.
We can build further intuition about Gaussian processes with RBF kernels, and hyperpa-
rameters such as length-scale , by sampling directly from the distribution over functions.
As before, this involves a simple procedure:
1.Choose the input ùë•points we want to query the GP: ùë•1,...,ùë•ùëõ.
2.Evaluateùëö¬πùë•ùëñ¬∫,ùëñ=1,...,ùëõ, andùëò¬πùë•ùëñ,ùë•ùëó¬∫forùëñ,ùëó=1,...,ùëõto respectively form the
mean vector and covariance matrix ùúáandùêæ, where¬πùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫¬∫N¬πùúá,ùêæ¬∫.
3.Sample from this multivariate Gaussian distribution to obtain the sample function val-
ues.
4.Sample more times to visualize more sample functions queried at those points.
We illustrate this process in the figure below.
813 Gaussian Process Priors
def rbfkernel (x1, x2, ls =4.): #@save
dist =distance_matrix(np .expand_dims(x1, 1), np .expand_dims(x2, 1))
return np.exp( -(1./ls/2)*(dist **2))
x_points =np.linspace( 0,5,50)
meanvec =np.zeros( len(x_points))
covmat =rbfkernel(x_points,x_points, 1)
prior_samples =np.random .multivariate_normal(meanvec, covmat, size =5);
d2l.plt.plot(x_points, prior_samples .T, alpha =0.5)
d2l.plt.show()
18.2.5TheNeuralNetworkKernel
Research on Gaussian processes in machine learning was triggered by research on neu-
ral networks. Radford Neal was pursuing ever larger Bayesian neural networks, ultimately
showing in 1994 (later published in 1996, as it was one of the most infamous NeurIPS
rejections) that such networks with an infinite number of hidden units become Gaussian
processes with particular kernel functions ( Neal, 1996 ). Interest in this derivation has re-
surfaced, with ideas like the neural tangent kernel being used to investigate the generaliza-
tion properties of neural networks ( Matthews et al., 2018) (Novaket al., 2018). We can
derive the neural network kernel as follows.
Consider a neural network function ùëì¬πùë•¬∫with one hidden layer:
ùëì¬πùë•¬∫=ùëè¬∏ùêΩ√ï
ùëñ=1ùë£ùëñ‚Ñé¬πùë•;ùë¢ùëñ¬∫. (18.2.8)
ùëèis a bias,ùë£ùëñare the hidden to output weights, ‚Ñéis any bounded hidden unit transfer
function,ùë¢ùëñare the input to hidden weights, and ùêΩis the number of hidden units. Let ùëèand
ùë£ùëñbe independent with zero mean and variances ùúé2
ùëèandùúé2
ùë£¬ùùêΩ, respectively, and let the ùë¢ùëñ
haveindependentidenticaldistributions. Wecanthenusethecentrallimittheoremtoshow
that any collection of function values ùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫has a joint multivariate Gaussian
distribution.
The mean and covariance function of the corresponding Gaussian process are:
ùëö¬πùë•¬∫=ùê∏¬ªùëì¬πùë•¬∫¬º=0 (18.2.9)
814 Gaussian Processes
ùëò¬πùë•,ùë•0¬∫=cov¬ªùëì¬πùë•¬∫, ùëì¬πùë•0¬∫¬º=ùê∏¬ªùëì¬πùë•¬∫ùëì¬πùë•0¬∫¬º=ùúé2
ùëè¬∏1
ùêΩùêΩ√ï
ùëñ=1ùúé2
ùë£ùê∏¬ª‚Ñéùëñ¬πùë•;ùë¢ùëñ¬∫‚Ñéùëñ¬πùë•0;ùë¢ùëñ¬∫¬º
(18.2.10)
In some cases, we can essentially evaluate this covariance function in closed form. Let
‚Ñé¬πùë•;ùë¢¬∫=erf¬πùë¢0¬∏√çùëÉ
ùëó=1ùë¢ùëóùë•ùëó¬∫, where erf¬πùëß¬∫=2pùúã¬Øùëß
0ùëí ùë°2ùëëùë°, andùë¢N¬π 0,Œ£¬∫. Then
ùëò¬πùë•,ùë•0¬∫=2
ùúãsin¬π2 Àúùë•>Œ£Àúùë•0p
¬π1¬∏2 Àúùë•>Œ£Àúùë•¬∫¬π1¬∏2 Àúùë•0>Œ£Àúùë•0¬∫¬∫.
The RBF kernel is stationary , meaning that it is translation invariant , and therefore can
be written as a function of ùúè=ùë• ùë•0. Intuitively, stationarity means that the high-level
properties of the function, such as rate of variation, do not change as we move in input
space. The neural network kernel, however, is non-stationary . Below, we show sample
functions from a Gaussian process with this kernel. We can see that the function looks
qualitatively different near the origin.
18.2.6Summary
The first step in performing Bayesian inference involves specifying a prior. Gaussian pro-
cesses can be used to specify a whole prior over functions. Starting from a traditional
‚Äúweightspace‚Äùviewofmodelling,wecaninduceaprioroverfunctionsbystartingwiththe
functional form of a model, and introducing a distribution over its parameters. We can al-
ternativelyspecifyapriordistributiondirectlyinfunctionspace,withpropertiescontrolled
by a kernel. The function-space approach has many advantages. We can build models that
actually correspond to an infinite number of parameters, but use a finite amount of com-
putation! Moreover, while these models have a great amount of flexibility, they also make
strong assumptions about what types of functions are a priori likely, leading to relatively
good generalization on small datasets.
Theassumptionsofmodelsinfunctionspaceareintuitivelycontrolledbykernels,whichof-
ten encode higher level properties of functions, such as smoothness and periodicity. Many
kernels are stationary, meaning that they are translation invariant. Functions drawn from
a Gaussian process with a stationary kernel have roughly the same high-level properties
(such as rate of variation) regardless of where we look in the input space.
Gaussianprocessesarearelativelygeneralmodelclass,containingmanyexamplesofmod-
els we are already familiar with, including polynomials, Fourier series, and so on, as long
as we have a Gaussian prior over the parameters. They also include neural networks with
an infinite number of parameters, even without Gaussian distributions over the parameters.
This connection, discovered by Radford Neal, triggered machine learning researchers to
move away from neural networks, and towards Gaussian processes.
18.2.7Exercises
1.DrawsamplepriorfunctionsfromaGPwithanOrnstein-Uhlenbeck(OU)kernel, ùëòOU¬πùë•,ùë•0¬∫=
exp
 1
2‚Ñìjjùë• ùë•0j
. If you fix the lengthscale ‚Ñìto be the same, how do these functions
look different than sample functions from a GP with an RBF kernel?
815 Gaussian Process Inference
260
2612.How does changing the amplitudeùëé2of the RBF kernel affect the distribution over
functions?
3.Supposeweform ùë¢¬πùë•¬∫=ùëì¬πùë•¬∫¬∏2ùëî¬πùë•¬∫,whereùëì¬πùë•¬∫GP¬πùëö1,ùëò1¬∫andùëî¬πùë•¬∫GP¬πùëö2,ùëò2¬∫.
Isùë¢¬πùë•¬∫a Gaussian process, and if so, what is its mean and covariance function?
4.Suppose we form ùëî¬πùë•¬∫=ùëé¬πùë•¬∫ùëì¬πùë•¬∫, whereùëì¬πùë•¬∫GP¬π 0,ùëò¬∫andùëé¬πùë•¬∫=ùë•2. Isùëî¬πùë•¬∫
a Gaussian process, and if so, what is its mean and covariance function? What is the
effect ofùëé¬πùë•¬∫? What do sample functions drawn from ùëî¬πùë•¬∫look like?
5.Supposeweform ùë¢¬πùë•¬∫=ùëì¬πùë•¬∫ùëî¬πùë•¬∫,whereùëì¬πùë•¬∫GP¬πùëö1,ùëò1¬∫andùëî¬πùë•¬∫GP¬πùëö2,ùëò2¬∫.
Isùë¢¬πùë•¬∫a Gaussian process, and if so, what is its mean and covariance function?
Discussions260.
18.3Gaussian ProcessInference
Inthissection,wewillshowhowtoperformposteriorinferenceandmakepredictionsusing
theGPpriorsweintroducedinthelastsection. Wewillstartwithregression,wherewecan
perform inference in closed form . This is a ‚ÄúGPs in a nutshell‚Äù section to quickly get up
andrunningwithGaussianprocessesinpractice. We‚Äôllstartcodingallthebasicoperations
fromscratch,andthenintroduce GPyTorch261,whichwillmakeworkingwithstate-of-the-
art Gaussian processes and integration with deep neural networks much more convenient.
We will consider these more advanced topics in depth in the next section. In that section,
we will also consider settings where approximate inference is required ‚Äî classification,
point processes, or any non-Gaussian likelihoods.
18.3.1PosteriorInferencefor Regression
Anobservation model relates the function we want to learn, ùëì¬πùë•¬∫, to our observations
ùë¶¬πùë•¬∫, both indexed by some input ùë•. In classification, ùë•could be the pixels of an image,
andùë¶couldbe the associated classlabel. In regression, ùë¶typicallyrepresents a continuous
output, such as a land surface temperature, a sea-level, a ùê∂ùëÇ 2concentration, etc.
In regression, we often assume the outputs are given by a latent noise-free function ùëì¬πùë•¬∫
plus i.i.d. Gaussian noise ùúñ¬πùë•¬∫:
ùë¶¬πùë•¬∫=ùëì¬πùë•¬∫¬∏ùúñ¬πùë•¬∫, (18.3.1)
withùúñ¬πùë•¬∫N¬π 0,ùúé2¬∫. Lety=ùë¶¬πùëã¬∫=¬πùë¶¬πùë•1¬∫,...,ùë¶¬πùë•ùëõ¬∫¬∫>be a vector of our training
observations, and f=¬πùëì¬πùë•1¬∫,..., ùëì¬πùë•ùëõ¬∫¬∫>be a vector of the latent noise-free function
values, queried at the training inputs ùëã=ùë•1,...,ùë•ùëõ.
We will assume ùëì¬πùë•¬∫GP¬πùëö,ùëò¬∫, which means that any collection of function values f
hasajointmultivariateGaussiandistribution,withmeanvector ùúáùëñ=ùëö¬πùë•ùëñ¬∫andcovariance
matrixùêæùëñùëó=ùëò¬πùë•ùëñ,ùë•ùëó¬∫. The RBF kernel ùëò¬πùë•ùëñ,ùë•ùëó¬∫=ùëé2exp
 1
2‚Ñì2jjùë•ùëñ ùë•ùëójj2
would be a
816 Gaussian Processes
standardchoiceofcovariancefunction. Fornotationalsimplicity,wewillassumethemean
functionùëö¬πùë•¬∫=0; our derivations can easily be generalized later on.
Suppose we want to make predictions at a set of inputs
ùëã=ùë•1,ùë•2,...,ùë•ùëö. (18.3.2)
Then we want to find ùë•2andùëù¬πfjy,ùëã¬∫. In the regression setting, we can conveniently
find this distribution by using Gaussian identities, after finding the joint distribution over
f=ùëì¬πùëã¬∫andy.
If we evaluate equation (18.3.1 )at the training inputs ùëã, we have y=f¬∏Ô¨Ñ. By the
definition of a Gaussian process (see last section), fN¬π 0,ùêæ¬πùëã,ùëã¬∫¬∫whereùêæ¬πùëã,ùëã¬∫is
anùëõùëõmatrix formed by evaluating our covariance function (aka kernel) at all possible
pairs of inputs ùë•ùëñ,ùë•ùëó2ùëã.Ô¨Ñis simply a vector comprised of iid samples from N¬π0,ùúé2¬∫
and thus has distribution N¬π0,ùúé2ùêº¬∫.yis therefore a sum of two independent multivariate
Gaussian variables, and thus has distribution N¬π0,ùêæ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬∫. One can also show
that cov¬πf,y¬∫=cov¬πy,f¬∫>=ùêæ¬πùëã,ùëã¬∫whereùêæ¬πùëã,ùëã¬∫is anùëöùëõmatrix formed by
evaluating the kernel at all pairs of test and training inputs.
y
f
N
0,A=ùêæ¬πùëã,ùëã¬∫¬∏ùúé2ùêº ùêæ¬πùëã,ùëã¬∫
ùêæ¬πùëã,ùëã¬∫ùêæ¬πùëã,ùëã¬∫
(18.3.3)
We can then use standard Gaussian identities to find the conditional distribution from the
joint distribution (see, e.g., Bishop Chapter 2), fjy,ùëã,ùëã N¬πùëö,ùëÜ¬∫, whereùëö=
ùêæ¬πùëã,ùëã¬∫¬ªùêæ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1y,andùëÜ=ùêæ¬πùëã,ùëã¬∫ ùêæ¬πùëã,ùëã¬∫¬ªùêæ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1ùêæ¬πùëã,ùëã¬∫.
Typically, we do not need to make use of the full predictive covariance matrix ùëÜ, and in-
stead use the diagonal of ùëÜfor uncertainty about each prediction. Often for this reason we
write the predictive distribution for a single test point ùë•, rather than a collection of test
points.
Thekernelmatrixhasparameters ùúÉthatwealsowishtoestimate, suchtheamplitude ùëéand
lengthscale‚Ñìof the RBF kernel above. For these purposes we use the marginallikelihood ,
ùëù¬πyjùúÉ,ùëã¬∫, which we already derived in working out the marginal distributions to find the
jointdistributionover y,f. Aswewillsee,themarginallikelihoodcompartmentalizesinto
modelfitandmodelcomplexityterms,andautomaticallyencodesanotionofOccam‚Äôsrazor
for learning hyperparameters. For a full discussion, see MacKay Ch. 28 ( MacKay, 2003 ),
and Rasmussen and Williams Ch. 5 ( Rasmussen and Williams, 2006 ).
import math
import os
import gpytorch
import matplotlib .pyplot asplt
import numpy asnp
import torch
from scipy import optimize
from scipy .spatial import distance_matrix
from d2l import torch asd2l
d2l.set_figsize()
817 Gaussian Process Inference
18.3.2EquationsforMaking Predictionsand Learning Kernel
Hyperparametersin GP Regression
Welistheretheequationsyouwilluseforlearninghyperparametersandmakingpredictions
in Gaussian process regression. Again, we assume a vector of regression targets y, indexed
byinputsùëã=fùë•1,...,ùë•ùëõg,andwewishtomakeapredictionatatestinput ùë•. Weassume
i.i.d. additive zero-mean Gaussian noise with variance ùúé2. We use a Gaussian process
priorùëì¬πùë•¬∫GP¬πùëö,ùëò¬∫forthelatentnoise-freefunction,withmeanfunction ùëöandkernel
functionùëò. The kernel itself has parameters ùúÉthat we want to learn. For example, if we
use an RBF kernel, ùëò¬πùë•ùëñ,ùë•ùëó¬∫=ùëé2exp
 1
2‚Ñì2jjùë• ùë•0jj2
, we want to learn ùúÉ=fùëé2,‚Ñì2g. Let
ùêæ¬πùëã,ùëã¬∫represent an ùëõùëõmatrix corresponding to evaluating the kernel for all possible
pairs ofùëõtraining inputs. Let ùêæ¬πùë•,ùëã¬∫represent a 1ùëõvector formed by evaluating
ùëò¬πùë•,ùë•ùëñ¬∫,ùëñ=1,...,ùëõ. Letùúábe a mean vector formed by evaluating the mean function
ùëö¬πùë•¬∫at every training points ùë•.
Typically in working with Gaussian processes, we follow a two-step procedure. 1. Learn
kernel hyperparameters ÀÜùúÉby maximizing the marginal likelihood with respect to these hy-
perparameters. 2. Use the predictive mean as a point predictor, and 2 times the predictive
standard deviation to form a 95% credible set, conditioning on these learned hyperparam-
eters ÀÜùúÉ.
The log marginal likelihood is simply a log Gaussian density, which has the form:
logùëù¬πyjùúÉ,ùëã¬∫= 1
2y>¬ªùêæùúÉ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1y 1
2logjùêæùúÉ¬πùëã,ùëã¬∫j¬∏ùëê (18.3.4)
The predictive distribution has the form:
ùëù¬πùë¶jùë•,y,ùúÉ¬∫=N¬πùëé,ùë£¬∫ (18.3.5)
ùëé=ùëòùúÉ¬πùë•,ùëã¬∫¬ªùêæùúÉ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1¬πy ùúá¬∫¬∏ùúá (18.3.6)
ùë£=ùëòùúÉ¬πùë•,ùë•¬∫ ùêæùúÉ¬πùë•,ùëã¬∫¬ªùêæùúÉ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1ùëòùúÉ¬πùëã,ùë•¬∫ (18.3.7)
18.3.3InterpretingEquations forLearning and Predictions
There are some key points to note about the predictive distributions for Gaussian pro-
cesses:
Despitetheflexibilityofthemodelclass,itispossibletodo exactBayesianinferencefor
GP regression in closed form . Aside from learning the kernel hyperparameters, there
is notraining. We can write down exactly what equations we want to use to make
predictions. Gaussian processes are relatively exceptional in this respect, and it has
greatly contributed to their convenience, versatility, and continued popularity.
The predictive mean ùëéis a linear combination of the training targets y, weighted by the
kernelùëòùúÉ¬πùë•,ùëã¬∫¬ªùêæùúÉ¬πùëã,ùëã¬∫¬∏ùúé2ùêº¬º 1. Aswewillsee,thekernel(anditshyperparam-
eters) thus plays a crucial role in the generalization properties of the model.
818 Gaussian Processes
The predictivemean explicitlydepends on the targetvalues ybut the predictivevariance
does not. The predictive uncertainty instead grows as the test input ùë•moves away
from the target locations ùëã, as governed by the kernel function. However, uncertainty
willimplicitlydependonthevaluesofthetargets ythroughthekernelhyperparameters
ùúÉ, which are learned from the data.
The marginal likelihood compartmentalizes into model fit and model complexity (log
determinant) terms. The marginal likelihood tends to select for hyperparameters that
provide the simplest fits that are still consistent with the data.
The key computational bottlenecks come from solving a linear system and computing
a log determinant over an ùëõùëõsymmetric positive definite matrix ùêæ¬πùëã,ùëã¬∫forùëõ
training points. Naively, these operations each incur O¬πùëõ3¬∫computations, as well as
O¬πùëõ2¬∫storage for each entry of the kernel (covariance) matrix, often starting with a
Choleskydecomposition. Historically,thesebottleneckshavelimitedGPstoproblems
with fewer than about 10,000 training points, and have given GPs a reputation for
‚Äúbeing slow‚Äù that has been inaccurate now for almost a decade. In advanced topics,
we will discuss how GPs can be scaled to problems with millions of points.
For popular choices of kernel functions, ùêæ¬πùëã,ùëã¬∫is often close to singular, which can
cause numerical issues when performing Cholesky decompositions or other opera-
tionsintendedtosolvelinearsystems. Fortunately,inregressionweareoftenworking
withùêæùúÉ¬πùëã,ùëã¬∫¬∏ùúé2ùêº, such that the noise variance ùúé2gets added to the diagonal of
ùêæ¬πùëã,ùëã¬∫, significantly improving its conditioning. If the noise variance is small, or
we are doing noise free regression, it is common practice to add a small amount of
‚Äújitter‚Äù to the diagonal, on the order of 10 6, to improve conditioning.
18.3.4WorkedExamplefromScratch
Let‚Äôs create some regression data, and then fit the data with a GP, implementing every step
from scratch. We‚Äôll sample data from
ùë¶¬πùë•¬∫=sin¬πùë•¬∫¬∏1
2sin¬π4ùë•¬∫¬∏ùúñ, (18.3.8)
withùúñN¬π 0,ùúé2¬∫. The noise free function we wish to find is ùëì¬πùë•¬∫=sin¬πùë•¬∫¬∏1
2sin¬π4ùë•¬∫.
We‚Äôll start by using a noise standard deviation ùúé=0.25.
def data_maker1 (x, sig):
return np.sin(x) +0.5 *np.sin( 4*x)+np.random .randn(x .shape[ 0])*sig
sig =0.25
train_x, test_x =np.linspace( 0,5,50), np .linspace( 0,5,500)
train_y, test_y =data_maker1(train_x, sig =sig), data_maker1(test_x, sig =0.)
d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y)
d2l.plt.xlabel( "x", fontsize =20)
d2l.plt.ylabel( "Observations y ", fontsize =20)
d2l.plt.show()
819 Gaussian Process Inference
Here we see the noisy observations as circles, and the noise-free function in blue that we
wish to find.
Now, let‚Äôs specify a GP prior over the latent noise-free function, ùëì¬πùë•¬∫GP¬πùëö,ùëò¬∫. We‚Äôll
use a mean function ùëö¬πùë•¬∫=0, and an RBF covariance function (kernel)
ùëò¬πùë•ùëñ,ùë•ùëó¬∫=ùëé2exp
 1
2‚Ñì2jjùë• ùë•0jj2
. (18.3.9)
mean =np.zeros(test_x .shape[ 0])
cov =d2l.rbfkernel(test_x, test_x, ls =0.2)
Wehavestartedwithalength-scaleof0.2. Beforewefitthedata,itisimportanttoconsider
whether we have specified a reasonable prior. Let‚Äôs visualize some sample functions from
this prior, as well as the 95% credible set (we believe there‚Äôs a 95% chance that the true
function is within this region).
prior_samples =np.random .multivariate_normal(mean =mean, cov =cov, size =5)
d2l.plt.plot(test_x, prior_samples .T, color ='black ', alpha =0.5)
d2l.plt.plot(test_x, mean, linewidth =2.)
d2l.plt.fill_between(test_x, mean -2*np.diag(cov), mean +2*np.diag(cov),
alpha =0.25 )
d2l.plt.show()
Do these samples look reasonable? Are the high-level properties of the functions aligned
with the type of data we are trying to model?
Nowlet‚Äôsformthemeanandvarianceoftheposteriorpredictivedistributionatanyarbitrary
820 Gaussian Processes
test pointùë•.
¬Øùëì=ùêæ¬πùë•,ùë•¬∫ùëá¬πùêæ¬πùë•,ùë•¬∫¬∏ùúé2ùêº¬∫ 1ùë¶ (18.3.10)
ùëâ¬πùëì¬∫=ùêæ¬πùë•,ùë•¬∫ ùêæ¬πùë•,ùë•¬∫ùëá¬πùêæ¬πùë•,ùë•¬∫¬∏ùúé2ùêº¬∫ 1ùêæ¬πùë•,ùë•¬∫ (18.3.11)
Before we make predictions, we should learn our kernel hyperparameters ùúÉand noise vari-
anceùúé2. Let‚Äôs initialize our length-scale at 0.75, as our prior functions looked too quickly
varying compared to the data we are fitting. We‚Äôll also guess a noise standard deviation ùúé
of 0.75.
In order to learn these parameters, we will maximize the marginal likelihood with respect
to these parameters.
logùëù¬πùë¶jùëã¬∫=log¬π
ùëù¬πùë¶jùëì,ùëã¬∫ùëù¬πùëìjùëã¬∫ùëëùëì (18.3.12)
logùëù¬πùë¶jùëã¬∫= 1
2ùë¶ùëá¬πùêæ¬πùë•,ùë•¬∫¬∏ùúé2ùêº¬∫ 1ùë¶ 1
2logjùêæ¬πùë•,ùë•¬∫¬∏ùúé2ùêºj ùëõ
2log 2ùúã(18.3.13)
Perhaps our prior functions were too quickly varying. Let‚Äôs guess a length-scale of 0.4.
We‚Äôll also guess a noise standard deviation of 0.75. These are simply hyperparameter ini-
tializations ‚Äî we will learn these parameters from the marginal likelihood.
ell_est =0.4
post_sig_est =0.5
def neg_MLL (pars):
K=d2l.rbfkernel(train_x, train_x, ls =pars[ 0])
kernel_term =-0.5 *train_y @\
np.linalg .inv(K +pars[ 1]**2*np.eye(train_x .shape[ 0])) @train_y
logdet =-0.5 *np.log(np .linalg .det(K +pars[ 1]**2*\
np.eye(train_x .shape[ 0])))
const =-train_x .shape[ 0]/2.*np.log( 2*np.pi)
return -(kernel_term +logdet +const)
learned_hypers =optimize .minimize(neg_MLL, x0 =np.array([ell_est,post_sig_
‚Ü©!est]),
bounds =((0.01 ,10.), ( 0.01 ,10.)))
ell =learned_hypers .x[0]
post_sig_est =learned_hypers .x[1]
In this instance, we learn a length-scale of 0.299, and a noise standard deviation of 0.24.
Note that the learned noise is extremely close to the true noise, which helps indicate that
our GP is a very well-specified to this problem.
In general, it is crucial to put careful thought into selecting the kernel and initializing the
hyperparameters. While marginal likelihood optimization can be relatively robust to ini-
tialization, it is not immune to poor initializations. Try running the above script with a
variety of initializations and see what results you find.
Now, let‚Äôs make predictions with these learned hypers.
821 Gaussian Process Inference
K_x_xstar =d2l.rbfkernel(train_x, test_x, ls =ell)
K_x_x =d2l.rbfkernel(train_x, train_x, ls =ell)
K_xstar_xstar =d2l.rbfkernel(test_x, test_x, ls =ell)
post_mean =K_x_xstar .T@np.linalg .inv((K_x_x +\
post_sig_est **2*np.eye(train_x .shape[ 0]))) @train_y
post_cov =K_xstar_xstar -K_x_xstar .T@np.linalg .inv((K_x_x +\
post_sig_est **2*np.eye(train_x .shape[ 0]))) @K_x_xstar
lw_bd =post_mean -2*np.sqrt(np .diag(post_cov))
up_bd =post_mean +2*np.sqrt(np .diag(post_cov))
d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y, linewidth =2.)
d2l.plt.plot(test_x, post_mean, linewidth =2.)
d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )
d2l.plt.legend([ 'Observed Data ','True Function ','Predictive Mean ','95%Set‚ê£
‚Ü©!on True Func '])
d2l.plt.show()
We see the posterior mean in orange almost perfectly matches the true noise free function!
Note that the 95% credible set we are showing is for the latent noise free (true) function,
andnotthedatapoints. Weseethatthiscrediblesetentirelycontainsthetruefunction,and
does not seem overly wide or narrow. We would not want nor expect it to contain the data
points. If we wish to have a credible set for the observations, we should compute
lw_bd_observed =post_mean -2*np.sqrt(np .diag(post_cov) +post_sig_est **2)
up_bd_observed =post_mean +2*np.sqrt(np .diag(post_cov) +post_sig_est **2)
There are two sources of uncertainty, epistemic uncertainty, representing reducible uncer-
tainty, and aleatoric orirreducible uncertainty. The epistemic uncertainty here represents
uncertainty about the true values of the noise free function. This uncertainty should grow
as we move away from the data points, as away from the data there are a greater variety of
function values consistent with our data. As we observe more and more data, our beliefs
about the true function become more confident, and the epistemic uncertainty disappears.
Thealeatoric uncertainty in this instance is the observation noise, since the data are given
to us with this noise, and it cannot be reduced.
Theepistemic uncertaintyinthedataiscapturedbyvarianceofthelatentnoisefreefunction
np.diag(post_cov). The aleatoric uncertaintyiscapturedbythenoisevariancepost_sig_est**2.
822 Gaussian Processes
Unfortunately, people are often careless about how they represent uncertainty, with many
papers showing error bars that are completely undefined, no clear sense of whether we are
visualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with
noise standard deviations, standard deviations with standard errors, confidence intervals
with credible sets, and so on. Without being precise about what the uncertainty represents,
it is essentially meaningless.
In the spirit of playing close attention to what our uncertainty represents, it is crucial to
note that we are taking twotimes thesquareroot of our variance estimate for the noise free
function. Since our predictive distribution is Gaussian, this quantity enables us to form a
95% credible set, representing our beliefs about the interval which is 95% likely to contain
the ground truth function. The noise variance is living on a completely different scale, and
is much less interpretable.
Finally, let‚Äôs take a look at 20 posterior samples. These samples tell us what types of
functions we believe might fit our data, a posteriori.
post_samples =np.random .multivariate_normal(post_mean, post_cov, size =20)
d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y, linewidth =2.)
d2l.plt.plot(test_x, post_mean, linewidth =2.)
d2l.plt.plot(test_x, post_samples .T, color ='gray ', alpha =0.25 )
d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )
plt.legend([ 'Observed Data ','True Function ','Predictive Mean ','Posterior ‚ê£
‚Ü©!Samples '])
d2l.plt.show()
In basic regression applications, it is most common to use the posterior predictive mean
and standard deviation as a point predictor and metric for uncertainty, respectively. In
more advanced applications, such as Bayesian optimization with Monte Carlo acquisition
functions, or Gaussian processes for model-based RL, it often necessary to take posterior
samples. However, even if not strictly required in the basic applications, these samples
give us more intuition about the fit we have for the data, and are often useful to include in
visualizations.
18.3.5Making LifeEasy with GPyTorch
As we have seen, it is actually pretty easy to implement basic Gaussian process regres-
sion entirely from scratch. However, as soon as we want to explore a variety of kernel
823 Gaussian Process Inference
262
263choices, considerapproximateinference(whichisneededevenforclassification), combine
GPs with neural networks, or even have a dataset larger than about 10,000 points, then an
implementationfromscratchbecomesunwieldyandcumbersome. Someofthemosteffec-
tive methods for scalable GP inference, such as SKI (also known as KISS-GP), can require
hundredsoflinesofcodeimplementingadvancednumericallinearalgebraroutines.
In these cases, the GPyTorch library will make our lives a lot easier. We‚Äôll be discussing
GPyTorchmoreinfuturenotebooksonGaussianprocessnumerics,andadvancedmethods.
The GPyTorch library contains many examples262. To get a feel for the package, we will
walkthroughthe simpleregressionexample263,showinghowitcanbeadaptedtoreproduce
ouraboveresultsusingGPyTorch. Thismayseemlikealotofcodetosimplyreproducethe
basicregressionabove,andinasense,itis. Butwecanimmediatelyuseavarietyofkernels,
scalable inference techniques, and approximate inference, by only changing a few lines of
code from below, instead of writing potentially thousands of lines of new code.
# First let's convert our data into tensors for use with PyTorch
train_x =torch .tensor(train_x)
train_y =torch .tensor(train_y)
test_y =torch .tensor(test_y)
# We are using exact GP inference with a zero mean and RBF kernel
class ExactGPModel (gpytorch .models .ExactGP):
def __init__ (self , train_x, train_y, likelihood):
super (ExactGPModel, self ).__init__ (train_x, train_y, likelihood)
self .mean_module =gpytorch .means .ZeroMean()
self .covar_module =gpytorch .kernels .ScaleKernel(
gpytorch .kernels .RBFKernel())
def forward (self , x):
mean_x =self .mean_module(x)
covar_x =self .covar_module(x)
return gpytorch .distributions .MultivariateNormal(mean_x, covar_x)
This code block puts the data in the right format for GPyTorch, and specifies that we are
using exact inference, as well the mean function (zero) and kernel function (RBF) that
we want to use. We can use any other kernel very easily, by calling, for instance, gpy-
torch.kernels.matern_kernel(), or gpyotrch.kernels.spectral_mixture_kernel(). So far, we
have only discussed exact inference, where it is possible to infer a predictive distribution
without making any approximations. For Gaussian processes, we can only perform exact
inferencewhenwehaveaGaussianlikelihood; morespecifically, whenweassumethatour
observationsaregeneratedasanoise-freefunctionrepresentedbyaGaussianprocess,plus
Gaussiannoise. Infuturenotebooks, wewillconsiderothersettings, suchasclassification,
where we cannot make these assumptions.
# Initialize Gaussian likelihood
likelihood =gpytorch .likelihoods .GaussianLikelihood()
model =ExactGPModel(train_x, train_y, likelihood)
training_iter =50
# Find optimal model hyperparameters
(continues on next page)
824 Gaussian Processes
(continued from previous page)
model .train()
likelihood .train()
# Use the adam optimizer, includes GaussianLikelihood parameters
optimizer =torch .optim .Adam(model .parameters(), lr =0.1)
# Set our loss as the negative log GP marginal likelihood
mll =gpytorch .mlls .ExactMarginalLogLikelihood(likelihood, model)
Here, we explicitly specify the likelihood we want to use (Gaussian), the objective we will
use for training kernel hyperparameters (here, the marginal likelihood), and the procedure
we we want to use for optimizing that objective (in this case, Adam). We note that while
we are using Adam, which is a ‚Äústochastic‚Äù optimizer, in this case, it is full-batch Adam.
Because the marginal likelihood does not factorize over data instances, we cannot use an
optimizer over ‚Äúmini-batches‚Äù of data and be guaranteed convergence. Other optimizers,
suchasL-BFGS,arealsosupportedbyGPyTorch. Unlikeinstandarddeeplearning, doing
a good job of optimizing the marginal likelihood corresponds strongly with good general-
ization, which often inclines us towards powerful optimizers like L-BFGS, assuming they
are not prohibitively expensive.
for iinrange (training_iter):
# Zero gradients from previous iteration
optimizer .zero_grad()
# Output from model
output =model(train_x)
# Calc loss and backprop gradients
loss =-mll(output, train_y)
loss .backward()
ifi%10==0:
print (f'Iter {i+1:d}/{training_iter :d}- Loss: {loss .item() :.3f}'
f'squared lengthscale: '
f'{model .covar_module .base_kernel .lengthscale .item() :.3f}'
f'noise variance: {model .likelihood .noise .item() :.3f}')
optimizer .step()
Iter 1/50-Loss: 1.000 squared lengthscale: 0.693 noise variance: 0.693
Iter 11/50-Loss: 0.711 squared lengthscale: 0.490 noise variance: 0.312
Iter 21/50-Loss: 0.451 squared lengthscale: 0.506 noise variance: 0.127
Iter 31/50-Loss: 0.330 squared lengthscale: 0.485 noise variance: 0.055
Iter 41/50-Loss: 0.344 squared lengthscale: 0.472 noise variance: 0.038
Here we actually run the optimization procedure, outputting the values of the loss every 10
iterations.
# Get into evaluation (predictive posterior) mode
test_x =torch .tensor(test_x)
model .eval()
likelihood .eval()
observed_pred =likelihood(model(test_x))
The above codeblock enables us to make predictions on our test inputs.
825 Gaussian Process Inference
with torch .no_grad():
# Initialize plot
f, ax =d2l.plt.subplots( 1,1, figsize =(4,3))
# Get upper and lower bounds for 95\% credible set (in this case, in
# observation space)
lower, upper =observed_pred .confidence_region()
ax.scatter(train_x .numpy(), train_y .numpy())
ax.plot(test_x .numpy(), test_y .numpy(), linewidth =2.)
ax.plot(test_x .numpy(), observed_pred .mean .numpy(), linewidth =2.)
ax.fill_between(test_x .numpy(), lower .numpy(), upper .numpy(), alpha =0.25 )
ax.set_ylim([ -1.5,1.5])
ax.legend([ 'True Function ','Predictive Mean ','Observed Data ',
'95%Credible Set '])
Finally, we plot the fit.
We see the fits are virtually identical. A few things to note: GPyTorch is working with
squared length-scales and observation noise. For example, our learned noise standard de-
viation in the for scratch code is about 0.283. The noise variance found by GPyTorch is
0.810.2832. In the GPyTorch plot, we also show the credible set in the observation
spacerather than the latent function space, to demonstrate that they indeed cover the ob-
served datapoints.
18.3.6Summary
We can combine a Gaussian process prior with data to form a posterior, which we use to
make predictions. We can also form a marginal likelihood, which is useful for automatic
learningofkernelhyperparameters,whichcontrolpropertiessuchastherateofvariationof
theGaussianprocess. Themechanicsofformingtheposteriorandlearningkernelhyperpa-
rameters for regression are simple, involving about a dozen lines of code. This notebook is
agoodreferenceforanyreaderwantingtoquicklyget‚Äúupandrunning‚ÄùwithGaussianpro-
cesses. We also introduced the GPyTorch library. Although the GPyTorch code for basic
regression is relatively long, it can be trivially modified for other kernel functions, or more
advanced functionality we will discuss in future notebooks, such as scalable inference, or
non-Gaussian likelihoods for classification.
18.3.7Exercises
826 Gaussian Processes
1.We have emphasized the importance of learning kernel hyperparameters, and the effect
of hyperparameters and kernels on the generalization properties of Gaussian processes.
Tryskippingthestepwherewelearnhypers,andinsteadguessavarietyoflength-scales
and noise variances, and check their effect on predictions. What happens when you use
a large length-scale? A small length-scale? A large noise variance? A small noise
variance?
2.We have said that the marginal likelihood is not a convex objective, but that hyperpa-
rameterslikelength-scaleandnoisevariancecanbereliablyestimatedinGPregression.
Thisisgenerallytrue‚Äîinfact,themarginallikelihoodis muchbetteratlearninglength-
scale hyperparameters than conventional approaches in spatial statistics, which involve
fittingempiricalautocorrelationfunctions(‚Äúcovariograms‚Äù). Arguably,thebiggestcon-
tributionfrommachinelearningtoGaussianprocessresearch,atleastbeforerecentwork
onscalableinference,wastheintroductionofthemarginallkelihoodforhyperparameter
learning.
However , different pairings of even these parameters provide interpretably different plau-
sible explanations for many datasets, leading to local optima in our objective. If we use a
large length-scale, then we assume the true underlying function is slowly varying. If the
observed data arevarying significantly, then the only we can plausibly have a large length-
scaleiswithalargenoise-variance. Ifweuseasmalllength-scale,ontheotherhand,ourfit
will be very sensitive to the variations in the data, leaving little room to explain variations
with noise (aleatoric uncertainty).
Try seeing if you can find these local optima: initialize with very large length-scale with
large noise, and small length-scales with small noise. Do you converge to different solu-
tions?
3.We have said that a fundamental advantage of Bayesian methods is in naturally repre-
sentingepistemic uncertainty. In the above example, we cannot fully see the effects of
epistemic uncertainty. Try instead to predict with test_x = np.linspace(0, 10,
1000). What happens to the 95% credible set as your predictions move beyond the
data? Does it cover the true function in that interval? What happens if you only visual-
ize aleatoric uncertainty in that region?
4.Try running the above example, but instead with 10,000, 20,000 and 40,000 training
points,andmeasuretheruntimes. Howdoesthetrainingtimescale? Alternatively,how
do the runtimes scale with the number of test points? Is it different for the predictive
mean and the predictive variance? Answer this question both by theoretically working
out the training and testing time complexities, and by running the code above with a
different number of points.
5.Try running the GPyTorch example with different covariance functions, such as the
Matern kernel. How do the results change? How about the spectral mixture kernel,
found in the GPyTorch library? Are some easier to train the marginal likelihood than
others? Are some more valuable for long-range versus short-range predictions?
6.In our GPyTorch example, we plotted the predictive distribution including observation
827 Gaussian Process Inference
264noise, while in our ‚Äúfrom scratch‚Äù example, we only included epistemic uncertainty.
Re-do the GPyTorch example, but this time only plotting epistemic uncertainty, and
comparetothefrom-scratchresults. Dothepredictivedistributionsnowlookthesame?
(They should.)
Discussions264.
19 Hyperparameter Optimization
Aaron Klein (Amazon),Matthias Seeger (Amazon), andCedric Archambeau (Ama-
zon)
The performance of every machine learning model depends on its hyperparameters. They
controlthelearningalgorithmorthestructureoftheunderlyingstatisticalmodel. However,
there is no general way to choose hyperparameters in practice. Instead, hyperparameters
are often set in a trial-and-error manner or sometimes left to their default values by practi-
tioners, leading to suboptimal generalization.
Hyperparameter optimization provides a systematic approach to this problem, by casting
it as an optimization problem: a good set of hyperparameters should (at least) minimize a
validation error. Compared to most other optimization problems arising in machine learn-
ing,hyperparameteroptimizationisanestedone,whereeachiterationrequirestrainingand
validating a machine learning model.
In this chapter, we will first introduce the basics of hyperparameter optimization. We will
alsopresentsomerecentadvancementsthatimprovetheoverallefficiencyofhyperparame-
ter optimization by exploiting cheap-to-evaluate proxies of the original objective function.
At the end of this chapter, you should be able to apply state-of-the-art hyperparameter
optimization techniques to optimize the hyperparameter of your own machine learning al-
gorithm.
19.1What Is HyperparameterOptimization?
As we have seen in the previous chapters, deep neural networks come with a large number
ofparametersorweightsthatarelearnedduringtraining. Ontopofthese,everyneuralnet-
work has additional hyperparameters that need to be configured by the user. For example,
to ensure that stochastic gradient descent converges to a local optimum of the training loss
(seeChapter 12 ), we have to adjust the learning rate and batch size. To avoid overfitting on
trainingdatasets,wemighthavetosetregularizationparameters,suchasweightdecay(see
Section 3.7 ) or dropout (see Section 5.6 ). We can define the capacity and inductive bias of
the model by setting the number of layers and number of units or filters per layer (i.e., the
effective number of weights).
828
829 What Is Hyperparameter Optimization?
Unfortunately, we cannot simply adjust these hyperparameters by minimizing the training
loss, because this would lead to overfitting on the training data. For example, setting reg-
ularization parameters, such as dropout or weight decay to zero leads to a small training
loss, but might hurt the generalization performance.
Set Hyperparameters
Train
Evaluate DeployLoop until validation
performance is maximised
tFig. 19.1.1 Typical workÔ¨Çow in machine learning that consists of training the model multiple times
with different hyperparameters.
Withouta different form of automation, hyperparameters haveto be setmanuallyin a trial-
and-errorfashion,inwhatamountstoatime-consuminganddifficultpartofmachinelearn-
ing workflows. For example, consider training a ResNet (see Section 8.6 ) on CIFAR-10,
which requires more than 2 hours on an Amazon Elastic Cloud Compute (EC2) g4dn.
xlarge instance. Even just trying ten hyperparameter configurations in sequence, this
would already take us roughly one day. To make matters worse, hyperparameters are usu-
allynotdirectlytransferableacrossarchitecturesanddatasets( Bardenet etal.,2013,Feurer
et al., 2022,Wistubaet al., 2018), and need to be re-optimized for every new task. Also,
for most hyperparameters, there are no rule-of-thumbs, and expert knowledge is required
to find sensible values.
Hyperparameter optimization (HPO) algorithms are designed to tackle this problem in a
principled and automated fashion ( Feurer and Hutter, 2018 ), by framing it as a global op-
timization problem. The default objective is the error on a hold-out validation dataset, but
could in principle be any other business metric. It can be combined with or constrained by
secondary objectives, such as training time, inference time, or model complexity.
Recently, hyperparameter optimization has been extended to neural architecture search
(NAS)(Elskenet al., 2018,Wistubaet al., 2019), where the goal is to find entirely new
neural network architectures. Compared to classical HPO, NAS is even more expensive in
terms of computation and requires additional efforts to remain feasible in practice. Both,
HPOandNAScanbeconsideredassub-fieldsofAutoML( Hutteretal.,2019),whichaims
to automate the entire ML pipeline.
In this section we will introduce HPO and show how we can automatically find the best
hyperparameters of the logistic regression example introduced in Section 4.5 .
19.1.1TheOptimizationProblem
We will start with a simple toy problem: searching for the learning rate of the multi-class
logisticregressionmodel SoftmaxRegression fromSection4.5 tominimizethevalidation
830 Hyperparameter Optimization
error on the Fashion MNIST dataset. While other hyperparameters like batch size or num-
ber of epochs are also worth tuning, we focus on learning rate alone for simplicity.
import numpy asnp
import torch
from scipy import stats
from torch import nn
from d2l import torch asd2l
Before we can run HPO, we first need to define two ingredients: the objective function and
the configuration space.
TheObjectiveFunction
The performance of a learning algorithm can be seen as a function ùëì:X! Rthat maps
from the hyperparameter space x2Xto the validation loss. For every evaluation of ùëì¬πx¬∫,
we have to train and validate our machine learning model, which can be time and compute
intensive in the case of deep neural networks trained on large datasets. Given our criterion
ùëì¬πx¬∫our goal is to find x‚òÖ2argminx2Xùëì¬πx¬∫.
Thereisnosimplewaytocomputegradientsof ùëìwithrespectto x,becauseitwouldrequire
to propagate the gradient through the entire training process. While there is recent work
(Franceschi etal.,2017,Maclaurin etal.,2015)todriveHPObyapproximate‚Äúhypergradi-
ents‚Äù, none of the existing approaches are competitive with the state-of-the-art yet, and we
willnotdiscussthemhere. Furthermore,thecomputationalburdenofevaluating ùëìrequires
HPO algorithms to approach the global optimum with as few samples as possible.
The training of neural networks is stochastic (e.g., weights are randomly initialized, mini-
batchesarerandomlysampled),sothatourobservationswillbenoisy: ùë¶ùëì¬πx¬∫¬∏ùúñ,where
we usually assume that the ùúñùëÅ¬π0,ùúé¬∫observation noise is Gaussian distributed.
Faced with all these challenges, we usually try to identify a small set of well performing
hyperparameter configurations quickly, instead of hitting the global optima exactly. How-
ever, due to large computational demands of most neural networks models, even this can
take days or weeks of compute. We will explore in Section 19.4 how we can speed-up the
optimization process by either distributing the search or using cheaper-to-evaluate approx-
imations of the objective function.
We begin with a method for computing the validation error of a model.
class HPOTrainer (d2l .Trainer): #@save
def validation_error (self ):
self .model .eval()
accuracy =0
val_batch_idx =0
for batch inself .val_dataloader:
with torch .no_grad():
x, y =self .prepare_batch(batch)
y_hat =self .model(x)
(continues on next page)
831 What Is Hyperparameter Optimization?
(continued from previous page)
accuracy +=self .model .accuracy(y_hat, y)
val_batch_idx +=1
return 1-accuracy /val_batch_idx
We optimize validation error with respect to the hyperparameter configuration config,
consistingofthe learning_rate . Foreachevaluation,wetrainourmodelfor max_epochs
epochs, then compute and return its validation error:
def hpo_objective_softmax_classification (config, max_epochs =8):
learning_rate =config[ "learning_rate "]
trainer =d2l.HPOTrainer(max_epochs =max_epochs)
data =d2l.FashionMNIST(batch_size =16)
model =d2l.SoftmaxRegression(num_outputs =10, lr =learning_rate)
trainer .fit(model =model, data =data)
return trainer .validation_error() .detach() .numpy()
TheConfiguration Space
Along with the objective function ùëì¬πx¬∫, we also need to define the feasible set x2Xto
optimize over, known as configuration space orsearch space . For our logistic regression
example, we will use:
config_space ={"learning_rate ": stats .loguniform( 1e-4 ,1)}
Here we use the use the loguniform object from SciPy, which represents a uniform distri-
bution between -4 and -1 in the logarithmic space. This object allows us to sample random
variables from this distribution.
Eachhyperparameterhasadatatype,suchas floatforlearning_rate ,aswellasaclosed
bounded range (i.e., lower and upper bounds). We usually assign a prior distribution (e.g,
uniformorlog-uniform)toeachhyperparametertosamplefrom. Somepositiveparameters,
such as learning_rate , are best represented on a logarithmic scale as optimal values can
differ by several orders of magnitude, while others, such as momentum, come with linear
scale.
Below we show a simple example of a configuration space consisting of typical hyperpa-
rameters of a multi-layer perceptron including their type and standard ranges.
: Example configuration space of multi-layer perceptron
Table 19.1.1: label: tab_example_configspace
832 Hyperparameter Optimization
Name Type Hyperparameter
Rangeslog-scale
learning rate float :math:‚Äò [10^{-
6},10^{-1}]‚Äòyes
batch size integer ¬ª8,256¬º yes
momentum float ¬ª0,0.99¬º no
activation function categorical :mat
h:{textrm{tanh}
, textrm{relu}}
number of units integer ¬ª32,1024¬º yes
number of layers integer ¬ª1,6¬º no
In general, the structure of the configuration space Xcan be complex and it can be quite
different from Rùëë. In practice, some hyperparameters may depend on the value of others.
For example, assume we try to tune the number of layers for a multi-layer perceptron, and
for each layer the number of units. The number of units of the ùëô-th layer is relevant only if
the network has at least ùëô¬∏1layers. These advanced HPO problems are beyond the scope
of this chapter. We refer the interested reader to ( Baptista and Poloczek, 2018 ,Hutteret
al., 2011,Jenattonetal., 2017).
The configuration space plays an important role for hyperparameter optimization, since
no algorithms can find something that is not included in the configuration space. On the
other hand, if the ranges are too large, the computation budget to find well performing
configurations might become infeasible.
19.1.2RandomSearch
Random search is the first hyperparameter optimization algorithm we will consider. The
main idea of random search is to independently sample from the configuration space until
a predefined budget (e.g maximum number of iterations) is exhausted, and to return the
bestobservedconfiguration. Allevaluationscanbeexecutedindependentlyinparallel(see
Section 19.3 ), but here we use a sequential loop for simplicity.
errors, values =[], []
num_iterations =5
for iinrange (num_iterations):
learning_rate =config_space[ "learning_rate "].rvs()
print (f"Trial {i}: learning_rate = {learning_rate }")
y=hpo_objective_softmax_classification({ "learning_rate ": learning_rate})
print (f" validation_error = {y}")
values .append(learning_rate)
errors .append(y)
validation_error =0.17070001363754272
The best learning rate is then simply the one with the lowest validation error.
833 19.1 What Is Hyperparameter Optimization?

834 Hyperparameter Optimization
best_idx =np.argmin(errors)
print (f"optimal learning rate = {values[best_idx] }")
optimal learning rate =0.09844872561810249
Duetoitssimplicityandgenerality, randomsearchisoneofthemostfrequentlyusedHPO
algorithms. It does not require any sophisticated implementation and can be applied to
any configuration space as long as we can define some probability distribution for each
hyperparameter.
Unfortunately random search also comes with a few shortcomings. First, it does not adapt
the sampling distribution based on the previous observations it collected so far. Hence,
it is equally likely to sample a poorly performing configuration than a better performing
configuration. Second, the same amount of resources are spent for all configurations, even
thoughsomemayshowpoorinitialperformanceandarelesslikelytooutperformpreviously
seen configurations.
In the next sections we will look at more sample efficient hyperparameter optimization
algorithms that overcome the shortcomings of random search by using a model to guide
the search. We will also look at algorithms that automatically stop the evaluation process
of poorly performing configurations to speed up the optimization process.
19.1.3Summary
In this section we introduced hyperparameter optimization (HPO) and how we can phrase
it as a global optimization by defining a configuration space and an objective function.
We also implemented our first HPO algorithm, random search, and applied it on a simple
softmax classification problem.
Whilerandomsearchisverysimple, itisthebetteralternativetogridsearch, whichsimply
evaluates a fixed set of hyperparameters. Random search somewhat mitigates the curse
of dimensionality ( Bellman, 1966 ), and can be far more efficient than grid search if the
criterion most strongly depends on a small subset of the hyperparameters.
19.1.4Exercises
835 What Is Hyperparameter Optimization?
2651.In this chapter, we optimize the validation error of a model after training on a disjoint
training set. For simplicity, our code uses Trainer.val_dataloader , which maps to a
loader around FashionMNIST.val .
1.Convince yourself (by looking at the code) that this means we use the original Fash-
ionMNISTtrainingset(60000examples)fortraining,andtheoriginal testset(10000
examples) for validation.
2.Whycouldthispracticebeproblematic? Hint: Re-read Section3.6 ,especiallyabout
model selection .
3.What should we have done instead?
2.Westatedabovethathyperparameteroptimizationbygradientdescentisveryhardtodo.
Considerasmallproblem,suchastrainingatwo-layerperceptronontheFashionMNIST
dataset ( Section 5.2 ) with a batch size of 256. We would like to tune the learning rate
of SGD in order to minimize a validation metric after one epoch of training.
1.Why cannot we use validation errorfor this purpose? What metric on the validation
set would you use?
2.Sketch (roughly) the computational graph of the validation metric after training for
oneepoch. Youmayassumethatinitialweightsandhyperparameters(suchaslearn-
ing rate) are input nodes to this graph. Hint: Re-read about computational graphs in
Section 5.3 .
3.Givearoughestimateofthenumberoffloatingpointvaluesyouneedtostoreduring
a forward pass on this graph. Hint: FashionMNIST has 60000 cases. Assume the
required memory is dominated by the activations after each layer, and look up the
layer widths in Section 5.2 .
4.Apart from the sheer amount of compute and storage required, what other issues
would gradient-based hyperparameter optimization run into? Hint: Re-read about
vanishing and exploding gradients in Section 5.4 .
5.Advanced : Read (Maclaurin et al., 2015) for an elegant (yet still somewhat unprac-
tical) approach to gradient-based HPO.
3.Grid search is another HPO baseline, where we define an equi-spaced grid for each hy-
perparameter,theniterateoverthe(combinatorial)Cartesianproductinordertosuggest
configurations.
1.We stated above that random search can be much more efficient than grid search for
HPOonasizablenumberofhyperparameters, ifthecriterionmoststronglydepends
on a small subset of the hyperparameters. Why is this? Hint: Read ( Bergstraet al.,
2011).
Discussions265.
836 Hyperparameter Optimization
19.2HyperparameterOptimizationAPI
Before we dive into the methodology, we will first discuss a basic code structure that al-
lows us to efficiently implement various HPO algorithms. In general, all HPO algorithms
consideredhereneedtoimplementtwodecisionmakingprimitives, searching andschedul-
ing. First, they need to sample new hyperparameter configurations, which often involves
some kind of search over the configuration space. Second, for each configuration, an HPO
algorithm needs to schedule its evaluation and decide how many resources to allocate for
it. Once we start to evaluate a configuration, we will refer to it as a trial. We map these
decisionstotwoclasses, HPOSearcher andHPOScheduler . Ontopofthat,wealsoprovide
aHPOTuner class that executes the optimization process.
This concept of scheduler and searcher is also implemented in popular HPO libraries, such
as Syne Tune ( Salinaset al., 2022), Ray Tune ( Liawet al., 2018) or Optuna ( Akibaet al.,
2019).
import time
from scipy import stats
from d2l import torch asd2l
19.2.1Searcher
Below we define a base class for searchers, which provides a new candidate configuration
through the sample_configuration function. A simple way to implement this function
would be to sample configurations uniformly at random, as we did for random search in
Section 19.1 . More sophisticated algorithms, such as Bayesian optimization, will make
these decisions based on the performance of previous trials. As a result, these algorithms
are able to sample more promising candidates over time. We add the updatefunction in
order to update the history of previous trials, which can then be exploited to improve our
sampling distribution.
class HPOSearcher (d2l .HyperParameters): #@save
def sample_configuration ()->dict :
raise NotImplementedError
def update (self , config: dict , error: float , additional_info =None ):
pass
The following code shows how to implement our random search optimizer from the pre-
vious section in this API. As a slight extension, we allow the user to prescribe the first
configuration to be evaluated via initial_config , while subsequent ones are drawn at
random.
class RandomSearcher (HPOSearcher): #@save
(continues on next page)
837 Hyperparameter Optimization API
(continued from previous page)
def __init__ (self , config_space: dict , initial_config =None ):
self .save_hyperparameters()
def sample_configuration (self )->dict :
ifself .initial_config isnot None :
result =self .initial_config
self .initial_config =None
else :
result ={
name: domain .rvs()
for name, domain inself .config_space .items()
}
return result
19.2.2Scheduler
Beyond sampling configurations for new trials, we also need to decide when and for how
long to run a trial. In practice, all these decisions are done by the HPOScheduler , which
delegates the choice of new configurations to a HPOSearcher . The suggest method is
called whenever some resource for training becomes available. Apart from invoking sam-
ple_configuration of a searcher, it may also decide upon parameters like max_epochs
(i.e.,howlongtotrainthemodelfor). The updatemethodiscalledwheneveratrialreturns
a new observation.
class HPOScheduler (d2l .HyperParameters): #@save
def suggest (self )->dict :
raise NotImplementedError
def update (self , config: dict , error: float , info =None ):
raise NotImplementedError
To implement random search, but also other HPO algorithms, we only need a basic sched-
ulerthatschedulesanewconfigurationeverytimenewresourcesbecomeavailable.
class BasicScheduler (HPOScheduler): #@save
def __init__ (self , searcher: HPOSearcher):
self .save_hyperparameters()
def suggest (self )->dict :
return self .searcher .sample_configuration()
def update (self , config: dict , error: float , info =None ):
self .searcher .update(config, error, additional_info =info)
19.2.3Tuner
Finally,weneedacomponentthatrunsthescheduler/searcheranddoessomebook-keeping
oftheresults. ThefollowingcodeimplementsasequentialexecutionoftheHPOtrialsthat
838 Hyperparameter Optimization
evaluates one training job after the next and will serve as a basic example. We will later
useSyne Tune for more scalable distributed HPO cases.
class HPOTuner (d2l .HyperParameters): #@save
def __init__ (self , scheduler: HPOScheduler, objective: callable ):
self .save_hyperparameters()
# Bookeeping results for plotting
self .incumbent =None
self .incumbent_error =None
self .incumbent_trajectory =[]
self .cumulative_runtime =[]
self .current_runtime =0
self .records =[]
def run(self , number_of_trials):
for iinrange (number_of_trials):
start_time =time .time()
config =self .scheduler .suggest()
print (f"Trial {i}: config = {config }")
error =self .objective( **config)
error =float (error .cpu() .detach() .numpy())
self .scheduler .update(config, error)
runtime =time .time() -start_time
self .bookkeeping(config, error, runtime)
print (f" error = {error }, runtime = {runtime }")
19.2.4Bookkeepingthe Performanceof HPO Algorithms
With any HPO algorithm, we are mostly interested in the best performing configuration
(calledincumbent ) and its validation error after a given wall-clock time. This is why we
track runtime per iteration, which includes both the time to run an evaluation (call of
objective ) and the time to make a decision (call of scheduler.suggest ). In the se-
quel, wewill plot cumulative_runtime against incumbent_trajectory in order to visu-
alize theany-timeperformance of the HPO algorithm defined in terms of scheduler (and
searcher ). This allows us to quantify not only how well the configuration found by an
optimizer works, but also how quickly an optimizer is able to find it.
@d2l .add_to_class(HPOTuner) #@save
def bookkeeping (self , config: dict , error: float , runtime: float ):
self .records .append({ "config ": config, "error ": error, "runtime ": runtime})
# Check if the last hyperparameter configuration performs better
# than the incumbent
ifself .incumbent isNone orself .incumbent_error >error:
self .incumbent =config
self .incumbent_error =error
# Add current best observed performance to the optimization trajectory
self .incumbent_trajectory .append( self .incumbent_error)
# Update runtime
self .current_runtime +=runtime
self .cumulative_runtime .append( self .current_runtime)
839 Hyperparameter Optimization API
19.2.5Example: Optimizingthe Hyperparametersof a Convolutional
NeuralNetwork
We now use our new implementation of random search to optimize the batch size and
learning rate of the LeNetconvolutional neural network from Section 7.6 . We being by
defining the objective function, which will once more be validation error.
def hpo_objective_lenet (learning_rate, batch_size, max_epochs =10): #@save
model =d2l.LeNet(lr =learning_rate, num_classes =10)
trainer =d2l.HPOTrainer(max_epochs =max_epochs, num_gpus =1)
data =d2l.FashionMNIST(batch_size =batch_size)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
trainer .fit(model =model, data =data)
validation_error =trainer .validation_error()
return validation_error
We also need to define the configuration space. Moreover, the first configuration to be
evaluated is the default setting used in Section 7.6 .
config_space ={
"learning_rate ": stats .loguniform( 1e-2 ,1),
"batch_size ": stats .randint( 32,256),
}
initial_config ={
"learning_rate ":0.1,
"batch_size ":128,
}
Now we can start our random search:
searcher =RandomSearcher(config_space, initial_config =initial_config)
scheduler =BasicScheduler(searcher =searcher)
tuner =HPOTuner(scheduler =scheduler, objective =hpo_objective_lenet)
tuner .run(number_of_trials =5)
error =0.9000097513198853 , runtime =62.85189199447632
Belowweplottheoptimizationtrajectoryoftheincumbenttogettheany-timeperformance
of random search:
840 Hyperparameter Optimization

841 Hyperparameter Optimization API
board =d2l.ProgressBoard(xlabel ="time ", ylabel ="error ")
for time_stamp, error inzip(
tuner .cumulative_runtime, tuner .incumbent_trajectory
):
board .draw(time_stamp, error, "random search ", every_n =1)
19.2.6ComparingHPO Algorithms
Just as with training algorithms or model architectures, it is important to understand how
to best compare different HPO algorithms. Each HPO run depends on two major sources
of randomness: the random effects of the training process, such as random weight initial-
ization or mini-batch ordering, and the intrinsic randomness of the HPO algorithm itself,
such as the random sampling of random search. Hence, when comparing different algo-
rithms, it is crucial to run each experiment several times and report statistics, such as mean
or median, across a population of multiple repetitions of an algorithm based on different
seeds of the random number generator.
To illustrate this, we compare random search (see Section 19.1.2 ) and Bayesian optimiza-
tion (Snoeket al., 2012) on tuning the hyperparameters of a feed-forward neural network.
Each algorithm was evaluated 50times with a different random seed. The solid line indi-
cates the average performance of the incumbent across these 50repetitions and the dashed
line the standard deviation. We can see that random search and Bayesian optimization per-
form roughly the same up to ~1000 seconds, but Bayesian optimization can make use of
the past observation to identify better configurations and thus quickly outperforms random
search afterwards.
tFig. 19.2.1 Example any-time performance plot to compare two algorithms A and B.
842 Hyperparameter Optimization
26619.2.7Summary
This section laid out a simple, yet flexible interface to implement various HPO algorithms
that we will look at in this chapter. Similar interfaces can be found in popular open-source
HPO frameworks. We also looked at how we can compare HPO algorithms, and potential
pitfall one needs to be aware.
19.2.8Exercises
1.The goal of this exercise is to implement the objective function for a slightly more chal-
lengingHPOproblem,andtorunmorerealisticexperiments. Wewillusethetwohidden
layer MLP DropoutMLP implemented in Section 5.6 .
1.Code up the objective function, which should depend on all hyperparameters of the
modeland batch_size . Use max_epochs=50 . GPUsdonothelphere,so num_gpus=0 .
Hint: Modify hpo_objective_lenet .
2.Chooseasensiblesearchspace,where num_hiddens_1 ,num_hiddens_2 areintegers
in¬ª8,1024¬º,anddropoutvaluesliein ¬ª0,0.95¬º,while batch_size liesin¬ª16,384¬º.
Provide code for config_space , using sensible distributions from scipy.stats .
3.Run random search on this example with number_of_trials=20 and plot the re-
sults. Make sure to first evaluate the default configuration of Section 5.6 , which
isinitial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256,
'dropout_1': 0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256} .
2.In this exercise, you will implement a new searcher (subclass of HPOSearcher ) which
makesdecisionsbasedonpastdata. Itdependsonparameters probab_local ,num_init_random .
Itssample_configuration method works as follows. For the first num_init_random
calls,dothesameas RandomSearcher.sample_configuration . Otherwise,withprob-
ability 1 - probab_local ,dothesameas RandomSearcher.sample_configuration .
Otherwise, pick the configuration which attained the smallest validation error so far,
select one of its hyperparameters at random, and sample its value randomly like in
RandomSearcher.sample_configuration , but leave all other values the same. Re-
turn this configuration, which is identical to the best configuration so far, except in this
one hyperparameter.
1.Code up this new LocalSearcher . Hint: Your searcher requires config_space as
argument at construction. Feel free to use a member of type RandomSearcher . You
will also have to implement the updatemethod.
2.Re-run the experiment from the previous exercise, but using your new searcher in-
stead of RandomSearcher . Experiment with different values for probab_local ,
num_init_random . However, note that a proper comparison between different HPO
methods requires repeating experiments several times, and ideally considering a
number of benchmark tasks.
Discussions266.
843 Asynchronous Random Search
19.3AsynchronousRandom Search
Aswehaveseenintheprevious Section19.2 , wemighthavetowaithoursorevendaysbe-
fore random search returns a good hyperparameter configuration, because of the expensive
evaluation of hyperparameter configurations. In practice, we have often access to a pool of
resources such as multiple GPUs on the same machine or multiple machines with a single
GPU. This begs the question: Howdo weeÔ¨Äiciently distributerandomsearch?
Ingeneral,wedistinguishbetweensynchronousandasynchronousparallelhyperparameter
optimization (see Fig. 19.3.1 ). In the synchronous setting, we wait for all concurrently
running trials to finish, before we start the next batch. Consider configuration spaces that
contain hyperparameters such as the number of filters or number of layers of a deep neural
network. Hyperparameter configurations that contain a larger number of layers of filters
willnaturallytakemoretimetofinish,andallothertrialsinthesamebatchwillhavetowait
atsynchronisationpoints(greyareain Fig.19.3.1 )beforewecancontinuetheoptimization
process.
In the asynchronous setting we immediately schedule a new trial as soon as resources be-
come available. This will optimally exploit our resources, since we can avoid any synchro-
nisation overhead. For random search, each new hyperparameter configuration is chosen
independentlyofallothers,andinparticularwithoutexploitingobservationsfromanyprior
evaluation. This means we can trivially parallelize random search asynchronously. This is
not straight-forward with more sophisticated methods that make decision based on previ-
ous observations (see Section 19.5 ). While we need access to more resources than in the
sequential setting, asynchronousrandom searchexhibits a linear speed-up, in that a certain
performance is reached ùêætimes faster if ùêætrials can be run in parallel.
Sequential Trial-0 Trial-1 Trial-2 Trial-3 Trial-4
Synchronous
AsynchronousTrial-0 Trial-2
Trial-3
Trial-0
TimeTrial-5
Trial-1
Trial-1Trial-4
Trial-5
Trial-3
Trial-2Trial-4
Trial-5
tFig. 19.3.1 Distributing the hyperparameter optimization process either synchronously or
asynchronously. Compared to the sequential setting, we can reduce the overall wall-clock
time while keep the total compute constant. Synchronous scheduling might lead to idling
workers in the case of stragglers.
In this notebook, we will look at asynchronous random search that, where trials are exe-
cuted in multiple python processes on the same machine. Distributed job scheduling and
execution is difficult to implement from scratch. We will use Syne Tune (Salinaset al.,
2022), which provides us with a simple interface for asynchronous HPO. Syne Tune is de-
844 Hyperparameter Optimization
signed to be run with different execution back-ends, and the interested reader is invited to
study its simple APIs in order to learn more about distributed HPO.
import logging
from d2l import torch asd2l
logging .basicConfig(level =logging .INFO)
from syne_tune import StoppingCriterion, Tuner
from syne_tune .backend .python_backend import PythonBackend
from syne_tune .config_space import loguniform, randint
from syne_tune .experiments import load_experiment
from syne_tune .optimizer .baselines import RandomSearch
INFO:root:SageMakerBackend isnot imported since dependencies are missing .You‚ê£
‚Ü©!can install them with
pip install 'syne-tune[extra] '
AWS dependencies are not imported since dependencies are missing .You can ‚ê£
‚Ü©!install them with
pip install 'syne-tune[aws] '
or(for everything)
pip install 'syne-tune[extra] '
AWS dependencies are not imported since dependencies are missing .You can ‚ê£
‚Ü©!install them with
pip install 'syne-tune[aws] '
or(for everything)
pip install 'syne-tune[extra] '
INFO:root:Ray Tune schedulers and searchers are not imported since ‚ê£
‚Ü©!dependencies are missing .You can install them with
pip install 'syne-tune[raytune] '
or(for everything)
pip install 'syne-tune[extra] '
19.3.1ObjectiveFunction
First, we have to define a new objective function such that it now returns the performance
back to Syne Tune via the reportcallback.
def hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):
from syne_tune import Reporter
from d2l import torch asd2l
model =d2l.LeNet(lr =learning_rate, num_classes =10)
trainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)
data =d2l.FashionMNIST(batch_size =batch_size)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
report =Reporter()
for epoch inrange (1, max_epochs +1):
ifepoch ==1:
# Initialize the state of Trainer
trainer .fit(model =model, data =data)
else :
trainer .fit_epoch()
(continues on next page)
845 Asynchronous Random Search
(continued from previous page)
validation_error =trainer .validation_error() .cpu() .detach() .numpy()
report(epoch =epoch, validation_error =float (validation_error))
Note that the PythonBackend of Syne Tune requires dependencies to be imported inside
the function definition.
19.3.2AsynchronousScheduler
First, we define the number of workers that evaluate trials concurrently. We also need to
specify how long we want to run random search, by defining an upper limit on the total
wall-clock time.
n_workers =2# Needs to be <= the number of available GPUs
max_wallclock_time =12*60 # 12 minutes
Next, we state which metric we want to optimize and whether we want to minimize or
maximize this metric. Namely, metricneeds to correspond to the argument name passed
to the reportcallback.
mode ="min"
metric ="validation_error "
We use the configuration space from our previous example. In Syne Tune, this dictionary
can also be used to pass constant attributes to the training script. We make use of this
feature in order to pass max_epochs . Moreover, we specify the first configuration to be
evaluated in initial_config .
config_space ={
"learning_rate ": loguniform( 1e-2 ,1),
"batch_size ": randint( 32,256),
"max_epochs ":10,
}
initial_config ={
"learning_rate ":0.1,
"batch_size ":128,
}
Next, we need to specify the back-end for job executions. Here we just consider the distri-
bution on a local machine where parallel jobs are executed as sub-processes. However, for
large scale HPO, we could run this also on a cluster or cloud environment, where each trial
consumes a full instance.
trial_backend =PythonBackend(
tune_function =hpo_objective_lenet_synetune,
config_space =config_space,
)
846 Hyperparameter Optimization
We can now create the scheduler for asynchronous random search, which is similar in be-
haviour to our BasicScheduler fromSection 19.2 .
scheduler =RandomSearch(
config_space,
metric =metric,
mode =mode,
points_to_evaluate =[initial_config],
)
INFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred ‚ê£
‚Ü©!from config_space
INFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =2737092907
Syne Tune also features a Tuner, where the main experiment loop and bookkeeping is
centralized, and interactions between scheduler and back-end are mediated.
stop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)
tuner =Tuner(
trial_backend =trial_backend,
scheduler =scheduler,
stop_criterion =stop_criterion,
n_workers =n_workers,
print_update_interval =int(max_wallclock_time *0.6),
)
LetusrunourdistributedHPOexperiment. Accordingtoourstoppingcriterion,itwillrun
for about 12 minutes.
tuner .run()
INFO:syne_tune .tuner:results of trials will be saved on /home /ci/syne -tune /
‚Ü©!python -entrypoint -2023 -08-18-19-45-39-958
INFO:root:Detected 4GPUs
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_function_root ‚ê£
‚Ü©!/home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/tune_function --
‚Ü©!tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_checkpoint_dir /
‚Ü©!home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/0/checkpoints
INFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,
‚Ü©!'batch_size ':128,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.1702844732454753 --batch_size 114 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!1/checkpoints
INFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.
(continues on next page)
847 Asynchronous Random Search
(continued from previous page)
‚Ü©!1702844732454753 ,'batch_size ':114,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 0completed .
INFO:syne_tune .tuner:Trial trial_id 1completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.34019846567238493 --batch_size 221 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!2/checkpoints
INFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.
‚Ü©!34019846567238493 ,'batch_size ':221,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.014628124155727769 --batch_size 88--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!3/checkpoints
INFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.
‚Ü©!014628124155727769 ,'batch_size ':88,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 2completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.1114831485450576 --batch_size 142 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!4/checkpoints
INFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.
‚Ü©!1114831485450576 ,'batch_size ':142,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 3completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.014076038679980779 --batch_size 223 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!5/checkpoints
INFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.
‚Ü©!014076038679980779 ,'batch_size ':223,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 4completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.02558173674804846 --batch_size 62--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!6/checkpoints
INFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.
‚Ü©!02558173674804846 ,'batch_size ':62,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 5completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.026035979388614055 --batch_size 139 --max_epochs 10--
(continues on next page)
848 Hyperparameter Optimization
(continued from previous page)
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!7/checkpoints
INFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.
‚Ü©!026035979388614055 ,'batch_size ':139,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 6completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.24202494130424274 --batch_size 231 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!8/checkpoints
INFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.
‚Ü©!24202494130424274 ,'batch_size ':231,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 7completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.10483132064775551 --batch_size 145 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!9/checkpoints
INFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.
‚Ü©!10483132064775551 ,'batch_size ':145,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 8completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.017898854850751864 --batch_size 51--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!10/checkpoints
INFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.
‚Ü©!017898854850751864 ,'batch_size ':51,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 9completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.9645419978270817 --batch_size 200 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!11/checkpoints
INFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.
‚Ü©!9645419978270817 ,'batch_size ':200,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 11completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.10559888854748693 --batch_size 40--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!12/checkpoints
INFO:syne_tune .tuner:(trial 12)-scheduled config { 'learning_rate ':0.
(continues on next page)
849 Asynchronous Random Search
(continued from previous page)
‚Ü©!10559888854748693 ,'batch_size ':40,'max_epochs ':10}
INFO:syne_tune .tuner:tuning status (last metric isreported)
trial_id status iter learning_rate batch_size max_epochs epoch ‚ê£
‚Ü©!validation_error worker -time
0Completed 10 0.100000 128 10 10.0 ‚ê£
‚Ü©! 0.277195 64.928907
1Completed 10 0.170284 114 10 10.0 ‚ê£
‚Ü©! 0.286225 65.434195
2Completed 10 0.340198 221 10 10.0 ‚ê£
‚Ü©! 0.218990 59.729758
3Completed 10 0.014628 88 10 10.0 ‚ê£
‚Ü©! 0.899920 81.001636
4Completed 10 0.111483 142 10 10.0 ‚ê£
‚Ü©! 0.268684 64.427400
5Completed 10 0.014076 223 10 10.0 ‚ê£
‚Ü©! 0.899922 61.264475
6Completed 10 0.025582 62 10 10.0 ‚ê£
‚Ü©! 0.399520 75.966186
7Completed 10 0.026036 139 10 10.0 ‚ê£
‚Ü©! 0.899988 62.261541
8Completed 10 0.242025 231 10 10.0 ‚ê£
‚Ü©! 0.257636 58.186485
9Completed 10 0.104831 145 10 10.0 ‚ê£
‚Ü©! 0.273898 59.771699
10InProgress 8 0.017899 51 10 8.0 ‚ê£
‚Ü©! 0.496118 66.999746
11 Completed 10 0.964542 200 10 10.0 ‚ê£
‚Ü©! 0.181600 59.159662
12InProgress 0 0.105599 40 10 - ‚ê£
‚Ü©! - -
2trials running, 11finished ( 11until the end), 436.60 s wallclock -time
INFO:syne_tune .tuner:Trial trial_id 10completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.5846051207380589 --batch_size 35--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!13/checkpoints
INFO:syne_tune .tuner:(trial 13)-scheduled config { 'learning_rate ':0.
‚Ü©!5846051207380589 ,'batch_size ':35,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 12completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.2468891379769198 --batch_size 146 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!14/checkpoints
INFO:syne_tune .tuner:(trial 14)-scheduled config { 'learning_rate ':0.
‚Ü©!2468891379769198 ,'batch_size ':146,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 13completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
(continues on next page)
850 Hyperparameter Optimization
(continued from previous page)
‚Ü©!py--learning_rate 0.12956867470224812 --batch_size 218 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!15/checkpoints
INFO:syne_tune .tuner:(trial 15)-scheduled config { 'learning_rate ':0.
‚Ü©!12956867470224812 ,'batch_size ':218,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 14completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.24900745354561854 --batch_size 103 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!16/checkpoints
INFO:syne_tune .tuner:(trial 16)-scheduled config { 'learning_rate ':0.
‚Ü©!24900745354561854 ,'batch_size ':103,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 15completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.03903577426988046 --batch_size 80--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!17/checkpoints
INFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.
‚Ü©!03903577426988046 ,'batch_size ':80,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 16completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.01846559300690354 --batch_size 183 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-
‚Ü©!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/
‚Ü©!18/checkpoints
INFO:syne_tune .tuner:(trial 18)-scheduled config { 'learning_rate ':0.
‚Ü©!01846559300690354 ,'batch_size ':183,'max_epochs ':10}
INFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping ‚ê£
‚Ü©!there .
INFO:syne_tune .tuner:Stopping trials that may still be running .
INFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /
‚Ü©!ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958
--------------------
Resource summary (last result isreported):
trial_id status iter learning_rate batch_size max_epochs epoch ‚ê£
‚Ü©!validation_error worker -time
0Completed 10 0.100000 128 10 10 ‚ê£
‚Ü©! 0.277195 64.928907
1Completed 10 0.170284 114 10 10 ‚ê£
‚Ü©! 0.286225 65.434195
2Completed 10 0.340198 221 10 10 ‚ê£
‚Ü©! 0.218990 59.729758
3Completed 10 0.014628 88 10 10 ‚ê£
‚Ü©! 0.899920 81.001636
4Completed 10 0.111483 142 10 10 ‚ê£
(continues on next page)
851 Asynchronous Random Search
(continued from previous page)
‚Ü©! 0.268684 64.427400
5Completed 10 0.014076 223 10 10 ‚ê£
‚Ü©! 0.899922 61.264475
6Completed 10 0.025582 62 10 10 ‚ê£
‚Ü©! 0.399520 75.966186
7Completed 10 0.026036 139 10 10 ‚ê£
‚Ü©! 0.899988 62.261541
8Completed 10 0.242025 231 10 10 ‚ê£
‚Ü©! 0.257636 58.186485
9Completed 10 0.104831 145 10 10 ‚ê£
‚Ü©! 0.273898 59.771699
10 Completed 10 0.017899 51 10 10 ‚ê£
‚Ü©! 0.405545 83.778503
11 Completed 10 0.964542 200 10 10 ‚ê£
‚Ü©! 0.181600 59.159662
12 Completed 10 0.105599 40 10 10 ‚ê£
‚Ü©! 0.182500 94.734384
13 Completed 10 0.584605 35 10 10 ‚ê£
‚Ü©! 0.153846 110.965637
14 Completed 10 0.246889 146 10 10 ‚ê£
‚Ü©! 0.215050 65.142847
15 Completed 10 0.129569 218 10 10 ‚ê£
‚Ü©! 0.313873 61.310455
16 Completed 10 0.249007 103 10 10 ‚ê£
‚Ü©! 0.196101 72.519127
17InProgress 9 0.039036 80 10 9 ‚ê£
‚Ü©! 0.369000 73.403000
18InProgress 5 0.018466 183 10 5 ‚ê£
‚Ü©! 0.900263 34.714568
2trials running, 17finished ( 17until the end), 722.84 s wallclock -time
validation_error: best 0.14451533555984497 for trial -id13
--------------------
The logs of all evaluated hyperparameter configurations are stored for further analysis. At
any time during the tuning job, we can easily get the results obtained so far and plot the
incumbent trajectory.
d2l.set_figsize()
tuning_experiment =load_experiment(tuner .name)
tuning_experiment .plot()
WARNING:matplotlib .legend:No artists with labels found to put inlegend .Note ‚ê£
‚Ü©!that artists whose label start with an underscore are ignored when legend() ‚ê£
‚Ü©!iscalled with no argument .
19.3.3Visualizethe AsynchronousOptimization Process
Below we visualize how the learning curves of every trial (each color in the plot represents
a trial) evolve during the asynchronous optimization process. At any point in time, there
are as many trials running concurrently as we have workers. Once a trial finishes, we
852 Hyperparameter Optimization
immediately start the next trial, without waiting for the other trials to finish. Idle time
of workers is reduced to a minimum with asynchronous scheduling.
d2l.set_figsize([ 6,2.5])
results =tuning_experiment .results
for trial_id inresults .trial_id .unique():
df=results[results[ "trial_id "]==trial_id]
d2l.plt.plot(
df["st_tuner_time "],
df["validation_error "],
marker ="o"
)
d2l.plt.xlabel( "wall-clock time ")
d2l.plt.ylabel( "objective function ")
Text( 0,0.5,'objective function ')
19.3.4Summary
Wecanreducethewaitingtimeforrandomsearchsubstantiallybydistributiontrialsacross
parallel resources. In general, we distinguish between synchronous scheduling and asyn-
chronous scheduling. Synchronous scheduling means that we sample a new batch of hy-
perparameter configurations once the previous batch finished. If we have a stragglers -
trials that takes more time to finish than other trials - our workers need to wait at synchro-
nization points. Asynchronous scheduling evaluates a new hyperparameter configurations
853 Multi-Fidelity Hyperparameter Optimization
267
268
269
270
271as soon as resources become available, and, hence, ensures that all workers are busy at
any point in time. While random search is easy to distribute asynchronously and does not
require any change of the actual algorithm, other methods require some additional modifi-
cations.
19.3.5Exercises
1.Consider the DropoutMLP model implemented in Section 5.6 , and used in Exercise 1 of
Section 19.2 .
1.Implementanobjectivefunction hpo_objective_dropoutmlp_synetune tobeused
withSyneTune. Makesurethatyourfunctionreportsthevalidationerrorafterevery
epoch.
2.Using the setup of Exercise 1 in Section 19.2 , compare random search to Bayesian
optimization. If you use SageMaker, feel free to use Syne Tune‚Äôs benchmarking
facilities in order to run experiments in parallel. Hint: Bayesian optimization is
provided as syne_tune.optimizer.baselines.BayesianOptimization .
3.For this exercise, you need to run on an instance with at least 4 CPU cores. For one
ofthemethodsusedabove(randomsearch,Bayesianoptimization),runexperiments
with n_workers=1 ,n_workers=2 ,n_workers=4 , and compare results (incumbent
trajectories). At least for random search, you should observe linear scaling with
respect to the number of workers. Hint: For robust results, you may have to average
over several repetitions each.
2.Advanced . The goal of this exercise is to implement a new scheduler in Syne Tune.
1.Create a virtual environment containing both the d2lbook267andsyne-tune268
sources.
2.Implement the LocalSearcher from Exercise2 in Section 19.2 as a new searcher in
Syne Tune. Hint: Read this tutorial269. Alternatively, you may follow this example
270.
3.Compare your new LocalSearcher with RandomSearch on the DropoutMLP bench-
mark.
Discussions271.
19.4Multi-Fidelity HyperparameterOptimization
Training neural networks can be expensive even on moderate size datasets. Depending
on the configuration space ( Section 19.1.1 ), hyperparameter optimization requires tens to
hundreds of function evaluations to find a well-performing hyperparameter configuration.
Aswehaveseenin Section19.3 ,wecansignificantlyspeeduptheoverallwall-clocktimeof
854 Hyperparameter Optimization
HPObyexploitingparallelresources, butthis does not reducethe total amount ofcompute
required.
In this section, we will show how the evaluation of hyperparameter configurations can be
sped up. Methods such as random search allocate the same amount of resources (e.g.,
number of epochs, training data points) to each hyperparameter evaluation. Fig. 19.4.1
depicts learning curves of a set of neural networks trained with different hyperparameter
configurations. Afterafewepochswearealreadyabletovisuallydistinguishbetweenwell-
performingandsuboptimalconfigurations. However, thelearningcurvesarenoisy, andwe
mightstillrequirethefullamountof100epochstoidentifythebestperformingone.
tFig. 19.4.1 Learning curves of random hyperparameter conÔ¨Ågurations
Multi-fidelityhyperparameteroptimizationallocatesmoreresourcestopromisingconfigu-
rations and stop evaluations of poorly performing ones early. This speeds up the optimiza-
tion process, since we can try a larger number of configurations for the same total amount
of resources.
Moreformally, weexpandourdefinitionin Section19.1.1 , suchthatourobjectivefunction
ùëì¬πx,ùëü¬∫gets an additional input ùëü2¬ªùëümin,ùëüùëöùëéùë•¬º, specifying the amount of resources that
we are willing to spend for the evaluation of configuration x. We assume that the error
ùëì¬πx,ùëü¬∫decreases with ùëü, whereas the computational cost ùëê¬πx,ùëü¬∫increases. Typically, ùëü
represents the number of epochs for training the neural network, but it could also be the
training subset size or the number of cross-validation folds.
from collections import defaultdict
import numpy asnp
from scipy import stats
from d2l import torch asd2l
(continues on next page)
855 Multi-Fidelity Hyperparameter Optimization
(continued from previous page)
d2l.set_figsize()
19.4.1SuccessiveHalving
One of the simplest ways to adapt random search to the multi-fidelity setting is successive
halving(JamiesonandTalwalkar, 2016 ,Karninetal., 2013). Thebasicideaistostartwith
ùëÅconfigurations,forexamplerandomlysampledfromtheconfigurationspace,andtotrain
eachofthemfor ùëüminepochsonly. Wethendiscardafractionoftheworstperformingtrials
and train the remaining ones for longer. Iterating this process, fewer trials run for longer,
until at least one trial reaches ùëüùëöùëéùë•epochs.
More formally, consider a minimum budget ùëümin(for example 1 epoch), a maximum bud-
getùëüùëöùëéùë•, for example max_epochs in our previous example, and a halving constant ùúÇ2
f2,3,...g. For simplicity, assume that ùëüùëöùëéùë•=ùëüminùúÇùêæ, withùêæ2I. The number of initial
configurationsisthen ùëÅ=ùúÇùêæ. Letusdefinethesetofrungs R=fùëümin,ùëüminùúÇ,ùëü minùúÇ2,...,ùëüùëöùëéùë•g.
One round of successive halving proceeds as follows. We start with running ùëÅtrials un-
til the first rung ùëümin. Sorting the validation errors, we keep the top 1¬ùùúÇfraction (which
amounts toùúÇùêæ 1configurations) and discard all the rest. The surviving trials are trained
forthenextrung( ùëüminùúÇepochs),andtheprocessisrepeated. Ateachrung,a 1¬ùùúÇfractionof
trialssurvivesandtheirtrainingcontinueswitha ùúÇtimeslargerbudget. Withthisparticular
choice ofùëÅ, only a single trial will be trained to the full budget ùëüùëöùëéùë•. Once such a round
of successive halving is done, we start the next one with a new set of initial configurations,
iterating until the total budget is spent.
tFig. 19.4.2 Learning curves of random hyperparameter conÔ¨Ågurations.
856 Hyperparameter Optimization
Wesubclassthe HPOScheduler baseclassfrom Section19.2 inordertoimplementsucces-
sive halving, allowing for a generic HPOSearcher object to sample configurations (which,
in our example below, will be a RandomSearcher ). Additionally, the user has to pass the
minimum resource ùëümin, the maximum resource ùëüùëöùëéùë•andùúÇas input. Inside our scheduler,
wemaintainaqueueofconfigurationsthatstillneedtobeevaluatedforthecurrentrung ùëüùëñ.
We update the queue every time we jump to the next rung.
class SuccessiveHalvingScheduler (d2l .HPOScheduler): #@save
def __init__ (self , searcher, eta, r_min, r_max, prefact =1):
self .save_hyperparameters()
# Compute K, which is later used to determine the number of ‚ê£
‚Ü©!configurations
self .K=int(np.log(r_max /r_min) /np.log(eta))
# Define the rungs
self .rung_levels =[r_min *eta **kfor kinrange (self .K+1)]
ifr_max not inself .rung_levels:
# The final rung should be r_max
self .rung_levels .append(r_max)
self .K+=1
# Bookkeeping
self .observed_error_at_rungs =defaultdict( list )
self .all_observed_error_at_rungs =defaultdict( list )
# Our processing queue
self .queue =[]
In the beginning our queue is empty, and we fill it with ùëõ=prefactùúÇùêæconfigurations,
which are first evaluated on the smallest rung ùëümin. Here, prefact allows us to reuse our
code in a different context. For the purpose of this section, we fix prefact =1. Every time
resources become available and the HPOTuner object queries the suggest function, we
return an element from the queue. Once we finish one round of successive halving, which
meansthatweevaluatedallsurvivingconfigurationsonthehighestresourcelevel ùëüùëöùëéùë•and
our queue is empty, we start the entire process again with a new, randomly sampled set of
configurations.
@d2l .add_to_class(SuccessiveHalvingScheduler) #@save
def suggest (self ):
iflen(self .queue) ==0:
# Start a new round of successive halving
# Number of configurations for the first rung:
n0=int(self .prefact *self .eta **self .K)
for _inrange (n0):
config =self .searcher .sample_configuration()
config[ "max_epochs "]=self .r_min # Set r = r_min
self .queue .append(config)
# Return an element from the queue
return self .queue .pop()
When we collected a new data point, we first update the searcher module. Afterwards we
check if we already collect all data points on the current rung. If so, we sort all configura-
tions and push the top1
ùúÇconfigurations into the queue.
857 Multi-Fidelity Hyperparameter Optimization
@d2l .add_to_class(SuccessiveHalvingScheduler) #@save
def update (self , config: dict , error: float , info =None ):
ri=int(config[ "max_epochs "]) # Rung r_i
# Update our searcher, e.g if we use Bayesian optimization later
self .searcher .update(config, error, additional_info =info)
self .all_observed_error_at_rungs[ri] .append((config, error))
ifri<self .r_max:
# Bookkeeping
self .observed_error_at_rungs[ri] .append((config, error))
# Determine how many configurations should be evaluated on this rung
ki=self .K-self .rung_levels .index(ri)
ni=int(self .prefact *self .eta **ki)
# If we observed all configuration on this rung r_i, we estimate the
# top 1 / eta configuration, add them to queue and promote them for
# the next rung r_{i+1}
iflen(self .observed_error_at_rungs[ri]) >=ni:
kiplus1 =ki-1
niplus1 =int(self .prefact *self .eta **kiplus1)
best_performing_configurations =self .get_top_n_configurations(
rung_level =ri, n =niplus1
)
riplus1 =self .rung_levels[ self .K-kiplus1] # r_{i+1}
# Queue may not be empty: insert new entries at the beginning
self .queue =[
dict (config, max_epochs =riplus1)
for config inbest_performing_configurations
]+self .queue
self .observed_error_at_rungs[ri] =[] # Reset
Configurationsaresortedbasedontheirobservedperformanceonthecurrentrung.
@d2l .add_to_class(SuccessiveHalvingScheduler) #@save
def get_top_n_configurations (self , rung_level, n):
rung =self .observed_error_at_rungs[rung_level]
ifnot rung:
return []
sorted_rung =sorted (rung, key =lambda x: x[ 1])
return [x[0]for xinsorted_rung[:n]]
Let us see how successive halving is doing on our neural network example. We will use
ùëümin=2,ùúÇ=2,ùëüùëöùëéùë•=10, so that rung levels are 2,4,8,10.
min_number_of_epochs =2
max_number_of_epochs =10
eta =2
num_gpus =1
config_space ={
"learning_rate ": stats .loguniform( 1e-2 ,1),
"batch_size ": stats .randint( 32,256),
}
initial_config ={
"learning_rate ":0.1,
(continues on next page)
858 Hyperparameter Optimization
(continued from previous page)
"batch_size ":128,
}
We just replace the scheduler with our new SuccessiveHalvingScheduler .
searcher =d2l.RandomSearcher(config_space, initial_config =initial_config)
scheduler =SuccessiveHalvingScheduler(
searcher =searcher,
eta=eta,
r_min =min_number_of_epochs,
r_max =max_number_of_epochs,
)
tuner =d2l.HPOTuner(
scheduler =scheduler,
objective =d2l.hpo_objective_lenet,
)
tuner .run(number_of_trials =30)
error =0.17762434482574463 , runtime =53.576584339141846
We can visualize the learning curves of all configurations that we evaluated. Most of the
configurationsarestoppedearlyandonlythebetterperformingconfigurationssurviveuntil
ùëüùëöùëéùë•. Compare this to vanilla random search, which would allocate ùëüùëöùëéùë•to every config-
uration.
859 19.4 Multi-Fidelity Hyperparameter Optimization

860 Hyperparameter Optimization

861 19.4 Multi-Fidelity Hyperparameter Optimization

862 Hyperparameter Optimization

863 19.4 Multi-Fidelity Hyperparameter Optimization

864 Hyperparameter Optimization

865 19.4 Multi-Fidelity Hyperparameter Optimization

866 Hyperparameter Optimization
272for rung_index, rung inscheduler .all_observed_error_at_rungs .items():
errors =[xi[ 1]for xiinrung]
d2l.plt.scatter([rung_index] *len(errors), errors)
d2l.plt.xlim(min_number_of_epochs -0.5, max_number_of_epochs +0.5)
d2l.plt.xticks(
np.arange(min_number_of_epochs, max_number_of_epochs +1),
np.arange(min_number_of_epochs, max_number_of_epochs +1)
)
d2l.plt.ylabel( "validation error ")
d2l.plt.xlabel( "epochs ")
Text( 0.5,0,'epochs ')
Finally,notesomeslightcomplexityinourimplementationof SuccessiveHalvingSched-
uler. Say that a worker is free to run a job, and suggest is called when the current rung
hasalmostbeencompletelyfilled,butanotherworkerisstillbusywithanevaluation. Since
welackthemetricvaluefromthisworker,wecannotdeterminethetop 1¬ùùúÇfractiontoopen
up the next rung. On the other hand, we want to assign a job to our free worker, so it does
not remain idle. Our solution is to start a new round of successive halving and assign our
worker to the first trial there. However, once a rung is completed in update, we make sure
to insert new configurations at the beginning of the queue, so they take precedence over
configurations from the next round.
19.4.2Summary
In this section, we introduced the concept of multi-fidelity hyperparameter optimization,
whereweassumetohaveaccesstocheap-to-evaluateapproximationsoftheobjectivefunc-
tion, such as validation error after a certain number of epochs of training as proxy to val-
idation error after the full number of epochs. Multi-fidelity hyperparameter optimization
allowstoreducetheoverallcomputationoftheHPOinsteadofjustreducingthewall-clock
time.
Weimplementedandevaluatedsuccessivehalving,asimpleyetefficientmulti-fidelityHPO
algorithm.
Discussions272.
867 Asynchronous Successive Halving
19.5AsynchronousSuccessiveHalving
As we have seen in Section 19.3 , we can accelerate HPO by distributing the evaluation of
hyperparameter configurations across either multiple instances or multiples CPUs / GPUs
on a single instance. However, compared to random search, it is not straightforward to
run successive halving (SH) asynchronously in a distributed setting. Before we can decide
which configuration to run next, we first have to collect all observations at the current rung
level. This requires to synchronize workers at each rung level. For example, for the lowest
rung levelùëümin, we first have to evaluate all ùëÅ=ùúÇùêæconfigurations, before we can promote
the1
ùúÇof them to the next rung level.
In any distributed system, synchronization typically implies idle time for workers. First,
we often observe high variations in training time across hyperparameter configurations.
For example, assuming the number of filters per layer is a hyperparameter, then networks
with less filters finish training faster than networks with more filters, which implies idle
worker time due to stragglers. Moreover, the number of slots in a rung level is not always a
multiple of the number of workers, in which case some workers may even sit idle for a full
batch.
FigureFig. 19.5.1 shows the scheduling of synchronous SH with ùúÇ=2for four different
trials with two workers. We start with evaluating Trial-0 and Trial-1 for one epoch and
immediately continue with the next two trials once they are finished. We first have to wait
until Trial-2 finishes, which takes substantially more time than the other trials, before we
can promote the best two trials, i.e., Trial-0 and Trial-3 to the next rung level. This causes
idle time for Worker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer
thanTrial-0,whichleadstoanadditionalidelingtimeofWorker-0. Once,wereachRung-2,
onlythebesttrial,Trial-0,remainswhichoccupiesonlyoneworker. ToavoidthatWorker-1
idles during that time, most implementaitons of SH continue already with the next round,
and start evaluating new trials (e.g Trial-4) on the first rung.
Trial-0
Trial-1Trial-2
Trial-3Trial-0
Trial-3Synchronous Successive Halving
Worker-0
Worker-1
Rung 0Trial-0
Rung 1 Rung 2Trial-4
tFig. 19.5.1 Synchronous successive halving with two workers.
Asynchronous successive halving (ASHA) ( Lietal., 2018) adapts SH to the asynchronous
parallel scenario. The main idea of ASHA is to promote configurations to the next rung
levelassoonaswecollectedatleast ùúÇobservationsonthecurrentrunglevel. Thisdecision
rule may lead to suboptimal promotions: configurations can be promoted to the next rung
level, which in hindsight do not compare favourably against most others at the same rung
868 Hyperparameter Optimization
level. Ontheotherhand, wegetridofallsynchronizationpointsthisway. Inpractice,such
suboptimalinitialpromotionshaveonlyamodestimpactonperformance,notonlybecause
the ranking of hyperparameter configurations is often fairly consistent across rung levels,
but also because rungs grow over time and reflect the distribution of metric values at this
level better and better. If a worker is free, but no configuration can be promoted, we start a
new configuration with ùëü=ùëümin, i.e the first rung level.
Fig. 19.5.2 shows the scheduling of the same configurations for ASHA. Once Trial-1 fin-
ishes, we collect the results of two trials (i.e Trial-0 and Trial-1) and immediately promote
the better of them (Trial-0) to the next rung level. After Trial-0 finishes on rung 1, there
are too few trials there in order to support a further promotion. Hence, we continue with
rung 0 and evaluate Trial-3. Once Trial-3 finishes, Trial-2 is still pending. At this point we
have 3 trials evaluated on rung 0 and one trial evaluated already on rung 1. Since Trial-3
performs worse than Trial-0 at rung 0, and ùúÇ=2, we cannot promote any new trial yet, and
Worker-1 starts Trial-4 from scratch instead. However, once Trial-2 finishes and scores
worse than Trial-3, the latter is promoted towards rung 1. Afterwards, we collected 2 eval-
uations on rung 1, which means we can now promote Trial-0 towards rung 2. At the same
time, Worker-1 continues with evaluating new trials (i.e., Trial-5) on rung 0.
Trial-0
Trial-1Trial-2
Trial-0 Trial-3Asynchronous Successive Halving
Worker-0
Worker-1Trial-3
Trial-4Trial-0
Promotion to Rung 1Promotion to Rung 1
Start new trial on Rung 0Promotion to Rung 2
Trial-5
Start new trial on Rung 0
tFig. 19.5.2 Asynchronous successive halving (ASHA) with two workers.
import logging
from d2l import torch asd2l
logging .basicConfig(level =logging .INFO)
import matplotlib .pyplot asplt
from syne_tune import StoppingCriterion, Tuner
from syne_tune .backend .python_backend import PythonBackend
from syne_tune .config_space import loguniform, randint
from syne_tune .experiments import load_experiment
from syne_tune .optimizer .baselines import ASHA
INFO:root:SageMakerBackend isnot imported since dependencies are missing .You‚ê£
‚Ü©!can install them with
pip install 'syne-tune[extra] '
AWS dependencies are not imported since dependencies are missing .You can ‚ê£
‚Ü©!install them with
pip install 'syne-tune[aws] '
or(for everything)
pip install 'syne-tune[extra] '
(continues on next page)
869 Asynchronous Successive Halving
(continued from previous page)
AWS dependencies are not imported since dependencies are missing .You can ‚ê£
‚Ü©!install them with
pip install 'syne-tune[aws] '
or(for everything)
pip install 'syne-tune[extra] '
INFO:root:Ray Tune schedulers and searchers are not imported since ‚ê£
‚Ü©!dependencies are missing .You can install them with
pip install 'syne-tune[raytune] '
or(for everything)
pip install 'syne-tune[extra] '
19.5.1ObjectiveFunction
We will use Syne Tune with the same objective function as in Section 19.3 .
def hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):
from syne_tune import Reporter
from d2l import torch asd2l
model =d2l.LeNet(lr =learning_rate, num_classes =10)
trainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)
data =d2l.FashionMNIST(batch_size =batch_size)
model .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)
report =Reporter()
for epoch inrange (1, max_epochs +1):
ifepoch ==1:
# Initialize the state of Trainer
trainer .fit(model =model, data =data)
else :
trainer .fit_epoch()
validation_error =trainer .validation_error() .cpu() .detach() .numpy()
report(epoch =epoch, validation_error =float (validation_error))
We will also use the same configuration space as before:
min_number_of_epochs =2
max_number_of_epochs =10
eta =2
config_space ={
"learning_rate ": loguniform( 1e-2 ,1),
"batch_size ": randint( 32,256),
"max_epochs ": max_number_of_epochs,
}
initial_config ={
"learning_rate ":0.1,
"batch_size ":128,
}
19.5.2AsynchronousScheduler
870 Hyperparameter Optimization
First, we define the number of workers that evaluate trials concurrently. We also need to
specify how long we want to run random search, by defining an upper limit on the total
wall-clock time.
n_workers =2# Needs to be <= the number of available GPUs
max_wallclock_time =12*60 # 12 minutes
ThecodeforrunningASHAisasimplevariationofwhatwedidforasynchronousrandom
search.
mode ="min"
metric ="validation_error "
resource_attr ="epoch "
scheduler =ASHA(
config_space,
metric =metric,
mode =mode,
points_to_evaluate =[initial_config],
max_resource_attr ="max_epochs ",
resource_attr =resource_attr,
grace_period =min_number_of_epochs,
reduction_factor =eta,
)
INFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred ‚ê£
‚Ü©!from config_space
INFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =3140976097
Here, metricandresource_attr specify the key names used with the reportcallback,
andmax_resource_attr denoteswhichinputtotheobjectivefunctioncorrespondsto ùëümax.
Moreover, grace_period providesùëümin, and reduction_factor isùúÇ. We can run Syne
Tune as before (this will take about 12 minutes):
trial_backend =PythonBackend(
tune_function =hpo_objective_lenet_synetune,
config_space =config_space,
)
stop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)
tuner =Tuner(
trial_backend =trial_backend,
scheduler =scheduler,
stop_criterion =stop_criterion,
n_workers =n_workers,
print_update_interval =int(max_wallclock_time *0.6),
)
tuner .run()
871 Asynchronous Successive Halving
INFO:syne_tune .tuner:results of trials will be saved on /home /ci/syne -tune /
‚Ü©!python -entrypoint -2023 -08-18-20-01-52-046
INFO:root:Detected 4GPUs
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_function_root ‚ê£
‚Ü©!/home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/tune_function --
‚Ü©!tune_function_hash e03d187e043d2a17cae636d6af164015 --st_checkpoint_dir /
‚Ü©!home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/0/checkpoints
INFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,
‚Ü©!'batch_size ':128,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.44639554136672527 --batch_size 196 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!1/checkpoints
INFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.
‚Ü©!44639554136672527 ,'batch_size ':196,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.011548051321691994 --batch_size 254 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!2/checkpoints
INFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.
‚Ü©!011548051321691994 ,'batch_size ':254,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.14942487313193167 --batch_size 132 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!3/checkpoints
INFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.
‚Ü©!14942487313193167 ,'batch_size ':132,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 1completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.06317157191455719 --batch_size 242 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!4/checkpoints
INFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.
‚Ü©!06317157191455719 ,'batch_size ':242,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.48801815412811467 --batch_size 41--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!5/checkpoints
INFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.
(continues on next page)
872 Hyperparameter Optimization
(continued from previous page)
‚Ü©!48801815412811467 ,'batch_size ':41,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.5904067586747807 --batch_size 244 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!6/checkpoints
INFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.
‚Ü©!5904067586747807 ,'batch_size ':244,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.08812857364095393 --batch_size 148 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!7/checkpoints
INFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.
‚Ü©!08812857364095393 ,'batch_size ':148,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.012271314788363914 --batch_size 235 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!8/checkpoints
INFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.
‚Ü©!012271314788363914 ,'batch_size ':235,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 5completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.08845692598296777 --batch_size 236 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!9/checkpoints
INFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.
‚Ü©!08845692598296777 ,'batch_size ':236,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.0825770880068151 --batch_size 75--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!10/checkpoints
INFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.
‚Ü©!0825770880068151 ,'batch_size ':75,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.20235201406823256 --batch_size 65--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!11/checkpoints
INFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.
(continues on next page)
873 Asynchronous Successive Halving
(continued from previous page)
‚Ü©!20235201406823256 ,'batch_size ':65,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.3359885631737537 --batch_size 58--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!12/checkpoints
INFO:syne_tune .tuner:(trial 12)-scheduled config { 'learning_rate ':0.
‚Ü©!3359885631737537 ,'batch_size ':58,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.7892434579795236 --batch_size 89--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!13/checkpoints
INFO:syne_tune .tuner:(trial 13)-scheduled config { 'learning_rate ':0.
‚Ü©!7892434579795236 ,'batch_size ':89,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.1233786579597858 --batch_size 176 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!14/checkpoints
INFO:syne_tune .tuner:(trial 14)-scheduled config { 'learning_rate ':0.
‚Ü©!1233786579597858 ,'batch_size ':176,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 13completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.13707981127012328 --batch_size 141 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!15/checkpoints
INFO:syne_tune .tuner:(trial 15)-scheduled config { 'learning_rate ':0.
‚Ü©!13707981127012328 ,'batch_size ':141,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.02913976299993913 --batch_size 116 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!16/checkpoints
INFO:syne_tune .tuner:(trial 16)-scheduled config { 'learning_rate ':0.
‚Ü©!02913976299993913 ,'batch_size ':116,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.033362897489792855 --batch_size 154 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!17/checkpoints
INFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.
(continues on next page)
874 Hyperparameter Optimization
(continued from previous page)
‚Ü©!033362897489792855 ,'batch_size ':154,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.29442952580755816 --batch_size 210 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!18/checkpoints
INFO:syne_tune .tuner:(trial 18)-scheduled config { 'learning_rate ':0.
‚Ü©!29442952580755816 ,'batch_size ':210,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.10214259921521483 --batch_size 239 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!19/checkpoints
INFO:syne_tune .tuner:(trial 19)-scheduled config { 'learning_rate ':0.
‚Ü©!10214259921521483 ,'batch_size ':239,'max_epochs ':10}
INFO:syne_tune .tuner:tuning status (last metric isreported)
trial_id status iter learning_rate batch_size max_epochs epoch ‚ê£
‚Ü©!validation_error worker -time
0 Stopped 4 0.100000 128 10 4.0 ‚ê£
‚Ü©! 0.430578 29.093798
1Completed 10 0.446396 196 10 10.0 ‚ê£
‚Ü©! 0.205652 72.747496
2 Stopped 2 0.011548 254 10 2.0 ‚ê£
‚Ü©! 0.900570 13.729115
3 Stopped 8 0.149425 132 10 8.0 ‚ê£
‚Ü©! 0.259171 58.980305
4 Stopped 4 0.063172 242 10 4.0 ‚ê£
‚Ü©! 0.900579 27.773950
5Completed 10 0.488018 41 10 10.0 ‚ê£
‚Ü©! 0.140488 113.171314
6 Stopped 10 0.590407 244 10 10.0 ‚ê£
‚Ü©! 0.193776 70.364757
7 Stopped 2 0.088129 148 10 2.0 ‚ê£
‚Ü©! 0.899955 14.169738
8 Stopped 2 0.012271 235 10 2.0 ‚ê£
‚Ü©! 0.899840 13.434274
9 Stopped 2 0.088457 236 10 2.0 ‚ê£
‚Ü©! 0.899801 13.034437
10 Stopped 4 0.082577 75 10 4.0 ‚ê£
‚Ü©! 0.385970 35.426524
11 Stopped 4 0.202352 65 10 4.0 ‚ê£
‚Ü©! 0.543102 34.653495
12 Stopped 10 0.335989 58 10 10.0 ‚ê£
‚Ü©! 0.149558 90.924182
13 Completed 10 0.789243 89 10 10.0 ‚ê£
‚Ü©! 0.144887 77.365970
14 Stopped 2 0.123379 176 10 2.0 ‚ê£
‚Ü©! 0.899987 12.422906
15 Stopped 2 0.137080 141 10 2.0 ‚ê£
‚Ü©! 0.899983 13.395153
16 Stopped 4 0.029140 116 10 4.0 ‚ê£
(continues on next page)
875 Asynchronous Successive Halving
(continued from previous page)
‚Ü©! 0.900532 27.834111
17 Stopped 2 0.033363 154 10 2.0 ‚ê£
‚Ü©! 0.899996 13.407285
18InProgress 1 0.294430 210 10 1.0 ‚ê£
‚Ü©! 0.899878 6.126259
19InProgress 0 0.102143 239 10 - ‚ê£
‚Ü©! - -
2trials running, 18finished ( 3until the end), 437.07 s wallclock -time
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.02846298236356246 --batch_size 115 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!20/checkpoints
INFO:syne_tune .tuner:(trial 20)-scheduled config { 'learning_rate ':0.
‚Ü©!02846298236356246 ,'batch_size ':115,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.037703019195187606 --batch_size 91--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!21/checkpoints
INFO:syne_tune .tuner:(trial 21)-scheduled config { 'learning_rate ':0.
‚Ü©!037703019195187606 ,'batch_size ':91,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.0741039859356903 --batch_size 192 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!22/checkpoints
INFO:syne_tune .tuner:(trial 22)-scheduled config { 'learning_rate ':0.
‚Ü©!0741039859356903 ,'batch_size ':192,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.3032613031191755 --batch_size 252 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!23/checkpoints
INFO:syne_tune .tuner:(trial 23)-scheduled config { 'learning_rate ':0.
‚Ü©!3032613031191755 ,'batch_size ':252,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.019823425532533637 --batch_size 252 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!24/checkpoints
INFO:syne_tune .tuner:(trial 24)-scheduled config { 'learning_rate ':0.
‚Ü©!019823425532533637 ,'batch_size ':252,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
(continues on next page)
876 Hyperparameter Optimization
(continued from previous page)
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.8203370335228594 --batch_size 77--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!25/checkpoints
INFO:syne_tune .tuner:(trial 25)-scheduled config { 'learning_rate ':0.
‚Ü©!8203370335228594 ,'batch_size ':77,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.2960420911378594 --batch_size 104 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!26/checkpoints
INFO:syne_tune .tuner:(trial 26)-scheduled config { 'learning_rate ':0.
‚Ü©!2960420911378594 ,'batch_size ':104,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.2993874715754653 --batch_size 192 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!27/checkpoints
INFO:syne_tune .tuner:(trial 27)-scheduled config { 'learning_rate ':0.
‚Ü©!2993874715754653 ,'batch_size ':192,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.08056711961080017 --batch_size 36--max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!28/checkpoints
INFO:syne_tune .tuner:(trial 28)-scheduled config { 'learning_rate ':0.
‚Ü©!08056711961080017 ,'batch_size ':36,'max_epochs ':10}
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.26868380288030347 --batch_size 151 --max_epochs 10--
‚Ü©!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-
‚Ü©!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!29/checkpoints
INFO:syne_tune .tuner:(trial 29)-scheduled config { 'learning_rate ':0.
‚Ü©!26868380288030347 ,'batch_size ':151,'max_epochs ':10}
INFO:syne_tune .tuner:Trial trial_id 29completed .
INFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/
‚Ü©!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .
‚Ü©!py--learning_rate 0.9197404791177789 --batch_size 66--max_epochs 10--tune_
‚Ü©!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_
‚Ü©!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/
‚Ü©!30/checkpoints
INFO:syne_tune .tuner:(trial 30)-scheduled config { 'learning_rate ':0.
‚Ü©!9197404791177789 ,'batch_size ':66,'max_epochs ':10}
INFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping ‚ê£
(continues on next page)
877 Asynchronous Successive Halving
(continued from previous page)
‚Ü©!there .
INFO:syne_tune .tuner:Stopping trials that may still be running .
INFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /
‚Ü©!ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046
--------------------
Resource summary (last result isreported):
trial_id status iter learning_rate batch_size max_epochs epoch ‚ê£
‚Ü©!validation_error worker -time
0 Stopped 4 0.100000 128 10 4 ‚ê£
‚Ü©! 0.430578 29.093798
1Completed 10 0.446396 196 10 10 ‚ê£
‚Ü©! 0.205652 72.747496
2 Stopped 2 0.011548 254 10 2 ‚ê£
‚Ü©! 0.900570 13.729115
3 Stopped 8 0.149425 132 10 8 ‚ê£
‚Ü©! 0.259171 58.980305
4 Stopped 4 0.063172 242 10 4 ‚ê£
‚Ü©! 0.900579 27.773950
5Completed 10 0.488018 41 10 10 ‚ê£
‚Ü©! 0.140488 113.171314
6 Stopped 10 0.590407 244 10 10 ‚ê£
‚Ü©! 0.193776 70.364757
7 Stopped 2 0.088129 148 10 2 ‚ê£
‚Ü©! 0.899955 14.169738
8 Stopped 2 0.012271 235 10 2 ‚ê£
‚Ü©! 0.899840 13.434274
9 Stopped 2 0.088457 236 10 2 ‚ê£
‚Ü©! 0.899801 13.034437
10 Stopped 4 0.082577 75 10 4 ‚ê£
‚Ü©! 0.385970 35.426524
11 Stopped 4 0.202352 65 10 4 ‚ê£
‚Ü©! 0.543102 34.653495
12 Stopped 10 0.335989 58 10 10 ‚ê£
‚Ü©! 0.149558 90.924182
13 Completed 10 0.789243 89 10 10 ‚ê£
‚Ü©! 0.144887 77.365970
14 Stopped 2 0.123379 176 10 2 ‚ê£
‚Ü©! 0.899987 12.422906
15 Stopped 2 0.137080 141 10 2 ‚ê£
‚Ü©! 0.899983 13.395153
16 Stopped 4 0.029140 116 10 4 ‚ê£
‚Ü©! 0.900532 27.834111
17 Stopped 2 0.033363 154 10 2 ‚ê£
‚Ü©! 0.899996 13.407285
18 Stopped 8 0.294430 210 10 8 ‚ê£
‚Ü©! 0.241193 52.089688
19 Stopped 2 0.102143 239 10 2 ‚ê£
‚Ü©! 0.900002 12.487762
20 Stopped 2 0.028463 115 10 2 ‚ê£
‚Ü©! 0.899995 14.100359
21 Stopped 2 0.037703 91 10 2 ‚ê£
‚Ü©! 0.900026 14.664848
22 Stopped 2 0.074104 192 10 2 ‚ê£
‚Ü©! 0.901730 13.312770
23 Stopped 2 0.303261 252 10 2 ‚ê£
(continues on next page)
878 Hyperparameter Optimization
(continued from previous page)
‚Ü©! 0.900009 12.725821
24 Stopped 2 0.019823 252 10 2 ‚ê£
‚Ü©! 0.899917 12.533380
25 Stopped 10 0.820337 77 10 10 ‚ê£
‚Ü©! 0.196842 81.816103
26 Stopped 10 0.296042 104 10 10 ‚ê£
‚Ü©! 0.198453 81.121330
27 Stopped 4 0.299387 192 10 4 ‚ê£
‚Ü©! 0.336183 24.610689
28InProgress 9 0.080567 36 10 9 ‚ê£
‚Ü©! 0.203052 104.303746
29 Completed 10 0.268684 151 10 10 ‚ê£
‚Ü©! 0.222814 68.217289
30InProgress 1 0.919740 66 10 1 ‚ê£
‚Ü©! 0.900037 10.070776
2trials running, 29finished ( 4until the end), 723.70 s wallclock -time
validation_error: best 0.1404876708984375 for trial -id5
--------------------
Note that we are running a variant of ASHA where underperforming trials are stopped
early. This is different to our implementation in Section 19.4.1 , where each training job is
started with a fixed max_epochs . In the latter case, a well-performing trial which reaches
the full 10 epochs, first needs to train 1, then 2, then 4, then 8 epochs, each time starting
fromscratch. Thistypeofpause-and-resumeschedulingcanbeimplementedefficientlyby
checkpointing the training state after each epoch, but we avoid this extra complexity here.
After the experiment has finished, we can retrieve and plot results.
d2l.set_figsize()
e=load_experiment(tuner .name)
e.plot()
WARNING:matplotlib .legend:No artists with labels found to put inlegend .Note ‚ê£
‚Ü©!that artists whose label start with an underscore are ignored when legend() ‚ê£
‚Ü©!iscalled with no argument .
19.5.3Visualizethe Optimization Process
879 Asynchronous Successive Halving
273Oncemore,wevisualizethelearningcurvesofeverytrial(eachcolorintheplotrepresents
a trial). Compare this to asynchronous random search in Section 19.3 . As we have seen
for successive halving in Section 19.4 , most of the trials are stopped at 1 or 2 epochs ( ùëümin
orùúÇùëümin). However, trials do not stop at the same point, because they require different
amount of time per epoch. If we ran standard successive halving instead of ASHA, we
would need to synchronize our workers, before we can promote configurations to the next
rung level.
d2l.set_figsize([ 6,2.5])
results =e.results
for trial_id inresults .trial_id .unique():
df=results[results[ "trial_id "]==trial_id]
d2l.plt.plot(
df["st_tuner_time "],
df["validation_error "],
marker ="o"
)
d2l.plt.xlabel( "wall-clock time ")
d2l.plt.ylabel( "objective function ")
Text( 0,0.5,'objective function ')
19.5.4Summary
Compared to random search, successive halving is not quite as trivial to run in an asyn-
chronous distributed setting. To avoid synchronisation points, we promote configurations
as quickly as possible to the next rung level, even if this means promoting some wrong
ones. In practice, this usually does not hurt much, and the gains of asynchronous versus
synchronous scheduling are usually much higher than the loss of the suboptimal decision
making.
Discussions273.
20 Generative Adversarial Networks
20.1GenerativeAdversarialNetworks
Throughout most of this book, we have talked about how to make predictions. In some
form or another, we used deep neural networks to learn mappings from data examples to
labels. This kind of learning is called discriminative learning, as in, we‚Äôd like to be able
to discriminate between photos of cats and photos of dogs. Classifiers and regressors are
both examples of discriminative learning. And neural networks trained by backpropaga-
tion have upended everything we thought we knew about discriminative learning on large
complicated datasets. Classification accuracies on high-res images have gone from useless
tohuman-level(withsomecaveats)injust5-6years. Wewillspareyouanotherspielabout
alltheotherdiscriminativetaskswheredeepneuralnetworksdoastoundinglywell.
But there is more to machine learning than just solving discriminative tasks. For example,
given a large dataset, without any labels, we might want to learn a model that concisely
captures the characteristics of this data. Given such a model, we could sample synthetic
dataexamplesthatresemblethedistributionofthetrainingdata. Forexample,givenalarge
corpus of photographs of faces, we might want to be able to generate a new photorealistic
image that looks like it might plausibly have come from the same dataset. This kind of
learning is called generative modeling.
Untilrecently,wehadnomethodthatcouldsynthesizenovelphotorealisticimages. Butthe
success of deep neural networks for discriminative learning opened up new possibilities.
One big trend over the last three years has been the application of discriminative deep
nets to overcome challenges in problems that we do not generally think of as supervised
learningproblems. Therecurrentneuralnetworklanguagemodelsareoneexampleofusing
a discriminative network (trained to predict the next character) that once trained can act as
a generative model.
In2014,abreakthroughpaperintroducedGenerativeadversarialnetworks(GANs)( Good-
fellowetal., 2014), a clever new way to leverage the power of discriminative models to get
goodgenerativemodels. Attheirheart, GANsrelyontheideathatadatageneratorisgood
if we cannot tell fake data apart from real data. In statistics, this is called a two-sample test
- a test to answer the question whether datasets ùëã=fùë•1,...,ùë•ùëõgandùëã0=fùë•0
1,...,ùë•0
ùëõg
weredrawnfromthesamedistribution. Themaindifferencebetweenmoststatisticspapers
880
881 Generative Adversarial Networks
274and GANs is that the latter use this idea in a constructive way. In other words, rather than
just training a model to say ‚Äúhey, these two datasets do not look like they came from the
same distribution‚Äù, they use the two-sample test274to provide training signals to a gener-
ative model. This allows us to improve the data generator until it generates something that
resembles the real data. At the very least, it needs to fool the classifier even if our classifier
is a state of the art deep neural network.
tFig. 20.1.1 Generative Adversarial Networks
The GAN architecture is illustrated in Fig. 20.1.1 . As you can see, there are two pieces
in GAN architecture - first off, we need a device (say, a deep network but it really could
be anything, such as a game rendering engine) that might potentially be able to generate
data that looks just like the real thing. If we are dealing with images, this needs to generate
images. If we are dealing with speech, it needs to generate audio sequences, and so on.
We call this the generator network. The second component is the discriminator network. It
attemptstodistinguishfakeandrealdatafromeachother. Bothnetworksareincompetition
with each other. The generator network attempts to fool the discriminator network. At that
point, the discriminator network adapts to the new fake data. This information, in turn is
used to improve the generator network, and so on.
Thediscriminatorisabinaryclassifiertodistinguishiftheinput ùë•isreal(fromrealdata)or
fake(fromthegenerator). Typically,thediscriminatoroutputsascalarprediction ùëú2Rfor
inputx, such as using a fully connected layer with hidden size 1, and then applies sigmoid
function to obtain the predicted probability ùê∑¬πx¬∫=1¬ù¬π1¬∏ùëí ùëú¬∫. Assume the label ùë¶
for the true data is 1and 0for the fake data. We train the discriminator to minimize the
cross-entropy loss, i.e.,
min
ùê∑f ùë¶logùê∑¬πx¬∫ ¬π 1 ùë¶¬∫log¬π1 ùê∑¬πx¬∫¬∫g, (20.1.1)
For the generator, it first draws some parameter z2Rùëëfrom a source of randomness, e.g.,
a normal distribution zN¬π 0,1¬∫. We often call zas the latent variable. It then applies
a function to generate x0=ùê∫¬πz¬∫. The goal of the generator is to fool the discriminator
to classify x0=ùê∫¬πz¬∫as true data, i.e., we wantùê∑¬πùê∫¬πz¬∫¬∫  1. In other words, for a
givendiscriminator ùê∑,weupdatetheparametersofthegenerator ùê∫tomaximizethecross-
entropy loss when ùë¶=0,i.e.,
max
ùê∫f ¬π1 ùë¶¬∫log¬π1 ùê∑¬πùê∫¬πz¬∫¬∫¬∫g=max
ùê∫f log¬π1 ùê∑¬πùê∫¬πz¬∫¬∫¬∫g. (20.1.2)
If the generator does a perfect job, then ùê∑¬πx0¬∫  1, so the above loss is near 0, which
882 Generative Adversarial Networks
results in the gradients that are too small to make good progress for the discriminator. So
commonly, we minimize the following loss:
min
ùê∫f ùë¶log¬πùê∑¬πùê∫¬πz¬∫¬∫¬∫g=min
ùê∫f log¬πùê∑¬πùê∫¬πz¬∫¬∫¬∫g, (20.1.3)
which is just feeding x0=ùê∫¬πz¬∫into the discriminator but giving label ùë¶=1.
To sum up, ùê∑andùê∫are playing a ‚Äúminimax‚Äù game with the comprehensive objective
function:
min
ùê∑max
ùê∫f ùê∏ùë•Datalogùê∑¬πx¬∫ ùê∏ùëßNoise log¬π1 ùê∑¬πùê∫¬πz¬∫¬∫¬∫g. (20.1.4)
Many of the GANs applications are in the context of images. As a demonstration purpose,
we are going to content ourselves with fitting a much simpler distribution first. We will
illustrate what happens if we use GANs to build the world‚Äôs most inefficient estimator of
parameters for a Gaussian. Let‚Äôs get started.
%matplotlib inline
import torch
from torch import nn
from d2l import torch asd2l
20.1.1GenerateSome ‚ÄúReal‚ÄùData
Since this is going to be the world‚Äôs lamest example, we simply generate data drawn from
a Gaussian.
X=torch .normal( 0.0,1, (1000 ,2))
A=torch .tensor([[ 1,2], [ -0.1,0.5]])
b=torch .tensor([ 1,2])
data =torch .matmul(X, A) +b
Let‚Äôs see what we got. This should be a Gaussian shifted in some rather arbitrary way with
meanùëèand covariance matrix ùê¥ùëáùê¥.
d2l.set_figsize()
d2l.plt.scatter(data[: 100, (0)].detach() .numpy(), data[: 100, (1)].detach() .
‚Ü©!numpy());
print (f'The covariance matrix is \n{torch .matmul(A .T,A)}')
The covariance matrix is
tensor([[ 1.0100 ,1.9500 ],
[1.9500 ,4.2500 ]])
batch_size =8
data_iter =d2l.load_array((data,), batch_size)
883 Generative Adversarial Networks
20.1.2Generator
Our generator network will be the simplest network possible - a single layer linear model.
This is since we will be driving that linear network with a Gaussian data generator. Hence,
it literally only needs to learn the parameters to fake things perfectly.
net_G =nn.Sequential(nn .Linear( 2,2))
20.1.3Discriminator
For the discriminator we will be a bit more discriminating: we will use an MLP with 3
layers to make things a bit more interesting.
net_D =nn.Sequential(
nn.Linear( 2,5), nn .Tanh(),
nn.Linear( 5,3), nn .Tanh(),
nn.Linear( 3,1))
20.1.4Training
First we define a function to update the discriminator.
#@save
def update_D (X, Z, net_D, net_G, loss, trainer_D):
"""Update discriminator."""
batch_size =X.shape[ 0]
ones =torch .ones((batch_size,), device =X.device)
zeros =torch .zeros((batch_size,), device =X.device)
trainer_D .zero_grad()
real_Y =net_D(X)
fake_X =net_G(Z)
# Do not need to compute gradient for `net_G`, detach it from
# computing gradients.
fake_Y =net_D(fake_X .detach())
loss_D =(loss(real_Y, ones .reshape(real_Y .shape)) +
loss(fake_Y, zeros .reshape(fake_Y .shape))) /2
loss_D .backward()
trainer_D .step()
return loss_D
884 Generative Adversarial Networks
The generator is updated similarly. Here we reuse the cross-entropy loss but change the
label of the fake data from 0to1.
#@save
def update_G (Z, net_D, net_G, loss, trainer_G):
"""Update generator."""
batch_size =Z.shape[ 0]
ones =torch .ones((batch_size,), device =Z.device)
trainer_G .zero_grad()
# We could reuse `fake_X` from `update_D` to save computation
fake_X =net_G(Z)
# Recomputing `fake_Y` is needed since `net_D` is changed
fake_Y =net_D(fake_X)
loss_G =loss(fake_Y, ones .reshape(fake_Y .shape))
loss_G .backward()
trainer_G .step()
return loss_G
Both the discriminator and the generator performs a binary logistic regression with the
cross-entropy loss. We use Adam to smooth the training process. In each iteration, we first
update the discriminator and then the generator. We visualize both losses and generated
examples.
def train (net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):
loss =nn.BCEWithLogitsLoss(reduction ='sum')
for winnet_D .parameters():
nn.init .normal_(w, 0,0.02 )
for winnet_G .parameters():
nn.init .normal_(w, 0,0.02 )
trainer_D =torch .optim .Adam(net_D .parameters(), lr =lr_D)
trainer_G =torch .optim .Adam(net_G .parameters(), lr =lr_G)
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[1, num_epochs], nrows =2, figsize =(5,5),
legend =['discriminator ','generator '])
animator .fig.subplots_adjust(hspace =0.3)
for epoch inrange (num_epochs):
# Train one epoch
timer =d2l.Timer()
metric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples
for (X,) indata_iter:
batch_size =X.shape[ 0]
Z=torch .normal( 0,1, size =(batch_size, latent_dim))
metric .add(update_D(X, Z, net_D, net_G, loss, trainer_D),
update_G(Z, net_D, net_G, loss, trainer_G),
batch_size)
# Visualize generated examples
Z=torch .normal( 0,1, size =(100, latent_dim))
fake_X =net_G(Z) .detach() .numpy()
animator .axes[ 1].cla()
animator .axes[ 1].scatter(data[:, 0], data[:, 1])
animator .axes[ 1].scatter(fake_X[:, 0], fake_X[:, 1])
animator .axes[ 1].legend([ 'real ','generated '])
# Show the losses
loss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]
(continues on next page)
885 Generative Adversarial Networks
275(continued from previous page)
animator .add(epoch +1, (loss_D, loss_G))
print (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'
f'{metric[ 2]/timer .stop() :.1f}examples/sec ')
Now we specify the hyperparameters to fit the Gaussian distribution.
lr_D, lr_G, latent_dim, num_epochs =0.05 ,0.005 ,2,20
train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,
latent_dim, data[: 100].detach() .numpy())
loss_D 0.693 , loss_G 0.693 ,1020.0 examples /sec
20.1.5Summary
Generative adversarial networks (GANs) composes of two deep networks, the generator
and the discriminator.
The generator generates the image as much closer to the true image as possible to fool
the discriminator, via maximizing the cross-entropy loss, i.e.,max log¬πùê∑¬πx0¬∫¬∫.
The discriminator tries to distinguish the generated images from the true images, via
minimizing the cross-entropy loss, i.e.,min ùë¶logùê∑¬πx¬∫ ¬π 1 ùë¶¬∫log¬π1 ùê∑¬πx¬∫¬∫.
20.1.6Exercises
Doesanequilibriumexistwherethegeneratorwins, i.e.thediscriminatorendsupunable
to distinguish the two distributions on finite samples?
Discussions275.
886 Generative Adversarial Networks
27620.2DeepConvolutionalGenerativeAdversarial
Networks
InSection 20.1 , we introduced the basic ideas behind how GANs work. We showed that
they can draw samples from some simple, easy-to-sample distribution, like a uniform or
normal distribution, and transform them into samples that appear to match the distribution
of some dataset. And while our example of matching a 2D Gaussian distribution got the
point across, it is not especially exciting.
In this section, we will demonstrate how you can use GANs to generate photorealistic im-
ages. WewillbebasingourmodelsonthedeepconvolutionalGANs(DCGAN)introduced
in Radford et al.(2015). We will borrow the convolutional architecture that have proven
so successful for discriminative computer vision problems and show how via GANs, they
can be leveraged to generate photorealistic images.
import warnings
import torch
import torchvision
from torch import nn
from d2l import torch asd2l
20.2.1The PokemonDataset
The dataset we will use is a collection of Pokemon sprites obtained from pokemondb276.
First download, extract and load this dataset.
#@save
d2l.DATA_HUB[ 'pokemon ']=(d2l .DATA_URL +'pokemon.zip ',
'c065c0e2593b8b161a2d7873e42418bf6a21106c ')
data_dir =d2l.download_extract( 'pokemon ')
pokemon =torchvision .datasets .ImageFolder(data_dir)
Downloading ../data /pokemon .zip from http ://d2l-data .s3-accelerate .amazonaws .
‚Ü©!com/pokemon .zip...
We resize each image into 6464. The ToTensor transformation will project the pixel
valueinto¬ª0,1¬º,whileourgeneratorwillusethetanhfunctiontoobtainoutputsin ¬ª 1,1¬º.
Therefore we normalize the data with 0.5mean and 0.5standard deviation to match the
value range.
batch_size =256
transformer =torchvision .transforms .Compose([
torchvision .transforms .Resize(( 64,64)),
(continues on next page)
887 Deep Convolutional Generative Adversarial Networks
(continued from previous page)
torchvision .transforms .ToTensor(),
torchvision .transforms .Normalize( 0.5,0.5)
])
pokemon .transform =transformer
data_iter =torch .utils .data .DataLoader(
pokemon, batch_size =batch_size,
shuffle =True , num_workers =d2l.get_dataloader_workers())
Let‚Äôs visualize the first 20 images.
warnings .filterwarnings( 'ignore ')
d2l.set_figsize(( 4,4))
for X, y indata_iter:
imgs =X[:20,:,:,:] .permute( 0,2,3,1)/2+0.5
d2l.show_images(imgs, num_rows =4, num_cols =5)
break
20.2.2The Generator
The generator needs to map the noise variable z2Rùëë, a length-ùëëvector, to a RGB image
withwidthandheighttobe 6464. InSection14.11 weintroducedthefullyconvolutional
networkthatusestransposedconvolutionlayer(referto Section14.10 )toenlargeinputsize.
The basic block of the generator contains a transposed convolution layer followed by the
batch normalization and ReLU activation.
class G_block (nn.Module):
def __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,
padding =1,**kwargs):
(continues on next page)
888 Generative Adversarial Networks
(continued from previous page)
super (G_block, self ).__init__ (**kwargs)
self .conv2d_trans =nn.ConvTranspose2d(in_channels, out_channels,
kernel_size, strides, padding, bias =False )
self .batch_norm =nn.BatchNorm2d(out_channels)
self .activation =nn.ReLU()
def forward (self , X):
return self .activation( self .batch_norm( self .conv2d_trans(X)))
In default, the transposed convolution layer uses a ùëò‚Ñé=ùëòùë§=4kernel, aùë†‚Ñé=ùë†ùë§=2
strides,anda ùëù‚Ñé=ùëùùë§=1padding. Withainputshapeof ùëõ0
‚Ñéùëõ0
ùë§=1616,thegenerator
block will double input‚Äôs width and height.
ùëõ0
‚Ñéùëõ0
ùë§=¬ª¬πùëõ‚Ñéùëò‚Ñé ¬πùëõ‚Ñé 1¬∫¬πùëò‚Ñé ùë†‚Ñé¬∫ 2ùëù‚Ñé¬º¬ª¬πùëõùë§ùëòùë§ ¬πùëõùë§ 1¬∫¬πùëòùë§ ùë†ùë§¬∫ 2ùëùùë§¬º
=¬ª¬πùëò‚Ñé¬∏ùë†‚Ñé¬πùëõ‚Ñé 1¬∫ 2ùëù‚Ñé¬º¬ª¬πùëòùë§¬∏ùë†ùë§¬πùëõùë§ 1¬∫ 2ùëùùë§¬º
=¬ª¬π4¬∏2¬π16 1¬∫ 21¬º¬ª¬π 4¬∏2¬π16 1¬∫ 21¬º
=3232.
(20.2.1)
x=torch .zeros(( 2,3,16,16))
g_blk =G_block( 20)
g_blk(x) .shape
torch .Size([ 2,20,32,32])
Ifchangingthetransposedconvolutionlayertoa 44kernel, 11stridesandzeropadding.
With a input size of 11, the output will have its width and height increased by 3 respec-
tively.
x=torch .zeros(( 2,3,1,1))
g_blk =G_block( 20, strides =1, padding =0)
g_blk(x) .shape
torch .Size([ 2,20,4,4])
Thegeneratorconsistsoffourbasicblocksthatincreaseinput‚Äôsbothwidthandheightfrom
1 to 32. At the same time, it first projects the latent variable into 648channels, and then
halve the channels each time. At last, a transposed convolution layer is used to generate
the output. It further doubles the width and height to match the desired 6464shape,
and reduces the channel size to 3. The tanh activation function is applied to project output
values into the¬π 1,1¬∫range.
n_G =64
net_G =nn.Sequential(
(continues on next page)
889 Deep Convolutional Generative Adversarial Networks
(continued from previous page)
G_block(in_channels =100, out_channels =n_G*8,
strides =1, padding =0), # Output: (64 * 8, 4, 4)
G_block(in_channels =n_G*8, out_channels =n_G*4),# Output: (64 * 4, 8, 8)
G_block(in_channels =n_G*4, out_channels =n_G*2),# Output: (64 * 2, 16, 16)
G_block(in_channels =n_G*2, out_channels =n_G), # Output: (64, 32, 32)
nn.ConvTranspose2d(in_channels =n_G, out_channels =3,
kernel_size =4, stride =2, padding =1, bias =False ),
nn.Tanh()) # Output: (3, 64, 64)
Generate a 100 dimensional latent variable to verify the generator‚Äôs output shape.
x=torch .zeros(( 1,100,1,1))
net_G(x) .shape
torch .Size([ 1,3,64,64])
20.2.3Discriminator
The discriminator is a normal convolutional network network except that it uses a leaky
ReLU as its activation function. Given ùõº2¬ª0,1¬º, its definition is
leaky ReLU¬πùë•¬∫=(
ùë•ifùë• >0
ùõºùë•otherwise. (20.2.2)
As it can be seen, it is normal ReLU if ùõº=0, and an identity function if ùõº=1. For
ùõº2¬π0,1¬∫, leaky ReLU is a nonlinear function that give a non-zero output for a negative
input. Itaimstofixthe‚ÄúdyingReLU‚Äùproblemthataneuronmightalwaysoutputanegative
value and therefore cannot make any progress since the gradient of ReLU is 0.
alphas =[0,.2,.4,.6,.8,1]
x=torch .arange( -2,1,0.1)
Y=[nn.LeakyReLU(alpha)(x) .detach() .numpy() for alpha inalphas]
d2l.plot(x .detach() .numpy(), Y, 'x','y', alphas)
Thebasicblockofthediscriminatorisaconvolutionlayerfollowedbyabatchnormalization
layerandaleakyReLUactivation. Thehyperparametersoftheconvolutionlayeraresimilar
to the transpose convolution layer in the generator block.
890 Generative Adversarial Networks
class D_block (nn.Module):
def __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,
padding =1, alpha =0.2,**kwargs):
super (D_block, self ).__init__ (**kwargs)
self .conv2d =nn.Conv2d(in_channels, out_channels, kernel_size,
strides, padding, bias =False )
self .batch_norm =nn.BatchNorm2d(out_channels)
self .activation =nn.LeakyReLU(alpha, inplace =True )
def forward (self , X):
return self .activation( self .batch_norm( self .conv2d(X)))
A basic block with default settings will halve the width and height of the inputs, as we
demonstratedin Section7.3 . Forexample,givenainputshape ùëõ‚Ñé=ùëõùë§=16,withakernel
shapeùëò‚Ñé=ùëòùë§=4, a stride shape ùë†‚Ñé=ùë†ùë§=2, and a padding shape ùëù‚Ñé=ùëùùë§=1, the
output shape will be:
ùëõ0
‚Ñéùëõ0
ùë§=b¬πùëõ‚Ñé ùëò‚Ñé¬∏2ùëù‚Ñé¬∏ùë†‚Ñé¬∫¬ùùë†‚Ñécb¬πùëõùë§ ùëòùë§¬∏2ùëùùë§¬∏ùë†ùë§¬∫¬ùùë†ùë§c
=b¬π16 4¬∏21¬∏2¬∫¬ù2cb¬π 16 4¬∏21¬∏2¬∫¬ù2c
=88.(20.2.3)
x=torch .zeros(( 2,3,16,16))
d_blk =D_block( 20)
d_blk(x) .shape
torch .Size([ 2,20,8,8])
The discriminator is a mirror of the generator.
n_D =64
net_D =nn.Sequential(
D_block(n_D), # Output: (64, 32, 32)
D_block(in_channels =n_D, out_channels =n_D*2), # Output: (64 * 2, 16, 16)
D_block(in_channels =n_D*2, out_channels =n_D*4), # Output: (64 * 4, 8, 8)
D_block(in_channels =n_D*4, out_channels =n_D*8), # Output: (64 * 8, 4, 4)
nn.Conv2d(in_channels =n_D*8, out_channels =1,
kernel_size =4, bias =False )) # Output: (1, 1, 1)
Itusesaconvolutionlayerwithoutputchannel 1asthelastlayertoobtainasingleprediction
value.
x=torch .zeros(( 1,3,64,64))
net_D(x) .shape
torch .Size([ 1,1,1,1])
20.2.4Training
891 Deep Convolutional Generative Adversarial Networks
Compared to the basic GAN in Section 20.1 , we use the same learning rate for both gen-
erator and discriminator since they are similar to each other. In addition, we change ùõΩ1in
Adam (Section 12.10 ) from 0.9to0.5. It decreases the smoothness of the momentum, the
exponentiallyweightedmovingaverageofpastgradients,totakecareoftherapidchanging
gradients because the generator and the discriminator fight with each other. Besides, the
random generated noise Z, is a 4-D tensor and we are using GPU to accelerate the compu-
tation.
def train (net_D, net_G, data_iter, num_epochs, lr, latent_dim,
device =d2l.try_gpu()):
loss =nn.BCEWithLogitsLoss(reduction ='sum')
for winnet_D .parameters():
nn.init .normal_(w, 0,0.02 )
for winnet_G .parameters():
nn.init .normal_(w, 0,0.02 )
net_D, net_G =net_D .to(device), net_G .to(device)
trainer_hp ={'lr': lr, 'betas ': [0.5,0.999 ]}
trainer_D =torch .optim .Adam(net_D .parameters(), **trainer_hp)
trainer_G =torch .optim .Adam(net_G .parameters(), **trainer_hp)
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[1, num_epochs], nrows =2, figsize =(5,5),
legend =['discriminator ','generator '])
animator .fig.subplots_adjust(hspace =0.3)
for epoch inrange (1, num_epochs +1):
# Train one epoch
timer =d2l.Timer()
metric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples
for X, _ indata_iter:
batch_size =X.shape[ 0]
Z=torch .normal( 0,1, size =(batch_size, latent_dim, 1,1))
X, Z =X.to(device), Z .to(device)
metric .add(d2l .update_D(X, Z, net_D, net_G, loss, trainer_D),
d2l.update_G(Z, net_D, net_G, loss, trainer_G),
batch_size)
# Show generated examples
Z=torch .normal( 0,1, size =(21, latent_dim, 1,1), device =device)
# Normalize the synthetic data to N(0, 1)
fake_x =net_G(Z) .permute( 0,2,3,1)/2+0.5
imgs =torch .cat(
[torch .cat([
fake_x[i *7+j].cpu() .detach() for jinrange (7)], dim =1)
for iinrange (len(fake_x) //7)], dim =0)
animator .axes[ 1].cla()
animator .axes[ 1].imshow(imgs)
# Show the losses
loss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]
animator .add(epoch, (loss_D, loss_G))
print (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'
f'{metric[ 2]/timer .stop() :.1f}examples/sec on {str(device) }')
We train the model with a small number of epochs just for demonstration. For better per-
formance, the variable num_epochs can be set to a larger number.
892 Generative Adversarial Networks
277latent_dim, lr, num_epochs =100,0.005 ,20
train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)
loss_D 0.023 , loss_G 7.359 ,2292.7 examples /sec on cuda: 0
20.2.5Summary
DCGANarchitecturehasfourconvolutionallayersfortheDiscriminatorandfour‚Äúfractionally-
strided‚Äù convolutional layers for the Generator.
The Discriminator is a 4-layer strided convolutions with batch normalization (except its
input layer) and leaky ReLU activations.
Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It
aims to fix the ‚Äúdying ReLU‚Äù problem and helps the gradients flow easier through the
architecture.
20.2.6Exercises
1.What will happen if we use standard ReLU activation rather than leaky ReLU?
2.ApplyDCGANonFashion-MNISTandseewhichcategoryworkswellandwhichdoes
not.
Discussions277.
21 Recommender Systems
ShuaiZhang (Amazon),AstonZhang (Amazon), andYiTay(Google)
Recommender systems are widely employed in industry and are ubiquitous in our daily
lives. These systems are utilized in a number of areas such as online shopping sites (e.g.,
amazon.com), music/movie services site (e.g., Netflix and Spotify), mobile application
stores (e.g., IOS app store and google play), online advertising, just to name a few.
The major goal of recommender systems is to help users discover relevant items such as
moviestowatch,texttoreadorproductstobuy,soastocreateadelightfuluserexperience.
Moreover, recommender systems are among the most powerful machine learning systems
that online retailers implement in order to drive incremental revenue. Recommender sys-
tems are replacements of search engines by reducing the efforts in proactive searches and
surprising users with offers they never searched for. Many companies managed to position
themselves ahead of their competitors with the help of more effective recommender sys-
tems. As such, recommender systems are central to not only our everyday lives but also
highly indispensable in some industries.
Inthischapter,wewillcoverthefundamentalsandadvancementsofrecommendersystems,
alongwithexploringsomecommonfundamentaltechniquesforbuildingrecommendersys-
temswith differentdata sources availableand theirimplementations. Specifically, youwill
learn how to predict the rating a user might give to a prospective item, how to generate a
recommendation list of items and how to predict the click-through rate from abundant fea-
tures. These tasks are commonplace in real-world applications. By studying this chapter,
you will get hands-on experience pertaining to solving real world recommendation prob-
lems with not only classical methods but the more advanced deep learning based models
as well.
21.1Overviewof RecommenderSystems
In the last decade, the Internet has evolved into a platform for large-scale online services,
which profoundly changed the way we communicate, read news, buy products, and watch
movies. Inthemeanwhile,theunprecedentednumberofitems(weusetheterm itemtorefer
to movies, news, books, and products.) offered online requires a system that can help us
discoveritemsthatwepreferred. Recommendersystemsarethereforepowerfulinformation
893
894 Recommender Systems
filtering tools that can facilitate personalized services and provide tailored experience to
individual users. In short, recommender systems play a pivotal role in utilizing the wealth
of data available to make choices manageable. Nowadays, recommender systems are at
the core of a number of online services providers such as Amazon, Netflix, and YouTube.
Recall the example of Deep learning books recommended by Amazon in Fig. 1.3.3 . The
benefits of employing recommender systems are two-folds: On the one hand, it can largely
reduce users‚Äô effort in finding items and alleviate the issue of information overload. On
the other hand, it can add business value to online service providers and is an important
source of revenue. This chapter will introduce the fundamental concepts, classic models
and recent advanceswith deep learning in the field of recommender systems, togetherwith
implemented examples.
tFig. 21.1.1 Illustration of the Recommendation Process
21.1.1CollaborativeFiltering
We start the journey with the important concept in recommender systems‚Äîcollaborative
filtering (CF), which was first coined by the Tapestry system ( Goldberg et al., 1992), re-
ferring to ‚Äúpeople collaborate to help one another perform the filtering process in order to
handlethelargeamountsofemailandmessagespostedtonewsgroups‚Äù. Thistermhasbeen
enriched with more senses. In a broad sense, it is the process of filtering for information or
patterns using techniques involving collaboration among multiple users, agents, and data
sources. CF has many forms and numerous CF methods proposed since its advent.
Overall, CF techniques can be categorized into: memory-based CF, model-based CF, and
their hybrid ( Su and Khoshgoftaar, 2009 ). Representative memory-based CF techniques
are nearest neighbor-based CF such as user-based CF and item-based CF ( Sarwaret al.,
2001). Latent factor models such as matrix factorization are examples of model-based CF.
Memory-based CF has limitations in dealing with sparse and large-scale data since it com-
putes the similarity values based on common items. Model-based methods become more
popular with its better capability in dealing with sparsity and scalability. Many model-
based CF approaches can be extended with neural networks, leading to more flexible and
scalable models with the computation acceleration in deep learning ( Zhanget al., 2019).
In general, CF only uses the user-item interaction data to make predictions and recom-
mendations. Besides CF, content-based and context-based recommender systems are also
useful in incorporating the content descriptions of items/users and contextual signals such
895 Overview of Recommender Systems
278as timestamps and locations. Obviously, we may need to adjust the model types/structures
when different input data is available.
21.1.2ExplicitFeedbackand ImplicitFeedback
Tolearnthepreferenceofusers,thesystemshallcollectfeedbackfromthem. Thefeedback
can be either explicit or implicit ( Huet al., 2008). For example, IMDb278collects star
ratings ranging from one to ten stars for movies. YouTube provides the thumbs-up and
thumbs-down buttons for users to show their preferences. It is apparent that gathering
explicitfeedbackrequiresusersto indicate theirinterestsproactively. Nonetheless, explicit
feedback is not always readily available as many users may be reluctant to rate products.
Relativelyspeaking,implicitfeedbackisoftenreadilyavailablesinceitismainlyconcerned
with modeling implicit behavior such as user clicks. As such, many recommender systems
arecenteredonimplicitfeedbackwhichindirectlyreflectsuser‚Äôsopinionthroughobserving
user behavior. There are diverse forms of implicit feedback including purchase history,
browsing history, watches and even mouse movements. For example, a user that purchased
many books by the same author probably likes that author. Note that implicit feedback is
inherently noisy. We can only guesstheir preferences and true motives. A user watched a
movie does not necessarily indicate a positive view of that movie.
21.1.3RecommendationTasks
A number of recommendation tasks have been investigated in the past decades. Based
on the domain of applications, there are movies recommendation, news recommendations,
point-of-interest recommendation ( Yeet al., 2011) and so forth. It is also possible to dif-
ferentiate the tasks based on the types of feedback and input data, for example, the rating
prediction task aims to predict the explicit ratings. Top- ùëõrecommendation (item ranking)
ranksallitemsforeachuserpersonallybasedontheimplicitfeedback. Iftime-stampinfor-
mation is also included, we can build sequence-aware recommendation ( Quadrana et al.,
2018). Another popular task is called click-through rate prediction, which is also based on
implicit feedback, but various categorical features can be utilized. Recommending for new
users and recommending new items to existing users are called cold-start recommendation
(Scheinetal., 2002).
21.1.4Summary
Recommender systems are important for individual users and industries. Collaborative
filtering is a key concept in recommendation.
There are two types of feedbacks: implicit feedback and explicit feedback. A number of
recommendation tasks have been explored during the last decade.
21.1.5Exercises
1.Can you explain how recommender systems influence your daily life?
2.What interesting recommendation tasks do you think can be investigated?
896 Recommender Systems
279Discussions279.
AMathematics for Deep
Learning
BrentWerness (Amazon),RachelHu (Amazon), and authors of this book
One of the wonderful parts of modern deep learning is the fact that much of it can be
understood and used without a full understanding of the mathematics below it. This is a
sign that the field is maturing. Just as most software developers no longer need to worry
about the theory of computable functions, neither should deep learning practitioners need
to worry about the theoretical foundations of maximum likelihood learning.
But, we are not quite there yet.
In practice, you will sometimes need to understand how architectural choices influence
gradientflow,ortheimplicitassumptionsyoumakebytrainingwithacertainlossfunction.
You might need to know what in the world entropy measures, and how it can help you
understand exactly what bits-per-character means in your model. These all require deeper
mathematical understanding.
This appendix aims to provide you the mathematical background you need to understand
the core theory of modern deep learning, but it is not exhaustive. We will begin with
examininglinearalgebraingreaterdepth. Wedevelopageometricunderstandingofallthe
common linear algebraic objects and operations that will enable us to visualize the effects
of various transformations on our data. A key element is the development of the basics of
eigen-decompositions.
Wenextdevelopthetheoryofdifferentialcalculustothepointthatwecanfullyunderstand
why the gradient is the direction of steepest descent, and why back-propagation takes the
form it does. Integral calculus is then discussed to the degree needed to support our next
topic, probability theory.
Problemsencounteredinpracticefrequentlyarenotcertain,andthusweneedalanguageto
speakaboutuncertainthings. Wereviewthetheoryofrandomvariablesandthemostcom-
monlyencountereddistributionssowemaydiscussmodelsprobabilistically. Thisprovides
thefoundationforthenaiveBayesclassifier,aprobabilisticclassificationtechnique.
Closely related to probability theory is the study of statistics. While statistics is far too
largeafield todo justiceina shortsection, wewillintroduce fundamentalconcepts thatall
machinelearningpractitionersshouldbeawareof,inparticular: evaluatingandcomparing
estimators, conducting hypothesis tests, and constructing confidence intervals.
Last, we turn to the topic of information theory, which is the mathematical study of infor-
897
898 Mathematics for Deep Learning
mationstorageandtransmission. Thisprovidesthecorelanguagebywhichwemaydiscuss
quantitatively how much information a model holds on a domain of discourse.
Takentogether,theseformthecoreofthemathematicalconceptsneededtobegindownthe
path towards a deep understanding of deep learning.
A.1Geometryand Linear AlgebraicOperations
InSection 2.3 , we encountered the basics of linear algebra and saw how it could be used
to express common operations for transforming our data. Linear algebra is one of the key
mathematical pillars underlying much of the work that we do in deep learning and in ma-
chine learning more broadly. While Section 2.3 contained enough machinery to commu-
nicate the mechanics of modern deep learning models, there is a lot more to the subject.
In this section, we will go deeper, highlighting some geometric interpretations of linear
algebra operations, and introducing a few fundamental concepts, including of eigenvalues
and eigenvectors.
A.1.1Geometryof Vectors
First, we need to discuss the two common geometric interpretations of vectors, as either
points or directions in space. Fundamentally, a vector is a list of numbers such as the
Python list below.
v=[1,7,0,1]
Mathematiciansmostoftenwritethisaseithera columnorrowvector,whichistosayeither
as
x=266666641
7
0
137777775, (A.1)
or
x>=
1 7 0 1
. (A.2)
These often have different interpretations, where data examples are column vectors and
weights used to form weighted sums are row vectors. However, it can be beneficial to be
flexible. Aswehavedescribedin Section2.3 ,thoughasinglevector‚Äôsdefaultorientationis
a column vector, for any matrix representing a tabular dataset, treating each data example
as a row vector in the matrix is more conventional.
Given a vector, the first interpretation that we should give it is as a point in space. In two
or three dimensions, we can visualize these points by using the components of the vectors
899 Geometry and Linear Algebraic Operations
to define the location of the points in space compared to a fixed reference called the origin.
This can be seen in Fig. A.1.
tFig. A.1 An illustration of visualizing vectors as points in the plane. The Ô¨Årst component of the
vector gives the x-coordinate, the second component gives the y-coordinate. Higher
dimensions are analogous, although much harder to visualize.
Thisgeometricpointofviewallowsustoconsidertheproblemonamoreabstractlevel. No
longerfacedwithsomeinsurmountableseemingproblemlikeclassifyingpicturesaseither
cats or dogs, we can start considering tasks abstractly as collections of points in space and
picturing the task as discovering how to separate two distinct clusters of points.
In parallel, there is a second point of view that people often take of vectors: as directions
in space. Not onlycan we think of the vector v=¬ª3,2¬º>as the location 3units to the right
and2units up from the origin, we can also think of it as the direction itself to take 3steps
to the right and 2steps up. In this way, we consider all the vectors in figure Fig. A.2 the
same.
tFig. A.2 Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a
representation of the vector ¬π3,2¬∫>.
Oneofthebenefitsofthisshiftisthatwecanmakevisualsenseoftheactofvectoraddition.
In particular, we follow the directions given by one vector, and then follow the directions
given by the other, as is seen in Fig. A.3.
Vector subtraction has a similar interpretation. By considering the identity that u=v¬∏
¬πu v¬∫, we see that the vector u vis the direction that takes us from the point vto the
pointu.
A.1.2DotProductsand Angles
900 Mathematics for Deep Learning
tFig. A.3 We can visualize vector addition by Ô¨Årst following one vector, and then another.
As we saw in Section 2.3 , if we take two column vectors uandv, we can form their dot
product by computing:
u>v=√ï
ùëñùë¢ùëñùë£ùëñ.(A.3)
Because (A.3)is symmetric, we will mirror the notation of classical multiplication and
write
uv=u>v=v>u, (A.4)
tohighlightthefactthatexchangingtheorderofthevectorswillyieldthesameanswer.
The dot product (A.3)also admits a geometric interpretation: it is closely related to the
angle between two vectors. Consider the angle shown in Fig. A.4.
tFig. A.4 Between any two vectors in the plane there is a well deÔ¨Åned angle ùúÉ. We will see this
angle is intimately tied to the dot product.
To start, let‚Äôs consider two specific vectors:
v=¬πùëü,0¬∫andw=¬πùë†cos¬πùúÉ¬∫,ùë†sin¬πùúÉ¬∫¬∫. (A.5)
The vector vis lengthùëüand runs parallel to the ùë•-axis, and the vector wis of length ùë†
and at angle ùúÉwith theùë•-axis. If we compute the dot product of these two vectors, we see
that
vw=ùëüùë†cos¬πùúÉ¬∫=kvkkwkcos¬πùúÉ¬∫. (A.6)
With some simple algebraic manipulation, we can rearrange terms to obtain
ùúÉ=arccosvw
kvkkwk
. (A.7)
901 Geometry and Linear Algebraic Operations
In short, for these two specific vectors, the dot product combined with the norms tell us
the angle between the two vectors. This same fact is true in general. We will not derive
the expression here, however, if we consider writing kv wk2in two ways: one with
the dot product, and the other geometrically using the law of cosines, we can obtain the
full relationship. Indeed, for any two vectors vandw, the angle between the two vectors
is
ùúÉ=arccosvw
kvkkwk
. (A.8)
This is a nice result since nothing in the computation references two-dimensions. Indeed,
we can use this in three or three million dimensions without issue.
As a simple example, let‚Äôs see how to compute the angle between a pair of vectors:
%matplotlib inline
import torch
import torchvision
from IPython import display
from torchvision import transforms
from d2l import torch asd2l
def angle (v, w):
return torch .acos(v .dot(w) /(torch .norm(v) *torch .norm(w)))
angle(torch .tensor([ 0,1,2], dtype =torch .float32), torch .tensor([ 2.0,3,4]))
tensor( 0.4190 )
We will not use it right now, but it is useful to know that we will refer to vectors for which
the angle is ùúã¬ù2(or equivalently 90) as being orthogonal . By examining the equation
above, we see that this happens when ùúÉ=ùúã¬ù2, which is the same thing as cos¬πùúÉ¬∫=0. The
only way this can happen is if the dot product itself is zero, and two vectors are orthogonal
ifandonlyif vw=0. Thiswillprovetobeahelpfulformulawhenunderstandingobjects
geometrically.
It is reasonable to ask: why is computing the angle useful? The answer comes in the kind
of invariance we expect data to have. Consider an image, and a duplicate image, where
every pixel value is the same but 10%the brightness. The values of the individual pixels
are in general far from the original values. Thus, if one computed the distance between
the original image and the darker one, the distance can be large. However, for most ML
applications,the contentisthesame‚Äîitisstillanimageofacatasfarasacat/dogclassifier
is concerned. However, if we consider the angle, it is not hard to see that for any vector v,
the angle between vand0.1vis zero. This corresponds to the fact that scaling vectors
keepsthesamedirectionandjustchangesthelength. Theangleconsidersthedarkerimage
identical.
Examples like this are everywhere. In text, we might want the topic being discussed to
not change if we write twice as long of document that says the same thing. For some
902 Mathematics for Deep Learning
encoding (such as counting the number of occurrences of words in some vocabulary), this
corresponds to a doubling of the vector encoding the document, so again we can use the
angle.
Cosine Similarity
In ML contextswhere the angle is employed to measure the closeness of twovectors, prac-
titioners adopt the term cosinesimilarity to refer to the portion
cos¬πùúÉ¬∫=vw
kvkkwk. (A.9)
The cosine takes a maximum value of 1when the two vectors point in the same direction,
a minimum value of  1when they point in opposite directions, and a value of 0when the
two vectors are orthogonal. Note that if the components of high-dimensional vectors are
sampled randomly with mean 0, their cosine will nearly always be close to 0.
A.1.3Hyperplanes
In addition to working with vectors, another key object that you must understand to go far
in linear algebra is the hyperplane , a generalization to higher dimensions of a line (two di-
mensions)orofaplane(threedimensions). Inan ùëë-dimensionalvectorspace,ahyperplane
hasùëë 1dimensions and divides the space into two half-spaces.
Let‚Äôs start with an example. Suppose that we have a column vector w=¬ª2,1¬º>. We want
to know, ‚Äúwhat are the points vwithwv=1?‚Äù By recalling the connection between dot
products and angles above (A.8), we can see that this is equivalent to
kvkkwkcos¬πùúÉ¬∫=1() k vkcos¬πùúÉ¬∫=1
kwk=1p
5. (A.10)
tFig. A.5 Recalling trigonometry, we see the formula kvkcos¬πùúÉ¬∫is the length of the projection of
the vector vonto the direction of w
If we consider the geometric meaning of this expression, we see that this is equivalent to
saying that the length of the projection of vonto the direction of wis exactly 1¬ùkwk, as
is shown in Fig. A.5. The set of all points where this is true is a line at right angles to the
vectorw. If we wanted, we could find the equation for this line and see that it is 2ùë•¬∏ùë¶=1
or equivalently ùë¶=1 2ùë•.
903 Geometry and Linear Algebraic Operations
If we now look at what happens when we ask about the set of points with wv>1or
wv<1, we can see that these are cases where the projections are longer or shorter than
1¬ùkwk, respectively. Thus, thosetwoinequalitiesdefineeithersideoftheline. Inthisway,
we have found a way to cut our space into two halves, where all the points on one side have
dot product below a threshold, and the other side above as we see in Fig. A.6.
tFig. A.6 If we now consider the inequality version of the expression, we see that our hyperplane (in
this case: just a line) separates the space into two halves.
The story in higher dimension is much the same. If we now take w=¬ª1,2,3¬º>and ask
aboutthepointsinthreedimensionswith wv=1, weobtainaplane atrightanglestothe
given vector w. The two inequalities again define the two sides of the plane as is shown in
Fig. A.7.
tFig. A.7 Hyperplanes in any dimension separate the space into two halves.
While our ability to visualize runs out at this point, nothing stops us from doing this in
tens, hundreds, or billions of dimensions. This occurs often when thinking about machine
learned models. For instance, we can understand linear classification models like those
fromSection 4.1 , as methods to find hyperplanes that separate the different target classes.
In this context, such hyperplanes are often referred to as decision planes . The majority of
deep learned classification models end with a linear layer fed into a softmax, so one can
interpret the role of the deep neural network to be to find a non-linear embedding such that
the target classes can be separated cleanly by hyperplanes.
To give a hand-built example, notice that we can produce a reasonable model to classify
tiny images of t-shirts and trousers from the Fashion-MNIST dataset (seen in Section 4.2 )
by just taking the vector between their means to define the decision plane and eyeball a
crude threshold. First we will load the data and compute the averages.
# Load in the dataset
trans =[]
(continues on next page)
904 Mathematics for Deep Learning
(continued from previous page)
trans .append(transforms .ToTensor())
trans =transforms .Compose(trans)
train =torchvision .datasets .FashionMNIST(root ="../data ", transform =trans,
train =True , download =True )
test =torchvision .datasets .FashionMNIST(root ="../data ", transform =trans,
train =False , download =True )
X_train_0 =torch .stack(
[x[0]*256 for xintrain ifx[1]==0]).type(torch .float32)
X_train_1 =torch .stack(
[x[0]*256 for xintrain ifx[1]==1]).type(torch .float32)
X_test =torch .stack(
[x[0]*256 for xintest ifx[1]==0orx[1]==1]).type(torch .float32)
y_test =torch .stack([torch .tensor(x[ 1])for xintest
ifx[1]==0orx[1]==1]).type(torch .float32)
# Compute averages
ave_0 =torch .mean(X_train_0, axis =0)
ave_1 =torch .mean(X_train_1, axis =0)
It can be informative to examine these averages in detail, so let‚Äôs plot what they look like.
In this case, we see that the average indeed resembles a blurry image of a t-shirt.
# Plot average t-shirt
d2l.set_figsize()
d2l.plt.imshow(ave_0 .reshape( 28,28).tolist(), cmap ='Greys ')
d2l.plt.show()
Inthesecondcase,weagainseethattheaverageresemblesablurryimageoftrousers.
# Plot average trousers
d2l.plt.imshow(ave_1 .reshape( 28,28).tolist(), cmap ='Greys ')
d2l.plt.show()
In a fully machine learned solution, we would learn the threshold from the dataset. In this
case, I simply eyeballed a threshold that looked good on the training data by hand.
# Print test set accuracy with eyeballed threshold
w=(ave_1 -ave_0) .T
(continues on next page)
905 Geometry and Linear Algebraic Operations
(continued from previous page)
# '@' is Matrix Multiplication operator in pytorch.
predictions =X_test .reshape( 2000 ,-1)@(w.flatten()) >-1500000
# Accuracy
torch .mean((predictions .type(y_test .dtype) ==y_test) .float(), dtype =torch .
‚Ü©!float64)
tensor( 0.7870 , dtype =torch .float64)
A.1.4Geometryof Linear Transformations
Through Section2.3 andtheabovediscussions,wehaveasolidunderstandingofthegeom-
etryofvectors,lengths,andangles. However,thereisoneimportantobjectwehaveomitted
discussing, and that is a geometric understanding of linear transformations represented by
matrices. Fully internalizing what matrices can do to transform data between two poten-
tially different high dimensional spaces takes significant practice, and is beyond the scope
of this appendix. However, we can start building up intuition in two dimensions.
Suppose that we have some matrix:
A=ùëé ùëè
ùëê ùëë
. (A.11)
If we want to apply this to an arbitrary vector v=¬ªùë•,ùë¶¬º>, we multiply and see that
Av=ùëé ùëè
ùëê ùëë ùë•
ùë¶
=ùëéùë•¬∏ùëèùë¶
ùëêùë•¬∏ùëëùë¶
=ùë•ùëé
ùëê
¬∏ùë¶ùëè
ùëë
=ùë•
A1
0
¬∏ùë¶
A0
1
.(A.12)
This may seem like an odd computation, where something clear became somewhat impen-
etrable. However, it tells us that we can write the way that a matrix transforms anyvector
906 Mathematics for Deep Learning
in terms of how it transforms two specific vectors :¬ª1,0¬º>and¬ª0,1¬º>. This is worth con-
sidering for a moment. We have essentially reduced an infinite problem (what happens to
any pair of real numbers) to a finite one (what happens to these specific vectors). These
vectors are an example a basis, where we can write any vector in our space as a weighted
sum of these basis vectors .
Let‚Äôs draw what happens when we use the specific matrix
A=1 2
 1 3
. (A.13)
If we look at the specific vector v=¬ª2, 1¬º>, we see this is 2¬ª1,0¬º>¬∏ 1¬ª0,1¬º>, and
thusweknowthatthematrix ùê¥willsendthisto 2¬πA¬ª1,0¬º>¬∫¬∏ 1¬πA¬ª0,1¬º¬∫>=2¬ª1, 1¬º> 
¬ª2,3¬º>=¬ª0, 5¬º>. If we follow this logic through carefully, say by considering the grid
of all integer pairs of points, we see that what happens is that the matrix multiplication
can skew, rotate, and scale the grid, but the grid structure must remain as you see in Fig.
A.8.
tFig. A.8 The matrix Aacting on the given basis vectors. Notice how the entire grid is transported
along with it.
This is the most important intuitive point to internalize about linear transformations rep-
resented by matrices. Matrices are incapable of distorting some parts of space differently
than others. All they can do is take the original coordinates on our space and skew, rotate,
and scale them.
Some distortions can be severe. For instance the matrix
B=2 1
4 2
, (A.14)
compressestheentiretwo-dimensionalplanedowntoasingleline. Identifyingandworking
with such transformations are the topic of a later section, but geometrically we can see
that this is fundamentally different from the types of transformations we saw above. For
instance, theresultfrommatrix Acanbe‚Äúbentback‚Äùtotheoriginalgrid. Theresultsfrom
matrix Bcannot because we will never know where the vector ¬ª1,2¬º>came from‚Äîwas it
¬ª1,1¬º>or¬ª0, 1¬º>?
Whilethispicturewasfora 22matrix,nothingpreventsusfromtakingthelessonslearned
intohigherdimensions. Ifwetakesimilarbasisvectorslike ¬ª1,0,..., 0¬ºandseewhereour
matrix sends them, we can start to get a feeling for how the matrix multiplication distorts
the entire space in whatever dimension space we are dealing with.
907 Geometry and Linear Algebraic Operations
A.1.5LinearDependence
Consider again the matrix
B=2 1
4 2
. (A.15)
This compresses the entire plane down to live on the single line ùë¶=2ùë•. The question now
arises: is there some way we can detect this just looking at the matrix itself? The answer is
that indeed we can. Let‚Äôs take b1=¬ª2,4¬º>andb2=¬ª 1, 2¬º>be the two columns of B.
Remember that we can write everything transformed bythe matrix Bas a weighted sum of
the columns of the matrix: like ùëé1b1¬∏ùëé2b2. We call this a linear combination . The fact
thatb1= 2b2means that we can write any linear combination of those two columns
entirely in terms of say b2since
ùëé1b1¬∏ùëé2b2= 2ùëé1b2¬∏ùëé2b2=¬πùëé2 2ùëé1¬∫b2. (A.16)
This means that one of the columns is, in a sense, redundant because it does not define a
unique direction in space. This should not surprise us too much since we already saw that
this matrix collapses the entire plane down into a single line. Moreover, we see that the
linear dependence b1= 2b2captures this. To make this more symmetrical between the
two vectors, we will write this as
b1¬∏2b2=0. (A.17)
Ingeneral,wewillsaythatacollectionofvectors v1,...,vùëòarelinearlydependent ifthere
exist coefficients ùëé1,...,ùëéùëònotall equaltozero so that
ùëò√ï
ùëñ=1ùëéùëñvi=0. (A.18)
Inthiscase, wecansolveforoneofthevectorsintermsofsomecombinationoftheothers,
and effectively render it redundant. Thus, a linear dependence in the columns of a matrix
is a witness to the fact that our matrix is compressing the space down to some lower di-
mension. If there is no linear dependence we say the vectors are linearly independent . If
the columns of a matrix are linearly independent, no compression occurs and the operation
can be undone.
A.1.6Rank
If we have a general ùëõùëömatrix, it is reasonable to ask what dimension space the matrix
maps into. A concept known as the rankwill be our answer. In the previous section, we
notedthatalineardependencebearswitnesstocompressionofspaceintoalowerdimension
and so we will be able to use this to define the notion of rank. In particular, the rank of
a matrix Ais the largest number of linearly independent columns amongst all subsets of
columns. For example, the matrix
B=2 4
 1 2
, (A.19)
908 Mathematics for Deep Learning
has rank¬πùêµ¬∫=1, since the two columns are linearly dependent, but either column by itself
is not linearly dependent. For a more challenging example, we can consider
C=266666641 3 0 1 0
 1 0 1 1 1
0 3 1 0 1
2 3 1 2 137777775, (A.20)
and show that Chas rank two since, for instance, the first two columns are linearly inde-
pendent, however any of the four collections of three columns are dependent.
This procedure, as described, is very inefficient. It requires looking at every subset of the
columnsofourgivenmatrix, andthusispotentiallyexponentialinthenumberofcolumns.
Later we will see a more computationally efficient way to compute the rank of a matrix,
but for now, this is sufficient to see that the concept is well defined and understand the
meaning.
A.1.7Invertibility
Wehaveseenabovethatmultiplicationbyamatrixwithlinearlydependentcolumnscannot
be undone, i.e., there is no inverse operation that can always recover the input. However,
multiplication by a full-rank matrix (i.e., some Athat isùëõùëõmatrix with rank ùëõ), we
should always be able to undo it. Consider the matrix
I=2666666641 0 0
0 1 0
............
0 0 1377777775. (A.21)
which is the matrix with ones along the diagonal, and zeros elsewhere. We call this the
identitymatrix. It is the matrix which leaves our data unchanged when applied. To find
a matrix which undoes what our matrix Ahas done, we want to find a matrix A 1such
that
A 1A=AA 1=I. (A.22)
If we look at this as a system, we have ùëõùëõunknowns (the entries of A 1) andùëõùëõ
equations (the equality that needs to hold between every entry of the product A 1Aand
every entry of I) so we should generically expect a solution to exist. Indeed, in the next
sectionwewillseeaquantitycalledthe determinant ,whichhasthepropertythataslongas
the determinant is not zero, we can find a solution. We call such a matrix A 1theinverse
matrix. As an example, if Ais the general 22matrix
A=ùëé ùëè
ùëê ùëë
, (A.23)
then we can see that the inverse is
1
ùëéùëë ùëèùëêùëë ùëè
 ùëê ùëé
. (A.24)
909 Geometry and Linear Algebraic Operations
We can test to see this by seeing that multiplying by the inverse given by the formula above
works in practice.
M=torch .tensor([[ 1,2], [ 1,4]], dtype =torch .float32)
M_inv =torch .tensor([[ 2,-1], [ -0.5,0.5]])
M_inv @M
tensor([[ 1.,0.],
[0.,1.]])
NumericalIssues
While the inverse of a matrix is useful in theory, we must say that most of the time we do
not wish to usethe matrix inverse to solve a problem in practice. In general, there are far
more numerically stable algorithms for solving linear equations like
Ax=b, (A.25)
than computing the inverse and multiplying to get
x=A 1b. (A.26)
Just as division by a small number can lead to numerical instability, so can inversion of a
matrix which is close to having low rank.
Moreover, it is common that the matrix Aissparse, which is to say that it contains only a
small number of non-zero values. If we were to explore examples, we would see that this
doesnotmeantheinverseissparse. Evenif Awasa 1millionby 1millionmatrixwithonly
5million non-zero entries (and thus we need only store those 5million), the inverse will
typically have almost every entry non-negative, requiring us to store all 1M2entries‚Äîthat
is1trillion entries!
While we do not have time to dive all the way into the thorny numerical issues frequently
encounteredwhenworkingwithlinearalgebra,wewanttoprovideyouwithsomeintuition
about when to proceed with caution, and generally avoiding inversion in practice is a good
rule of thumb.
A.1.8Determinant
The geometric view of linear algebra gives an intuitive way to interpret a fundamental
quantity known as the determinant . Consider the grid image from before, but now with a
highlighted region ( Fig. A.9).
Look at the highlighted square. This is a square with edges given by ¬π0,1¬∫and¬π1,0¬∫and
thusithasareaone. After Atransformsthissquare,weseethatitbecomesaparallelogram.
There is no reason this parallelogram should have the same area that we started with, and
910 Mathematics for Deep Learning
tFig. A.9 The matrix Aagain distorting the grid. This time, I want to draw particular attention to
what happens to the highlighted square.
indeed in the specific case shown here of
A=1 2
 1 3
, (A.27)
itisanexerciseincoordinategeometrytocomputetheareaofthisparallelogramandobtain
that the area is 5.
In general, if we have a matrix
A=ùëé ùëè
ùëê ùëë
, (A.28)
we can see with some computation that the area of the resulting parallelogram is ùëéùëë ùëèùëê.
This area is referred to as the determinant .
Let‚Äôs check this quickly with some example code.
torch .det(torch .tensor([[ 1,-1], [ 2,3]], dtype =torch .float32))
tensor( 5.)
The eagle-eyed amongst us will notice that this expression can be zero or even negative.
For the negative term, this is a matter of convention taken generally in mathematics: if the
matrix flips the figure, we say the area is negated. Let‚Äôs see now that when the determinant
is zero, we learn more.
Let‚Äôs consider
B=2 4
 1 2
. (A.29)
If we compute the determinant of this matrix, we get 2¬π 2¬∫ 4¬π 1¬∫=0. Given our
understanding above, this makes sense. Bcompresses the square from the original image
down to a line segment, which has zero area. And indeed, being compressed into a lower
dimensional space is the only way to have zero area after the transformation. Thus we see
the following result is true: a matrix ùê¥is invertible if and only if the determinant is not
equal to zero.
911 Geometry and Linear Algebraic Operations
As a final comment, imagine that we have any figure drawn on the plane. Thinking like
computer scientists, we can decompose that figure into a collection of little squares so that
the area of the figure is in essence just the number of squares in the decomposition. If we
nowtransformthatfigurebyamatrix,wesendeachofthesesquarestoparallelograms,each
oneofwhichhasareagivenbythedeterminant. Weseethatforanyfigure,thedeterminant
gives the (signed) number that a matrix scales the area of any figure.
Computing determinants for larger matrices can be laborious, but the intuition is the same.
Thedeterminantremainsthefactorthat ùëõùëõmatricesscale ùëõ-dimensionalvolumes.
A.1.9Tensorsand Common Linear AlgebraOperations
InSection 2.3 the concept of tensors was introduced. In this section, we will dive more
deeply into tensor contractions (the tensor equivalent of matrix multiplication), and see
how it can provide a unified view on a number of matrix and vector operations.
With matrices and vectors we knew how to multiply them to transform data. We need
to have a similar definition for tensors if they are to be useful to us. Think about matrix
multiplication:
C=AB, (A.30)
or equivalently
ùëêùëñ,ùëó=√ï
ùëòùëéùëñ,ùëòùëèùëò,ùëó.(A.31)
This pattern is one we can repeat for tensors. For tensors, there is no one case of what to
sum over that can be universally chosen, so we need specify exactly which indices we want
to sum over. For instance we could consider
ùë¶ùëñùëô=√ï
ùëóùëòùë•ùëñùëóùëòùëôùëéùëóùëò.(A.32)
Such a transformation is called a tensor contraction . It can represent a far more flexible
family of transformations that matrix multiplication alone.
As a often-used notational simplification, we can notice that the sum is over exactly those
indices that occur more than once in the expression, thus people often work with Einstein
notation, where the summation is implicitly taken over all repeated indices. This gives the
compact expression:
ùë¶ùëñùëô=ùë•ùëñùëóùëòùëôùëéùëóùëò. (A.33)
Common ExamplesfromLinear Algebra
Let‚Äôsseehowmanyofthelinearalgebraicdefinitionswehaveseenbeforecanbeexpressed
in this compressed tensor notation:
vw=√ç
ùëñùë£ùëñùë§ùëñ
kvk2
2=√ç
ùëñùë£ùëñùë£ùëñ
912 Mathematics for Deep Learning
¬πAv¬∫ùëñ=√ç
ùëóùëéùëñùëóùë£ùëó
¬πAB¬∫ùëñùëò=√ç
ùëóùëéùëñùëóùëèùëóùëò
tr¬πA¬∫=√ç
ùëñùëéùëñùëñ
In this way, we can replace a myriad of specialized notations with short tensor expres-
sions.
Expressingin Code
Tensors may flexibly be operated on in code as well. As seen in Section 2.3 , we can create
tensors as is shown below.
# Define tensors
B=torch .tensor([[[ 1,2,3], [ 4,5,6]], [[ 7,8,9], [ 10,11,12]]])
A=torch .tensor([[ 1,2], [ 3,4]])
v=torch .tensor([ 1,2])
# Print out the shapes
A.shape, B .shape, v .shape
(torch .Size([ 2,2]), torch .Size([ 2,2,3]), torch .Size([ 2]))
Einstein summation has been implemented directly. The indices that occurs in the Einstein
summationcanbepassedasastring,followedbythetensorsthatarebeingactedupon. For
instance,toimplementmatrixmultiplication,wecanconsidertheEinsteinsummationseen
above( Av=ùëéùëñùëóùë£ùëó)andstripouttheindicesthemselvestogettheimplementation:
# Reimplement matrix multiplication
torch .einsum( "ij, j -> i ", A, v), A @v
(tensor([ 5,11]), tensor([ 5,11]))
This is a highly flexible notation. For instance if we want to compute what would be tradi-
tionally written as
ùëêùëòùëô=√ï
ùëñùëóbùëñùëóùëòaùëñùëôùë£ùëó.(A.34)
it can be implemented via Einstein summation as:
torch .einsum( "ijk, il, j -> kl ", B, A, v)
tensor([[ 90,126],
[102,144],
[114,162]])
913 Geometry and Linear Algebraic Operations
Thisnotationisreadableandefficientforhumans,howeverbulkyifforwhateverreasonwe
need to generate a tensor contraction programmatically. For this reason, einsumprovides
an alternative notation by providing integer indices for each tensor. For example, the same
tensor contraction can also be written as:
# PyTorch does not support this type of notation.
Either notation allows for concise and efficient representation of tensor contractions in
code.
A.1.10Summary
Vectors can be interpreted geometrically as either points or directions in space.
Dot products define the notion of angle to arbitrarily high-dimensional spaces.
Hyperplanesarehigh-dimensionalgeneralizationsoflinesandplanes. Theycanbeused
to define decision planes that are often used as the last step in a classification task.
Matrix multiplication can be geometrically interpreted as uniform distortions of the un-
derlying coordinates. They represent a very restricted, but mathematically clean, way
to transform vectors.
Lineardependenceisawaytotellwhenacollectionofvectorsareinalowerdimensional
space than we would expect (say you have 3vectors living in a 2-dimensional space).
The rank of a matrix is the size of the largest subset of its columns that are linearly
independent.
Whenamatrix‚Äôsinverseisdefined,matrixinversionallowsustofindanothermatrixthat
undoes the action of the first. Matrix inversion is useful in theory, but requires care in
practice owing to numerical instability.
Determinants allow us to measure how much a matrix expands or contracts a space. A
nonzero determinant implies an invertible (non-singular) matrix and a zero-valued
determinant means that the matrix is non-invertible (singular).
Tensor contractions and Einstein summation provide for a neat and clean notation for
expressing many of the computations that are seen in machine learning.
A.1.11Exercises
1.What is the angle between
¬Æùë£1=266666641
0
 1
237777775,¬Æùë£2=266666643
1
0
137777775? (A.35)
2.True or false:1 2
0 1
and1 2
0 1
are inverses of one another?
914 Mathematics for Deep Learning
2803.Suppose that we draw a shape in the plane with area 100m2. What is the area after
transforming the figure by the matrix
2 3
1 2
. (A.36)
4.Which of the following sets of vectors are linearly independent?
8>>> <
>>>:¬©¬≠¬≠
¬´1
0
 1¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´2
1
 1¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´3
1
1¬™¬Æ¬Æ
¬¨9>>> =
>>>;
8>>> <
>>>:¬©¬≠¬≠
¬´3
1
1¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´1
1
1¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´0
0
0¬™¬Æ¬Æ
¬¨9>>> =
>>>;
8>>> <
>>>:¬©¬≠¬≠
¬´1
1
0¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´0
1
 1¬™¬Æ¬Æ
¬¨,¬©¬≠¬≠
¬´1
0
1¬™¬Æ¬Æ
¬¨9>>> =
>>>;
5.Suppose that you have a matrix written as ùê¥=ùëê
ùëë

ùëé ùëè
for some choice of values
ùëé,ùëè,ùëê, andùëë. True or false: the determinant of such a matrix is always 0?
6.The vectorsùëí1=1
0
andùëí2=0
1
are orthogonal. What is the condition on a matrix ùê¥
so thatùê¥ùëí1andùê¥ùëí2are orthogonal?
7.How can you write tr ¬πA4¬∫in Einstein notation for an arbitrary matrix ùê¥?
Discussions280.
A.2Eigendecompositions
Eigenvaluesareoftenoneofthemostusefulnotionswewillencounterwhenstudyinglinear
algebra,however,asabeginner,itiseasytooverlooktheirimportance. Below,weintroduce
eigendecomposition and try to convey some sense of just why it is so important.
Suppose that we have a matrix ùê¥with the following entries:
A=2 0
0 1
. (A.1)
If we applyùê¥to any vector v=¬ªùë•,ùë¶¬º>, we obtain a vector Av=¬ª2ùë•, ùë¶¬º>. This has an
intuitive interpretation: stretch the vector to be twice as wide in the ùë•-direction, and then
flip it in theùë¶-direction.
915 Eigendecompositions
However, there are somevectors for which something remains unchanged. Namely ¬ª1,0¬º>
gets sent to¬ª2,0¬º>and¬ª0,1¬º>gets sent to¬ª0, 1¬º>. These vectors are still in the same
line, and the only modification is that the matrix stretches them by a factor of 2and 1
respectively. We call such vectors eigenvectors and the factor they are stretched by eigen-
values.
In general, if we can find a number ùúÜand a vector vsuch that
Av=ùúÜv. (A.2)
We say that vis an eigenvector for ùê¥andùúÜis an eigenvalue.
A.2.1Finding Eigenvalues
Let‚Äôs figure out how to find them. By subtracting off the ùúÜvfrom both sides, and then
factoring out the vector, we see the above is equivalent to:
¬πA ùúÜI¬∫v=0. (A.3)
For(A.3)to happen, we see that ¬πA ùúÜI¬∫must compress some direction down to zero,
henceitisnotinvertible,andthusthedeterminantiszero. Thus,wecanfindthe eigenvalues
by finding for what ùúÜisdet¬πA ùúÜI¬∫=0. Once we find the eigenvalues, we can solve
Av=ùúÜvto find the associated eigenvector(s) .
AnExample
Let‚Äôs see this with a more challenging matrix
A=2 1
2 3
. (A.4)
If we consider det¬πA ùúÜI¬∫=0, we see this is equivalent to the polynomial equation
0=¬π2 ùúÜ¬∫¬π3 ùúÜ¬∫ 2=¬π4 ùúÜ¬∫¬π1 ùúÜ¬∫. Thus, two eigenvalues are 4and1. To find the
associated vectors, we then need to solve
2 1
2 3 ùë•
ùë¶
=ùë•
ùë¶
and2 1
2 3 ùë•
ùë¶
=4ùë•
4ùë¶
. (A.5)
We can solve this with the vectors ¬ª1, 1¬º>and¬ª1,2¬º>respectively.
We can check this in code using the built-in numpy.linalg.eig routine.
%matplotlib inline
import torch
from IPython import display
from d2l import torch asd2l
torch .linalg .eig(torch .tensor([[ 2,1], [ 2,3]], dtype =torch .float64))
916 Mathematics for Deep Learning
torch .return_types .linalg_eig(
eigenvalues =tensor([ 1.+0.j,4.+0.j], dtype =torch .complex128),
eigenvectors =tensor([[ -0.7071 +0.j,-0.4472 +0.j],
[0.7071 +0.j,-0.8944 +0.j]], dtype =torch .complex128))
Note that numpynormalizes the eigenvectors to be of length one, whereas we took ours to
be of arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors
computed are parallel to the ones we found by hand with the same eigenvalues.
A.2.2DecomposingMatrices
Let‚Äôs continue the previous example one step further. Let
W=1 1
 1 2
, (A.6)
be the matrix where the columns are the eigenvectors of the matrix A. Let
ùö∫=1 0
0 4
, (A.7)
bethematrixwiththeassociatedeigenvaluesonthediagonal. Thenthedefinitionofeigen-
values and eigenvectors tells us that
AW=Wùö∫. (A.8)
The matrixùëäis invertible, so we may multiply both sides by ùëä 1on the right, we see that
we may write
A=Wùö∫W 1. (A.9)
In the next section we will see some nice consequences of this, but for now we need only
knowthatsuchadecompositionwillexistaslongaswecanfindafullcollectionoflinearly
independent eigenvectors (so that ùëäis invertible).
A.2.3Operationson Eigendecompositions
One nice thing about eigendecompositions (A.9)is that we can write many operations we
usually encounter cleanly in terms of the eigendecomposition. As a first example, con-
sider:
Aùëõ=ùëõtimesz   }|   {
AA=ùëõtimesz                              }|                              {
¬πWùö∫W 1¬∫¬πWùö∫W 1¬∫=Wùëõtimesz  }|  {
ùö∫ùö∫W 1=Wùö∫ùëõW 1.(A.10)
This tells us that for any positive power of a matrix, the eigendecomposition is obtained by
justraisingtheeigenvaluestothesamepower. Thesamecanbeshownfornegativepowers,
so if we want to invert a matrix we need only consider
A 1=Wùö∫ 1W 1, (A.11)
917 Eigendecompositions
or in other words, just invert each eigenvalue. This will work as long as each eigenvalue is
non-zero, so we see that invertible is the same as having no zero eigenvalues.
Indeed, additional work can show that if ùúÜ1,...,ùúÜùëõare the eigenvalues of a matrix, then
the determinant of that matrix is
det¬πA¬∫=ùúÜ1ùúÜùëõ, (A.12)
ortheproductofalltheeigenvalues. Thismakessenseintuitivelybecausewhateverstretch-
ingWdoes,ùëä 1undoes it, so in the end the only stretching that happens is by multipli-
cation by the diagonal matrix ùö∫, which stretches volumes by the product of the diagonal
elements.
Finally, recall that the rank was the maximum number of linearly independent columns of
your matrix. By examining the eigendecomposition closely, we can see that the rank is the
same as the number of non-zero eigenvalues of A.
The examples could continue, but hopefully the point is clear: eigendecomposition can
simplify many linear-algebraic computations and is a fundamental operation underlying
many numerical algorithms and much of the analysis that we do in linear algebra.
A.2.4Eigendecompositionsof SymmetricMatrices
It is not always possible to find enough linearly independent eigenvectors for the above
process to work. For instance the matrix
A=1 1
0 1
, (A.13)
has only a single eigenvector, namely ¬π1,0¬∫>. To handle such matrices, we require more
advancedtechniquesthanwecancover(suchastheJordanNormalForm,orSingularValue
Decomposition). We will often need to restrict our attention to those matrices where we
can guarantee the existence of a full set of eigenvectors.
The most commonly encountered family are the symmetric matrices , which are those ma-
trices where A=A>. In this case, we may take ùëäto be anorthogonal matrix ‚Äîa matrix
whose columns are all length one vectors that are at right angles to one another, where
W>=W 1‚Äîand all the eigenvalues will be real. Thus, in this special case, we can write
(A.9)as
A=Wùö∫W>. (A.14)
A.2.5GershgorinCircleTheorem
Eigenvalues are often difficult to reason with intuitively. If presented an arbitrary matrix,
thereislittlethatcanbesaidaboutwhattheeigenvaluesarewithoutcomputingthem. There
is, however, one theorem that can make it easy to approximate well if the largest values are
on the diagonal.
LetA=¬πùëéùëñùëó¬∫be any square matrix ( ùëõùëõ). We will define ùëüùëñ=√ç
ùëó‚â†ùëñjùëéùëñùëój. LetDùëñ
918 Mathematics for Deep Learning
represent the disc in the complex plane with center ùëéùëñùëñradiusùëüùëñ. Then, every eigenvalue of
Ais contained in one of the Dùëñ.
This can be a bit to unpack, so let‚Äôs look at an example. Consider the matrix:
A=266666641.0 0.1 0.1 0.1
0.1 3.0 0.2 0.3
0.1 0.2 5.0 0.5
0.1 0.3 0.5 9.037777775. (A.15)
We haveùëü1=0.3,ùëü2=0.6,ùëü3=0.8andùëü4=0.9. The matrix is symmetric, so all
eigenvalues are real. This means that all of our eigenvalues will be in one of the ranges
of
¬ªùëé11 ùëü1,ùëé11¬∏ùëü1¬º=¬ª0.7,1.3¬º, (A.16)
¬ªùëé22 ùëü2,ùëé22¬∏ùëü2¬º=¬ª2.4,3.6¬º, (A.17)
¬ªùëé33 ùëü3,ùëé33¬∏ùëü3¬º=¬ª4.2,5.8¬º, (A.18)
¬ªùëé44 ùëü4,ùëé44¬∏ùëü4¬º=¬ª8.1,9.9¬º. (A.19)
Performing the numerical computation shows that the eigenvalues are approximately 0.99,
2.97,4.95,9.08, all comfortably inside the ranges provided.
A=torch .tensor([[ 1.0,0.1,0.1,0.1],
[0.1,3.0,0.2,0.3],
[0.1,0.2,5.0,0.5],
[0.1,0.3,0.5,9.0]])
v, _ =torch .linalg .eig(A)
v
tensor([ 0.9923 +0.j,9.0803 +0.j,4.9539 +0.j,2.9734 +0.j])
Inthisway,eigenvaluescanbeapproximated,andtheapproximationswillbefairlyaccurate
in the case that the diagonal is significantly larger than all the other elements.
It is a small thing, but with a complex and subtle topic like eigendecomposition, it is good
to get any intuitive grasp we can.
A.2.6AUseful Application: The Growthof Iterated Maps
Now that we understand what eigenvectors are in principle, let‚Äôs see how they can be used
to provide a deep understanding of a problem central to neural network behavior: proper
weight initialization.
919 Eigendecompositions
Eigenvectorsas Long TermBehavior
The full mathematical investigation of the initialization of deep neural networks is beyond
the scope of the text, but we can see a toy version here to understand how eigenvalues can
help us see how these models work. As we know, neural networks operate by interspersing
layers of linear transformations with non-linear operations. For simplicity here, we will
assumethatthereisnonon-linearity,andthatthetransformationisasinglerepeatedmatrix
operationùê¥, so that the output of our model is
vùëúùë¢ùë°=AAAvùëñùëõ=AùëÅvùëñùëõ. (A.20)
When these models are initialized, ùê¥is taken to be a random matrix with Gaussian entries,
solet‚Äôsmakeoneofthose. Tobeconcrete,westartwithameanzero,varianceoneGaussian
distributed 55matrix.
torch .manual_seed( 42)
k=5
A=torch .randn(k, k, dtype =torch .float64)
A
tensor([[ 0.2996 ,0.2424 ,0.2832 ,-0.2329 ,0.6712 ],
[0.7818 ,-1.7903 ,-1.7484 ,0.1735 ,-0.1182 ],
[-1.7446 ,-0.4695 ,0.4573 ,0.5177 ,-0.2771 ],
[-0.6641 ,0.6551 ,0.2616 ,-1.5265 ,-0.3311 ],
[-0.6378 ,0.1072 ,0.7096 ,0.3009 ,-0.2869 ]], dtype =torch .float64)
Behavioron Random Data
For simplicity in our toy model, we will assume that the data vector we feed in vùëñùëõis a
random five dimensional Gaussian vector. Let‚Äôs think about what we want to have happen.
Forcontext,letsthinkofagenericMLproblem,wherewearetryingtoturninputdata,like
an image, into a prediction, like the probability the image is a picture of a cat. If repeated
application of Astretches a random vector out to be very long, then small changes in input
willbeamplifiedintolargechangesinoutput‚Äîtinymodificationsoftheinputimagewould
lead to vastly different predictions. This does not seem right!
Ontheflipside,if Ashrinksrandomvectorstobeshorter,thenafterrunningthroughmany
layers, the vector will essentially shrink to nothing, and the output will not depend on the
input. This is also clearly not right either!
We need to walk the narrow line between growth and decay to make sure that our output
changes depending on our input, but not much!
Let‚Äôs see what happens when we repeatedly multiply our matrix Aagainst a random input
vector, and keep track of the norm.
920 Mathematics for Deep Learning
# Calculate the sequence of norms after repeatedly applying `A`
v_in =torch .randn(k, 1, dtype =torch .float64)
norm_list =[torch .norm(v_in) .item()]
for iinrange (1,100):
v_in =A@v_in
norm_list .append(torch .norm(v_in) .item())
d2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')
The norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a
pattern.
# Compute the scaling factor of the norms
norm_ratio_list =[]
for iinrange (1,100):
norm_ratio_list .append(norm_list[i] /norm_list[i -1])
d2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')
If we look at the last portion of the above computation, we see that the random vector is
stretched by a factor of 1.974459321485[...] , where the portion at the end shifts a little,
but the stretching factor is stable.
921 Eigendecompositions
RelatingBackto Eigenvectors
We have seen that eigenvectors and eigenvalues correspond to the amount something is
stretched, but that was for specific vectors, and specific stretches. Let‚Äôs take a look at what
they are for A. A bit of a caveat here: it turns out that to see them all, we will need to go
to complex numbers. You can think of these as stretches and rotations. By taking the norm
of the complex number (square root of the sums of squares of real and imaginary parts) we
can measure that stretching factor. Let‚Äôs also sort them.
# Compute the eigenvalues
eigs =torch .linalg .eig(A) .eigenvalues .tolist()
norm_eigs =[torch .abs(torch .tensor(x)) for xineigs]
norm_eigs .sort()
print (f'norms of eigenvalues: {norm_eigs }')
norms of eigenvalues: [tensor( 0.3490 ), tensor( 1.1296 ), tensor( 1.1296 ),‚ê£
‚Ü©!tensor( 1.1828 ), tensor( 2.4532 )]
AnObservation
Weseesomethingabitunexpectedhappeninghere: thatnumberweidentifiedbeforeforthe
longtermstretchingofourmatrix Aappliedtoarandomvectoris exactly(accuratetothir-
teendecimalplaces!) thelargesteigenvalueof A. Thisisclearlynotacoincidence!
But,ifwenowthinkaboutwhatishappeninggeometrically,thisstartstomakesense. Con-
siderarandomvector. Thisrandomvectorpointsalittleineverydirection,soinparticular,
it points at least a little bit in the same direction as the eigenvector of Aassociated with
the largest eigenvalue. This is so important that it is called the principle eigenvalue and
principle eigenvector . After applying A, our random vector gets stretched in every possi-
ble direction, as is associated with every possible eigenvector, but it is stretched most of
all in the direction associated with this principle eigenvector. What this means is that after
apply inùê¥, our random vector is longer, and points in a direction closer to being aligned
with the principle eigenvector. After applying the matrix many times, the alignment with
the principle eigenvector becomes closer and closer until, for all practical purposes, our
random vector has been transformed into the principle eigenvector! Indeed this algorithm
is the basis for what is known as the power iteration for finding the largest eigenvalue and
eigenvector of a matrix. For details see, for example, ( Golub and Van Loan, 1996 ).
Fixing the Normalization
Now, from above discussions, we concluded that we do not want a random vector to be
stretched or squished at all, we would like random vectors to stay about the same size
throughout the entire process. To do so, we now rescale our matrix by this principle eigen-
value so that the largest eigenvalue is instead now just one. Let‚Äôs see what happens in this
case.
922 Mathematics for Deep Learning
# Rescale the matrix `A`
A/=norm_eigs[ -1]
# Do the same experiment again
v_in =torch .randn(k, 1, dtype =torch .float64)
norm_list =[torch .norm(v_in) .item()]
for iinrange (1,100):
v_in =A@v_in
norm_list .append(torch .norm(v_in) .item())
d2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')
We can also plot the ratio between consecutive norms as before and see that indeed it sta-
bilizes.
# Also plot the ratio
norm_ratio_list =[]
for iinrange (1,100):
norm_ratio_list .append(norm_list[i] /norm_list[i -1])
d2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')
A.2.7Discussion
We now see exactly what we hoped for! After normalizing the matrices by the principal
eigenvalue, we see that the random data does not explode as before, but rather eventually
equilibrates to a specific value. It would be nice to be able to do these things from first
923 Eigendecompositions
281principles, and it turns out that if we look deeply at the mathematics of it, we can see that
the largest eigenvalue of a large random matrix with independent mean zero, variance one
Gaussian entries is on average aboutpùëõ, or in our casep
52.2, due to a fascinating fact
known as the circularlaw (Ginibre, 1965 ). The relationship between the eigenvalues (and
a related object called singular values) of random matrices has been shown to have deep
connections to proper initialization of neural networks as was discussed in Pennington et
al.(2017) and subsequent works.
A.2.8Summary
Eigenvectors are vectors which are stretched by a matrix without changing direction.
Eigenvalues are the amount that the eigenvectors are stretched by the application of the
matrix.
The eigendecomposition of a matrix can allow for many operations to be reduced to
operations on the eigenvalues.
The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of
a matrix.
Thebehaviorofiteratedmatrixpowersdependsprimarilyonthesizeofthelargesteigen-
value. This understanding has many applications in the theory of neural network ini-
tialization.
A.2.9Exercises
1.What are the eigenvalues and eigenvectors of
A=2 1
1 2
? (A.21)
2.What are the eigenvalues and eigenvectors of the following matrix, and what is strange
about this example compared to the previous one?
A=2 1
0 2
. (A.22)
3.Without computing the eigenvalues, is it possible that the smallest eigenvalue of the
following matrix is less that 0.5?Note: this problem can be done in your head.
A=266666643.0 0.1 0.3 1.0
0.1 1.0 0.1 0.2
0.3 0.1 5.0 0.0
1.0 0.2 0.0 1.837777775. (A.23)
Discussions281.
924 Mathematics for Deep Learning
A.3SingleVariableCalculus
InSection 2.4 , we saw the basic elements of differential calculus. This section takes a
deeper dive into the fundamentals of calculus and how we can understand and apply it in
the context of machine learning.
A.3.1DifferentialCalculus
Differentialcalculusisfundamentallythestudyofhowfunctionsbehaveundersmallchanges.
To see why this is so core to deep learning, let‚Äôs consider an example.
Suppose that we have a deep neural network where the weights are, for convenience, con-
catenated into a single vector w=¬πùë§1,...,ùë§ùëõ¬∫. Given a training dataset, we consider the
loss of our neural network on this dataset, which we will write as L¬πw¬∫.
This function is extraordinarily complex, encoding the performance of all possible models
ofthegivenarchitectureonthisdataset,soitisnearlyimpossibletotellwhatsetofweights
wwill minimize the loss. Thus, in practice, we often start by initializing our weights ran-
domly, and then iteratively take small steps in the direction which makes the loss decrease
as rapidly as possible.
The question then becomes something that on the surface is no easier: how do we find
the direction which makes the weights decrease as quickly as possible? To dig into this,
let‚Äôs first examine the case with only a single weight: ùêø¬πw¬∫=ùêø¬πùë•¬∫for a single real value
ùë•.
Let‚Äôs takeùë•and try to understand what happens when we change it by a small amount to
ùë•¬∏ùúñ. If you wish to be concrete, think a number like ùúñ=0.0000001 . To help us visualize
what happens, let‚Äôs graph an example function, ùëì¬πùë•¬∫=sin¬πùë•ùë•¬∫, over the¬ª0,3¬º.
%matplotlib inline
import torch
from IPython import display
from d2l import torch asd2l
torch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch
# Plot a function in a normal range
x_big =torch .arange( 0.01 ,3.01 ,0.01 )
ys=torch .sin(x_big **x_big)
d2l.plot(x_big, ys, 'x','f(x) ')
Atthislargescale,thefunction‚Äôsbehaviorisnotsimple. However,ifwereduceourrangeto
something smaller like ¬ª1.75,2.25¬º, we see that the graph becomes much simpler.
# Plot a the same function in a tiny range
x_med =torch .arange( 1.75 ,2.25 ,0.001 )
(continues on next page)
925 Single Variable Calculus
(continued from previous page)
ys=torch .sin(x_med **x_med)
d2l.plot(x_med, ys, 'x','f(x) ')
Takingthistoanextreme,ifwezoomintoatinysegment,thebehaviorbecomesfarsimpler:
it is just a straight line.
# Plot a the same function in a tiny range
x_small =torch .arange( 2.0,2.01 ,0.0001 )
ys=torch .sin(x_small **x_small)
d2l.plot(x_small, ys, 'x','f(x) ')
This is the key observation of single variable calculus: the behavior of familiar functions
can be modeled by a line in a small enough range. This means that for most functions, it
is reasonable to expect that as we shift the ùë•value of the function by a little bit, the output
ùëì¬πùë•¬∫willalsobeshiftedbyalittlebit. Theonlyquestionweneedtoansweris, ‚ÄúHowlarge
926 Mathematics for Deep Learning
is the change in the output compared to the change in the input? Is it half as large? Twice
as large?‚Äù
Thus, we can consider the ratio of the change in the output of a function for a small change
in the input of the function. We can write this formally as
ùêø¬πùë•¬∏ùúñ¬∫ ùêø¬πùë•¬∫
¬πùë•¬∏ùúñ¬∫ ùë•=ùêø¬πùë•¬∏ùúñ¬∫ ùêø¬πùë•¬∫
ùúñ. (A.1)
This is already enough to start to play around with in code. For instance, suppose that we
know thatùêø¬πùë•¬∫=ùë•2¬∏1701¬πùë• 4¬∫3, then we can see how large this value is at the point
ùë•=4as follows.
# Define our function
def L(x):
return x**2+1701 *(x-4)**3
# Print the difference divided by epsilon for several epsilon
for epsilon in[0.1,0.001 ,0.0001 ,0.00001 ]:
print (f'epsilon = {epsilon :.5f}->{(L(4+epsilon) -L(4))/epsilon :.5f}')
epsilon =0.10000 ->25.11000
epsilon =0.00100 ->8.00270
epsilon =0.00010 ->8.00012
epsilon =0.00001 ->8.00001
Now,ifweareobservant,wewillnoticethattheoutputofthisnumberissuspiciouslyclose
to8. Indeed,ifwedecrease ùúñ,wewillseevaluebecomesprogressivelycloserto 8. Thuswe
may conclude, correctly, that the value we seek (the degree a change in the input changes
the output) should be 8at the pointùë•=4. The way that a mathematician encodes this fact
is
lim
ùúñ!0ùêø¬π4¬∏ùúñ¬∫ ùêø¬π4¬∫
ùúñ=8. (A.2)
As a bit of a historical digression: in the first few decades of neural network research, sci-
entists used this algorithm (the methodoffinitedifferences ) to evaluate how a loss function
changed under small perturbation: just change the weights and see how the loss changed.
Thisiscomputationallyinefficient,requiringtwoevaluationsofthelossfunctiontoseehow
a single change of one variable influenced the loss. If we tried to do this with even a pal-
try few thousand parameters, it would require several thousand evaluations of the network
over the entire dataset! It was not solved until 1986 that the backpropagation algorithm
introduced in Rumelhart et al.(1988) provided a way to calculate how anychange of the
weightstogetherwouldchangethelossinthesamecomputationtimeasasingleprediction
of the network over the dataset.
Back in our example, this value 8is different for different values of ùë•, so it makes sense to
define it as a function of ùë•. More formally, this value dependent rate of change is referred
to as thederivative which is written as
ùëëùëì
ùëëùë•¬πùë•¬∫=lim
ùúñ!0ùëì¬πùë•¬∏ùúñ¬∫ ùëì¬πùë•¬∫
ùúñ. (A.3)
927 Single Variable Calculus
Different texts will use different notations for the derivative. For instance, all of the below
notations indicate the same thing:
ùëëùëì
ùëëùë•=ùëë
ùëëùë•ùëì=ùëì0=rùë•ùëì=ùê∑ùë•ùëì=ùëìùë•. (A.4)
Most authors will pick a single notation and stick with it, however even that is not guaran-
teed. It is best to be familiar with all of these. We will use the notationùëëùëì
ùëëùë•throughout this
text, unless we want to take the derivative of a complex expression, in which case we will
useùëë
ùëëùë•ùëìto write expressions like
ùëë
ùëëùë•
ùë•4¬∏cosùë•2¬∏1
2ùë• 1
. (A.5)
Oftentimes, it is intuitively useful to unravel the definition of derivative (A.3)again to see
how a function changes when we make a small change of ùë•:
ùëëùëì
ùëëùë•¬πùë•¬∫=lim
ùúñ!0ùëì¬πùë•¬∏ùúñ¬∫ ùëì¬πùë•¬∫
ùúñ=)ùëëùëì
ùëëùë•¬πùë•¬∫ùëì¬πùë•¬∏ùúñ¬∫ ùëì¬πùë•¬∫
ùúñ
=)ùúñùëëùëì
ùëëùë•¬πùë•¬∫ùëì¬πùë•¬∏ùúñ¬∫ ùëì¬πùë•¬∫
=)ùëì¬πùë•¬∏ùúñ¬∫ùëì¬πùë•¬∫¬∏ùúñùëëùëì
ùëëùë•¬πùë•¬∫.(A.6)
The last equation is worth explicitly calling out. It tells us that if you take any function and
change the input by a small amount, the output would change by that small amount scaled
by the derivative.
In this way, we can understand the derivative as the scaling factor that tells us how large of
change we get in the output from a change in the input.
A.3.2Rulesof Calculus
We now turn to the task of understanding how to compute the derivative of an explicit
function. A full formal treatment of calculus would derive everything from first principles.
We will not indulge in this temptation here, but rather provide an understanding of the
common rules encountered.
Common Derivatives
As was seen in Section 2.4 , when computing derivatives one can oftentimes use a series of
rules to reduce the computation to a few core functions. We repeat them here for ease of
reference.
Derivativeof constants.ùëë
ùëëùë•ùëê=0.
Derivativeof linear functions.ùëë
ùëëùë•¬πùëéùë•¬∫=ùëé.
Powerrule.ùëë
ùëëùë•ùë•ùëõ=ùëõùë•ùëõ 1.
Derivativeof exponentials.ùëë
ùëëùë•ùëíùë•=ùëíùë•.
Derivativeof the logarithm.ùëë
ùëëùë•log¬πùë•¬∫=1
ùë•.
928 Mathematics for Deep Learning
DerivativeRules
If every derivative needed to be separately computed and stored in a table, differential cal-
culus would be near impossible. It is a gift of mathematics that we can generalize the
above derivatives and compute more complex derivatives like finding the derivative of
ùëì¬πùë•¬∫=log 1¬∏¬πùë• 1¬∫10. As was mentioned in Section 2.4 , the key to doing so is to
codify what happens when we take functions and combine them in various ways, most
importantly: sums, products, and compositions.
Sum rule.ùëë
ùëëùë•¬πùëî¬πùë•¬∫¬∏‚Ñé¬πùë•¬∫¬∫=ùëëùëî
ùëëùë•¬πùë•¬∫¬∏ùëë‚Ñé
ùëëùë•¬πùë•¬∫.
Productrule.ùëë
ùëëùë•¬πùëî¬πùë•¬∫‚Ñé¬πùë•¬∫¬∫=ùëî¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫¬∏ùëëùëî
ùëëùë•¬πùë•¬∫‚Ñé¬πùë•¬∫.
Chain rule.ùëë
ùëëùë•ùëî¬π‚Ñé¬πùë•¬∫¬∫=ùëëùëî
ùëë‚Ñé¬π‚Ñé¬πùë•¬∫¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫.
Let‚Äôs see how we may use (A.6)to understand these rules. For the sum rule, consider
following chain of reasoning:
ùëì¬πùë•¬∏ùúñ¬∫=ùëî¬πùë•¬∏ùúñ¬∫¬∏‚Ñé¬πùë•¬∏ùúñ¬∫
ùëî¬πùë•¬∫¬∏ùúñùëëùëî
ùëëùë•¬πùë•¬∫¬∏‚Ñé¬πùë•¬∫¬∏ùúñùëë‚Ñé
ùëëùë•¬πùë•¬∫
=ùëî¬πùë•¬∫¬∏‚Ñé¬πùë•¬∫¬∏ùúñùëëùëî
ùëëùë•¬πùë•¬∫¬∏ùëë‚Ñé
ùëëùë•¬πùë•¬∫
=ùëì¬πùë•¬∫¬∏ùúñùëëùëî
ùëëùë•¬πùë•¬∫¬∏ùëë‚Ñé
ùëëùë•¬πùë•¬∫
.(A.7)
By comparing this result with the fact that ùëì¬πùë•¬∏ùúñ¬∫ùëì¬πùë•¬∫¬∏ùúñùëëùëì
ùëëùë•¬πùë•¬∫, we see thatùëëùëì
ùëëùë•¬πùë•¬∫=
ùëëùëî
ùëëùë•¬πùë•¬∫¬∏ùëë‚Ñé
ùëëùë•¬πùë•¬∫asdesired. Theintuitionhereis: whenwechangetheinput ùë•,ùëîand‚Ñéjointly
contribute to the change of the output byùëëùëî
ùëëùë•¬πùë•¬∫andùëë‚Ñé
ùëëùë•¬πùë•¬∫.
The product is more subtle, and will require a new observation about how to work with
these expressions. We will begin as before using (A.6):
ùëì¬πùë•¬∏ùúñ¬∫=ùëî¬πùë•¬∏ùúñ¬∫‚Ñé¬πùë•¬∏ùúñ¬∫

ùëî¬πùë•¬∫¬∏ùúñùëëùëî
ùëëùë•¬πùë•¬∫

‚Ñé¬πùë•¬∫¬∏ùúñùëë‚Ñé
ùëëùë•¬πùë•¬∫
=ùëî¬πùë•¬∫‚Ñé¬πùë•¬∫¬∏ùúñ
ùëî¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫¬∏ùëëùëî
ùëëùë•¬πùë•¬∫‚Ñé¬πùë•¬∫
¬∏ùúñ2ùëëùëî
ùëëùë•¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫
=ùëì¬πùë•¬∫¬∏ùúñ
ùëî¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫¬∏ùëëùëî
ùëëùë•¬πùë•¬∫‚Ñé¬πùë•¬∫
¬∏ùúñ2ùëëùëî
ùëëùë•¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫.(A.8)
This resembles the computation done above, and indeed we see our answer (ùëëùëì
ùëëùë•¬πùë•¬∫=
ùëî¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫¬∏ùëëùëî
ùëëùë•¬πùë•¬∫‚Ñé¬πùë•¬∫) sitting next to ùúñ, but there is the issue of that term of size ùúñ2.
We will refer to this as a higher-orderterm , since the power of ùúñ2is higher than the power
ofùúñ1. We will see in a later section that we will sometimes want to keep track of these,
however for now observe that if ùúñ=0.0000001 , thenùúñ2=0.0000000000001 , which is
vastly smaller. As we send ùúñ!0, we may safely ignore the higher order terms. As a
general convention in this appendix, we will use ‚Äú ‚Äù to denote that the two terms are equal
929 Single Variable Calculus
up to higher order terms. However, if we wish to be more formal we may examine the
difference quotient
ùëì¬πùë•¬∏ùúñ¬∫ ùëì¬πùë•¬∫
ùúñ=ùëî¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫¬∏ùëëùëî
ùëëùë•¬πùë•¬∫‚Ñé¬πùë•¬∫¬∏ùúñùëëùëî
ùëëùë•¬πùë•¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫, (A.9)
and see that as we send ùúñ!0, the right hand term goes to zero as well.
Finally,withthechainrule,wecanagainprogressasbeforeusing (A.6)andseethat
ùëì¬πùë•¬∏ùúñ¬∫=ùëî¬π‚Ñé¬πùë•¬∏ùúñ¬∫¬∫
ùëî
‚Ñé¬πùë•¬∫¬∏ùúñùëë‚Ñé
ùëëùë•¬πùë•¬∫
ùëî¬π‚Ñé¬πùë•¬∫¬∫¬∏ùúñùëë‚Ñé
ùëëùë•¬πùë•¬∫ùëëùëî
ùëë‚Ñé¬π‚Ñé¬πùë•¬∫¬∫
=ùëì¬πùë•¬∫¬∏ùúñùëëùëî
ùëë‚Ñé¬π‚Ñé¬πùë•¬∫¬∫ùëë‚Ñé
ùëëùë•¬πùë•¬∫,(A.10)
where in the second line we view the function ùëîas having its input ( ‚Ñé¬πùë•¬∫) shifted by the
tiny quantity ùúñùëë‚Ñé
ùëëùë•¬πùë•¬∫.
These rule provide us with a flexible set of tools to compute essentially any expression
desired. For instance,
ùëë
ùëëùë•h
log
1¬∏¬πùë• 1¬∫10i
=
1¬∏¬πùë• 1¬∫10 1ùëë
ùëëùë•
1¬∏¬πùë• 1¬∫10
=
1¬∏¬πùë• 1¬∫10 1ùëë
ùëëùë•¬ª1¬º¬∏ùëë
ùëëùë•¬ª¬πùë• 1¬∫10¬º
=
1¬∏¬πùë• 1¬∫10 1
0¬∏10¬πùë• 1¬∫9ùëë
ùëëùë•¬ªùë• 1¬º
=10
1¬∏¬πùë• 1¬∫10 1
¬πùë• 1¬∫9
=10¬πùë• 1¬∫9
1¬∏¬πùë• 1¬∫10.(A.11)
Where each line has used the following rules:
1.The chain rule and derivative of logarithm.
2.The sum rule.
3.The derivative of constants, chain rule, and power rule.
4.The sum rule, derivative of linear functions, derivative of constants.
Two things should be clear after doing this example:
1.Anyfunctionwecanwritedownusingsums,products,constants,powers,exponentials,
and logarithms can have its derivate computed mechanically by following these rules.
2.Having a human follow these rules can be tedious and error prone!
Thankfully, these two facts together hint towards a way forward: this is a perfect candidate
930 Mathematics for Deep Learning
for mechanization! Indeed backpropagation, which we will revisit later in this section, is
exactly that.
Linear Approximation
Whenworkingwithderivatives, itis oftenusefultogeometricallyinterprettheapproxima-
tion used above. In particular, note that the equation
ùëì¬πùë•¬∏ùúñ¬∫ùëì¬πùë•¬∫¬∏ùúñùëëùëì
ùëëùë•¬πùë•¬∫, (A.12)
approximates the value of ùëìby a line which passes through the point ¬πùë•, ùëì¬πùë•¬∫¬∫and has
slopeùëëùëì
ùëëùë•¬πùë•¬∫. In this way we say that the derivative gives a linear approximation to the
functionùëì, as illustrated below:
# Compute sin
xs=torch .arange( -torch .pi, torch .pi, 0.01 )
plots =[torch .sin(xs)]
# Compute some linear approximations. Use d(sin(x))/dx = cos(x)
for x0in[-1.5,0.0,2.0]:
plots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *
torch .cos(torch .tensor(x0)))
d2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])
HigherOrderDerivatives
Let‚Äôs now do something that may on the surface seem strange. Take a function ùëìand
compute the derivativeùëëùëì
ùëëùë•. This gives us the rate of change of ùëìat any point.
However, the derivative,ùëëùëì
ùëëùë•, can be viewed as a function itself, so nothing stops us from
computing the derivative ofùëëùëì
ùëëùë•to getùëë2ùëì
ùëëùë•2=ùëëùëì
ùëëùë•
ùëëùëì
ùëëùë•
. We will call this the second deriva-
tive ofùëì. This function is the rate of change of the rate of change of ùëì, or in other words,
how the rate of change is changing. We may apply the derivative any number of times to
obtain what is called the ùëõ-th derivative. To keep the notation clean, we will denote the
931 Single Variable Calculus
ùëõ-th derivative as
ùëì¬πùëõ¬∫¬πùë•¬∫=ùëëùëõùëì
ùëëùë•ùëõ=ùëë
ùëëùë•ùëõ
ùëì. (A.13)
Let‚Äôs try to understand whythis is a useful notion. Below, we visualize ùëì¬π2¬∫¬πùë•¬∫,ùëì¬π1¬∫¬πùë•¬∫,
andùëì¬πùë•¬∫.
First,considerthecasethatthesecondderivative ùëì¬π2¬∫¬πùë•¬∫isapositiveconstant. Thismeans
that the slope of the first derivative is positive. As a result, the first derivative ùëì¬π1¬∫¬πùë•¬∫may
start out negative, becomes zero at a point, and then becomes positive in the end. This tells
ustheslopeofouroriginalfunction ùëìandtherefore,thefunction ùëìitselfdecreases,flattens
out, then increases. In other words, the function ùëìcurves up, and has a single minimum as
is shown in Fig. A.1.
tFig. A.1 If we assume the second derivative is a positive constant, then the Ô¨Åst derivative in
increasing, which implies the function itself has a minimum.
Second, if the second derivative is a negative constant, that means that the first derivative
is decreasing. This implies the first derivative may start out positive, becomes zero at a
point, and then becomes negative. Hence, the function ùëìitself increases, flattens out, then
decreases. In other words, the function ùëìcurves down, and has a single maximum as is
shown in Fig. A.2.
tFig. A.2 If we assume the second derivative is a negative constant, then the Ô¨Åst derivative in
decreasing, which implies the function itself has a maximum.
Third,ifthesecondderivativeisaalwayszero,thenthefirstderivativewillneverchange‚Äî
it is constant! This means that ùëìincreases (or decreases) at a fixed rate, and ùëìis itself a
straight line as is shown in Fig. A.3.
Tosummarize,thesecondderivativecanbeinterpretedasdescribingthewaythatthefunc-
tionùëìcurves. A positive second derivative leads to a upwards curve, while a negative sec-
ond derivative means that ùëìcurves downwards, and a zero second derivative means that ùëì
does not curve at all.
932 Mathematics for Deep Learning
tFig. A.3 If we assume the second derivative is zero, then the Ô¨Åst derivative is constant, which
implies the function itself is a straight line.
Let‚Äôs take this one step further. Consider the function ùëî¬πùë•¬∫=ùëéùë•2¬∏ùëèùë•¬∏ùëê. We can then
compute that
ùëëùëî
ùëëùë•¬πùë•¬∫=2ùëéùë•¬∏ùëè
ùëë2ùëî
ùëëùë•2¬πùë•¬∫=2ùëé.(A.14)
If we have some original function ùëì¬πùë•¬∫in mind, we may compute the first two derivatives
and find the values for ùëé,ùëè, andùëêthat make them match this computation. Similarly to
the previous section where we saw that the first derivative gave the best approximation
with a straight line, this construction provides the best approximation by a quadratic. Let‚Äôs
visualize this for ùëì¬πùë•¬∫=sin¬πùë•¬∫.
# Compute sin
xs=torch .arange( -torch .pi, torch .pi, 0.01 )
plots =[torch .sin(xs)]
# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)
for x0in[-1.5,0.0,2.0]:
plots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *
torch .cos(torch .tensor(x0)) -(xs -x0)**2*
torch .sin(torch .tensor(x0)) /2)
d2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])
We will extend this idea to the idea of a Taylorseries in the next section.
933 Single Variable Calculus
TaylorSeries
TheTaylorseries providesamethodtoapproximatethefunction ùëì¬πùë•¬∫ifwearegivenvalues
for the firstùëõderivatives at a point ùë•0, i.e.,
ùëì¬πùë•0¬∫, ùëì¬π1¬∫¬πùë•0¬∫, ùëì¬π2¬∫¬πùë•0¬∫,..., ùëì¬πùëõ¬∫¬πùë•0¬∫	
. The
ideawillbetofindadegree ùëõpolynomialthatmatchesallthegivenderivativesat ùë•0.
We saw the case of ùëõ=2in the previous section and a little algebra shows this is
ùëì¬πùë•¬∫1
2ùëë2ùëì
ùëëùë•2¬πùë•0¬∫¬πùë• ùë•0¬∫2¬∏ùëëùëì
ùëëùë•¬πùë•0¬∫¬πùë• ùë•0¬∫¬∏ùëì¬πùë•0¬∫. (A.15)
As we can see above, the denominator of 2is there to cancel out the 2we get when we take
two derivatives of ùë•2, while the other terms are all zero. Same logic applies for the first
derivative and the value itself.
If we push the logic further to ùëõ=3, we will conclude that
ùëì¬πùë•¬∫ùëë3ùëì
ùëëùë•3¬πùë•0¬∫
6¬πùë• ùë•0¬∫3¬∏ùëë2ùëì
ùëëùë•2¬πùë•0¬∫
2¬πùë• ùë•0¬∫2¬∏ùëëùëì
ùëëùë•¬πùë•0¬∫¬πùë• ùë•0¬∫¬∏ùëì¬πùë•0¬∫.(A.16)
wherethe 6=32=3!comesfromtheconstantwegetinfrontifwetakethreederivatives
ofùë•3.
Furthermore, we can get a degree ùëõpolynomial by
ùëÉùëõ¬πùë•¬∫=ùëõ√ï
ùëñ=0ùëì¬πùëñ¬∫¬πùë•0¬∫
ùëñ!¬πùë• ùë•0¬∫ùëñ. (A.17)
where the notation
ùëì¬πùëõ¬∫¬πùë•¬∫=ùëëùëõùëì
ùëëùë•ùëõ=ùëë
ùëëùë•ùëõ
ùëì. (A.18)
Indeed,ùëÉùëõ¬πùë•¬∫canbeviewedasthebest ùëõ-thdegreepolynomialapproximationtoourfunc-
tionùëì¬πùë•¬∫.
While we are not going to dive all the way into the error of the above approximations, it
is worth mentioning the infinite limit. In this case, for well behaved functions (known as
realanalyticfunctions)like cos¬πùë•¬∫orùëíùë•, wecanwriteouttheinfinitenumberoftermsand
approximate the exactly same function
ùëì¬πùë•¬∫=1√ï
ùëõ=0ùëì¬πùëõ¬∫¬πùë•0¬∫
ùëõ!¬πùë• ùë•0¬∫ùëõ. (A.19)
Takeùëì¬πùë•¬∫=ùëíùë•as am example. Since ùëíùë•is its own derivative, we know that ùëì¬πùëõ¬∫¬πùë•¬∫=ùëíùë•.
Therefore,ùëíùë•can be reconstructed by taking the Taylor series at ùë•0=0, i.e.,
ùëíùë•=1√ï
ùëõ=0ùë•ùëõ
ùëõ!=1¬∏ùë•¬∏ùë•2
2¬∏ùë•3
6¬∏. (A.20)
Let‚Äôs see how this works in code and observe how increasing the degree of the Taylor
approximation brings us closer to the desired function ùëíùë•.
934 Mathematics for Deep Learning
# Compute the exponential function
xs=torch .arange( 0,3,0.01 )
ys=torch .exp(xs)
# Compute a few Taylor series approximations
P1=1+xs
P2=1+xs+xs**2/2
P5=1+xs+xs**2/2+xs**3/6+xs**4/24+xs**5/120
d2l.plot(xs, [ys, P1, P2, P5], 'x','f(x) ', legend =[
"Exponential ","Degree 1 Taylor Series ","Degree 2 Taylor Series ",
"Degree 5 Taylor Series "])
Taylor series have two primary applications:
1.Theoretical applications : Often when we try to understand a too complex function,
usingTaylorseriesenablesustoturnitintoapolynomialthatwecanworkwithdirectly.
2.Numerical applications : Some functions like ùëíùë•orcos¬πùë•¬∫are difficult for machines to
compute. Theycanstoretablesofvaluesatafixedprecision(andthisisoftendone),but
it still leaves open questions like ‚ÄúWhat is the 1000-th digit of cos¬π1¬∫?‚Äù Taylor series
are often helpful to answer such questions.
A.3.3Summary
Derivatives can be used to express how functions change when we change the input by a
small amount.
Elementaryderivativescanbecombinedusingderivativerulestocreatearbitrarilycom-
plex derivatives.
Derivatives can be iterated to get second or higher order derivatives. Each increase in
order provides more fine grained information on the behavior of the function.
Using information in the derivatives of a single data example, we can approximate well
behaved functions by polynomials obtained from the Taylor series.
A.3.4Exercises
1.What is the derivative of ùë•3 4ùë•¬∏1?
935 Multivariable Calculus
2822.What is the derivative of log¬π1
ùë•¬∫?
3.True or False: If ùëì0¬πùë•¬∫=0thenùëìhas a maximum or minimum at ùë•?
4.Where is the minimum of ùëì¬πùë•¬∫=ùë•log¬πùë•¬∫forùë•0(where we assume that ùëìtakes the
limiting value of 0atùëì¬π0¬∫)?
Discussions282.
A.4MultivariableCalculus
Now that we have a fairly strong understanding of derivatives of a function of a single
variable, let‚Äôs return to our original question where we were considering a loss function of
potentially billions of weights.
A.4.1Higher-DimensionalDifferentiation
WhatSection A.3 tells us is that if we change a single one of these billions of weights
leaving every other one fixed, we know what will happen! This is nothing more than a
function of a single variable, so we can write
ùêø¬πùë§1¬∏ùúñ1,ùë§2,...,ùë§ùëÅ¬∫ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫¬∏ùúñ1ùëë
ùëëùë§ 1ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫.(A.1)
Wewillcallthederivativeinonevariablewhilefixingtheothervariablesthe partialderiva-
tive, and we will use the notationùúï
ùúïùë§ 1for the derivative in (A.1).
Now, let‚Äôs take this and change ùë§2a little bit to ùë§2¬∏ùúñ2:
ùêø¬πùë§1¬∏ùúñ1,ùë§2¬∏ùúñ2,...,ùë§ùëÅ¬∫ùêø¬πùë§1,ùë§2¬∏ùúñ2,...,ùë§ùëÅ¬∫¬∏ùúñ1ùúï
ùúïùë§ 1ùêø¬πùë§1,ùë§2¬∏ùúñ2,...,ùë§ùëÅ¬∏ùúñùëÅ¬∫
ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
¬∏ùúñ2ùúï
ùúïùë§ 2ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
¬∏ùúñ1ùúï
ùúïùë§ 1ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
¬∏ùúñ1ùúñ2ùúï
ùúïùë§ 2ùúï
ùúïùë§ 1ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
¬∏ùúñ2ùúï
ùúïùë§ 2ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫
¬∏ùúñ1ùúï
ùúïùë§ 1ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫.
(A.2)
Wehaveagainusedtheideathat ùúñ1ùúñ2isahigherordertermthatwecandiscardinthesame
936 Mathematics for Deep Learning
way we could discard ùúñ2in the previous section, along with what we saw in (A.1). By
continuing in this manner, we may write that
ùêø¬πùë§1¬∏ùúñ1,ùë§2¬∏ùúñ2,...,ùë§ùëÅ¬∏ùúñùëÅ¬∫ùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫¬∏√ï
ùëñùúñùëñùúï
ùúïùë§ùëñùêø¬πùë§1,ùë§2,...,ùë§ùëÅ¬∫.
(A.3)
This may look like a mess, but we can make this more familiar by noting that the sum on
the right looks exactly like a dot product, so if we let
ùùê=¬ªùúñ1,...,ùúñùëÅ¬º>andrxùêø=ùúïùêø
ùúïùë•1,...,ùúïùêø
ùúïùë•ùëÅ>
, (A.4)
then
ùêø¬πw¬∏ùùê¬∫ùêø¬πw¬∫¬∏ùùêrwùêø¬πw¬∫. (A.5)
We will call the vector rwùêøthegradient ofùêø.
Equation (A.5)isworthponderingforamoment. Ithasexactlytheformatthatweencoun-
tered in one dimension, just we have converted everything to vectors and dot products.
It allows us to tell approximately how the function ùêøwill change given any perturbation
to the input. As we will see in the next section, this will provide us with an important
tool in understanding geometrically how we can learn using information contained in the
gradient.
Butfirst,let‚Äôsseethisapproximationatworkwithanexample. Supposethatweareworking
with the function
ùëì¬πùë•,ùë¶¬∫=log¬πùëíùë•¬∏ùëíùë¶¬∫with gradientrùëì¬πùë•,ùë¶¬∫=ùëíùë•
ùëíùë•¬∏ùëíùë¶,ùëíùë¶
ùëíùë•¬∏ùëíùë¶
. (A.6)
If we look at a point like ¬π0,log¬π2¬∫¬∫, we see that
ùëì¬πùë•,ùë¶¬∫=log¬π3¬∫with gradientrùëì¬πùë•,ùë¶¬∫=1
3,2
3
. (A.7)
Thus, if we want to approximate ùëìat¬πùúñ1,log¬π2¬∫¬∏ùúñ2¬∫, we see that we should have the
specific instance of (A.5):
ùëì¬πùúñ1,log¬π2¬∫¬∏ùúñ2¬∫log¬π3¬∫¬∏1
3ùúñ1¬∏2
3ùúñ2. (A.8)
We can test this in code to see how good the approximation is.
%matplotlib inline
import numpy asnp
import torch
from IPython import display
from mpl_toolkits import mplot3d
from d2l import torch asd2l
def f(x, y):
(continues on next page)
937 Multivariable Calculus
(continued from previous page)
return torch .log(torch .exp(x) +torch .exp(y))
def grad_f (x, y):
return torch .tensor([torch .exp(x) /(torch .exp(x) +torch .exp(y)),
torch .exp(y) /(torch .exp(x) +torch .exp(y))])
epsilon =torch .tensor([ 0.01 ,-0.03 ])
grad_approx =f(torch .tensor([ 0.]), torch .log(
torch .tensor([ 2.]))) +epsilon .dot(
grad_f(torch .tensor([ 0.]), torch .log(torch .tensor( 2.))))
true_value =f(torch .tensor([ 0.])+epsilon[ 0], torch .log(
torch .tensor([ 2.])) +epsilon[ 1])
f'approximation: {grad_approx }, true Value: {true_value }'
'approximation: tensor([1.0819]), true Value: tensor([1.0821]) '
A.4.2Geometryof Gradients and Gradient Descent
Consider the expression from (A.5)again:
ùêø¬πw¬∏ùùê¬∫ùêø¬πw¬∫¬∏ùùêrwùêø¬πw¬∫. (A.9)
Let‚Äôs suppose that I want to use this to help minimize our loss ùêø. Let‚Äôs understand geomet-
rically the algorithm of gradient descent first described in Section 2.5 . What we will do is
the following:
1.Start with a random choice for the initial parameters w.
2.Find the direction vthat makesùêødecrease the most rapidly at w.
3.Take a small step in that direction: w!w¬∏ùúñv.
4.Repeat.
The only thing we do not know exactly how to do is to compute the vector vin the second
step. We will call such a direction the direction of steepest descent . Using the geometric
understandingofdotproductsfrom SectionA.1 ,weseethatwecanrewrite (A.5)as
ùêø¬πw¬∏v¬∫ùêø¬πw¬∫¬∏vrwùêø¬πw¬∫=ùêø¬πw¬∫¬∏kr wùêø¬πw¬∫kcos¬πùúÉ¬∫. (A.10)
Note that we have taken our direction to have length one for convenience, and used ùúÉfor
the angle between vandrwùêø¬πw¬∫. If we want to find the direction that decreases ùêøas
rapidly as possible, we want to make this expression as negative as possible. The only way
the direction we pick enters into this equation is through cos¬πùúÉ¬∫, and thus we wish to make
this cosine as negative as possible. Now, recalling the shape of cosine, we can make this as
negative as possible by making cos¬πùúÉ¬∫= 1or equivalently making the angle between the
gradient and our chosen direction to be ùúãradians, or equivalently 180degrees. The only
way to achieve this is to head in the exact opposite direction: pick vto point in the exact
opposite direction to rwùêø¬πw¬∫!
This brings us to one of the most important mathematical concepts in machine learning:
938 Mathematics for Deep Learning
the direction of steepest decent points in the direction of  rwùêø¬πw¬∫. Thus our informal
algorithm can be rewritten as follows.
1.Start with a random choice for the initial parameters w.
2.Computerwùêø¬πw¬∫.
3.Take a small step in the opposite of that direction: w w ùúñrwùêø¬πw¬∫.
4.Repeat.
This basic algorithm has been modified and adapted many ways by many researchers, but
thecore conceptremains the samein allof them. Use thegradient tofind the directionthat
decreases the loss as rapidly as possible, and update the parameters to take a step in that
direction.
A.4.3A Noteon Mathematical Optimization
Throughoutthisbook,wefocussquarelyonnumericaloptimizationtechniquesfortheprac-
tical reason that all functions we encounter in the deep learning setting are too complex to
minimize explicitly.
However, it is a useful exercise to consider what the geometric understanding we obtained
above tells us about optimizing functions directly.
Suppose that we wish to find the value of x0which minimizes some function ùêø¬πx¬∫. Let‚Äôs
suppose that moreover someone gives us a value and tells us that it is the value that mini-
mizesùêø. Is there anything we can check to see if their answer is even plausible?
Again consider (A.5):
ùêø¬πx0¬∏ùùê¬∫ùêø¬πx0¬∫¬∏ùùêrxùêø¬πx0¬∫. (A.11)
If the gradient is not zero, we know that we can take a step in the direction  ùúñrxùêø¬πx0¬∫to
find a value of ùêøthat is smaller. Thus, if we truly are at a minimum, this cannot be the
case! We can conclude that if x0is a minimum, then rxùêø¬πx0¬∫=0. We call points with
rxùêø¬πx0¬∫=0critical points .
This is nice, because in some rare settings, we canexplicitly find all the points where the
gradient is zero, and find the one with the smallest value.
For a concrete example, consider the function
ùëì¬πùë•¬∫=3ùë•4 4ùë•3 12ùë•2. (A.12)
This function has derivative
ùëëùëì
ùëëùë•=12ùë•3 12ùë•2 24ùë•=12ùë•¬πùë• 2¬∫¬πùë•¬∏1¬∫. (A.13)
The only possible location of minima are at ùë•= 1,0,2, where the function takes the
values 5,0, 32respectively, and thus we can conclude that we minimize our function
whenùë•=2. A quick plot confirms this.
939 Multivariable Calculus
x=torch .arange( -2,3,0.01 )
f=(3*x**4)-(4*x**3)-(12*x**2)
d2l.plot(x, f, 'x','f(x) ')
Thishighlightsanimportantfacttoknowwhenworkingeithertheoreticallyornumerically:
theonlypossiblepointswherewecanminimize(ormaximize)afunctionwillhavegradient
equal to zero, however, not every point with gradient zero is the true globalminimum (or
maximum).
A.4.4MultivariateChain Rule
Let‚Äôs suppose that we have a function of four variables ( ùë§,ùë•,ùë¶, andùëß) which we can make
by composing many terms:
ùëì¬πùë¢,ùë£¬∫=¬πùë¢¬∏ùë£¬∫2
ùë¢¬πùëé,ùëè¬∫=¬πùëé¬∏ùëè¬∫2, ùë£¬πùëé,ùëè¬∫=¬πùëé ùëè¬∫2,
ùëé¬πùë§,ùë•,ùë¶,ùëß¬∫=¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2, ùëè¬πùë§,ùë•,ùë¶,ùëß¬∫=¬πùë§¬∏ùë• ùë¶ ùëß¬∫2.(A.14)
Such chains of equations are common when working with neural networks, so trying to
understand how to compute gradients of such functions is key. We can start to see visual
hints of this connection in Fig. A.1 if we take a look at what variables directly relate to one
another.
tFig. A.1 The function relations above where nodes represent values and edges show functional
dependence.
Nothing stops us from just composing everything from (A.14 )and writing out that
ùëì¬πùë§,ùë•,ùë¶,ùëß¬∫=
¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2¬∏¬πùë§¬∏ùë• ùë¶ ùëß¬∫22
¬∏
¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2 ¬πùë§¬∏ùë• ùë¶ ùëß¬∫222
.
(A.15)
940 Mathematics for Deep Learning
We may then take the derivative by just using single variable derivatives, but if we did that
we would quickly find ourself swamped with terms, many of which are repeats! Indeed,
one can see that, for instance:
ùúïùëì
ùúïùë§=2
2¬π2¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫ 2¬πùë§¬∏ùë• ùë¶ ùëß¬∫¬∫
¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2 ¬πùë§¬∏ùë• ùë¶ ùëß¬∫2
¬∏
2¬π2¬πùë§¬∏ùë• ùë¶ ùëß¬∫¬∏2¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫¬∫
¬πùë§¬∏ùë• ùë¶ ùëß¬∫2¬∏¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2


¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2 ¬πùë§¬∏ùë• ùë¶ ùëß¬∫22
¬∏
¬πùë§¬∏ùë• ùë¶ ùëß¬∫2¬∏¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫22
.
(A.16)
If we then also wanted to computeùúïùëì
ùúïùë•, we would end up with a similar equation again with
many repeated terms, and many sharedrepeated terms between the two derivatives. This
representsamassivequantityofwastedwork, andifweneededtocomputederivativesthis
way, the whole deep learning revolution would have stalled out before it began!
Let‚Äôs break up the problem. We will start by trying to understand how ùëìchanges when we
changeùëé, essentially assuming that ùë§,ùë•,ùë¶, andùëßall do not exist. We will reason as we
did back when we worked with the gradient for the first time. Let‚Äôs take ùëéand add a small
amountùúñto it.
ùëì¬πùë¢¬πùëé¬∏ùúñ,ùëè¬∫,ùë£¬πùëé¬∏ùúñ,ùëè¬∫¬∫
ùëì
ùë¢¬πùëé,ùëè¬∫¬∏ùúñùúïùë¢
ùúïùëé¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∏ùúñùúïùë£
ùúïùëé¬πùëé,ùëè¬∫
ùëì¬πùë¢¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∫¬∏ùúñùúïùëì
ùúïùë¢¬πùë¢¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∫ùúïùë¢
ùúïùëé¬πùëé,ùëè¬∫¬∏ùúïùëì
ùúïùë£¬πùë¢¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∫ùúïùë£
ùúïùëé¬πùëé,ùëè¬∫
.
(A.17)
The first line follows from the definition of partial derivative, and the second follows from
the definition of gradient. It is notationally burdensome to track exactly where we evaluate
every derivative, as in the expressionùúïùëì
ùúïùë¢¬πùë¢¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∫, so we often abbreviate this to
the much more memorable
ùúïùëì
ùúïùëé=ùúïùëì
ùúïùë¢ùúïùë¢
ùúïùëé¬∏ùúïùëì
ùúïùë£ùúïùë£
ùúïùëé. (A.18)
It is useful to think about the meaning of the process. We are trying to understand how
a function of the form ùëì¬πùë¢¬πùëé,ùëè¬∫,ùë£¬πùëé,ùëè¬∫¬∫changes its value with a change in ùëé. There
are two pathways this can occur: there is the pathway where ùëé!ùë¢!ùëìand where
ùëé!ùë£!ùëì. We can compute both of these contributions via the chain rule:ùúïùë§
ùúïùë¢ùúïùë¢
ùúïùë•and
ùúïùë§
ùúïùë£ùúïùë£
ùúïùë•respectively, and added up.
Imagine we have a different network of functions where the functions on the right depend
on those that are connected to on the left as is shown in Fig. A.2.
tFig. A.2 Another more subtle example of the chain rule.
941 Multivariable Calculus
To compute something likeùúïùëì
ùúïùë¶, we need to sum over all (in this case 3) paths from ùë¶toùëì
giving
ùúïùëì
ùúïùë¶=ùúïùëì
ùúïùëéùúïùëé
ùúïùë¢ùúïùë¢
ùúïùë¶¬∏ùúïùëì
ùúïùë¢ùúïùë¢
ùúïùë¶¬∏ùúïùëì
ùúïùëèùúïùëè
ùúïùë£ùúïùë£
ùúïùë¶. (A.19)
Understandingthechainruleinthiswaywillpaygreatdividendswhentryingtounderstand
how gradients flow through networks, and why various architectural choices like those in
LSTMs ( Section 10.1 ) or residual layers ( Section 8.6 ) can help shape the learning process
by controlling gradient flow.
A.4.5TheBackpropagationAlgorithm
Let‚Äôs return to the example of (A.14 )the previous section where
ùëì¬πùë¢,ùë£¬∫=¬πùë¢¬∏ùë£¬∫2
ùë¢¬πùëé,ùëè¬∫=¬πùëé¬∏ùëè¬∫2, ùë£¬πùëé,ùëè¬∫=¬πùëé ùëè¬∫2,
ùëé¬πùë§,ùë•,ùë¶,ùëß¬∫=¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫2, ùëè¬πùë§,ùë•,ùë¶,ùëß¬∫=¬πùë§¬∏ùë• ùë¶ ùëß¬∫2.(A.20)
If we want to compute sayùúïùëì
ùúïùë§we may apply the multi-variate chain rule to see:
ùúïùëì
ùúïùë§=ùúïùëì
ùúïùë¢ùúïùë¢
ùúïùë§¬∏ùúïùëì
ùúïùë£ùúïùë£
ùúïùë§,
ùúïùë¢
ùúïùë§=ùúïùë¢
ùúïùëéùúïùëé
ùúïùë§¬∏ùúïùë¢
ùúïùëèùúïùëè
ùúïùë§,
ùúïùë£
ùúïùë§=ùúïùë£
ùúïùëéùúïùëé
ùúïùë§¬∏ùúïùë£
ùúïùëèùúïùëè
ùúïùë§.(A.21)
Let‚Äôs try using this decomposition to computeùúïùëì
ùúïùë§. Notice that all we need here are the
various single step partials:
ùúïùëì
ùúïùë¢=2¬πùë¢¬∏ùë£¬∫,ùúïùëì
ùúïùë£=2¬πùë¢¬∏ùë£¬∫,
ùúïùë¢
ùúïùëé=2¬πùëé¬∏ùëè¬∫,ùúïùë¢
ùúïùëè=2¬πùëé¬∏ùëè¬∫,
ùúïùë£
ùúïùëé=2¬πùëé ùëè¬∫,ùúïùë£
ùúïùëè= 2¬πùëé ùëè¬∫,
ùúïùëé
ùúïùë§=2¬πùë§¬∏ùë•¬∏ùë¶¬∏ùëß¬∫,ùúïùëè
ùúïùë§=2¬πùë§¬∏ùë• ùë¶ ùëß¬∫.(A.22)
If we write this out into code this becomes a fairly manageable expression.
# Compute the value of the function from inputs to outputs
w, x, y, z =-1,0,-2,1
a, b =(w+x+y+z)**2, (w +x-y-z)**2
u, v =(a+b)**2, (a -b)**2
f=(u+v)**2
print (f' f at {w},{x},{y},{z}is{f}')
# Compute the single step partials
df_du, df_dv =2*(u+v), 2*(u+v)
(continues on next page)
942 Mathematics for Deep Learning
(continued from previous page)
du_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)
da_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)
# Compute the final result from inputs to outputs
du_dw, dv_dw =du_da *da_dw +du_db *db_dw, dv_da *da_dw +dv_db *db_dw
df_dw =df_du *du_dw +df_dv *dv_dw
print (f'df/dw at {w},{x},{y},{z}is{df_dw }')
f at -1,0,-2,1is1024
df/dw at -1,0,-2,1is-4096
However, note that this still does not make it easy to compute something likeùúïùëì
ùúïùë•. The
reasonforthatisthe waywechosetoapplythechainrule. Ifwelookatwhatwedidabove,
we always kept ùúïùë§in the denominator when we could. In this way, we chose to apply the
chainruleseeinghow ùë§changedeveryothervariable. Ifthatiswhatwewanted,thiswould
be a good idea. However, think back to our motivation from deep learning: we want to see
how every parameter changes the loss. In essence, we want to apply the chain rule keeping
ùúïùëìin the numerator whenever we can!
To be more explicit, note that we can write
ùúïùëì
ùúïùë§=ùúïùëì
ùúïùëéùúïùëé
ùúïùë§¬∏ùúïùëì
ùúïùëèùúïùëè
ùúïùë§,
ùúïùëì
ùúïùëé=ùúïùëì
ùúïùë¢ùúïùë¢
ùúïùëé¬∏ùúïùëì
ùúïùë£ùúïùë£
ùúïùëé,
ùúïùëì
ùúïùëè=ùúïùëì
ùúïùë¢ùúïùë¢
ùúïùëè¬∏ùúïùëì
ùúïùë£ùúïùë£
ùúïùëè.(A.23)
Notethatthisapplicationofthechainrulehasusexplicitlycomputeùúïùëì
ùúïùë¢,ùúïùëì
ùúïùë£,ùúïùëì
ùúïùëé,ùúïùëì
ùúïùëè,andùúïùëì
ùúïùë§.
Nothing stops us from also including the equations:
ùúïùëì
ùúïùë•=ùúïùëì
ùúïùëéùúïùëé
ùúïùë•¬∏ùúïùëì
ùúïùëèùúïùëè
ùúïùë•,
ùúïùëì
ùúïùë¶=ùúïùëì
ùúïùëéùúïùëé
ùúïùë¶¬∏ùúïùëì
ùúïùëèùúïùëè
ùúïùë¶,
ùúïùëì
ùúïùëß=ùúïùëì
ùúïùëéùúïùëé
ùúïùëß¬∏ùúïùëì
ùúïùëèùúïùëè
ùúïùëß.(A.24)
and then keeping track of how ùëìchanges when we change anynode in the entire network.
Let‚Äôs implement it.
# Compute the value of the function from inputs to outputs
w, x, y, z =-1,0,-2,1
a, b =(w+x+y+z)**2, (w +x-y-z)**2
u, v =(a+b)**2, (a -b)**2
f=(u+v)**2
print (f'f at {w},{x},{y},{z}is{f}')
# Compute the derivative using the decomposition above
(continues on next page)
943 Multivariable Calculus
(continued from previous page)
# First compute the single step partials
df_du, df_dv =2*(u+v), 2*(u+v)
du_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)
da_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)
da_dx, db_dx =2*(w+x+y+z), 2*(w+x-y-z)
da_dy, db_dy =2*(w+x+y+z), -2*(w+x-y-z)
da_dz, db_dz =2*(w+x+y+z), -2*(w+x-y-z)
# Now compute how f changes when we change any value from output to input
df_da, df_db =df_du *du_da +df_dv *dv_da, df_du *du_db +df_dv *dv_db
df_dw, df_dx =df_da *da_dw +df_db *db_dw, df_da *da_dx +df_db *db_dx
df_dy, df_dz =df_da *da_dy +df_db *db_dy, df_da *da_dz +df_db *db_dz
print (f'df/dw at {w},{x},{y},{z}is{df_dw }')
print (f'df/dx at {w},{x},{y},{z}is{df_dx }')
print (f'df/dy at {w},{x},{y},{z}is{df_dy }')
print (f'df/dz at {w},{x},{y},{z}is{df_dz }')
f at -1,0,-2,1is1024
df/dw at -1,0,-2,1is-4096
df/dx at -1,0,-2,1is-4096
df/dy at -1,0,-2,1is-4096
df/dz at -1,0,-2,1is-4096
The fact that we compute derivatives from ùëìback towards the inputs rather than from the
inputs forward to the outputs (as we did in the first code snippet above) is what gives this
algorithm its name: backpropagation . Note that there are two steps: 1. Compute the value
of the function, and the single step partials from front to back. While not done above, this
can be combined into a single forward pass . 2. Compute the gradient of ùëìfrom back to
front. We call this the backwardspass .
This is precisely what every deep learning algorithm implements to allow the computation
of the gradient of the loss with respect to every weight in the network at one pass. It is an
astonishing fact that we have such a decomposition.
To see how to encapsulated this, let‚Äôs take a quick look at this example.
# Initialize as ndarrays, then attach gradients
w=torch .tensor([ -1.], requires_grad =True )
x=torch .tensor([ 0.], requires_grad =True )
y=torch .tensor([ -2.], requires_grad =True )
z=torch .tensor([ 1.], requires_grad =True )
# Do the computation like usual, tracking gradients
a, b =(w+x+y+z)**2, (w +x-y-z)**2
u, v =(a+b)**2, (a -b)**2
f=(u+v)**2
# Execute backward pass
f.backward()
print (f'df/dw at {w.data .item() },{x.data .item() },{y.data .item() },'
(continues on next page)
944 Mathematics for Deep Learning
(continued from previous page)
f'{z.data .item() }is{w.grad .data .item() }')
print (f'df/dx at {w.data .item() },{x.data .item() },{y.data .item() },'
f'{z.data .item() }is{x.grad .data .item() }')
print (f'df/dy at {w.data .item() },{x.data .item() },{y.data .item() },'
f'{z.data .item() }is{y.grad .data .item() }')
print (f'df/dz at {w.data .item() },{x.data .item() },{y.data .item() },'
f'{z.data .item() }is{z.grad .data .item() }')
df/dw at -1.0,0.0,-2.0,1.0 is-4096.0
df/dx at -1.0,0.0,-2.0,1.0 is-4096.0
df/dy at -1.0,0.0,-2.0,1.0 is-4096.0
df/dz at -1.0,0.0,-2.0,1.0 is-4096.0
All of what we did above can be done automatically by calling f.backwards() .
A.4.6Hessians
As with single variable calculus, it is useful to consider higher-order derivatives in order
to get a handle on how we can obtain a better approximation to a function than using the
gradient alone.
There is one immediate problem one encounters when working with higher order deriva-
tives of functions of several variables, and that is there are a large number of them. If we
have a function ùëì¬πùë•1,...,ùë•ùëõ¬∫ofùëõvariables, then we can take ùëõ2many second derivatives,
namely for any choice of ùëñandùëó:
ùëë2ùëì
ùëëùë•ùëñùëëùë•ùëó=ùëë
ùëëùë•ùëñùëë
ùëëùë•ùëóùëì
. (A.25)
This is traditionally assembled into a matrix called the Hessian:
Hùëì=26666664ùëë2ùëì
ùëëùë•1ùëëùë•1ùëë2ùëì
ùëëùë•1ùëëùë•ùëõ.........
ùëë2ùëì
ùëëùë•ùëõùëëùë•1ùëë2ùëì
ùëëùë•ùëõùëëùë•ùëõ37777775. (A.26)
Not every entry of this matrix is independent. Indeed, we can show that as long as both
mixed partials (partial derivatives with respect to more than one variable) exist and are
continuous, we can say that for any ùëñ, andùëó,
ùëë2ùëì
ùëëùë•ùëñùëëùë•ùëó=ùëë2ùëì
ùëëùë•ùëóùëëùë•ùëñ. (A.27)
This follows by considering first perturbing a function in the direction of ùë•ùëñ, and then per-
turbing it inùë•ùëóand then comparing the result of that with what happens if we perturb first
ùë•ùëóand thenùë•ùëñ, with the knowledge that both of these orders lead to the same final change
in the output of ùëì.
As with single variables, we can use these derivatives to get a far better idea of how the
945 Multivariable Calculus
function behaves near a point. In particular, we can use it to find the best fitting quadratic
near a point x0, as we saw in a single variable.
Let‚Äôs see an example. Suppose that ùëì¬πùë•1,ùë•2¬∫=ùëé¬∏ùëè1ùë•1¬∏ùëè2ùë•2¬∏ùëê11ùë•2
1¬∏ùëê12ùë•1ùë•2¬∏ùëê22ùë•2
2.
This is the general form for a quadratic in two variables. If we look at the value of the
function, its gradient, and its Hessian (A.26 ), all at the point zero:
ùëì¬π0,0¬∫=ùëé,
rùëì¬π0,0¬∫=ùëè1
ùëè2
,
Hùëì¬π0,0¬∫=2ùëê11ùëê12
ùëê12 2ùëê22
,(A.28)
we can get our original polynomial back by saying
ùëì¬πx¬∫=ùëì¬π0¬∫¬∏rùëì¬π0¬∫x¬∏1
2x>Hùëì¬π0¬∫x. (A.29)
In general, if we computed this expansion any point x0, we see that
ùëì¬πx¬∫=ùëì¬πx0¬∫¬∏rùëì¬πx0¬∫¬πx x0¬∫¬∏1
2¬πx x0¬∫>Hùëì¬πx0¬∫¬πx x0¬∫. (A.30)
Thisworksforanydimensionalinput,andprovidesthebestapproximatingquadratictoany
function at a point. To give an example, let‚Äôs plot the function
ùëì¬πùë•,ùë¶¬∫=ùë•ùëí ùë•2 ùë¶2. (A.31)
One can compute that the gradient and Hessian are
rùëì¬πùë•,ùë¶¬∫=ùëí ùë•2 ùë¶21 2ùë•2
 2ùë•ùë¶
andHùëì¬πùë•,ùë¶¬∫=ùëí ùë•2 ùë¶24ùë•3 6ùë• 4ùë•2ùë¶ 2ùë¶
4ùë•2ùë¶ 2ùë¶4ùë•ùë¶2 2ùë•
.
(A.32)
And thus, with a little algebra, see that the approximating quadratic at ¬ª 1,0¬º>is
ùëì¬πùë•,ùë¶¬∫ùëí 1
 1 ¬πùë•¬∏1¬∫¬∏¬πùë•¬∏1¬∫2¬∏ùë¶2
. (A.33)
# Construct grid and compute function
x, y =torch .meshgrid(torch .linspace( -2,2,101),
torch .linspace( -2,2,101))
z=x*torch .exp( -x**2-y**2)
# Compute approximating quadratic with gradient and Hessian at (1, 0)
w=torch .exp(torch .tensor([ -1.]))*(-1-(x+1)+2*(x+1)**2+2*y**2)
# Plot function
ax=d2l.plt.figure() .add_subplot( 111, projection ='3d')
ax.plot_wireframe(x .numpy(), y .numpy(), z .numpy(),
**{'rstride ':10,'cstride ':10})
ax.plot_wireframe(x .numpy(), y .numpy(), w .numpy(),
**{'rstride ':10,'cstride ':10}, color ='purple ')
(continues on next page)
946 Mathematics for Deep Learning
(continued from previous page)
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'y')
d2l.set_figsize()
ax.set_xlim( -2,2)
ax.set_ylim( -2,2)
ax.set_zlim( -1,1)
ax.dist =12
This forms the basis for Newton‚Äôs Algorithm discussed in Section 12.3 , where we perform
numerical optimization iteratively finding the best fitting quadratic, and then exactly mini-
mizing that quadratic.
A.4.7ALittle Matrix Calculus
Derivatives of functions involving matrices turn out to be particularly nice. This section
canbecomenotationallyheavy,somaybeskippedinafirstreading,butitisusefultoknow
how derivatives of functions involving common matrix operations are often much cleaner
than one might initially anticipate, particularly given how central matrix operations are to
deep learning applications.
Let‚Äôs begin with an example. Suppose that we have some fixed column vector ùú∑, and we
wanttotaketheproductfunction ùëì¬πx¬∫=ùú∑>x,andunderstandhowthedotproductchanges
when we change x.
A bit of notation that will be useful when working with matrix derivatives in ML is called
thedenominator layout matrix derivative where we assemble our partial derivatives into
the shape of whatever vector, matrix, or tensor is in the denominator of the differential. In
this case, we will write
ùëëùëì
ùëëx=26666664ùëëùëì
ùëëùë•1...
ùëëùëì
ùëëùë•ùëõ37777775, (A.34)
where we matched the shape of the column vector x.
If we write out our function into components this is
ùëì¬πx¬∫=ùëõ√ï
ùëñ=1ùõΩùëñùë•ùëñ=ùõΩ1ùë•1¬∏¬∏ùõΩùëõùë•ùëõ. (A.35)
947 Multivariable Calculus
If we now take the partial derivative with respect to say ùõΩ1, note that everything is zero but
the first term, which is just ùë•1multiplied by ùõΩ1, so we obtain that
ùëëùëì
ùëëùë•1=ùõΩ1, (A.36)
or more generally that
ùëëùëì
ùëëùë•ùëñ=ùõΩùëñ. (A.37)
We can now reassemble this into a matrix to see
ùëëùëì
ùëëx=26666664ùëëùëì
ùëëùë•1...
ùëëùëì
ùëëùë•ùëõ37777775=2666664ùõΩ1
...
ùõΩùëõ3777775=ùú∑. (A.38)
This illustrates a few factors about matrix calculus that we will often counter throughout
this section:
First, The computations will get rather involved.
Second, The final results are much cleaner than the intermediate process, and will al-
ways look similar to the single variable case. In this case, note thatùëë
ùëëùë•¬πùëèùë•¬∫=ùëèand
ùëë
ùëëx¬πùú∑>x¬∫=ùú∑are both similar.
Third, transposes can often appear seemingly from nowhere. The core reason for this is
the convention that we match the shape of the denominator, thus when we multiply
matrices, we will need to take transposes to match back to the shape of the original
term.
To keep building intuition, let‚Äôs try a computation that is a little harder. Suppose that we
have a column vector x, and a square matrix ùê¥and we want to compute
ùëë
ùëëx¬πx>ùê¥x¬∫. (A.39)
To drive towards easier to manipulate notation, let‚Äôs consider this problem using Einstein
notation. In this case we can write the function as
x>ùê¥x=ùë•ùëñùëéùëñùëóùë•ùëó. (A.40)
To compute our derivative, we need to understand for every ùëò, what is the value of
ùëë
ùëëùë•ùëò¬πx>ùê¥x¬∫=ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó. (A.41)
By the product rule, this is
ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó=ùëëùë•ùëñ
ùëëùë•ùëòùëéùëñùëóùë•ùëó¬∏ùë•ùëñùëéùëñùëóùëëùë•ùëó
ùëëùë•ùëò. (A.42)
For a term likeùëëùë•ùëñ
ùëëùë•ùëò, it is not hard to see that this is one when ùëñ=ùëòand zero otherwise.
This means that every term where ùëñandùëòare different vanish from this sum, so the only
948 Mathematics for Deep Learning
terms that remain in that first sum are the ones where ùëñ=ùëò. The same reasoning holds for
the second term where we need ùëó=ùëò. This gives
ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó=ùëéùëòùëóùë•ùëó¬∏ùë•ùëñùëéùëñùëò. (A.43)
Now, the names of the indices in Einstein notation are arbitrary‚Äîthe fact that ùëñandùëóare
differentisimmaterialtothiscomputationatthispoint,sowecanre-indexsothattheyboth
useùëñto see that
ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó=ùëéùëòùëñùë•ùëñ¬∏ùë•ùëñùëéùëñùëò=¬πùëéùëòùëñ¬∏ùëéùëñùëò¬∫ùë•ùëñ. (A.44)
Now, here is where we start to need some practice to go further. Let‚Äôs try and identify this
outcome in terms of matrix operations. ùëéùëòùëñ¬∏ùëéùëñùëòis theùëò,ùëñ-th component of A¬∏A>. This
gives
ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó=¬ªA¬∏A>¬ºùëòùëñùë•ùëñ. (A.45)
Similarly, this term is now the product of the matrix A¬∏A>by the vector x, so we see
thatùëë
ùëëx¬πx>ùê¥x¬∫
ùëò=ùëë
ùëëùë•ùëòùë•ùëñùëéùëñùëóùë•ùëó=¬ª¬πA¬∏A>¬∫x¬ºùëò. (A.46)
Thus, we see that the ùëò-th entry of the desired derivative from (A.39 )is just theùëò-th entry
of the vector on the right, and thus the two are the same. Thus yields
ùëë
ùëëx¬πx>ùê¥x¬∫=¬πA¬∏A>¬∫x. (A.47)
This required significantly more work than our last one, but the final result is small. More
thanthat,considerthefollowingcomputationfortraditionalsinglevariablederivatives:
ùëë
ùëëùë•¬πùë•ùëéùë•¬∫=ùëëùë•
ùëëùë•ùëéùë•¬∏ùë•ùëéùëëùë•
ùëëùë•=¬πùëé¬∏ùëé¬∫ùë•. (A.48)
Equivalentlyùëë
ùëëùë•¬πùëéùë•2¬∫=2ùëéùë•=¬πùëé¬∏ùëé¬∫ùë•. Again, we get a result that looks rather like the
single variable result but with a transpose tossed in.
At this point, the pattern should be looking rather suspicious, so let‚Äôs try to figure out
why. When we take matrix derivatives like this, let‚Äôs first assume that the expression we
get will be another matrix expression: an expression we can write it in terms of products
and sums of matrices and their transposes. If such an expression exists, it will need to be
true for all matrices. In particular, it will need to be true of 11matrices, in which case
the matrix product is just the product of the numbers, the matrix sum is just the sum, and
the transpose does nothing at all! In other words, whatever expression we get mustmatch
the single variable expression. This means that, with some practice, one can often guess
matrixderivativesjustbyknowingwhattheassociatedsinglevariableexpressionmustlook
like!
Let‚Äôs try this out. Suppose that Xis aùëõùëömatrix, Uis anùëõùëüandVis anùëüùëö. Let‚Äôs
try to compute
ùëë
ùëëVkX UVk2
2=? (A.49)
949 Multivariable Calculus
This computation is important in an area called matrix factorization. For us, however, it is
just a derivative to compute. Let‚Äôs try to imagine what this would be for 11matrices. In
that case, we get the expression
ùëë
ùëëùë£¬πùë• ùë¢ùë£¬∫2= 2¬πùë• ùë¢ùë£¬∫ùë¢, (A.50)
where, the derivative is rather standard. If we try to convert this back into a matrix expres-
sion we get
ùëë
ùëëVkX UVk2
2= 2¬πX UV¬∫U. (A.51)
However, if we look at this it does not quite work. Recall that Xisùëõùëö, as isUV, so the
matrix 2¬πX UV¬∫isùëõùëö. On the other hand Uisùëõùëü, and we cannot multiply a ùëõùëö
and aùëõùëümatrix since the dimensions do not match!
We want to getùëë
ùëëV, which is the same shape as V, which isùëüùëö. So somehow we need
to take aùëõùëömatrix and a ùëõùëümatrix, multiply them together (perhaps with some
transposes) to get a ùëüùëö. We can do this by multiplying ùëà>by¬πX UV¬∫. Thus, we can
guess the solution to (A.49 )is
ùëë
ùëëVkX UVk2
2= 2U>¬πX UV¬∫. (A.52)
To show that this works, we would be remiss to not provide a detailed computation. If
we already believe that this rule-of-thumb works, feel free to skip past this derivation. To
compute
ùëë
ùëëVkX UVk2
2, (A.53)
we must find for every ùëé, andùëè
ùëë
ùëëùë£ùëéùëèkX UVk2
2=ùëë
ùëëùë£ùëéùëè√ï
ùëñ,ùëó 
ùë•ùëñùëó √ï
ùëòùë¢ùëñùëòùë£ùëòùëó!2
. (A.54)
Recalling that all entries of XandUare constants as far asùëë
ùëëùë£ùëéùëèis concerned, we may
push the derivative inside the sum, and apply the chain rule to the square to get
ùëë
ùëëùë£ùëéùëèkX UVk2
2=√ï
ùëñ,ùëó2 
ùë•ùëñùëó √ï
ùëòùë¢ùëñùëòùë£ùëòùëó!  
 √ï
ùëòùë¢ùëñùëòùëëùë£ùëòùëó
ùëëùë£ùëéùëè!
. (A.55)
As in the previous derivation, we may note thatùëëùë£ùëò ùëó
ùëëùë£ùëéùëèis only non-zero if the ùëò=ùëéand
ùëó=ùëè. If either of those conditions do not hold, the term in the sum is zero, and we may
freely discard it. We see that
ùëë
ùëëùë£ùëéùëèkX UVk2
2= 2√ï
ùëñ 
ùë•ùëñùëè √ï
ùëòùë¢ùëñùëòùë£ùëòùëè!
ùë¢ùëñùëé. (A.56)
An important subtlety here is that the requirement that ùëò=ùëédoes not occur inside the
950 Mathematics for Deep Learning
inner sum since that ùëòis a dummy variable which we are summing over inside the inner
term. For a notationally cleaner example, consider why
ùëë
ùëëùë•1 √ï
ùëñùë•ùëñ!2
=2 √ï
ùëñùë•ùëñ!
. (A.57)
From this point, we may start identifying components of the sum. First,
√ï
ùëòùë¢ùëñùëòùë£ùëòùëè=¬ªUV¬ºùëñùëè.(A.58)
So the entire expression in the inside of the sum is
ùë•ùëñùëè √ï
ùëòùë¢ùëñùëòùë£ùëòùëè=¬ªX UV¬ºùëñùëè.(A.59)
This means we may now write our derivative as
ùëë
ùëëùë£ùëéùëèkX UVk2
2= 2√ï
ùëñ¬ªX UV¬ºùëñùëèùë¢ùëñùëé. (A.60)
We want this to look like the ùëé,ùëèelement of a matrix so we can use the technique as in the
previous example to arrive at a matrix expression, which means that we need to exchange
the order of the indices on ùë¢ùëñùëé. If we notice that ùë¢ùëñùëé=¬ªU>¬ºùëéùëñ, we can then write
ùëë
ùëëùë£ùëéùëèkX UVk2
2= 2√ï
ùëñ¬ªU>¬ºùëéùëñ¬ªX UV¬ºùëñùëè. (A.61)
This is a matrix product, and thus we can conclude that
ùëë
ùëëùë£ùëéùëèkX UVk2
2= 2¬ªU>¬πX UV¬∫¬ºùëéùëè. (A.62)
and thus we may write the solution to (A.49 )
ùëë
ùëëVkX UVk2
2= 2U>¬πX UV¬∫. (A.63)
This matches the solution we guessed above!
It is reasonable to ask at this point, ‚ÄúWhy can I not just write down matrix versions of
all the calculus rules I have learned? It is clear this is still mechanical. Why do we not
just get it over with!‚Äù And indeed there are such rules and ( Petersen and Pedersen, 2008 )
provides an excellent summary. However, due to the plethora of ways matrix operations
can be combined compared to single values, there are many more matrix derivative rules
thansinglevariableones. Itisoftenthecasethatitisbesttoworkwiththeindices,orleave
it up to automatic differentiation when appropriate.
A.4.8Summary
In higher dimensions, we can define gradients which serve the same purpose as deriva-
tives in one dimension. These allow us to see how a multi-variable function changes
when we make an arbitrary small change to the inputs.
951 Integral Calculus
283The backpropagation algorithm can be seen to be a method of organizing the multi-
variable chain rule to allow for the efficient computation of many partial derivatives.
Matrix calculus allows us to write the derivatives of matrix expressions in concise ways.
A.4.9Exercises
1.Givenacolumnvector ùú∑,computethederivativesofboth ùëì¬πx¬∫=ùú∑>xandùëî¬πx¬∫=x>ùú∑.
Why do you get the same answer?
2.Letvbe anùëõdimension vector. What isùúï
ùúïvkvk2?
3.Letùêø¬πùë•,ùë¶¬∫=log¬πùëíùë•¬∏ùëíùë¶¬∫. Compute the gradient. What is the sum of the components
of the gradient?
4.Letùëì¬πùë•,ùë¶¬∫=ùë•2ùë¶¬∏ùë•ùë¶2. Show that the only critical point is ¬π0,0¬∫. By considering
ùëì¬πùë•,ùë•¬∫, determine if¬π0,0¬∫is a maximum, minimum, or neither.
5.Suppose that we are minimizing a function ùëì¬πx¬∫=ùëî¬πx¬∫¬∏‚Ñé¬πx¬∫. How can we geomet-
rically interpret the condition of rùëì=0in terms ofùëîand‚Ñé?
Discussions283.
A.5IntegralCalculus
Differentiation only makes up half of the content of a traditional calculus education. The
other pillar, integration, starts out seeming a rather disjoint question, ‚ÄúWhat is the area
underneath this curve?‚Äù While seemingly unrelated, integration is tightly intertwined with
the differentiation via what is known as the fundamentaltheoremof calculus .
Atthelevelofmachinelearningwediscussinthisbook,wewillnotneedadeepunderstand-
ing of integration. However, we will provide a brief introduction to lay the groundwork for
any further applications we will encounter later on.
A.5.1GeometricInterpretation
Supposethatwehaveafunction ùëì¬πùë•¬∫. Forsimplicity,let‚Äôsassumethat ùëì¬πùë•¬∫isnon-negative
(never takes a value less than zero). What we want to try and understand is: what is the
area contained between ùëì¬πùë•¬∫and theùë•-axis?
%matplotlib inline
import torch
from IPython import display
from mpl_toolkits import mplot3d
from d2l import torch asd2l
(continues on next page)
952 Mathematics for Deep Learning
(continued from previous page)
x=torch .arange( -2,2,0.01 )
f=torch .exp( -x**2)
d2l.set_figsize()
d2l.plt.plot(x, f, color ='black ')
d2l.plt.fill_between(x .tolist(), f .tolist())
d2l.plt.show()
In most cases, this area will be infinite or undefined (consider the area under ùëì¬πùë•¬∫=ùë•2),
so people will often talk about the area between a pair of ends, say ùëéandùëè.
x=torch .arange( -2,2,0.01 )
f=torch .exp( -x**2)
d2l.set_figsize()
d2l.plt.plot(x, f, color ='black ')
d2l.plt.fill_between(x .tolist()[ 50:250], f .tolist()[ 50:250])
d2l.plt.show()
We will denote this area by the integral symbol below:
Area¬πA¬∫=¬πùëè
ùëéùëì¬πùë•¬∫ùëëùë•. (A.1)
The inner variable is a dummy variable, much like the index of a sum in a√ç, and so this
can be equivalently written with any inner value we like:
¬πùëè
ùëéùëì¬πùë•¬∫ùëëùë•=¬πùëè
ùëéùëì¬πùëß¬∫ùëëùëß. (A.2)
953 Integral Calculus
There is a traditional way to try and understand how we might try to approximate such
integrals: we can imagine taking the region in-between ùëéandùëèand chopping it into ùëÅ
vertical slices. If ùëÅis large, we can approximate the area of each slice by a rectangle, and
then add up the areas to get the total area under the curve. Let‚Äôs take a look at an example
doing this in code. We will see how to get the true value in a later section.
epsilon =0.05
a=0
b=2
x=torch .arange(a, b, epsilon)
f=x/(1+x**2)
approx =torch .sum(epsilon *f)
true =torch .log(torch .tensor([ 5.])) /2
d2l.set_figsize()
d2l.plt.bar(x, f, width =epsilon, align ='edge ')
d2l.plt.plot(x, f, color ='black ')
d2l.plt.ylim([ 0,1])
d2l.plt.show()
f'approximation: {approx }, truth: {true }'
'approximation: 0.7944855690002441, truth: tensor([0.8047]) '
The issue is that while it can be done numerically, we can do this approach analytically for
only the simplest functions like
¬πùëè
ùëéùë• ùëëùë•. (A.3)
Anything somewhat more complex like our example from the code above
¬πùëè
ùëéùë•
1¬∏ùë•2ùëëùë•. (A.4)
is beyond what we can solve with such a direct method.
We will instead take a different approach. We will work intuitively with the notion of the
area,andlearnthemaincomputationaltoolusedtofindintegrals: the fundamentaltheorem
ofcalculus . This will be the basis for our study of integration.
954 Mathematics for Deep Learning
A.5.2The Fundamental Theoremof Calculus
To dive deeper into the theory of integration, let‚Äôs introduce a function
ùêπ¬πùë•¬∫=¬πùë•
0ùëì¬πùë¶¬∫ùëëùë¶. (A.5)
This function measures the area between 0andùë•depending on how we change ùë•. Notice
that this is everything we need since
¬πùëè
ùëéùëì¬πùë•¬∫ùëëùë•=ùêπ¬πùëè¬∫ ùêπ¬πùëé¬∫. (A.6)
This is a mathematical encoding of the fact that we can measure the area out to the far end-
point and then subtract off the area to the near end point as indicated in Fig. A.1.
tFig. A.1 Visualizing why we may reduce the problem of computing the area under a curve between
two points to computing the area to the left of a point.
Thus, we can figure out what the integral over any interval is by figuring out what ùêπ¬πùë•¬∫
is.
To do so, let‚Äôs consider an experiment. As we often do in calculus, let‚Äôs imagine what hap-
pens when we shift the value by a tiny bit. From the comment above, we know that
ùêπ¬πùë•¬∏ùúñ¬∫ ùêπ¬πùë•¬∫=¬πùë•¬∏ùúñ
ùë•ùëì¬πùë¶¬∫ùëëùë¶. (A.7)
This tells us that the function changes by the area under a tiny sliver of a function.
Thisis the point at whichwemakean approximation. If welookat a tinysliverofarea like
this, it looks like this area is close to the rectangular area with height the value of ùëì¬πùë•¬∫and
the base width ùúñ. Indeed, one can show that as ùúñ!0this approximation becomes better
and better. Thus we can conclude:
ùêπ¬πùë•¬∏ùúñ¬∫ ùêπ¬πùë•¬∫ùúñùëì¬πùë•¬∫. (A.8)
However, we can now notice: this is exactly the pattern we expect if we were computing
the derivative of ùêπ! Thus we see the following rather surprising fact:
ùëëùêπ
ùëëùë•¬πùë•¬∫=ùëì¬πùë•¬∫. (A.9)
This is the fundamentaltheoremof calculus . We may write it in expanded form as
ùëë
ùëëùë•¬πùë•
0ùëì¬πùë¶¬∫ùëëùë¶=ùëì¬πùë•¬∫. (A.10)
It takes the concept of finding areas ( a priori rather hard), and reduces it to a statement
derivatives (something much more completely understood). One last comment that we
955 Integral Calculus
must make is that this does not tell us exactly what ùêπ¬πùë•¬∫is. Indeedùêπ¬πùë•¬∫¬∏ùê∂for anyùê∂has
the same derivative. This is a fact-of-life in the theory of integration. Thankfully, notice
that when working with definite integrals, the constants drop out, and thus are irrelevant to
the outcome.
¬πùëè
ùëéùëì¬πùë•¬∫ùëëùë•=¬πùêπ¬πùëè¬∫¬∏ùê∂¬∫ ¬πùêπ¬πùëé¬∫¬∏ùê∂¬∫=ùêπ¬πùëè¬∫ ùêπ¬πùëé¬∫. (A.11)
Thismayseemlikeabstractnon-sense,butlet‚Äôstakeamomenttoappreciatethatithasgiven
us a whole new perspective on computing integrals. Our goal is no-longer to do some sort
of chop-and-sum process to try and recover the area, rather we need only find a function
whose derivative is the function we have! This is incredible since we can now list many
rather difficult integrals by just reversing the table from Section A.3.2 . For instance, we
know that the derivative of ùë•ùëõisùëõùë•ùëõ 1. Thus, we can say using the fundamental theorem
(A.10 )that
¬πùë•
0ùëõùë¶ùëõ 1ùëëùë¶=ùë•ùëõ 0ùëõ=ùë•ùëõ. (A.12)
Similarly, we know that the derivative of ùëíùë•is itself, so that means
¬πùë•
0ùëíùë•ùëëùë•=ùëíùë• ùëí0=ùëíùë• 1. (A.13)
Inthisway,wecandeveloptheentiretheoryofintegrationleveragingideasfromdifferential
calculus freely. Every integration rule derives from this one fact.
A.5.3Changeof Variables
Just as with differentiation, there are a number of rules which make the computation of
integrals more tractable. In fact, every rule of differential calculus (like the product rule,
sumrule,andchainrule)hasacorrespondingruleforintegralcalculus(integrationbyparts,
linearity of integration, and the change of variables formula respectively). In this section,
we will dive into what is arguably the most important from the list: the change of variables
formula.
First, suppose that we have a function which is itself an integral:
ùêπ¬πùë•¬∫=¬πùë•
0ùëì¬πùë¶¬∫ùëëùë¶. (A.14)
Let‚Äôs suppose that we want to know how this function looks when we compose it with
another to obtain ùêπ¬πùë¢¬πùë•¬∫¬∫. By the chain rule, we know
ùëë
ùëëùë•ùêπ¬πùë¢¬πùë•¬∫¬∫=ùëëùêπ
ùëëùë¢¬πùë¢¬πùë•¬∫¬∫ùëëùë¢
ùëëùë•. (A.15)
We can turn this into a statement about integration by using the fundamental theorem
(A.10 )as above. This gives
ùêπ¬πùë¢¬πùë•¬∫¬∫ ùêπ¬πùë¢¬π0¬∫¬∫=¬πùë•
0ùëëùêπ
ùëëùë¢¬πùë¢¬πùë¶¬∫¬∫ùëëùë¢
ùëëùë¶ùëëùë¶. (A.16)
956 Mathematics for Deep Learning
Recalling that ùêπis itself an integral gives that the left hand side may be rewritten to
be
¬πùë¢¬πùë•¬∫
ùë¢¬π0¬∫ùëì¬πùë¶¬∫ùëëùë¶=¬πùë•
0ùëëùêπ
ùëëùë¢¬πùë¢¬πùë¶¬∫¬∫ùëëùë¢
ùëëùë¶ùëëùë¶. (A.17)
Similarly, recalling that ùêπis an integral allows us to recognize thatùëëùêπ
ùëëùë•=ùëìusing the
fundamental theorem (A.10 ), and thus we may conclude
¬πùë¢¬πùë•¬∫
ùë¢¬π0¬∫ùëì¬πùë¶¬∫ùëëùë¶=¬πùë•
0ùëì¬πùë¢¬πùë¶¬∫¬∫ùëëùë¢
ùëëùë¶ùëëùë¶. (A.18)
This is the changeof variables formula.
Foramoreintuitivederivation,considerwhathappenswhenwetakeanintegralof ùëì¬πùë¢¬πùë•¬∫¬∫
betweenùë•andùë•¬∏ùúñ. For a small ùúñ, this integral is approximately ùúñùëì¬πùë¢¬πùë•¬∫¬∫, the area of
the associated rectangle. Now, let‚Äôs compare this with the integral of ùëì¬πùë¶¬∫fromùë¢¬πùë•¬∫to
ùë¢¬πùë•¬∏ùúñ¬∫. We know that ùë¢¬πùë•¬∏ùúñ¬∫ùë¢¬πùë•¬∫¬∏ùúñùëëùë¢
ùëëùë•¬πùë•¬∫, so the area of this rectangle is approx-
imatelyùúñùëëùë¢
ùëëùë•¬πùë•¬∫ùëì¬πùë¢¬πùë•¬∫¬∫. Thus, to make the area of these two rectangles to agree, we need
to multiply the first one byùëëùë¢
ùëëùë•¬πùë•¬∫as is illustrated in Fig. A.2.
tFig. A.2 Visualizing the transformation of a single thin rectangle under the change of variables.
This tells us that
¬πùë•¬∏ùúñ
ùë•ùëì¬πùë¢¬πùë¶¬∫¬∫ùëëùë¢
ùëëùë¶¬πùë¶¬∫ùëëùë¶=¬πùë¢¬πùë•¬∏ùúñ¬∫
ùë¢¬πùë•¬∫ùëì¬πùë¶¬∫ùëëùë¶. (A.19)
This is the change of variables formula expressed for a single small rectangle.
Ifùë¢¬πùë•¬∫andùëì¬πùë•¬∫are properly chosen, this can allow for the computation of incredibly
complex integrals. For instance, if we even chose ùëì¬πùë¶¬∫=1andùë¢¬πùë•¬∫=ùëí ùë•2(which means
ùëëùë¢
ùëëùë•¬πùë•¬∫= 2ùë•ùëí ùë•2), this can show for instance that
ùëí 1 1=¬πùëí 1
ùëí 01ùëëùë¶= 2¬π1
0ùë¶ùëí ùë¶2ùëëùë¶, (A.20)
and thus by rearranging that
¬π1
0ùë¶ùëí ùë¶2ùëëùë¶=1 ùëí 1
2. (A.21)
A.5.4A Comment on Sign Conventions
957 Integral Calculus
Keen-eyed readers will observe something strange about the computations above. Namely,
computations like
¬πùëí 1
ùëí 01ùëëùë¶=ùëí 1 1<0, (A.22)
can produce negative numbers. When thinking about areas, it can be strange to see a neg-
ative value, and so it is worth digging into what the convention is.
Mathematicians take the notion of signed areas. This manifests itself in two ways. First, if
we consider a function ùëì¬πùë•¬∫which is sometimes less than zero, then the area will also be
negative. So for instance
¬π1
0¬π 1¬∫ùëëùë•= 1. (A.23)
Similarly, integrals which progress from right to left, rather than left to right are also taken
to be negative areas
¬π 1
01ùëëùë•= 1. (A.24)
The standard area (from left to right of a positive function) is always positive. Anything
obtained by flipping it (say flipping over the ùë•-axis to get the integral of a negative number,
or flipping over the ùë¶-axis to get an integral in the wrong order) will produce a negative
area. And indeed, flipping twice will give a pair of negative signs that cancel out to have
positive area
¬π 1
0¬π 1¬∫ùëëùë•=1. (A.25)
If this discussion sounds familiar, it is! In Section A.1 we discussed how the determinant
represented the signed area in much the same way.
A.5.5MultipleIntegrals
In some cases, we will need to work in higher dimensions. For instance, suppose that we
have a function of two variables, like ùëì¬πùë•,ùë¶¬∫and we want to know the volume under ùëì
whenùë•ranges over¬ªùëé,ùëè¬ºandùë¶ranges over¬ªùëê,ùëë¬º.
# Construct grid and compute function
x, y =torch .meshgrid(torch .linspace( -2,2,101), torch .linspace( -2,2,101))
z=torch .exp( -x**2-y**2)
# Plot function
ax=d2l.plt.figure() .add_subplot( 111, projection ='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'y')
d2l.plt.xticks([ -2,-1,0,1,2])
d2l.plt.yticks([ -2,-1,0,1,2])
d2l.set_figsize()
(continues on next page)
958 Mathematics for Deep Learning
(continued from previous page)
ax.set_xlim( -2,2)
ax.set_ylim( -2,2)
ax.set_zlim( 0,1)
ax.dist =12
We write this as¬π
¬ªùëé,ùëè¬º¬ªùëê,ùëë¬ºùëì¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶. (A.26)
Supposethatwewishtocomputethisintegral. Myclaimisthatwecandothisbyiteratively
computing first the integral in ùë•and then shifting to the integral in ùë¶, that is to say
¬π
¬ªùëé,ùëè¬º¬ªùëê,ùëë¬ºùëì¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶ =¬πùëë
ùëê¬πùëè
ùëéùëì¬πùë•,ùë¶¬∫ùëëùë•
ùëëùë¶. (A.27)
Let‚Äôs see why this is.
Considerthefigureabovewherewehavesplitthefunctioninto ùúñùúñsquareswhichwewill
index with integer coordinates ùëñ,ùëó. In this case, our integral is approximately
√ï
ùëñ,ùëóùúñ2ùëì¬πùúñùëñ,ùúñùëó¬∫.(A.28)
Once we discretize the problem, we may add up the values on these squares in whatever
order we like, and not worry about changing the values. This is illustrated in Fig. A.3. In
particular, we can say that
√ï
ùëóùúñ √ï
ùëñùúñùëì¬πùúñùëñ,ùúñùëó¬∫!
. (A.29)
tFig. A.3 Illustrating how to decompose a sum over many squares as a sum over Ô¨Årst the columns
(1), then adding the column sums together (2).
959 Integral Calculus
The sum on the inside is precisely the discretization of the integral
ùê∫¬πùúñùëó¬∫=¬πùëè
ùëéùëì¬πùë•,ùúñùëó¬∫ùëëùë•. (A.30)
Finally, notice that if we combine these two expressions we get
√ï
ùëóùúñùê∫¬πùúñùëó¬∫¬πùëë
ùëêùê∫¬πùë¶¬∫ùëëùë¶=¬π
¬ªùëé,ùëè¬º¬ªùëê,ùëë¬ºùëì¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶. (A.31)
Thus putting it all together, we have that
¬π
¬ªùëé,ùëè¬º¬ªùëê,ùëë¬ºùëì¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶ =¬πùëë
ùëê¬πùëè
ùëéùëì¬πùë•,ùë¶¬∫ùëëùë•
ùëëùë¶. (A.32)
Notice that, once discretized, all we did was rearrange the order in which we added a list
of numbers. This may make it seem like it is nothing, however this result (called Fubini‚Äôs
Theorem ) is not always true! For the type of mathematics encountered when doing ma-
chine learning (continuous functions), there is no concern, however it is possible to create
exampleswhereitfails(forexamplethefunction ùëì¬πùë•,ùë¶¬∫=ùë•ùë¶¬πùë•2 ùë¶2¬∫¬ù¬πùë•2¬∏ùë¶2¬∫3overthe
rectangle¬ª0,2¬º¬ª 0,1¬º).
Notethatthechoicetodotheintegralin ùë•first, andthentheintegralin ùë¶wasarbitrary. We
could have equally well chosen to do ùë¶first and then ùë•to see
¬π
¬ªùëé,ùëè¬º¬ªùëê,ùëë¬ºùëì¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶ =¬πùëè
ùëé¬πùëë
ùëêùëì¬πùë•,ùë¶¬∫ùëëùë¶
ùëëùë•. (A.33)
Often times, we will condense down to vector notation, and say that for ùëà=¬ªùëé,ùëè¬º¬ªùëê,ùëë¬º
this is¬π
ùëàùëì¬πx¬∫ùëëx. (A.34)
A.5.6Changeof Variablesin Multiple Integrals
As with single variables in (A.18 ), the ability to change variables inside a higher dimen-
sional integral is a key tool. Let‚Äôs summarize the result without derivation.
We need a function that reparametrizes our domain of integration. We can take this to be
ùúô:Rùëõ!Rùëõ, that is any function which takes in ùëõreal variables and returns another ùëõ. To
keep the expressions clean, we will assume that ùúôisinjective which is to say it never folds
over itself (ùúô¬πx¬∫=ùúô¬πy¬∫=)x=y).
In this case, we can say that
¬π
ùúô¬πùëà¬∫ùëì¬πx¬∫ùëëx=¬π
ùëàùëì¬πùúô¬πx¬∫¬∫jdet¬πùê∑ùúô¬πx¬∫¬∫jùëëx. (A.35)
whereùê∑ùúôistheJacobian ofùúô,whichisthematrixofpartialderivativesof ùùì=¬πùúô1¬πùë•1,...,ùë•ùëõ¬∫,...,ùúôùëõ¬πùë•1,...,ùë•ùëõ¬∫¬∫,
ùê∑ùùì=26666664ùúïùúô 1
ùúïùë•1ùúïùúô 1
ùúïùë•ùëõ.........
ùúïùúôùëõ
ùúïùë•1ùúïùúôùëõ
ùúïùë•ùëõ37777775. (A.36)
960 Mathematics for Deep Learning
Looking closely, we see that this is similar to the single variable chain rule (A.18 ), except
we have replaced the termùëëùë¢
ùëëùë•¬πùë•¬∫withjdet¬πùê∑ùúô¬πx¬∫¬∫j. Let‚Äôs see how we can to interpret
this term. Recall that theùëëùë¢
ùëëùë•¬πùë•¬∫term existed to say how much we stretched our ùë•-axis by
applyingùë¢. The same process in higher dimensions is to determine how much we stretch
thearea(orvolume,orhyper-volume)ofalittlesquare(orlittle hyper-cube )byapplying ùùì.
Ifùùìwas the multiplication by a matrix, then we know how the determinant already gives
the answer.
With some work, one can show that the Jacobian provides the best approximation to a
multivariable function ùùìat a point by a matrix in the same way we could approximate by
linesorplaneswithderivativesandgradients. ThusthedeterminantoftheJacobianexactly
mirrors the scaling factor we identified in one dimension.
It takes some work to fill in the details to this, so do not worry if they are not clear now.
Let‚Äôs see at least one example we will make use of later on. Consider the integral
¬π1
 1¬π1
 1ùëí ùë•2 ùë¶2ùëëùë• ùëëùë¶. (A.37)
Playing with this integral directly will get us no-where, but if we change variables, we can
make significant progress. If we let ùùì¬πùëü,ùúÉ¬∫=¬πùëücos¬πùúÉ¬∫,ùëüsin¬πùúÉ¬∫¬∫(which is to say that
ùë•=ùëücos¬πùúÉ¬∫,ùë¶=ùëüsin¬πùúÉ¬∫), then we can apply the change of variable formula to see that
this is the same thing as
¬π1
0¬π2ùúã
0ùëí ùëü2jdet¬πùê∑≈í¬πx¬∫¬∫jùëëùúÉ ùëëùëü, (A.38)
where
jdet¬πùê∑≈í¬πx¬∫¬∫j=detcos¬πùúÉ¬∫  ùëüsin¬πùúÉ¬∫
sin¬πùúÉ¬∫ùëücos¬πùúÉ¬∫=ùëü¬πcos2¬πùúÉ¬∫¬∏sin2¬πùúÉ¬∫¬∫=ùëü. (A.39)
Thus, the integral is
¬π1
0¬π2ùúã
0ùëüùëí ùëü2ùëëùúÉ ùëëùëü =2ùúã¬π1
0ùëüùëí ùëü2ùëëùëü=ùúã, (A.40)
where the final equality follows by the same computation that we used in section Section
A.5.3.
We will meet this integral again when we study continuous random variables in Section
A.6.
A.5.7Summary
The theory of integration allows us to answer questions about areas or volumes.
The fundamental theorem of calculus allows us to leverage knowledge about derivatives
to compute areas via the observation that the derivative of the area up to some point
is given by the value of the function being integrated.
Integrals in higher dimensions can be computed by iterating single variable integrals.
961 Random Variables
284A.5.8Exercises
1.What is¬Ø2
11
ùë•ùëëùë•?
2.Use the change of variables formula to integrate¬Øpùúã
0ùë•sin¬πùë•2¬∫ùëëùë•.
3.What is¬Ø
¬ª0,1¬º2ùë•ùë¶ ùëëùë• ùëëùë¶ ?
4.Use the change of variables formula to compute¬Ø2
0¬Ø1
0ùë•ùë¶¬πùë•2 ùë¶2¬∫¬ù¬πùë•2¬∏ùë¶2¬∫3ùëëùë¶ ùëëùë•
and¬Ø1
0¬Ø2
0ùëì¬πùë•,ùë¶¬∫=ùë•ùë¶¬πùë•2 ùë¶2¬∫¬ù¬πùë•2¬∏ùë¶2¬∫3ùëëùë• ùëëùë¶to see they are different.
Discussions284.
A.6Random Variables
InSection 2.6 we saw the basics of how to work with discrete random variables, which in
our case refer to those random variables which take either a finite set of possible values, or
the integers. In this section, we develop the theory of continuousrandomvariables , which
are random variables which can take on any real value.
A.6.1Continuous Random Variables
Continuous random variables are a significantly more subtle topic than discrete random
variables. A fair analogy to make is that the technical jump is comparable to the jump
between adding lists of numbers and integrating functions. As such, we will need to take
some time to develop the theory.
FromDiscreteto Continuous
To understand the additional technical challenges encountered when working with contin-
uous random variables, let‚Äôs perform a thought experiment. Suppose that we are throwing
a dart at the dart board, and we want to know the probability that it hits exactly 2cm from
the center of the board.
To start with, we imagine measuring a single digit of accuracy, that is to say with bins for
0cm,1cm,2cm, and so on. We throw say 100darts at the dart board, and if 20of them fall
into the bin for 2cm we conclude that 20%of the darts we throw hit the board 2cm away
from the center.
However,whenwelookcloser,thisdoesnotmatchourquestion! Wewantedexactequality,
whereas these bins hold all that fell between say 1.5cm and 2.5cm.
Undeterred, we continue further. We measure even more precisely, say 1.9cm, 2.0cm,
2.1cm, and now see that perhaps 3of the 100darts hit the board in the 2.0cm bucket. Thus
we conclude the probability is 3%.
962 Mathematics for Deep Learning
However,thisdoesnotsolveanything! Wehavejustpushedtheissuedownonedigitfurther.
Let‚Äôs abstract a bit. Imagine we know the probability that the first ùëòdigits match with
2.00000...and we want to know the probability it matches for the first ùëò¬∏1digits. It is
fairly reasonable to assume that the ùëò¬∏1thdigit is essentially a random choice from the
setf0,1,2,..., 9g. At least, we cannot conceive of a physically meaningful process which
would force the number of micrometers away form the center to prefer to end in a 7vs a
3.
What this means is that in essence each additional digit of accuracy we require should
decrease probability of matching by a factor of 10. Or put another way, we would expect
that
ùëÉ¬πdistance is 2.00...,toùëòdigits¬∫ùëù10 ùëò. (A.1)
Thevalueùëùessentiallyencodeswhathappenswiththefirstfewdigits,andthe 10 ùëòhandles
the rest.
Notice that if we know the position accurate to ùëò=4digits after the decimal, that means
we know the value falls within the interval say ¬ª1.99995,2.00005¬ºwhich is an interval of
length 2.00005 1.99995 =10 4. Thus, if we call the length of this interval ùúñ, we can
say
ùëÉ¬πdistance is in an ùúñ-sized interval around 2¬∫ùúñùëù. (A.2)
Let‚Äôs take this one final step further. We have been thinking about the point 2the entire
time, but never thinking about other points. Nothing is different there fundamentally, but
it is the case that the value ùëùwill likely be different. We would at least hope that a dart
thrower was more likely to hit a point near the center, like 2cm rather than 20cm. Thus, the
valueùëùis not fixed, but rather should depend on the point ùë•. This tells us that we should
expect
ùëÉ¬πdistance is in an ùúñ-sized interval around ùë•¬∫ùúñùëù¬πùë•¬∫. (A.3)
Indeed, (A.3)preciselydefinesthe probabilitydensityfunction . Itisafunction ùëù¬πùë•¬∫which
encodes the relative probability of hitting near one point vs. another. Let‚Äôs visualize what
such a function might look like.
%matplotlib inline
import torch
from IPython import display
from d2l import torch asd2l
torch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch
# Plot the probability density function for some random variable
x=torch .arange( -5,5,0.01 )
p=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\
0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))
d2l.plot(x, p, 'x','Density ')
963 Random Variables
The locations where the function value is large indicates regions where we are more likely
to find the random value. The low portions are areas where we are unlikely to find the
random value.
ProbabilityDensity Functions
Let‚Äôsnowinvestigatethisfurther. Wehavealreadyseenwhataprobabilitydensityfunction
is intuitively for a random variable ùëã, namely the density function is a function ùëù¬πùë•¬∫so
that
ùëÉ¬πùëãis in anùúñ-sized interval around ùë•¬∫ùúñùëù¬πùë•¬∫. (A.4)
But what does this imply for the properties of ùëù¬πùë•¬∫?
First, probabilities are never negative, thus we should expect that ùëù¬πùë•¬∫0as well.
Second, let‚Äôs imagine that we slice up the Rinto an infinite number of slices which are ùúñ
wide,saywithslices ¬πùúñùëñ,ùúñ¬πùëñ¬∏1¬∫¬º. Foreachofthese,weknowfrom (A.4)theprobability
is approximately
ùëÉ¬πùëãis in anùúñ-sized interval around ùë•¬∫ùúñùëù¬πùúñùëñ¬∫, (A.5)
so summed over all of them it should be
ùëÉ¬πùëã2R¬∫√ï
ùëñùúñùëù¬πùúñùëñ¬∫.(A.6)
This is nothing more than the approximation of an integral discussed in Section A.5 , thus
we can say that
ùëÉ¬πùëã2R¬∫=¬π1
 1ùëù¬πùë•¬∫ùëëùë•. (A.7)
We know that ùëÉ¬πùëã2R¬∫=1, since the random variable must take on somenumber, we
can conclude that for any density
¬π1
 1ùëù¬πùë•¬∫ùëëùë•=1. (A.8)
Indeed, digging into this further shows that for any ùëé, andùëè, we see that
ùëÉ¬πùëã2¬πùëé,ùëè¬º¬∫=¬πùëè
ùëéùëù¬πùë•¬∫ùëëùë•. (A.9)
964 Mathematics for Deep Learning
We may approximate this in code by using the same discrete approximation methods as
before. Inthiscasewecanapproximatetheprobabilityoffallingintheblueregion.
# Approximate probability using numerical integration
epsilon =0.01
x=torch .arange( -5,5,0.01 )
p=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\
0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))
d2l.set_figsize()
d2l.plt.plot(x, p, color ='black ')
d2l.plt.fill_between(x .tolist()[ 300:800], p .tolist()[ 300:800])
d2l.plt.show()
f'approximate Probability: {torch .sum(epsilon *p[300:800])}'
'approximate Probability: 0.773617148399353 '
It turns out that these two properties describe exactly the space of possible probability
density functions (or p.d.f.‚Äôs for the commonly encountered abbreviation). They are non-
negative functions ùëù¬πùë•¬∫0such that
¬π1
 1ùëù¬πùë•¬∫ùëëùë•=1. (A.10)
Weinterpretthisfunctionbyusingintegrationtoobtaintheprobabilityourrandomvariable
is in a specific interval:
ùëÉ¬πùëã2¬πùëé,ùëè¬º¬∫=¬πùëè
ùëéùëù¬πùë•¬∫ùëëùë•. (A.11)
InSection A.8 we will see a number of common distributions, but let‚Äôs continue working
in the abstract.
CumulativeDistribution Functions
In the previous section, we saw the notion of the p.d.f. In practice, this is a commonly en-
counteredmethodtodiscusscontinuousrandomvariables,butithasonesignificantpitfall:
that the values of the p.d.f. are not themselves probabilities, but rather a function that we
must integrate to yield probabilities. There is nothing wrong with a density being larger
965 Random Variables
than 10, as long as it is not larger than 10for more than an interval of length 1¬ù10. This
can be counter-intuitive, so people often also think in terms of the cumulative distribution
function, or c.d.f., which isa probability.
In particular, by using (A.11 ), we define the c.d.f. for a random variable ùëãwith density
ùëù¬πùë•¬∫by
ùêπ¬πùë•¬∫=¬πùë•
 1ùëù¬πùë•¬∫ùëëùë•=ùëÉ¬πùëãùë•¬∫. (A.12)
Let‚Äôs observe a few properties.
ùêπ¬πùë•¬∫! 0asùë•! 1.
ùêπ¬πùë•¬∫! 1asùë•!1.
ùêπ¬πùë•¬∫is non-decreasing ( ùë¶ >ùë• =)ùêπ¬πùë¶¬∫ùêπ¬πùë•¬∫).
ùêπ¬πùë•¬∫is continuous (has no jumps) if ùëãis a continuous random variable.
With the fourth bullet point, note that this would not be true if ùëãwere discrete, say taking
the values 0and1both with probability 1¬ù2. In that case
ùêπ¬πùë•¬∫=8>>> <
>>>:0ùë• <0,
1
2ùë• <1,
1ùë•1.(A.13)
In this example, we see one of the benefits of working with the c.d.f., the ability to deal
with continuous or discrete random variables in the same framework, or indeed mixtures
of the two (flip a coin: if heads return the roll of a die, if tails return the distance of a dart
throw from the center of a dart board).
Means
Suppose that we are dealing with a random variables ùëã. The distribution itself can be hard
to interpret. It is often useful to be able to summarize the behavior of a random variable
concisely. Numbers that help us capture the behavior of a random variable are called sum-
marystatistics . The most commonly encountered ones are the mean, thevariance , and the
standarddeviation .
Themeanencodes the average value of a random variable. If we have a discrete random
variableùëã, which takes the values ùë•ùëñwith probabilities ùëùùëñ, then the mean is given by the
weighted average: sum the values times the probability that the random variable takes on
that value:
ùúáùëã=ùê∏¬ªùëã¬º=√ï
ùëñùë•ùëñùëùùëñ.(A.14)
The way we should interpret the mean (albeit with caution) is that it tells us essentially
where the random variable tends to be located.
As a minimalistic example that we will examine throughout this section, let‚Äôs take ùëãto be
966 Mathematics for Deep Learning
therandomvariablewhichtakesthevalue ùëé 2withprobability ùëù,ùëé¬∏2withprobability ùëù
andùëéwith probability 1 2ùëù. We can compute using (A.14 )that, for any possible choice
ofùëéandùëù, the mean is
ùúáùëã=ùê∏¬ªùëã¬º=√ï
ùëñùë•ùëñùëùùëñ=¬πùëé 2¬∫ùëù¬∏ùëé¬π1 2ùëù¬∫¬∏¬πùëé¬∏2¬∫ùëù=ùëé.(A.15)
Thus we see that the mean is ùëé. This matches the intuition since ùëéis the location around
which we centered our random variable.
Because they are helpful, let‚Äôs summarize a few properties.
For any random variable ùëãand numbers ùëéandùëè, we have that ùúáùëéùëã¬∏ùëè=ùëéùúáùëã¬∏ùëè.
If we have two random variables ùëãandùëå, we haveùúáùëã¬∏ùëå=ùúáùëã¬∏ùúáùëå.
Meansareusefulforunderstandingtheaveragebehaviorofarandomvariable,howeverthe
meanisnotsufficienttoevenhaveafullintuitiveunderstanding. Makingaprofitof $10$1
per sale is very different from making $10$15per sale despite having the same average
value. The second one has a much larger degree of fluctuation, and thus represents a much
largerrisk. Thus,tounderstandthebehaviorofarandomvariable,wewillneedatminimum
one more measure: some measure of how widely a random variable fluctuates.
Variances
This leads us to consider the variance of a random variable. This is a quantitative measure
of how far a random variable deviates from the mean. Consider the expression ùëã ùúáùëã.
This is the deviation of the random variable from its mean. This value can be positive
or negative, so we need to do something to make it positive so that we are measuring the
magnitude of the deviation.
A reasonable thing to try is to look at jùëã ùúáùëãj, and indeed this leads to a useful quan-
tity called the mean absolute deviation , however due to connections with other areas of
mathematics and statistics, people often use a different solution.
Inparticular,theylookat ¬πùëã ùúáùëã¬∫2.Ifwelookatthetypicalsizeofthisquantitybytaking
the mean, we arrive at the variance
ùúé2
ùëã=Var¬πùëã¬∫=ùê∏
¬πùëã ùúáùëã¬∫2
=ùê∏¬ªùëã2¬º ùúá2
ùëã. (A.16)
Thelastequalityin (A.16 )holdsbyexpandingoutthedefinitioninthemiddle,andapplying
the properties of expectation.
Let‚Äôs look at our example where ùëãis the random variable which takes the value ùëé 2with
probabilityùëù,ùëé¬∏2with probability ùëùandùëéwith probability 1 2ùëù. In this case ùúáùëã=ùëé,
so all we need to compute is ùê∏
ùëã2
. This can readily be done:
ùê∏
ùëã2
=¬πùëé 2¬∫2ùëù¬∏ùëé2¬π1 2ùëù¬∫¬∏¬πùëé¬∏2¬∫2ùëù=ùëé2¬∏8ùëù. (A.17)
Thus, we see that by (A.16 )our variance is
ùúé2
ùëã=Var¬πùëã¬∫=ùê∏¬ªùëã2¬º ùúá2
ùëã=ùëé2¬∏8ùëù ùëé2=8ùëù. (A.18)
967 Random Variables
This result again makes sense. The largest ùëùcan be is 1¬ù2which corresponds to picking
ùëé 2orùëé¬∏2with a coin flip. The variance of this being 4corresponds to the fact that
bothùëé 2andùëé¬∏2are2units away from the mean, and 22=4. On the other end of the
spectrum, if ùëù=0, this random variable always takes the value 0and so it has no variance
at all.
We will list a few properties of variance below:
For any random variable ùëã, Var¬πùëã¬∫0, with Var¬πùëã¬∫=0if and only if ùëãis a constant.
Foranyrandomvariable ùëãandnumbers ùëéandùëè,wehavethatVar¬πùëéùëã¬∏ùëè¬∫=ùëé2Var¬πùëã¬∫.
If we have two independent random variables ùëãandùëå, we have Var¬πùëã¬∏ùëå¬∫=Var¬πùëã¬∫¬∏
Var¬πùëå¬∫.
When interpreting these values, there can be a bit of a hiccup. In particular, let‚Äôs try imag-
iningwhathappensifwekeeptrackofunitsthroughthiscomputation. Supposethatweare
workingwiththestarratingassignedtoaproductonthewebpage. Then ùëé,ùëé 2, andùëé¬∏2
are all measured in units of stars. Similarly, the mean ùúáùëãis then also measured in stars
(being a weighted average). However, if we get to the variance, we immediately encounter
an issue, which is we want to look at ¬πùëã ùúáùëã¬∫2, which is in units of squared stars . This
means that the variance itself is not comparable to the original measurements. To make it
interpretable, we will need to return to our original units.
StandardDeviations
Thissummarystatisticscanalwaysbededucedfromthevariancebytakingthesquareroot!
Thus we define the standarddeviation to be
ùúéùëã=p
Var¬πùëã¬∫. (A.19)
In our example, this means we now have the standard deviation is ùúéùëã=2p
2ùëù. If we are
dealing with units of stars for our review example, ùúéùëãis again in units of stars.
The properties we had for the variance can be restated for the standard deviation.
For any random variable ùëã,ùúéùëã0.
For any random variable ùëãand numbers ùëéandùëè, we have that ùúéùëéùëã¬∏ùëè=jùëéjùúéùëã
If we have two independent random variables ùëãandùëå, we haveùúéùëã¬∏ùëå=q
ùúé2
ùëã¬∏ùúé2
ùëå.
It is natural at this moment to ask, ‚ÄúIf the standard deviation is in the units of our original
random variable, does it represent something we can draw with regards to that random
variable?‚Äù The answer is a resounding yes! Indeed much like the mean told us the typical
location of our random variable, the standard deviation gives the typical range of variation
of that random variable. We can make this rigorous with what is known as Chebyshev‚Äôs
inequality:
ùëÉ¬πùëã‚àâ¬ªùúáùëã ùõºùúéùëã,ùúáùëã¬∏ùõºùúéùëã¬º¬∫1
ùõº2. (A.20)
968 Mathematics for Deep Learning
Or to state it verbally in the case of ùõº=10,99%of the samples from any random variable
fall within 10standard deviations of the mean. This gives an immediate interpretation to
our standard summary statistics.
To see how this statement is rather subtle, let‚Äôs take a look at our running example again
whereùëãis the random variable which takes the value ùëé 2with probability ùëù,ùëé¬∏2with
probabilityùëùandùëéwith probability 1 2ùëù. We saw that the mean was ùëéand the standard
deviation was 2p
2ùëù. This means, if we take Chebyshev‚Äôs inequality (A.20 )withùõº=2,
we see that the expression is
ùëÉ
ùëã‚àâ¬ªùëé 4p
2ùëù,ùëé¬∏4p
2ùëù¬º
1
4. (A.21)
This means that 75%of the time, this random variable will fall within this interval for any
value ofùëù. Now, notice that as ùëù!0, this interval also converges to the single point ùëé.
Butweknowthatourrandomvariabletakesthevalues ùëé 2,ùëé,andùëé¬∏2onlysoeventually
we can be certain ùëé 2andùëé¬∏2will fall outside the interval! The question is, at what ùëù
does that happen. So we want to solve: for what ùëùdoesùëé¬∏4p
2ùëù=ùëé¬∏2, which is solved
whenùëù=1¬ù8,whichis exactlythefirstùëùwhereitcouldpossiblyhappenwithoutviolating
our claim that no more than 1¬ù4of samples from the distribution would fall outside the
interval ( 1¬ù8to the left, and 1¬ù8to the right).
Let‚Äôsvisualizethis. Wewillshowtheprobabilityofgettingthethreevaluesasthreevertical
bars with height proportional to the probability. The interval will be drawn as a horizontal
line in the middle. The first plot shows what happens for ùëù > 1¬ù8where the interval safely
contains all points.
# Define a helper to plot these figures
def plot_chebyshev (a, p):
d2l.set_figsize()
d2l.plt.stem([a -2, a, a +2], [p, 1-2*p, p], use_line_collection =True )
d2l.plt.xlim([ -4,4])
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.hlines( 0.5, a -4*torch .sqrt( 2*p),
a+4*torch .sqrt( 2*p), 'black ', lw =4)
d2l.plt.vlines(a -4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)
d2l.plt.vlines(a +4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)
d2l.plt.title( f'p = {p:.3f}')
d2l.plt.show()
# Plot interval when p > 1/8
plot_chebyshev( 0.0, torch .tensor( 0.2))
The second shows that at ùëù=1¬ù8, the interval exactly touches the two points. This shows
that the inequality is sharp, since no smaller interval could be taken while keeping the
inequality true.
969 Random Variables
# Plot interval when p = 1/8
plot_chebyshev( 0.0, torch .tensor( 0.125 ))
Thethirdshowsthatfor ùëù < 1¬ù8theintervalonlycontainsthecenter. Thisdoesnotinvali-
date the inequality since we only needed to ensure that no more than 1¬ù4of the probability
falls outside the interval, which means that once ùëù < 1¬ù8, the two points at ùëé 2andùëé¬∏2
can be discarded.
# Plot interval when p < 1/8
plot_chebyshev( 0.0, torch .tensor( 0.05 ))

970 Mathematics for Deep Learning
Means and Variancesin the Continuum
This has all been in terms of discrete random variables, but the case of continuous random
variables is similar. To intuitively understand how this works, imagine that we split the
real number line into intervals of length ùúñgiven by¬πùúñùëñ,ùúñ¬πùëñ¬∏1¬∫¬º. Once we do this, our
continuousrandomvariablehasbeenmadediscreteandwecanuse (A.14 )saythat
ùúáùëã√ï
ùëñ¬πùúñùëñ¬∫ùëÉ¬πùëã2¬πùúñùëñ,ùúñ¬πùëñ¬∏1¬∫¬º¬∫
√ï
ùëñ¬πùúñùëñ¬∫ùëùùëã¬πùúñùëñ¬∫ùúñ,(A.22)
whereùëùùëãis the density of ùëã. This is an approximation to the integral of ùë•ùëùùëã¬πùë•¬∫, so we
can conclude that
ùúáùëã=¬π1
 1ùë•ùëùùëã¬πùë•¬∫ùëëùë•. (A.23)
Similarly, using (A.16 )the variance can be written as
ùúé2
ùëã=ùê∏¬ªùëã2¬º ùúá2
ùëã=¬π1
 1ùë•2ùëùùëã¬πùë•¬∫ùëëùë• ¬π1
 1ùë•ùëùùëã¬πùë•¬∫ùëëùë•2
. (A.24)
Everything stated above about the mean, the variance, and the standard deviation still ap-
plies in this case. For instance, if we consider the random variable with density
ùëù¬πùë•¬∫=(
1ùë•2¬ª0,1¬º,
0otherwise.(A.25)
we can compute
ùúáùëã=¬π1
 1ùë•ùëù¬πùë•¬∫ùëëùë•=¬π1
0ùë• ùëëùë•=1
2. (A.26)
and
ùúé2
ùëã=¬π1
 1ùë•2ùëù¬πùë•¬∫ùëëùë• 1
22
=1
3 1
4=1
12. (A.27)
As a warning, let‚Äôs examine one more example, known as the Cauchy distribution . This is
the distribution with p.d.f. given by
ùëù¬πùë•¬∫=1
1¬∏ùë•2. (A.28)
# Plot the Cauchy distribution p.d.f.
x=torch .arange( -5,5,0.01 )
p=1/(1+x**2)
d2l.plot(x, p, 'x','p.d.f. ')
This function looks innocent, and indeed consulting a table of integrals will show it has
area one under it, and thus it defines a continuous random variable.
971 Random Variables
To see what goes astray, let‚Äôs try to compute the variance of this. This would involve using
(A.16 )computing
¬π1
 1ùë•2
1¬∏ùë•2ùëëùë•. (A.29)
The function on the inside looks like this:
# Plot the integrand needed to compute the variance
x=torch .arange( -20,20,0.01 )
p=x**2/(1+x**2)
d2l.plot(x, p, 'x','integrand ')
This function clearly has infinite area under it since it is essentially the constant one with a
small dip near zero, and indeed we could show that
¬π1
 1ùë•2
1¬∏ùë•2ùëëùë•=1. (A.30)
This means it does not have a well-defined finite variance.
However, looking deeper shows an even more disturbing result. Let‚Äôs try to compute the
mean using (A.14 ). Using the change of variables formula, we see
ùúáùëã=¬π1
 1ùë•
1¬∏ùë•2ùëëùë•=1
2¬π1
11
ùë¢ùëëùë¢. (A.31)
The integral inside is the definition of the logarithm, so this is in essence log¬π1¬∫=1, so
there is no well-defined average value either!
972 Mathematics for Deep Learning
Machine learning scientists define their models so that we most often do not need to deal
with these issues, and will in the vast majority of cases deal with random variables with
well-defined means and variances. However, every so often random variables with heavy
tails(thatisthoserandomvariableswheretheprobabilitiesofgettinglargevaluesarelarge
enoughtomakethingslikethemeanorvarianceundefined)arehelpfulinmodelingphysical
systems, thus it is worth knowing that they exist.
Joint Density Functions
Theaboveworkallassumesweareworkingwithasinglerealvaluedrandomvariable. But
what if we are dealing with two or more potentially highly correlated random variables?
This circumstance is the norm in machine learning: imagine random variables like ùëÖùëñ,ùëó
which encode the red value of the pixel at the ¬πùëñ,ùëó¬∫coordinate in an image, or ùëÉùë°which is
arandomvariablegivenbyastockpriceattime ùë°. Nearbypixelstendtohavesimilarcolor,
and nearby times tend to have similar prices. We cannot treat them as separate random
variables, and expect to create a successful model (we will see in Section A.9 a model that
under-performsduetosuchanassumption). Weneedtodevelopthemathematicallanguage
to handle these correlated continuous random variables.
Thankfully, with the multiple integrals in Section A.5 we can develop such a language.
Suppose that we have, for simplicity, two random variables ùëã,ùëåwhich can be correlated.
Then, similar to the case of a single variable, we can ask the question:
ùëÉ¬πùëãis in anùúñ-sized interval around ùë•andùëåis in anùúñ-sized interval around ùë¶¬∫.(A.32)
Similarreasoningtothesinglevariablecaseshowsthatthisshouldbeapproximately
ùëÉ¬πùëãis in anùúñ-sized interval around ùë•andùëåis in anùúñ-sized interval around ùë¶¬∫ùúñ2ùëù¬πùë•,ùë¶¬∫,
(A.33)
for some function ùëù¬πùë•,ùë¶¬∫. This is referred to as the joint density of ùëãandùëå. Similar
properties are true for this as we saw in the single variable case. Namely:
ùëù¬πùë•,ùë¶¬∫0;
¬Ø
R2ùëù¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶ =1;
ùëÉ¬π¬πùëã,ùëå¬∫2D¬∫ =¬Ø
Dùëù¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶.
In this way, we can deal with multiple, potentially correlated random variables. If we wish
to work with more than two random variables, we can extend the multivariate density to as
many coordinates as desired by considering ùëù¬πx¬∫=ùëù¬πùë•1,...,ùë•ùëõ¬∫. The same properties of
being non-negative, and having total integral of one still hold.
Marginal Distributions
When dealing with multiple variables, we oftentimes want to be able to ignore the rela-
tionships and ask, ‚Äúhow is this one variable distributed?‚Äù Such a distribution is called a
marginaldistribution .
973 Random Variables
To be concrete, let‚Äôs suppose that we have two random variables ùëã,ùëåwith joint density
given byùëùùëã,ùëå¬πùë•,ùë¶¬∫. We will be using the subscript to indicate what random variables the
density is for. The question of finding the marginal distribution is taking this function, and
using it to find ùëùùëã¬πùë•¬∫.
As with most things, it is best to return to the intuitive picture to figure out what should be
true. Recall that the density is the function ùëùùëãso that
ùëÉ¬πùëã2¬ªùë•,ùë•¬∏ùúñ¬º¬∫ùúñùëùùëã¬πùë•¬∫. (A.34)
There is no mention of ùëå, but if all we are given is ùëùùëã,ùëå, we need to include ùëåsomehow.
We can first observe that this is the same as
ùëÉ¬πùëã2¬ªùë•,ùë•¬∏ùúñ¬º, andùëå2R¬∫ùúñùëùùëã¬πùë•¬∫. (A.35)
Our density does not directly tell us about what happens in this case, we need to split into
small intervals in ùë¶as well, so we can write this as
ùúñùëùùëã¬πùë•¬∫√ï
ùëñùëÉ¬πùëã2¬ªùë•,ùë•¬∏ùúñ¬º, andùëå2¬ªùúñùëñ,ùúñ¬πùëñ¬∏1¬∫¬º¬∫
√ï
ùëñùúñ2ùëùùëã,ùëå¬πùë•,ùúñùëñ¬∫.(A.36)
tFig. A.1 By summing along the columns of our array of probabilities, we are able to obtain the
marginal distribution for just the random variable represented along the x-axis.
Thistellsustoaddupthevalueofthedensityalongaseriesofsquaresinalineasisshown
inFig. A.1. Indeed, after canceling one factor of epsilon from both sides, and recognizing
the sum on the right is the integral over ùë¶, we can conclude that
ùëùùëã¬πùë•¬∫√ï
ùëñùúñùëùùëã,ùëå¬πùë•,ùúñùëñ¬∫
¬π1
 1ùëùùëã,ùëå¬πùë•,ùë¶¬∫ùëëùë¶.(A.37)
Thus we see
ùëùùëã¬πùë•¬∫=¬π1
 1ùëùùëã,ùëå¬πùë•,ùë¶¬∫ùëëùë¶. (A.38)
This tells us that to get a marginal distribution, we integrate over the variables we do not
974 Mathematics for Deep Learning
care about. This process is often referred to as integrating out ormarginalized out the
unneeded variables.
Covariance
When dealing with multiple random variables, there is one additional summary statistic
which is helpful to know: the covariance . This measures the degree that two random vari-
able fluctuate together.
Suppose that we have two random variables ùëãandùëå, to begin with, let‚Äôs suppose they
are discrete, taking on values ¬πùë•ùëñ,ùë¶ùëó¬∫with probability ùëùùëñùëó. In this case, the covariance is
defined as
ùúéùëãùëå=Cov¬πùëã,ùëå¬∫=√ï
ùëñ,ùëó¬πùë•ùëñ ùúáùëã¬∫¬πùë¶ùëó ùúáùëå¬∫ùëùùëñùëó.=ùê∏¬ªùëãùëå¬º ùê∏¬ªùëã¬ºùê∏¬ªùëå¬º.(A.39)
To think about this intuitively: consider the following pair of random variables. Suppose
thatùëãtakes the values 1and3, andùëåtakes the values 1and3. Suppose that we have the
following probabilities
ùëÉ¬πùëã=1andùëå= 1¬∫=ùëù
2,
ùëÉ¬πùëã=1andùëå=3¬∫=1 ùëù
2,
ùëÉ¬πùëã=3andùëå= 1¬∫=1 ùëù
2,
ùëÉ¬πùëã=3andùëå=3¬∫=ùëù
2,(A.40)
whereùëùisaparameterin¬ª0,1¬ºwegettopick. Noticethatif ùëù=1thentheyarebothalways
theirminimumormaximumvaluessimultaneously,andif ùëù=0theyareguaranteedtotake
their flipped values simultaneously (one is large when the other is small and vice versa).
Ifùëù=1¬ù2, then the four possibilities are all equally likely, and neither should be related.
Let‚Äôs compute the covariance. First, note ùúáùëã=2andùúáùëå=1, so we may compute using
(A.39 ):
Cov¬πùëã,ùëå¬∫=√ï
ùëñ,ùëó¬πùë•ùëñ ùúáùëã¬∫¬πùë¶ùëó ùúáùëå¬∫ùëùùëñùëó
=¬π1 2¬∫¬π 1 1¬∫ùëù
2¬∏¬π1 2¬∫¬π3 1¬∫1 ùëù
2¬∏¬π3 2¬∫¬π 1 1¬∫1 ùëù
2¬∏¬π3 2¬∫¬π3 1¬∫ùëù
2
=4ùëù 2.
(A.41)
Whenùëù=1(thecasewheretheyarebothmaximallypositiveornegativeatthesametime)
has a covariance of 2. Whenùëù=0(the case where they are flipped) the covariance is  2.
Finally, when ùëù=1¬ù2(the case where they are unrelated), the covariance is 0. Thus we
see that the covariance measures how these two random variables are related.
A quick note on the covariance is that it only measures these linear relationships. More
complex relationships like ùëã=ùëå2whereùëåis randomly chosen from f 2, 1,0,1,2gwith
975 Random Variables
equalprobabilitycanbemissed. Indeedaquickcomputationshowsthattheserandomvari-
ables have covariance zero, despite one being a deterministic function of the other.
For continuous random variables, much the same story holds. At this point, we are pretty
comfortable with doing the transition between discrete and continuous, so we will provide
the continuous analogue of (A.39 )without any derivation.
ùúéùëãùëå=¬π
R2¬πùë• ùúáùëã¬∫¬πùë¶ ùúáùëå¬∫ùëù¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶. (A.42)
For visualization, let‚Äôs take a look at a collection of random variables with tunable covari-
ance.
# Plot a few random variables adjustable covariance
covs =[-0.9,0.0,1.2]
d2l.plt.figure(figsize =(12,3))
for iinrange (3):
X=torch .randn( 500)
Y=covs[i] *X+torch .randn( 500)
d2l.plt.subplot( 1,4, i+1)
d2l.plt.scatter(X .numpy(), Y .numpy())
d2l.plt.xlabel( 'X')
d2l.plt.ylabel( 'Y')
d2l.plt.title( f'cov = {covs[i] }')
d2l.plt.show()
Let‚Äôs see some properties of covariances:
For any random variable ùëã, Cov¬πùëã,ùëã¬∫=Var¬πùëã¬∫.
Foranyrandomvariables ùëã,ùëåandnumbers ùëéandùëè,Cov¬πùëéùëã¬∏ùëè,ùëå¬∫=Cov¬πùëã,ùëéùëå¬∏ùëè¬∫=
ùëéCov¬πùëã,ùëå¬∫.
Ifùëãandùëåare independent then Cov ¬πùëã,ùëå¬∫=0.
In addition, we can use the covariance to expand a relationship we saw before. Recall that
isùëãandùëåare two independent random variables then
Var¬πùëã¬∏ùëå¬∫=Var¬πùëã¬∫¬∏Var¬πùëå¬∫. (A.43)
976 Mathematics for Deep Learning
Withknowledgeofcovariances,wecanexpandthisrelationship. Indeed,somealgebracan
show that in general,
Var¬πùëã¬∏ùëå¬∫=Var¬πùëã¬∫¬∏Var¬πùëå¬∫¬∏2Cov¬πùëã,ùëå¬∫. (A.44)
Thisallowsustogeneralizethevariancesummationruleforcorrelatedrandomvariables.
Correlation
As we did in the case of means and variances, let‚Äôs now consider units. If ùëãis measured in
oneunit(sayinches),and ùëåismeasuredinanother(saydollars),thecovarianceismeasured
intheproductofthesetwounitsinches dollars. Theseunitscanbehardtointerpret. What
we will often want in this case is a unit-less measurement of relatedness. Indeed, often we
do not care about exact quantitative correlation, but rather ask if the correlation is in the
same direction, and how strong the relationship is.
To see what makes sense, let‚Äôs perform a thought experiment. Suppose that we convert
our random variables in inches and dollars to be in inches and cents. In this case the ran-
dom variable ùëåis multiplied by 100. If we work through the definition, this means that
Cov¬πùëã,ùëå¬∫willbemultipliedby 100. Thusweseethatinthiscaseachangeofunitschange
the covariance by a factor of 100. Thus, to find our unit-invariant measure of correlation,
we will need to divide by something else that also gets scaled by 100. Indeed we have a
clear candidate, the standard deviation! Indeed if we define the correlation coeÔ¨Äicient to
be
ùúå¬πùëã,ùëå¬∫=Cov¬πùëã,ùëå¬∫
ùúéùëãùúéùëå, (A.45)
we see that this is a unit-less value. A little mathematics can show that this number is
between 1and 1with 1meaning maximally positively correlated, whereas  1means
maximally negatively correlated.
Returning to our explicit discrete example above, we can see that ùúéùëã=1andùúéùëå=2,
so we can compute the correlation between the two random variables using (A.45 )to see
that
ùúå¬πùëã,ùëå¬∫=4ùëù 2
12=2ùëù 1. (A.46)
Thisnowrangesbetween  1and1withtheexpectedbehaviorof 1meaningmostcorrelated,
and 1meaning minimally correlated.
As another example, consider ùëãas any random variable, and ùëå=ùëéùëã¬∏ùëèas any linear
deterministic function of ùëã. Then, one can compute that
ùúéùëå=ùúéùëéùëã¬∏ùëè=jùëéjùúéùëã, (A.47)
Cov¬πùëã,ùëå¬∫=Cov¬πùëã,ùëéùëã¬∏ùëè¬∫=ùëéCov¬πùëã,ùëã¬∫=ùëéVar¬πùëã¬∫, (A.48)
and thus by (A.45 )that
ùúå¬πùëã,ùëå¬∫=ùëéVar¬πùëã¬∫
jùëéjùúé2
ùëã=ùëé
jùëéj=sign¬πùëé¬∫. (A.49)
977 Random Variables
Thus we see that the correlation is ¬∏1for anyùëé > 0, and 1for anyùëé < 0illustrating that
correlation measures the degree and directionality the two random variables are related,
not the scale that the variation takes.
Let‚Äôs again plot a collection of random variables with tunable correlation.
# Plot a few random variables adjustable correlations
cors =[-0.9,0.0,1.0]
d2l.plt.figure(figsize =(12,3))
for iinrange (3):
X=torch .randn( 500)
Y=cors[i] *X+torch .sqrt(torch .tensor( 1)-
cors[i] **2)*torch .randn( 500)
d2l.plt.subplot( 1,4, i +1)
d2l.plt.scatter(X .numpy(), Y .numpy())
d2l.plt.xlabel( 'X')
d2l.plt.ylabel( 'Y')
d2l.plt.title( f'cor = {cors[i] }')
d2l.plt.show()
Let‚Äôs list a few properties of the correlation below.
For any random variable ùëã,ùúå¬πùëã,ùëã¬∫=1.
For any random variables ùëã,ùëåand numbers ùëéandùëè,ùúå¬πùëéùëã¬∏ùëè,ùëå¬∫=ùúå¬πùëã,ùëéùëå¬∏ùëè¬∫=
ùúå¬πùëã,ùëå¬∫.
Ifùëãandùëåare independent with non-zero variance then ùúå¬πùëã,ùëå¬∫=0.
Asafinalnote, youmayfeellikesomeoftheseformulaearefamiliar. Indeed, ifweexpand
everything out assuming that ùúáùëã=ùúáùëå=0, we see that this is
ùúå¬πùëã,ùëå¬∫=√ç
ùëñ,ùëóùë•ùëñùë¶ùëñùëùùëñùëóq√ç
ùëñ,ùëóùë•2
ùëñùëùùëñùëóq√ç
ùëñ,ùëóùë¶2
ùëóùëùùëñùëó.(A.50)
This looks like a sum of a product of terms divided by the square root of sums of terms.
This is exactly the formula for the cosine of the angle between two vectors v,wwith the
978 Mathematics for Deep Learning
different coordinates weighted by ùëùùëñùëó:
cos¬πùúÉ¬∫=vw
kvkkwk=√ç
ùëñùë£ùëñùë§ùëñq√ç
ùëñùë£2
ùëñq√ç
ùëñùë§2
ùëñ.(A.51)
Indeed if we think of norms as being related to standard deviations, and correlations as
being cosines of angles, much of the intuition we have from geometry can be applied to
thinking about random variables.
A.6.2Summary
Continuous random variables are random variables that can take on a continuum of val-
ues. They have some technical difficulties that make them more challenging to work
with compared to discrete random variables.
The probability density function allows us to work with continuous random variables by
givingafunctionwheretheareaunderthecurveonsomeintervalgivestheprobability
of finding a sample point in that interval.
Thecumulativedistributionfunctionistheprobabilityofobservingtherandomvariable
to be less than a given threshold. It can provide a useful alternate viewpoint which
unifies discrete and continuous variables.
The mean is the average value of a random variable.
The variance is the expected square of the difference between the random variable and
its mean.
The standard deviation is the square root of the variance. It can be thought of as mea-
suring the range of values the random variable may take.
Chebyshev‚Äôs inequality allows us to make this intuition rigorous by giving an explicit
interval that contains the random variable most of the time.
Joint densities allow us to work with correlated random variables. We may marginalize
joint densities by integrating over unwanted random variables to get the distribution
of the desired random variable.
The covariance and correlation coefficient provide a way to measure any linear relation-
ship between two correlated random variables.
A.6.3Exercises
1.Suppose that we have the random variable with density given by ùëù¬πùë•¬∫=1
ùë•2forùë•1
andùëù¬πùë•¬∫=0otherwise. What is ùëÉ¬πùëã > 2¬∫?
2.The Laplace distribution is a random variable whose density is given by ùëù¬πùë•=1
2ùëí jùë•j.
Whatisthemeanandthestandarddeviationofthisfunction? Asahint,¬Ø1
0ùë•ùëí ùë•ùëëùë•=1
and¬Ø1
0ùë•2ùëí ùë•ùëëùë•=2.
979 Maximum Likelihood
2853.I walk up to you on the street and say ‚ÄúI have a random variable with mean 1, standard
deviation 2, and I observed 25%of my samples taking a value larger than 9.‚Äù Do you
believe me? Why or why not?
4.Supposethatyouhavetworandomvariables ùëã,ùëå,withjointdensitygivenby ùëùùëãùëå¬πùë•,ùë¶¬∫=
4ùë•ùë¶forùë•,ùë¶2¬ª0,1¬ºandùëùùëãùëå¬πùë•,ùë¶¬∫=0otherwise. What is the covariance of ùëãandùëå?
Discussions285.
A.7Maximum Likelihood
One of the most commonly encountered way of thinking in machine learning is the maxi-
mum likelihood point of view. This is the concept that when working with a probabilistic
model with unknown parameters, the parameters which make the data have the highest
probability are the most likely ones.
A.7.1The Maximum LikelihoodPrinciple
This has a Bayesian interpretation which can be helpful to think about. Suppose that we
have a model with parameters ùúΩand a collection of data examples ùëã. For concreteness, we
canimaginethat ùúΩisasinglevaluerepresentingtheprobabilitythatacoincomesupheads
when flipped, and ùëãis a sequence of independent coin flips. We will look at this example
in depth later.
If we want to find the most likely value for the parameters of our model, that means we
want to find
argmaxùëÉ¬πùúΩjùëã¬∫. (A.1)
By Bayes‚Äô rule, this is the same thing as
argmaxùëÉ¬πùëãjùúΩ¬∫ùëÉ¬πùúΩ¬∫
ùëÉ¬πùëã¬∫. (A.2)
The expression ùëÉ¬πùëã¬∫, a parameter agnostic probability of generating the data, does not
depend on ùúΩat all, and so can be dropped without changing the best choice of ùúΩ. Similarly,
we may now posit that we have no prior assumption on which set of parameters are better
than any others, so we may declare that ùëÉ¬πùúΩ¬∫does not depend on theta either! This, for
instance,makessenseinourcoinflippingexamplewheretheprobabilityitcomesupheads
could be any value in ¬ª0,1¬ºwithout any prior belief it is fair or not (often referred to as an
uninformative prior ). Thus we see that our application of Bayes‚Äô rule shows that our best
choice of ùúΩis the maximum likelihood estimate for ùúΩ:
ÀÜùúΩ=argmax
ùúΩùëÉ¬πùëãjùúΩ¬∫. (A.3)
Asamatterofcommonterminology,theprobabilityofthedatagiventheparameters( ùëÉ¬πùëãj
ùúΩ¬∫) is referred to as the likelihood .
980 Mathematics for Deep Learning
A ConcreteExample
Let‚Äôs see how this works in a concrete example. Suppose that we have a single parameter ùúÉ
representing the probability that a coin flip is heads. Then the probability of getting a tails
is1 ùúÉ, and so if our observed data ùëãis a sequence with ùëõùêªheads andùëõùëátails, we can
use the fact that independent probabilities multiply to see that
ùëÉ¬πùëãjùúÉ¬∫=ùúÉùëõùêª¬π1 ùúÉ¬∫ùëõùëá. (A.4)
If we flip 13coins and get the sequence ‚ÄúHHHTHTTHHHHHT‚Äù, which has ùëõùêª=9and
ùëõùëá=4, we see that this is
ùëÉ¬πùëãjùúÉ¬∫=ùúÉ9¬π1 ùúÉ¬∫4. (A.5)
One nice thing about this example will be that we know the answer going in. Indeed, if
we said verbally, ‚ÄúI flipped 13 coins, and 9 came up heads, what is our best guess for the
probabilitythatthecoincomesusheads?,‚Äùeveryonewouldcorrectlyguess 9¬ù13. Whatthis
maximum likelihood method will give us is a way to get that number from first principals
in a way that will generalize to vastly more complex situations.
For our example, the plot of ùëÉ¬πùëãjùúÉ¬∫is as follows:
%matplotlib inline
import torch
from d2l import torch asd2l
theta =torch .arange( 0,1,0.001 )
p=theta **9*(1-theta) **4.
d2l.plot(theta, p, 'theta ','likelihood ')
This has its maximum value somewhere near our expected 9¬ù130.7.... To see if it
is exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the
function is flat. Thus, we could find the maximum likelihood estimate (A.1)by finding
the values of ùúÉwhere the derivative is zero, and finding the one that gives the highest
981 Maximum Likelihood
probability. We compute:
0=ùëë
ùëëùúÉùëÉ¬πùëãjùúÉ¬∫
=ùëë
ùëëùúÉùúÉ9¬π1 ùúÉ¬∫4
=9ùúÉ8¬π1 ùúÉ¬∫4 4ùúÉ9¬π1 ùúÉ¬∫3
=ùúÉ8¬π1 ùúÉ¬∫3¬π9 13ùúÉ¬∫.(A.6)
This has three solutions: 0,1and9¬ù13. The first two are clearly minima, not maxima as
they assign probability 0to our sequence. The final value does notassign zero probability
to our sequence, and thus must be the maximum likelihood estimate ÀÜùúÉ=9¬ù13.
A.7.2NumericalOptimizationand the NegativeLog-Likelihood
The previous example is nice, but what if we have billions of parameters and data exam-
ples?
First, notice that if we make the assumption that all the data examples are independent, we
can no longer practically consider the likelihood itself as it is a product of many probabili-
ties. Indeed,eachprobabilityisin ¬ª0,1¬º,saytypicallyofvalueabout 1¬ù2,andtheproductof
¬π1¬ù2¬∫1000000000is far below machine precision. We cannot work with that directly.
However, recall that the logarithm turns products to sums, in which case
log¬π¬π1¬ù2¬∫1000000000¬∫=1000000000log¬π1¬ù2¬∫  301029995.6... (A.7)
This number fits perfectly within even a single precision 32-bit float. Thus, we should
consider the log-likelihood , which is
log¬πùëÉ¬πùëãjùúΩ¬∫¬∫. (A.8)
Since the function ùë•7!log¬πùë•¬∫is increasing, maximizing the likelihood is the same thing
asmaximizingthelog-likelihood. Indeedin SectionA.9 wewillseethisreasoningapplied
when working with the specific example of the naive Bayes classifier.
We often work with loss functions, where we wish to minimize the loss. We may turn
maximum likelihood into the minimization of a loss by taking  log¬πùëÉ¬πùëãjùúΩ¬∫¬∫, which is
thenegativelog-likelihood .
To illustrate this, consider the coin flipping problem from before, and pretend that we do
not know the closed form solution. We may compute that
 log¬πùëÉ¬πùëãjùúΩ¬∫¬∫= log¬πùúÉùëõùêª¬π1 ùúÉ¬∫ùëõùëá¬∫= ¬πùëõùêªlog¬πùúÉ¬∫¬∏ùëõùëálog¬π1 ùúÉ¬∫¬∫. (A.9)
This can be written into code, and freely optimized even for billions of coin flips.
# Set up our data
n_H =8675309
n_T =256245
(continues on next page)
982 Mathematics for Deep Learning
(continued from previous page)
# Initialize our paramteres
theta =torch .tensor( 0.5, requires_grad =True )
# Perform gradient descent
lr=1e-9
for iter inrange (100):
loss =-(n_H *torch .log(theta) +n_T *torch .log( 1-theta))
loss .backward()
with torch .no_grad():
theta -=lr*theta .grad
theta .grad .zero_()
# Check output
theta, n_H /(n_H +n_T)
(tensor( 0.9713 , requires_grad =True ),0.9713101437890875 )
Numericalconvenienceisnottheonlyreasonwhypeopleliketousenegativelog-likelihoods.
There are several other reasons why it is preferable.
The second reason we consider the log-likelihood is the simplified application of calcu-
lus rules. As discussed above, due to independence assumptions, most probabilities we
encounter in machine learning are products of individual probabilities.
ùëÉ¬πùëãjùúΩ¬∫=ùëù¬πùë•1jùúΩ¬∫ùëù¬πùë•2jùúΩ¬∫ùëù¬πùë•ùëõjùúΩ¬∫. (A.10)
Thismeansthatifwedirectlyapplytheproductruletocomputeaderivativeweget
ùúï
ùúïùúΩùëÉ¬πùëãjùúΩ¬∫=ùúï
ùúïùúΩùëÉ¬πùë•1jùúΩ¬∫
ùëÉ¬πùë•2jùúΩ¬∫ùëÉ¬πùë•ùëõjùúΩ¬∫
¬∏ùëÉ¬πùë•1jùúΩ¬∫ùúï
ùúïùúΩùëÉ¬πùë•2jùúΩ¬∫
ùëÉ¬πùë•ùëõjùúΩ¬∫
...
¬∏ùëÉ¬πùë•1jùúΩ¬∫ùëÉ¬πùë•2jùúΩ¬∫ùúï
ùúïùúΩùëÉ¬πùë•ùëõjùúΩ¬∫
.(A.11)
This requires ùëõ¬πùëõ 1¬∫multiplications, along with ¬πùëõ 1¬∫additions, so it is proportional
to quadratic time in the inputs! Sufficient cleverness in grouping terms will reduce this
to linear time, but it requires some thought. For the negative log-likelihood we have in-
stead
 log¬πùëÉ¬πùëãjùúΩ¬∫¬∫= log¬πùëÉ¬πùë•1jùúΩ¬∫¬∫  log¬πùëÉ¬πùë•2jùúΩ¬∫¬∫  log¬πùëÉ¬πùë•ùëõjùúΩ¬∫¬∫,(A.12)
which then gives
 ùúï
ùúïùúΩlog¬πùëÉ¬πùëãjùúΩ¬∫¬∫=1
ùëÉ¬πùë•1jùúΩ¬∫ùúï
ùúïùúΩùëÉ¬πùë•1jùúΩ¬∫
¬∏¬∏1
ùëÉ¬πùë•ùëõjùúΩ¬∫ùúï
ùúïùúΩùëÉ¬πùë•ùëõjùúΩ¬∫
.
(A.13)
983 Maximum Likelihood
This requires only ùëõdivides andùëõ 1sums, and thus is linear time in the inputs.
The third and final reason to consider the negative log-likelihood is the relationship to
information theory, which we will discuss in detail in Section A.11 . This is a rigorous
mathematicaltheorywhichgivesawaytomeasurethedegreeofinformationorrandomness
in a random variable. The key object of study in that field is the entropy which is
ùêª¬πùëù¬∫= √ï
ùëñùëùùëñlog2¬πùëùùëñ¬∫,(A.14)
which measures the randomness of a source. Notice that this is nothing more than the av-
erage logprobability, and thus if we take our negative log-likelihood and divide by the
numberofdataexamples,wegetarelativeofentropyknownascross-entropy. Thistheoret-
icalinterpretationalonewouldbesufficientlycompellingtomotivatereportingtheaverage
negative log-likelihood over the dataset as a way of measuring model performance.
A.7.3MaximumLikelihoodforContinuous Variables
Everything that we have done so far assumes we are working with discrete random vari-
ables, but what if we want to work with continuous ones?
The short summary is that nothing at all changes, except we replace all the instances of the
probability with the probability density. Recalling that we write densities with lower case
ùëù, this means that for example we now say
 log¬πùëù¬πùëãjùúΩ¬∫¬∫= log¬πùëù¬πùë•1jùúΩ¬∫¬∫  log¬πùëù¬πùë•2jùúΩ¬∫¬∫  log¬πùëù¬πùë•ùëõjùúΩ¬∫¬∫= √ï
ùëñlog¬πùëù¬πùë•ùëñjùúÉ¬∫¬∫.
(A.15)
Thequestionbecomes,‚ÄúWhyisthisOK?‚ÄùAfterall,thereasonweintroduceddensitieswas
because probabilities of getting specific outcomes themselves was zero, and thus is not the
probability of generating our data for any set of parameters zero?
Indeed, this is the case, and understanding why we can shift to densities is an exercise in
tracing what happens to the epsilons.
Let‚Äôs first re-define our goal. Suppose that for continuous random variables we no longer
want to compute the probability of getting exactly the right value, but instead matching to
withinsomerange ùúñ. Forsimplicity,weassumeourdataisrepeatedobservations ùë•1,...,ùë•ùëÅ
of identically distributed random variables ùëã1,...,ùëãùëÅ. As we have seen previously, this
can be written as
ùëÉ¬πùëã12¬ªùë•1,ùë•1¬∏ùúñ¬º,ùëã22¬ªùë•2,ùë•2¬∏ùúñ¬º,...,ùëãùëÅ2¬ªùë•ùëÅ,ùë•ùëÅ¬∏ùúñ¬ºjùúΩ¬∫
ùúñùëÅùëù¬πùë•1jùúΩ¬∫ùëù¬πùë•2jùúΩ¬∫ùëù¬πùë•ùëõjùúΩ¬∫.(A.16)
Thus, if we take negative logarithms of this we obtain
 log¬πùëÉ¬πùëã12¬ªùë•1,ùë•1¬∏ùúñ¬º,ùëã22¬ªùë•2,ùë•2¬∏ùúñ¬º,...,ùëãùëÅ2¬ªùë•ùëÅ,ùë•ùëÅ¬∏ùúñ¬ºjùúΩ¬∫¬∫
 ùëÅlog¬πùúñ¬∫ √ï
ùëñlog¬πùëù¬πùë•ùëñjùúΩ¬∫¬∫.(A.17)
If we examine this expression, the only place that the ùúñoccurs is in the additive constant
984 Mathematics for Deep Learning
286 ùëÅlog¬πùúñ¬∫. Thisdoesnotdependontheparameters ùúΩatall,sotheoptimalchoiceof ùúΩdoes
not depend on our choice of ùúñ! If we demand four digits or four-hundred, the best choice
ofùúΩremains the same, thus we may freely drop the epsilon to see that what we want to
optimize is
 √ï
ùëñlog¬πùëù¬πùë•ùëñjùúΩ¬∫¬∫.(A.18)
Thus, we see that the maximum likelihood point of view can operate with continuous ran-
domvariablesaseasilyaswithdiscreteonesbyreplacingtheprobabilitieswithprobability
densities.
A.7.4Summary
The maximum likelihood principle tells us that the best fit model for a given dataset is
the one that generates the data with the highest probability.
Often people work with the negative log-likelihood instead for a variety of reasons: nu-
merical stability, conversion of products to sums (and the resulting simplification of
gradient computations), and theoretical ties to information theory.
While simplest to motivate in the discrete setting, it may be freely generalized to the
continuous setting as well by maximizing the probability density assigned to the dat-
apoints.
A.7.5Exercises
1.Supposethatyouknowthatanon-negativerandomvariablehasdensity ùõºùëí ùõºùë•forsome
valueùõº > 0. You obtain a single observation from the random variable which is the
number 3. What is the maximum likelihood estimate for ùõº?
2.Supposethatyouhaveadatasetofsamples fùë•ùëñgùëÅ
ùëñ=1drawnfromaGaussianwithunknown
mean, but variance 1. What is the maximum likelihood estimate for the mean?
Discussions286.
A.8Distributions
Nowthat wehavelearned howto workwith probability in both the discrete and the contin-
uous setting, let‚Äôs get to know some of the common distributions encountered. Depending
on the area of machine learning, we may need to be familiar with vastly more of these, or
for some areas of deep learning potentially none at all. This is, however, a good basic list
to be familiar with. Let‚Äôs first import some common libraries.
985 Distributions
%matplotlib inline
from math import erf, factorial
import torch
from IPython import display
from d2l import torch asd2l
torch .pi=torch .acos(torch .zeros( 1))*2# Define pi in torch
A.8.1Bernoulli
This is the simplest random variable usually encountered. This random variable encodes a
coin flip which comes up 1with probability ùëùand0with probability 1 ùëù. If we have a
random variable ùëãwith this distribution, we will write
ùëãBernoulli¬πùëù¬∫. (A.1)
The cumulative distribution function is
ùêπ¬πùë•¬∫=8>>> <
>>>:0ùë• <0,
1 ùëù0ùë• <1,
1ùë• >=1.(A.2)
The probability mass function is plotted below.
p=0.3
d2l.set_figsize()
d2l.plt.stem([ 0,1], [ 1-p, p], use_line_collection =True )
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.show()
Now, let‚Äôs plot the cumulative distribution function (A.2).
x=torch .arange( -1,2,0.01 )
def F(x):
return 0ifx<0else 1ifx>1else 1-p
(continues on next page)
986 Mathematics for Deep Learning
(continued from previous page)
d2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')
IfùëãBernoulli¬πùëù¬∫, then:
ùúáùëã=ùëù,
ùúé2
ùëã=ùëù¬π1 ùëù¬∫.
WecansampleanarrayofarbitraryshapefromaBernoullirandomvariableasfollows.
1*(torch .rand( 10,10)<p)
tensor([[ 0,1,0,0,1,0,0,0,0,0],
[0,1,0,0,0,0,1,0,0,0],
[0,1,0,0,1,0,0,0,0,1],
[1,0,0,0,0,0,0,0,0,0],
[0,0,0,1,0,0,1,0,0,1],
[0,0,0,0,0,0,1,1,0,0],
[1,1,0,0,1,1,1,1,1,0],
[1,0,0,0,1,0,1,1,0,0],
[0,0,0,0,1,0,0,0,0,0],
[1,0,1,1,1,1,0,1,0,0]])
A.8.2DiscreteUniform
Thenextcommonlyencounteredrandomvariableisadiscreteuniform. Forourdiscussion
here, we will assume that it is supported on the integers f1,2,...,ùëõg, however any other
set of values can be freely chosen. The meaning of the word uniform in this context is that
every possible value is equally likely. The probability for each value ùëñ2f1,2,3,...,ùëõgis
ùëùùëñ=1
ùëõ. We will denote a random variable ùëãwith this distribution as
ùëãùëà¬πùëõ¬∫. (A.3)
987 Distributions
The cumulative distribution function is
ùêπ¬πùë•¬∫=8>>> <
>>>:0ùë• <1,
ùëò
ùëõùëòùë• < ùëò¬∏1with 1ùëò <ùëõ,
1ùë• >=ùëõ.(A.4)
Let‚Äôs first plot the probability mass function.
n=5
d2l.plt.stem([i +1for iinrange (n)], n *[1/n], use_line_collection =True )
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.show()
Now, let‚Äôs plot the cumulative distribution function (A.4).
x=torch .arange( -1,6,0.01 )
def F(x):
return 0ifx<1else 1ifx>nelse torch .floor(x) /n
d2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')
Ifùëãùëà¬πùëõ¬∫, then:
ùúáùëã=1¬∏ùëõ
2,
ùúé2
ùëã=ùëõ2 1
12.
988 Mathematics for Deep Learning
We can sample an array of arbitrary shape from a discrete uniform random variable as
follows.
torch .randint( 1, n, size =(10,10))
tensor([[ 1,4,3,2,1,1,3,1,1,4],
[4,1,1,4,4,1,4,3,2,4],
[2,4,4,1,4,2,4,3,2,1],
[1,2,3,1,1,4,2,4,1,3],
[1,2,4,1,4,3,3,2,2,1],
[1,2,2,4,1,3,2,4,2,3],
[1,2,3,4,1,3,4,1,4,3],
[3,1,1,4,4,1,3,1,1,2],
[2,2,4,3,4,2,3,4,2,4],
[1,4,3,3,2,3,3,4,1,3]])
A.8.3Continuous Uniform
Next, let‚Äôs discuss the continuous uniform distribution. The idea behind this random vari-
able is that if we increase the ùëõin the discrete uniform distribution, and then scale it to fit
within the interval ¬ªùëé,ùëè¬º, we will approach a continuous random variable that just picks
an arbitrary value in ¬ªùëé,ùëè¬ºall with equal probability. We will denote this distribution
as
ùëãùëà¬πùëé,ùëè¬∫. (A.5)
The probability density function is
ùëù¬πùë•¬∫=(
1
ùëè ùëéùë•2¬ªùëé,ùëè¬º,
0ùë•‚àâ¬ªùëé,ùëè¬º.(A.6)
The cumulative distribution function is
ùêπ¬πùë•¬∫=8>>> <
>>>:0ùë• <ùëé,
ùë• ùëé
ùëè ùëéùë•2¬ªùëé,ùëè¬º,
1ùë• >=ùëè.(A.7)
Let‚Äôs first plot the probability density function (A.6).
a, b =1,3
x=torch .arange( 0,4,0.01 )
p=(x>a).type(torch .float32) *(x<b).type(torch .float32) /(b-a)
d2l.plot(x, p, 'x','p.d.f. ')
Now, let‚Äôs plot the cumulative distribution function (A.7).
989 Distributions
def F(x):
return 0ifx<aelse 1ifx>belse (x-a)/(b-a)
d2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')
Ifùëãùëà¬πùëé,ùëè¬∫, then:
ùúáùëã=ùëé¬∏ùëè
2,
ùúé2
ùëã=¬πùëè ùëé¬∫2
12.
Wecansampleanarrayofarbitraryshapefromauniformrandomvariableasfollows. Note
that it by default samples from a ùëà¬π0,1¬∫, so if we want a different range we need to scale
it.
(b-a)*torch .rand( 10,10)+a
tensor([[ 2.4857 ,2.2461 ,1.6809 ,2.7434 ,2.7072 ,2.6190 ,1.4883 ,1.2517 ,1.
‚Ü©!3454 ,
2.4754 ],
[1.0974 ,1.5680 ,1.8788 ,2.8231 ,2.1695 ,2.6461 ,1.4914 ,1.4887 ,1.
‚Ü©!3860 ,
1.9090 ],
[1.3746 ,1.7773 ,1.2412 ,1.1950 ,2.7281 ,2.8356 ,1.2266 ,2.4724 ,2.
‚Ü©!4641 ,
2.8991 ],
[2.4018 ,2.6727 ,1.0308 ,1.1951 ,1.9390 ,1.6486 ,2.8314 ,1.1025 ,1.
(continues on next page)
990 Mathematics for Deep Learning
(continued from previous page)
‚Ü©!3354 ,
1.0130 ],
[1.1281 ,1.8000 ,2.3788 ,2.6580 ,1.6750 ,2.2081 ,1.2705 ,1.0757 ,2.
‚Ü©!3311 ,
2.6557 ],
[2.9912 ,1.2263 ,1.8115 ,1.5940 ,1.9321 ,1.6469 ,2.2990 ,2.1473 ,1.
‚Ü©!8165 ,
1.2806 ],
[1.1672 ,1.1536 ,1.9649 ,2.1655 ,1.7170 ,1.0284 ,1.3305 ,2.1904 ,1.
‚Ü©!4036 ,
2.1958 ],
[2.5891 ,2.5840 ,2.2679 ,2.0687 ,2.9249 ,1.6741 ,1.2238 ,2.4463 ,2.
‚Ü©!2235 ,
2.7038 ],
[1.8697 ,2.4965 ,1.5785 ,2.7890 ,2.3319 ,2.1434 ,2.3333 ,1.0286 ,1.
‚Ü©!9245 ,
1.7640 ],
[1.2504 ,1.7558 ,1.4322 ,1.5226 ,1.3380 ,1.1388 ,1.8707 ,2.2330 ,2.
‚Ü©!3818 ,
2.2087 ]])
A.8.4Binomial
Let‚Äôs make things a little more complex and examine the binomial random variable. This
randomvariableoriginatesfromperformingasequenceof ùëõindependentexperiments,each
of which has probability ùëùof succeeding, and asking how many successes we expect to
see.
Let‚Äôs express this mathematically. Each experiment is an independent random variable ùëãùëñ
wherewewilluse 1toencodesuccess,and 0toencodefailure. Sinceeachisanindependent
coin flip which is successful with probability ùëù, we can say that ùëãùëñBernoulli¬πùëù¬∫. Then,
the binomial random variable is
ùëã=ùëõ√ï
ùëñ=1ùëãùëñ. (A.8)
In this case, we will write
ùëãBinomial¬πùëõ,ùëù¬∫. (A.9)
To get the cumulative distribution function, we need to notice that getting exactly ùëòsuc-
cesses can occur in ùëõ
ùëò=ùëõ!
ùëò!¬πùëõ ùëò¬∫!ways each of which has a probability of ùëùùëò¬π1 ùëù¬∫ùëõ ùëò
of occurring. Thus the cumulative distribution function is
ùêπ¬πùë•¬∫=8>>> <
>>>:0 ùë• <0,
√ç
ùëöùëò ùëõ
ùëöùëùùëö¬π1 ùëù¬∫ùëõ ùëöùëòùë• < ùëò¬∏1with 0ùëò <ùëõ,
1 ùë• >=ùëõ.(A.10)
Let‚Äôs first plot the probability mass function.
991 Distributions
n, p =10,0.2
# Compute binomial coefficient
def binom (n, k):
comb =1
for iinrange (min(k, n -k)):
comb =comb *(n-i)//(i+1)
return comb
pmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i) for iinrange (n+1)])
d2l.plt.stem([i for iinrange (n+1)], pmf, use_line_collection =True )
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.show()
Now, let‚Äôs plot the cumulative distribution function (A.10 ).
x=torch .arange( -1,11,0.01 )
cmf =torch .cumsum(pmf, dim =0)
def F(x):
return 0ifx<0else 1ifx>nelse cmf[ int(x)]
d2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')
IfùëãBinomial¬πùëõ,ùëù¬∫, then:
ùúáùëã=ùëõùëù,
992 Mathematics for Deep Learning
ùúé2
ùëã=ùëõùëù¬π1 ùëù¬∫.
This follows from the linearity of expected value over the sum of ùëõBernoulli random vari-
ables, and the fact that the variance of the sum of independent random variables is the sum
of the variances. This can be sampled as follows.
m=torch .distributions .binomial .Binomial(n, p)
m.sample(sample_shape =(10,10))
tensor([[ 6.,3.,4.,3.,3.,1.,3.,3.,3.,3.],
[3.,1.,2.,2.,3.,2.,1.,3.,1.,4.],
[6.,1.,0.,3.,0.,3.,1.,0.,1.,1.],
[1.,2.,3.,1.,2.,2.,2.,2.,3.,2.],
[2.,2.,5.,4.,1.,3.,4.,3.,2.,0.],
[2.,0.,2.,2.,3.,1.,1.,4.,3.,1.],
[1.,1.,3.,2.,4.,2.,2.,2.,1.,0.],
[0.,3.,2.,1.,1.,3.,2.,1.,1.,3.],
[2.,3.,2.,3.,4.,3.,1.,2.,1.,2.],
[1.,2.,1.,1.,3.,2.,4.,3.,3.,2.]])
A.8.5Poisson
Let‚Äôs now perform a thought experiment. We are standing at a bus stop and we want to
know how many buses will arrive in the next minute. Let‚Äôs start by considering ùëã¬π1¬∫
Bernoulli¬πùëù¬∫which is simply the probability that a bus arrives in the one minute window.
Forbusstopsfarfromanurbancenter, thismightbeaprettygoodapproximation. Wemay
never see more than one bus in a minute.
However, if we are in a busy area, it is possible or even likely that two buses will arrive.
We can model this by splitting our random variable into two parts for the first 30 seconds,
or the second 30 seconds. In this case we can write
ùëã¬π2¬∫ùëã¬π2¬∫
1¬∏ùëã¬π2¬∫
2, (A.11)
whereùëã¬π2¬∫is the total sum, and ùëã¬π2¬∫
ùëñBernoulli¬πùëù¬ù2¬∫. The total distribution is then
ùëã¬π2¬∫Binomial¬π2,ùëù¬ù2¬∫.
Why stop here? Let‚Äôs continue to split that minute into ùëõparts. By the same reasoning as
above, we see that
ùëã¬πùëõ¬∫Binomial¬πùëõ,ùëù¬ùùëõ¬∫. (A.12)
Consider these random variables. By the previous section, we know that (A.12 )has mean
ùúáùëã¬πùëõ¬∫=ùëõ¬πùëù¬ùùëõ¬∫=ùëù, and variance ùúé2
ùëã¬πùëõ¬∫=ùëõ¬πùëù¬ùùëõ¬∫¬π1 ¬πùëù¬ùùëõ¬∫¬∫=ùëù¬π1 ùëù¬ùùëõ¬∫. If we take
ùëõ!1,wecanseethatthesenumbersstabilizeto ùúáùëã¬π1¬∫=ùëù,andvariance ùúé2
ùëã¬π1¬∫=ùëù. This
indicatesthatthere couldbe somerandomvariablewecandefineinthisinfinitesubdivision
limit.
This should not come as too much of a surprise, since in the real world we can just count
993 Distributions
the number of bus arrivals, however it is nice to see that our mathematical model is well
defined. This discussion can be made formal as the lawof rareevents .
Following through this reasoning carefully, we can arrive at the following model. We will
say thatùëãPoisson¬πùúÜ¬∫if it is a random variable which takes the values f0,1,2,...gwith
probability
ùëùùëò=ùúÜùëòùëí ùúÜ
ùëò!. (A.13)
The valueùúÜ > 0is known as the rate(or theshapeparameter), and denotes the average
number of arrivals we expect in one unit of time.
Wemaysumthisprobabilitymassfunctiontogetthecumulativedistributionfunction.
ùêπ¬πùë•¬∫=(
0 ùë• <0,
ùëí ùúÜ√çùëò
ùëö=0ùúÜùëö
ùëö!ùëòùë• < ùëò¬∏1with 0ùëò.(A.14)
Let‚Äôs first plot the probability mass function (A.13 ).
lam =5.0
xs=[ifor iinrange (20)]
pmf =torch .tensor([torch .exp(torch .tensor( -lam)) *lam**k
/factorial(k) for kinxs])
d2l.plt.stem(xs, pmf, use_line_collection =True )
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.show()
Now, let‚Äôs plot the cumulative distribution function (A.14 ).
x=torch .arange( -1,21,0.01 )
cmf =torch .cumsum(pmf, dim =0)
def F(x):
return 0ifx<0else 1ifx>nelse cmf[ int(x)]
d2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')
As we saw above, the means and variances are particularly concise. If ùëãPoisson¬πùúÜ¬∫,
then:
994 Mathematics for Deep Learning
ùúáùëã=ùúÜ,
ùúé2
ùëã=ùúÜ.
This can be sampled as follows.
m=torch .distributions .poisson .Poisson(lam)
m.sample(( 10,10))
tensor([[ 1.,4.,6.,8.,4.,4.,4.,7.,6.,4.],
[3.,6.,7.,7.,5.,7.,7.,3.,5.,4.],
[4.,1.,3.,3.,10.,5.,5.,3.,7.,5.],
[4.,3.,4.,10.,8.,6.,4.,6.,5.,5.],
[5.,11.,1.,5.,7.,5.,2.,4.,3.,5.],
[6.,6.,4.,4.,3.,1.,5.,8.,4.,5.],
[2.,9.,7.,2.,6.,5.,2.,8.,6.,10.],
[1.,4.,3.,7.,3.,1.,7.,5.,3.,6.],
[5.,4.,6.,4.,9.,8.,3.,3.,1.,8.],
[3.,12.,9.,13.,2.,14.,3.,2.,0.,3.]])
A.8.6Gaussian
Now Let‚Äôs try a different, but related experiment. Let‚Äôs say we again are performing ùëõ
independentBernoulli ¬πùëù¬∫measurements ùëãùëñ. Thedistributionofthesumoftheseis ùëã¬πùëõ¬∫
Binomial¬πùëõ,ùëù¬∫. Rather than taking a limit as ùëõincreases and ùëùdecreases, Let‚Äôs fix ùëù, and
then sendùëõ!1. In this case ùúáùëã¬πùëõ¬∫=ùëõùëù!1andùúé2
ùëã¬πùëõ¬∫=ùëõùëù¬π1 ùëù¬∫!1, so there is
no reason to think this limit should be well defined.
However, not all hope is lost! Let‚Äôs just make the mean and variance be well behaved by
defining
ùëå¬πùëõ¬∫=ùëã¬πùëõ¬∫ ùúáùëã¬πùëõ¬∫
ùúéùëã¬πùëõ¬∫. (A.15)
This can be seen to have mean zero and variance one, and so it is plausible to believe that
it will converge to some limiting distribution. If we plot what these distributions look like,
we will become even more convinced that it will work.
995 Distributions
p=0.2
ns=[1,10,100,1000 ]
d2l.plt.figure(figsize =(10,3))
for iinrange (4):
n=ns[i]
pmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i)
for iinrange (n+1)])
d2l.plt.subplot( 1,4, i +1)
d2l.plt.stem([(i -n*p)/torch .sqrt(torch .tensor(n *p*(1-p)))
for iinrange (n+1)], pmf,
use_line_collection =True )
d2l.plt.xlim([ -4,4])
d2l.plt.xlabel( 'x')
d2l.plt.ylabel( 'p.m.f. ')
d2l.plt.title( "n = {}".format(n))
d2l.plt.show()
One thing to note: compared to the Poisson case, we are now dividing by the standard de-
viation which means that we are squeezing the possible outcomes into smaller and smaller
areas. This is an indication that our limit will no longer be discrete, but rather continu-
ous.
A derivation of what occurs is beyond the scope of this document, but the central limit
theorem states that as ùëõ!1, this will yield the Gaussian Distribution (or sometimes
normal distribution). More explicitly, for any ùëé,ùëè:
lim
ùëõ!1ùëÉ¬πùëå¬πùëõ¬∫2¬ªùëé,ùëè¬º¬∫=ùëÉ¬πN¬π0,1¬∫2¬ªùëé,ùëè¬º¬∫, (A.16)
where we say a random variable is normally distributed with given mean ùúáand variance
ùúé2, writtenùëãN¬πùúá,ùúé2¬∫ifùëãhas density
ùëùùëã¬πùë•¬∫=1p
2ùúãùúé2ùëí ¬πùë• ùúá¬∫2
2ùúé2. (A.17)
Let‚Äôs first plot the probability density function (A.17 ).
mu, sigma =0,1
(continues on next page)
996 Mathematics for Deep Learning
(continued from previous page)
x=torch .arange( -3,3,0.01 )
p=1/torch .sqrt( 2*torch .pi*sigma **2)*torch .exp(
-(x-mu)**2/(2*sigma **2))
d2l.plot(x, p, 'x','p.d.f. ')
Now, let‚Äôs plot the cumulative distribution function. It is beyond the scope of this ap-
pendix, but the Gaussian c.d.f. does not have a closed-form formula in terms of more
elementary functions. We will use erfwhich provides a way to compute this integral nu-
merically.
def phi(x):
return (1.0 +erf((x -mu) /(sigma *torch .sqrt(torch .tensor( 2.))))) /2.0
d2l.plot(x, torch .tensor([phi(y) for yinx.tolist()]), 'x','c.d.f. ')
Keen-eyedreaderswillrecognizesomeoftheseterms. Indeed,weencounteredthisintegral
inSection A.5 . Indeed we need exactly that computation to see that this ùëùùëã¬πùë•¬∫has total
area one and is thus a valid density.
Our choice of working with coin flips made computations shorter, but nothing about that
choice was fundamental. Indeed, if we take any collection of independent identically dis-
tributed random variables ùëãùëñ, and form
ùëã¬πùëÅ¬∫=ùëÅ√ï
ùëñ=1ùëãùëñ. (A.18)
997 Distributions
Then
ùëã¬πùëÅ¬∫ ùúáùëã¬πùëÅ¬∫
ùúéùëã¬πùëÅ¬∫(A.19)
willbeapproximatelyGaussian. Thereareadditionalrequirementsneededtomakeitwork,
most commonly ùê∏¬ªùëã4¬º<1, but the philosophy is clear.
The central limit theorem is the reason why the Gaussian is fundamental to probability,
statistics, and machine learning. Whenever we can say that something we measured is a
sumofmanysmallindependentcontributions,wecanassumethatthethingbeingmeasured
will be close to Gaussian.
TherearemanymorefascinatingpropertiesofGaussians,andwewouldliketodiscussone
more here. The Gaussian is what is known as a maximum entropy distribution . We will
get into entropy more deeply in Section A.11 , however all we need to know at this point
is that it is a measure of randomness. In a rigorous mathematical sense, we can think of
the Gaussian as the mostrandom choice of random variable with fixed mean and variance.
Thus, if we know that our random variable has some mean and variance, the Gaussian is in
a sense the most conservative choice of distribution we can make.
To close the section, let‚Äôs recall that if ùëãN¬πùúá,ùúé2¬∫, then:
ùúáùëã=ùúá,
ùúé2
ùëã=ùúé2.
WecansamplefromtheGaussian(orstandardnormal)distributionasshownbelow.
torch .normal(mu, sigma, size =(10,10))
tensor([[ 1.3588 ,0.0473 ,-1.5805 ,-0.0108 ,0.4253 ,0.7924 ,-0.6547 ,0.
‚Ü©!7313 ,
-0.3038 ,1.1935 ],
[0.0089 ,0.8951 ,1.0055 ,0.0956 ,-1.1109 ,-0.6342 ,1.6772 ,1.
‚Ü©!0314 ,
0.3819 ,-1.7822 ],
[-0.0604 ,-1.0318 ,0.9113 ,1.3118 ,-1.8370 ,-0.9023 ,1.0365 ,0.
‚Ü©!9052 ,
-0.6411 ,-0.8949 ],
[-0.1713 ,-0.2347 ,0.0767 ,-0.6375 ,-0.4612 ,-1.6875 ,-0.1570 ,1.
‚Ü©!0591 ,
0.8377 ,0.5097 ],
[0.2762 ,-0.6213 ,-0.3422 ,0.9449 ,-0.7544 ,-0.2150 ,1.0240 ,1.
‚Ü©!0253 ,
-0.9182 ,1.1536 ],
[0.0614 ,0.2758 ,-0.3610 ,-1.0577 ,-0.5513 ,-0.9158 ,0.7539 ,0.
‚Ü©!9204 ,
-0.5908 ,0.9113 ],
[1.6190 ,-0.9213 ,-0.7944 ,-2.2621 ,0.5826 ,-1.8287 ,1.4097 ,-0.
‚Ü©!5744 ,
-0.0668 ,1.2074 ],
[-0.0624 ,0.1928 ,1.3002 ,0.6756 ,1.1590 ,1.0144 ,1.1840 ,-0.
(continues on next page)
998 Mathematics for Deep Learning
(continued from previous page)
‚Ü©!5010 ,
0.6026 ,-0.7722 ],
[-2.0148 ,0.6958 ,0.9940 ,0.8477 ,1.0957 ,-0.5253 ,0.2353 ,-0.
‚Ü©!2663 ,
1.2275 ,0.5993 ],
[0.4651 ,-0.8218 ,-0.5441 ,-2.0338 ,-0.6930 ,-0.0674 ,-0.4448 ,-0.
‚Ü©!8397 ,
0.0360 ,-0.7089 ]])
A.8.7ExponentialFamily
One shared property for all the distributions listed above is that they all belong to which
is known as the exponential family . The exponential family is a set of distributions whose
density can be expressed in the following form:
ùëù¬πxjùúº¬∫=‚Ñé¬πx¬∫exp ùúº>ùëá¬πx¬∫ ùê¥¬πùúº¬∫(A.20)
As this definition can be a little subtle, let‚Äôs examine it closely.
First,‚Ñé¬πx¬∫is known as the underlying measure or thebase measure . This can be viewed
as an original choice of measure we are modifying with our exponential weight.
Second, we have the vector ùúº=¬πùúÇ1,ùúÇ2,...,ùúÇùëô¬∫ 2Rùëôcalled the natural parameters or
canonical parameters . These define how the base measure will be modified. The natural
parametersenterintothenewmeasurebytakingthedotproductoftheseparametersagainst
some function ùëá¬π¬∫ofx=¬πùë•1,ùë•2,...,ùë•ùëõ¬∫ 2Rùëõand exponentiated. The vector ùëá¬πx¬∫=
¬πùëá1¬πx¬∫,ùëá2¬πx¬∫,...,ùëáùëô¬πx¬∫¬∫is called the suÔ¨Äicientstatistics forùúº. This name is used since the
information represented by ùëá¬πx¬∫is sufficient to calculate the probability density and no
other information from the sample x‚Äôs are required.
Third, we have ùê¥¬πùúº¬∫, which is referred to as the cumulantfunction , which ensures that the
above distribution (A.20 )integrates to one, i.e.,
ùê¥¬πùúº¬∫=log¬π
‚Ñé¬πx¬∫exp ùúº>ùëá¬πx¬∫ùëëx
. (A.21)
To be concrete, let‚Äôs consider the Gaussian. Assuming that xis an univariate variable, we
saw that it had a density of
ùëù¬πùë•jùúá,ùúé¬∫=1p
2ùúãùúé2exp ¬πùë• ùúá¬∫2
2ùúé2
=1p
2ùúãexpùúá
ùúé2ùë• 1
2ùúé2ùë•2 1
2ùúé2ùúá2¬∏log¬πùúé¬∫
.(A.22)
This matches the definition of the exponential family with:
underlyingmeasure :‚Ñé¬πùë•¬∫=1p
2ùúã,
naturalparameters :ùúº=ùúÇ1
ùúÇ2
=ùúá
ùúé2
1
2ùúé2
,
999 Naive Bayes
287suÔ¨Äicient statistics :ùëá¬πùë•¬∫=ùë•
 ùë•2
, and
cumulant function :ùê¥¬πùúº¬∫=1
2ùúé2ùúá2¬∏log¬πùúé¬∫=ùúÇ2
1
4ùúÇ2 1
2log¬π2ùúÇ2¬∫.
Itisworthnotingthattheexactchoiceofeachofabovetermsissomewhatarbitrary. Indeed,
theimportantfeatureisthatthedistributioncanbeexpressedinthisform,nottheexactform
itself.
As we allude to in Section 4.1.2 , a widely used technique is to assume that the final output
yfollows an exponential family distribution. The exponential family is a common and
powerful family of distributions encountered frequently in machine learning.
A.8.8Summary
Bernoulli random variables can be used to model events with a yes/no outcome.
Discrete uniform distributions model selects from a finite set of possibilities.
Continuous uniform distributions select from an interval.
BinomialdistributionsmodelaseriesofBernoullirandomvariables,andcountthenum-
ber of successes.
Poisson random variables model the arrival of rare events.
Gaussian random variables model the result of adding a large number of independent
random variables together.
All the above distributions belong to exponential family.
A.8.9Exercises
1.What is the standard deviation of a random variable that is the difference ùëã ùëåof two
independent binomial random variables ùëã,ùëåBinomial¬π16,1¬ù2¬∫.
2.If we take a Poisson random variable ùëãPoisson¬πùúÜ¬∫and consider¬πùëã ùúÜ¬∫¬ùp
ùúÜas
ùúÜ!1, we can show that this becomes approximately Gaussian. Why does this make
sense?
3.Whatistheprobabilitymassfunctionforasumoftwodiscreteuniformrandomvariables
onùëõelements?
Discussions287.
A.9NaiveBayes
Throughout the previous sections, we learned about the theory of probability and random
variables. To put this theory to work, let‚Äôs introduce the naive Bayes classifier. This
1000 Mathematics for Deep Learning
uses nothing but probabilistic fundamentals to allow us to perform classification of dig-
its.
Learningisallaboutmakingassumptions. Ifwewanttoclassifyanewdataexamplethatwe
have never seen before we have to make some assumptions about which data examples are
similartoeachother. ThenaiveBayesclassifier,apopularandremarkablyclearalgorithm,
assumes all features are independent from each other to simplify the computation. In this
section, we will apply this model to recognize characters in images.
%matplotlib inline
import math
import torch
import torchvision
from d2l import torch asd2l
d2l.use_svg_display()
A.9.1OpticalCharacter Recognition
MNIST ( LeCunet al., 1998) is one of widely used datasets. It contains 60,000 images for
training and 10,000 images for validation. Each image contains a handwritten digit from 0
to 9. The task is classifying each image into the corresponding digit.
Gluon provides a MNISTclass in the data.vision module to automatically retrieve the
datasetfromtheInternet. Subsequently,Gluonwillusethealready-downloadedlocalcopy.
We specify whether we are requesting the training set or the test set by setting the value of
the parameter traintoTrueorFalse, respectively. Each image is a grayscale image with
both width and height of 28with shape ( 28,28,1). We use a customized transformation to
remove the last channel dimension. In addition, the dataset represents each pixel by an un-
signed 8-bitinteger. Wequantizethemintobinaryfeaturestosimplifytheproblem.
data_transform =torchvision .transforms .Compose([
torchvision .transforms .ToTensor(),
lambda x: torch .floor(x *255 /128).squeeze(dim =0)
])
mnist_train =torchvision .datasets .MNIST(
root ='./temp ', train =True , transform =data_transform, download =True )
mnist_test =torchvision .datasets .MNIST(
root ='./temp ', train =False , transform =data_transform, download =True )
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./
‚Ü©!temp/MNIST/raw/train-images-idx3-ubyte.gz
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 9912422/9912422 [00:00<00:00, 115752065.81it/s]
Extracting ./temp/MNIST/raw/train-images-idx3-ubyte.gz to ./temp/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./
(continues on next page)
1001 Naive Bayes
(continued from previous page)
‚Ü©!temp/MNIST/raw/train-labels-idx1-ubyte.gz
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 28881/28881 [00:00<00:00, 5234904.66it/s]
Extracting ./temp/MNIST/raw/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./
‚Ü©!temp/MNIST/raw/t10k-images-idx3-ubyte.gz
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 1648877/1648877 [00:00<00:00, 43715298.68it/s]Extracting ./
‚Ü©!temp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./
‚Ü©!temp/MNIST/raw/t10k-labels-idx1-ubyte.gz
100%|ÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøøÔøø| 4542/4542 [00:00<00:00, 21501725.47it/s]
Extracting ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw
We can access a particular example, which contains the image and the corresponding la-
bel.
image, label =mnist_train[ 2]
image .shape, label
(torch .Size([ 28,28]), 4)
Our example, stored here in the variable image, corresponds to an image with a height and
width of 28pixels.
image .shape, image .dtype
(torch .Size([ 28,28]), torch .float32)
Our code stores the label of each image as a scalar. Its type is a 32-bit integer.
label, type (label)
(4,int)
We can also access multiple examples at the same time.
images =torch .stack([mnist_train[i][ 0]for iinrange (10,38)], dim =0)
labels =torch .tensor([mnist_train[i][ 1]for iinrange (10,38)])
images .shape, labels .shape
1002 Mathematics for Deep Learning
(torch .Size([ 28,28,28]), torch .Size([ 28]))
Let‚Äôs visualize these examples.
d2l.show_images(images, 2,9);
A.9.2TheProbabilisticModel forClassification
In a classification task, we map an example into a category. Here an example is a grayscale
2828image, and a category is a digit. (Refer to Section 4.1 for a more detailed expla-
nation.) One natural way to express the classification task is via the probabilistic question:
what is the most likely label given the features (i.e., image pixels)? Denote by x2Rùëëthe
features of the example and ùë¶2Rthe label. Here features are image pixels, where we can
reshape a 2-dimensional image to a vector so that ùëë=282=784, and labels are digits.
The probability of the label given the features is ùëù¬πùë¶jx¬∫. If we are able to compute these
probabilities, which are ùëù¬πùë¶jx¬∫forùë¶=0,..., 9in our example, then the classifier will
output the prediction ÀÜùë¶given by the expression:
ÀÜùë¶=argmaxùëù¬πùë¶jx¬∫. (A.1)
Unfortunately, this requires that we estimate ùëù¬πùë¶jx¬∫for every value of x=ùë•1,...,ùë•ùëë.
Imaginethateachfeaturecouldtakeoneof 2values. Forexample,thefeature ùë•1=1might
signify that the word apple appears in a given document and ùë•1=0would signify that it
does not. If we had 30such binary features, that would mean that we need to be prepared
to classify any of 230(over 1 billion!) possible values of the input vector x.
Moreover,whereisthelearning? Ifweneedtoseeeverysinglepossibleexampleinorderto
predictthecorrespondinglabelthenwearenotreallylearningapatternbutjustmemorizing
the dataset.
A.9.3The NaiveBayesClassifier
Fortunately, by making some assumptions about conditional independence, we can intro-
duce some inductive bias and build a model capable of generalizing from a comparatively
modest selection of training examples. To begin, let‚Äôs use Bayes theorem, to express the
classifier as
ÀÜùë¶=argmaxùë¶ùëù¬πùë¶jx¬∫=argmaxùë¶ùëù¬πxjùë¶¬∫ùëù¬πùë¶¬∫
ùëù¬πx¬∫. (A.2)
1003 Naive Bayes
Notethatthedenominatoristhenormalizingterm ùëù¬πx¬∫whichdoesnotdependonthevalue
of the labelùë¶. As a result, we only need to worry about comparing the numerator across
different values of ùë¶. Even if calculating the denominator turned out to be intractable, we
could get away with ignoring it, so long as we could evaluate the numerator. Fortunately,
even if we wanted to recover the normalizing constant, we could. We can always recover
the normalization term since√ç
ùë¶ùëù¬πùë¶jx¬∫=1.
Now, let‚Äôs focus on ùëù¬πxjùë¶¬∫. Using the chain rule of probability, we can express the term
ùëù¬πxjùë¶¬∫as
ùëù¬πùë•1jùë¶¬∫ùëù¬πùë•2jùë•1,ùë¶¬∫...ùëù¬πùë•ùëëjùë•1,...,ùë•ùëë 1,ùë¶¬∫. (A.3)
By itself, this expression does not get us any further. We still must estimate roughly 2ùëë
parameters. However, if we assume that thefeaturesareconditionallyindependentofeach
other,giventhelabel , then suddenly we are in much better shape, as this term simplifies to√é
ùëñùëù¬πùë•ùëñjùë¶¬∫, giving us the predictor
ÀÜùë¶=argmaxùë¶ùëë√ñ
ùëñ=1ùëù¬πùë•ùëñjùë¶¬∫ùëù¬πùë¶¬∫. (A.4)
If we can estimate ùëù¬πùë•ùëñ=1jùë¶¬∫for everyùëñandùë¶, and save its value in ùëÉùë•ùë¶¬ªùëñ,ùë¶¬º, hereùëÉùë•ùë¶
is aùëëùëõmatrix withùëõbeing the number of classes and ùë¶2f1,...,ùëõg, then we can also
use this to estimate ùëù¬πùë•ùëñ=0jùë¶¬∫, i.e.,
ùëù¬πùë•ùëñ=ùë°ùëñjùë¶¬∫=(
ùëÉùë•ùë¶¬ªùëñ,ùë¶¬ºforùë°ùëñ=1;
1 ùëÉùë•ùë¶¬ªùëñ,ùë¶¬ºforùë°ùëñ=0.(A.5)
In addition, we estimate ùëù¬πùë¶¬∫for everyùë¶and save it in ùëÉùë¶¬ªùë¶¬º, withùëÉùë¶aùëõ-length vector.
Then, for any new example t=¬πùë°1,ùë°2,...,ùë°ùëë¬∫, we could compute
ÀÜùë¶=argmaxùë¶ùëù¬πùë¶¬∫ùëë√ñ
ùëñ=1ùëù¬πùë•ùë°=ùë°ùëñjùë¶¬∫
=argmaxùë¶ùëÉùë¶¬ªùë¶¬ºùëë√ñ
ùëñ=1ùëÉùë•ùë¶¬ªùëñ,ùë¶¬ºùë°ùëñ 1 ùëÉùë•ùë¶¬ªùëñ,ùë¶¬º1 ùë°ùëñ(A.6)
for anyùë¶. So our assumption of conditional independence has taken the complexity of
our model from an exponential dependence on the number of features O¬π2ùëëùëõ¬∫to a linear
dependence, which is O¬πùëëùëõ¬∫.
A.9.4Training
The problem now is that we do not know ùëÉùë•ùë¶andùëÉùë¶. So we need to estimate their values
given some training data first. This is training the model. Estimating ùëÉùë¶is not too hard.
Since we are only dealing with 10classes, we may count the number of occurrences ùëõùë¶for
each of the digits and divide it by the total amount of data ùëõ. For instance, if digit 8 occurs
ùëõ8=5,800times and we have a total of ùëõ=60,000images, the probability estimate is
ùëù¬πùë¶=8¬∫=0.0967.
1004 Mathematics for Deep Learning
X=torch .stack([mnist_train[i][ 0]for iinrange (len(mnist_train))], dim =0)
Y=torch .tensor([mnist_train[i][ 1]for iinrange (len(mnist_train))])
n_y =torch .zeros( 10)
for yinrange (10):
n_y[y] =(Y==y).sum()
P_y =n_y /n_y.sum()
P_y
tensor([ 0.0987 ,0.1124 ,0.0993 ,0.1022 ,0.0974 ,0.0904 ,0.0986 ,0.1044 ,0.0975 ,
0.0992 ])
Now on to slightly more difficult things ùëÉùë•ùë¶. Since we picked black and white images,
ùëù¬πùë•ùëñjùë¶¬∫denotes the probability that pixel ùëñis switched on for class ùë¶. Just like before we
can go and count the number of times ùëõùëñùë¶such that an event occurs and divide it by the
totalnumberofoccurrencesof ùë¶,i.e.,ùëõùë¶. Butthereissomethingslightlytroubling: certain
pixels may never be black (e.g., for well cropped images the corner pixels might always be
white). Aconvenientwayforstatisticianstodealwiththisproblemistoaddpseudocounts
to all occurrences. Hence, rather than ùëõùëñùë¶we useùëõùëñùë¶¬∏1and instead of ùëõùë¶we useùëõùë¶¬∏2
(since there are two possible values pixel ùëñcan take - it can either be black or white). This
is also called LaplaceSmoothing . It may seem ad-hoc, however it can be motivated from a
Bayesian point-of-view by a Beta-binomial model.
n_x =torch .zeros(( 10,28,28))
for yinrange (10):
n_x[y] =torch .tensor(X .numpy()[Y .numpy() ==y].sum(axis =0))
P_xy =(n_x +1)/(n_y +2).reshape( 10,1,1)
d2l.show_images(P_xy, 2,5);
By visualizing these 102828probabilities (for each pixel for each class) we could get
some mean looking digits.
Now we can use (A.6)to predict a new image. Given x, the following functions computes
ùëù¬πxjùë¶¬∫ùëù¬πùë¶¬∫for everyùë¶.
1005 Naive Bayes
def bayes_pred (x):
x=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)
p_xy =P_xy *x+(1-P_xy) *(1-x)
p_xy =p_xy .reshape( 10,-1).prod(dim =1)# p(x|y)
return p_xy *P_y
image, label =mnist_test[ 0]
bayes_pred(image)
tensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
This went horribly wrong! To find out why, let‚Äôs look at the per pixel probabilities. They
are typically numbers between 0.001and1. We are multiplying 784of them. At this point
it is worth mentioning that we are calculating these numbers on a computer, hence with a
fixedrangefortheexponent. Whathappensisthatweexperience numericalunderflow ,i.e.,
multiplying all the small numbers leads to something even smaller until it is rounded down
to zero. We discussed this as a theoretical issue in Section A.7 , but we see the phenomena
clearly here in practice.
As discussed in that section, we fix this by use the fact that logùëéùëè=logùëé¬∏logùëè, i.e.,
we switch to summing logarithms. Even if both ùëéandùëèare small numbers, the logarithm
values should be in a proper range.
a=0.1
print ('underflow: ', a**784)
print ('logarithm is normal: ',784*math .log(a))
underflow: 0.0
logarithm isnormal: -1805.2267129073316
Since the logarithm is an increasing function, we can rewrite (A.6)as
ÀÜùë¶=argmaxùë¶logùëÉùë¶¬ªùë¶¬º¬∏ùëë√ï
ùëñ=1h
ùë°ùëñlogùëÉùë•ùë¶¬ªùë•ùëñ,ùë¶¬º¬∏¬π 1 ùë°ùëñ¬∫log¬π1 ùëÉùë•ùë¶¬ªùë•ùëñ,ùë¶¬º¬∫i
.(A.7)
We can implement the following stable version:
log_P_xy =torch .log(P_xy)
log_P_xy_neg =torch .log( 1-P_xy)
log_P_y =torch .log(P_y)
def bayes_pred_stable (x):
x=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)
p_xy =log_P_xy *x+log_P_xy_neg *(1-x)
p_xy =p_xy .reshape( 10,-1).sum(axis =1)# p(x|y)
return p_xy +log_P_y
py=bayes_pred_stable(image)
py
1006 Mathematics for Deep Learning
tensor([ -268.9725 ,-301.7044 ,-245.1951 ,-218.8738 ,-193.4570 ,-206.0909 ,
-292.5226 ,-114.6257 ,-220.3313 ,-163.1784 ])
We may now check if the prediction is correct.
py.argmax(dim =0)==label
tensor( True )
If we now predict a few validation examples, we can see the Bayes classifier works pretty
well.
def predict (X):
return [bayes_pred_stable(x) .argmax(dim =0).type(torch .int32) .item()
for xinX]
X=torch .stack([mnist_test[i][ 0]for iinrange (18)], dim =0)
y=torch .tensor([mnist_test[i][ 1]for iinrange (18)])
preds =predict(X)
d2l.show_images(X, 2,9, titles =[str(d) for dinpreds]);
Finally, let‚Äôs compute the overall accuracy of the classifier.
X=torch .stack([mnist_test[i][ 0]for iinrange (len(mnist_test))], dim =0)
y=torch .tensor([mnist_test[i][ 1]for iinrange (len(mnist_test))])
preds =torch .tensor(predict(X), dtype =torch .int32)
float ((preds ==y).sum()) /len(y) # Validation accuracy
0.8427
Modern deep networks achieve error rates of less than 0.01. The relatively poor perfor-
mance is due to the incorrect statistical assumptions that we made in our model: we as-
sumed that each and every pixel are independently generated, depending only on the label.
This is clearly not how humans write digits, and this wrong assumption led to the downfall
of our overly naive (Bayes) classifier.
A.9.5Summary
1007 Statistics
288Using Bayes‚Äô rule, a classifier can be made by assuming all observed features are inde-
pendent.
This classifier can be trained on a dataset by counting the number of occurrences of
combinations of labels and pixel values.
This classifier was the gold standard for decades for tasks such as spam detection.
A.9.6Exercises
1.Consider the dataset ¬ª¬ª0,0¬º,¬ª0,1¬º,¬ª1,0¬º,¬ª1,1¬º¬ºwith labels given by the XOR of the
two elements¬ª0,1,1,0¬º. What are the probabilities for a Naive Bayes classifier built
on this dataset. Does it successfully classify our points? If not, what assumptions are
violated?
2.Suppose that we did not use Laplace smoothing when estimating probabilities and a
dataexamplearrivedattestingtimewhichcontainedavalueneverobservedintraining.
What would the model output?
3.The naive Bayes classifier is a specific example of a Bayesian network, where the de-
pendenceofrandomvariablesareencodedwithagraphstructure. Whilethefulltheory
is beyond the scope of this section (see Koller and Friedman ( 2009) for full details),
explain why allowing explicit dependence between the two input variables in the XOR
model allows for the creation of a successful classifier.
Discussions288.
A.10Statistics
Undoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art
and high accurate models is crucial. However, it is often unclear when improvements are
significant, or only the result of random fluctuations in the training process. To be able to
discuss uncertainty in estimated values, we must learn some statistics.
Theearliestreferenceof statistics canbetracedbacktoanArabscholarAl-Kindiinthe 9th-
century, who gave a detailed description of how to use statistics and frequency analysis to
decipher encrypted messages. After 800 years, the modern statistics arose from Germany
in 1700s, when the researchers focused on the demographic and economic data collection
andanalysis. Today,statisticsisthesciencesubjectthatconcernsthecollection,processing,
analysis,interpretationandvisualizationofdata. Whatismore,thecoretheoryofstatistics
has been widely used in the research within academia, industry, and government.
Morespecifically, statisticscan be divided to descriptivestatistics andstatisticalinference .
The former focus on summarizing and illustrating the features of a collection of observed
data, which is referred to as a sample. The sample is drawn from a population , denotes
1008 Mathematics for Deep Learning
thetotalsetofsimilarindividuals, items, oreventsofourexperimentinterests. Contraryto
descriptivestatistics, statisticalinference furtherdeducesthecharacteristicsofapopulation
fromthegiven samples,basedontheassumptionsthatthesampledistributioncanreplicate
the population distribution at some degree.
You may wonder: ‚ÄúWhat is the essential difference between machine learning and statis-
tics?‚Äù Fundamentally speaking, statistics focuses on the inference problem. This type of
problems includes modeling the relationship between the variables, such as causal infer-
ence, and testing the statistically significance of model parameters, such as A/B testing. In
contrast, machine learning emphasizes on making accurate predictions, without explicitly
programming and understanding each parameter‚Äôs functionality.
Inthissection,wewillintroducethreetypesofstatisticsinferencemethods: evaluatingand
comparing estimators, conducting hypothesis tests, and constructing confidence intervals.
These methods can help us infer the characteristics of a given population, i.e., the true
parameterùúÉ. For brevity, we assume that the true parameter ùúÉof a given population is a
scalar value. It is straightforward to extend to the case where ùúÉis a vector or a tensor, thus
we omit it in our discussion.
A.10.1Evaluatingand ComparingEstimators
Instatistics, an estimator isafunctionofgivensamplesusedtoestimatethetrueparameter
ùúÉ. We will write ÀÜùúÉùëõ=ÀÜùëì¬πùë•1,...,ùë•ùëõ¬∫for the estimate of ùúÉafter observing the samples
{ùë•1,ùë•2,...,ùë•ùëõ}.
We have seen simple examples of estimators before in section Section A.7 . If you have a
number of samples from a Bernoulli random variable, then the maximum likelihood esti-
matefortheprobabilitytherandomvariableisonecanbeobtainedbycountingthenumber
of ones observed and dividing by the total number of samples. Similarly, an exercise asked
youto showthat the maximum likelihoodestimateof the mean of a Gaussian givena num-
berofsamplesisgivenbytheaveragevalueofallthesamples. Theseestimatorswillalmost
never give the true value of the parameter, but ideally for a large number of samples the
estimate will be close.
As an example, we show below the true density of a Gaussian random variable with mean
zeroandvarianceone,alongwithacollectionsamplesfromthatGaussian. Weconstructed
theùë¶coordinate so every point is visible and the relationship to the original density is
clearer.
import torch
from d2l import torch asd2l
torch .pi=torch .acos(torch .zeros( 1))*2#define pi in torch
# Sample datapoints and create y coordinate
epsilon =0.1
torch .manual_seed( 8675309 )
xs=torch .randn(size =(300,))
(continues on next page)
1009 Statistics
(continued from previous page)
ys=torch .tensor(
[torch .sum(torch .exp( -(xs[:i] -xs[i]) **2/(2*epsilon **2))\
/torch .sqrt( 2*torch .pi*epsilon **2))/len(xs)\
for iinrange (len(xs))])
# Compute true density
xd=torch .arange(torch .min(xs), torch .max(xs), 0.01 )
yd=torch .exp( -xd**2/2)/torch .sqrt( 2*torch .pi)
# Plot the results
d2l.plot(xd, yd, 'x','density ')
d2l.plt.scatter(xs, ys)
d2l.plt.axvline(x =0)
d2l.plt.axvline(x =torch .mean(xs), linestyle ='--', color ='purple ')
d2l.plt.title( f'sample mean: {float (torch .mean(xs) .item()) :.2f}')
d2l.plt.show()
There can be many ways to compute an estimator of a parameter ÀÜùúÉùëõ. In this section, we
introduce three common methods to evaluate and compare estimators: the mean squared
error, the standard deviation, and statistical bias.
Mean SquaredError
Perhaps the simplest metric used to evaluate estimators is the mean squared error (MSE)
(orùëô2loss) estimator which can be defined as
MSE¬πÀÜùúÉùëõ,ùúÉ¬∫=ùê∏¬ª¬πÀÜùúÉùëõ ùúÉ¬∫2¬º. (A.1)
Thisallowsustoquantifytheaveragesquareddeviationfromthetruevalue. MSEisalways
non-negative. Ifyouhaveread Section3.1 ,youwillrecognizeitasthemostcommonlyused
regressionlossfunction. Asameasuretoevaluateanestimator, thecloseritsvaluetozero,
the closer the estimator is close to the true parameter ùúÉ.
Statistical Bias
The MSE provides a natural metric, but we can easily imagine multiple different phenom-
ena that might make it large. Two fundamentally important are fluctuation in the estimator
1010 Mathematics for Deep Learning
duetorandomnessinthedataset,andsystematicerrorintheestimatorduetotheestimation
procedure.
First, let‚Äôs measure the systematic error. For an estimator ÀÜùúÉùëõ, the mathematical illustration
ofstatisticalbias can be defined as
bias¬πÀÜùúÉùëõ¬∫=ùê∏¬πÀÜùúÉùëõ ùúÉ¬∫=ùê∏¬πÀÜùúÉùëõ¬∫ ùúÉ. (A.2)
Note that when bias ¬πÀÜùúÉùëõ¬∫=0, the expectation of the estimator ÀÜùúÉùëõis equal to the true value
of parameter. In this case, we say ÀÜùúÉùëõis an unbiased estimator. In general, an unbiased
estimator is better than a biased estimator since its expectation is the same as the true pa-
rameter.
It is worth being aware, however, that biased estimators are frequently used in practice.
There are cases where unbiased estimators do not exist without further assumptions, or
are intractable to compute. This may seem like a significant flaw in an estimator, however
the majority of estimators encountered in practice are at least asymptotically unbiased in
the sense that the bias tends to zero as the number of available samples tends to infinity:
limùëõ!1bias¬πÀÜùúÉùëõ¬∫=0.
Varianceand StandardDeviation
Second, let‚Äôs measure the randomness in the estimator. Recall from Section A.6 , thestan-
dard deviation (orstandard error ) is defined as the squared root of the variance. We may
measure the degree of fluctuation of an estimator by measuring the standard deviation or
variance of that estimator.
ùúéÀÜùúÉùëõ=q
Var¬πÀÜùúÉùëõ¬∫=q
ùê∏¬ª¬πÀÜùúÉùëõ ùê∏¬πÀÜùúÉùëõ¬∫¬∫2¬º. (A.3)
It is important to compare (A.3)to(A.1). In this equation we do not compare to the true
population value ùúÉ, but instead to ùê∏¬πÀÜùúÉùëõ¬∫, the expected sample mean. Thus we are not
measuring how far the estimator tends to be from the true value, but instead we measuring
the fluctuation of the estimator itself.
TheBias-VarianceTrade-off
It is intuitively clear that these two main components contribute to the mean squared error.
What is somewhat shocking is that we can show that this is actually a decomposition of
the mean squared error into these two contributions plus a third one. That is to say that we
can write the mean squared error as the sum of the square of the bias, the variance and the
1011 Statistics
irreducible error.
MSE¬πÀÜùúÉùëõ,ùúÉ¬∫=ùê∏¬ª¬πÀÜùúÉùëõ ùúÉ¬∫2¬º
=ùê∏¬ª¬πÀÜùúÉùëõ¬∫2¬º¬∏ùê∏¬ªùúÉ2¬º 2ùê∏¬ªÀÜùúÉùëõùúÉ¬º
=Var¬ªÀÜùúÉùëõ¬º¬∏ùê∏¬ªÀÜùúÉùëõ¬º2¬∏Var¬ªùúÉ¬º¬∏ùê∏¬ªùúÉ¬º2 2ùê∏¬ªÀÜùúÉùëõ¬ºùê∏¬ªùúÉ¬º
=¬πùê∏¬ªÀÜùúÉùëõ¬º ùê∏¬ªùúÉ¬º¬∫2¬∏Var¬ªÀÜùúÉùëõ¬º¬∏Var¬ªùúÉ¬º
=¬πùê∏¬ªÀÜùúÉùëõ ùúÉ¬º¬∫2¬∏Var¬ªÀÜùúÉùëõ¬º¬∏Var¬ªùúÉ¬º
=¬πbias¬ªÀÜùúÉùëõ¬º¬∫2¬∏Var¬πÀÜùúÉùëõ¬∫¬∏Var¬ªùúÉ¬º.(A.4)
We refer the above formula as bias-variance trade-off . The mean squared error can be di-
videdintothreesourcesoferror: theerrorfromhighbias,theerrorfromhighvarianceand
the irreducible error. The bias error is commonly seen in a simple model (such as a linear
regression model), which cannot extract high dimensional relations between the features
and the outputs. If a model suffers from high bias error, we often say it is underfitting or
lack offlexibilty as introduced in ( Section 3.6 ). The high variance usually results from a
too complex model, which overfits the training data. As a result, an overfitting model is
sensitive to small fluctuations in the data. If a model suffers from high variance, we often
sayitisoverfitting andlackof generalization asintroducedin( Section3.6 ). Theirreducible
error is the result from noise in the ùúÉitself.
EvaluatingEstimators in Code
Since the standard deviation of an estimator has been implementing by simply calling a.
std()foratensor a,wewillskipitbutimplementthestatisticalbiasandthemeansquared
error.
# Statistical bias
def stat_bias (true_theta, est_theta):
return (torch .mean(est_theta) -true_theta)
# Mean squared error
def mse(data, true_theta):
return (torch .mean(torch .square(data -true_theta)))
Toillustratetheequationofthebias-variancetrade-off,let‚Äôssimulateofnormaldistribution
N¬πùúÉ,ùúé2¬∫with 10,000samples. Here, we use a ùúÉ=1andùúé=4. As the estimator is a
function of the given samples, here we use the mean of the samples as an estimator for true
ùúÉin this normal distribution N¬πùúÉ,ùúé2¬∫.
theta_true =1
sigma =4
sample_len =10000
samples =torch .normal(theta_true, sigma, size =(sample_len, 1))
theta_est =torch .mean(samples)
theta_est
1012 Mathematics for Deep Learning
tensor( 1.0170 )
Let‚Äôs validate the trade-off equation by calculating the summation of the squared bias and
the variance of our estimator. First, calculate the MSE of our estimator.
mse(samples, theta_true)
tensor( 16.0298 )
Next, we calculate Var ¬πÀÜùúÉùëõ¬∫¬∏¬ªbias¬πÀÜùúÉùëõ¬∫¬º2as below. As you can see, the two values agree to
numerical precision.
bias =stat_bias(theta_true, theta_est)
torch .square(samples .std(unbiased =False ))+torch .square(bias)
tensor( 16.0298 )
A.10.2Conducting HypothesisTests
The most commonly encountered topic in statistical inference is hypothesis testing. While
hypothesis testing was popularized in the early 20th century, the first use can be traced
back to John Arbuthnot in the 1700s. John tracked 80-year birth records in London and
concluded that more men were born than women each year. Following that, the modern
significance testing is the intelligence heritage by Karl Pearson who invented ùëù-value and
Pearson‚Äôschi-squaredtest,WilliamGossetwhoisthefatherofStudent‚Äôst-distribution,and
Ronald Fisher who initialed the null hypothesis and the significance test.
Ahypothesistest isawayofevaluatingsomeevidenceagainstthedefaultstatementabouta
population. We refer the default statement as the nullhypothesis ùêª0, which we try to reject
using the observed data. Here, we use ùêª0as a starting point for the statistical significance
testing. The alternative hypothesis ùêªùê¥(orùêª1) is a statement that is contrary to the null
hypothesis. A null hypothesis is often stated in a declarative form which posits a relation-
ship between variables. It should reflect the brief as explicit as possible, and be testable by
statistics theory.
Imagine you are a chemist. After spending thousands of hours in the lab, you develop a
new medicine which can dramatically improve one‚Äôs ability to understand math. To show
its magic power, you need to test it. Naturally, you may need some volunteers to take
the medicine and see whether it can help them learn mathematics better. How do you get
started?
First, you will need carefully random selected two groups of volunteers, so that there is no
difference between their mathematical understanding ability measured by some metrics.
The two groups are commonly referred to as the test group and the control group. The
testgroup (ortreatmentgroup )isagroupofindividualswhowillexperiencethemedicine,
1013 Statistics
while the control group represents the group of users who are set aside as a benchmark,
i.e., identical environment setups except taking this medicine. In this way, the influence
of all the variables are minimized, except the impact of the independent variable in the
treatment.
Second, after a period of taking the medicine, you will need to measure the two groups‚Äô
mathematicalunderstandingbythesamemetrics,suchaslettingthevolunteersdothesame
tests after learning a new mathematical formula. Then, you can collect their performance
and compare the results. In this case, our null hypothesis will be that there is no difference
between the two groups, and our alternate will be that there is.
This is still not fully formal. There are many details you have to think of carefully. For
example,whatisthesuitablemetricstotesttheirmathematicalunderstandingability? How
many volunteers for your test so you can be confident to claim the effectiveness of your
medicine? How long should you run the test? How do you decide if there is a difference
betweenthetwogroups? Doyoucareabouttheaverageperformanceonly,oralsotherange
of variation of the scores? And so on.
Inthisway,hypothesistestingprovidesaframeworkforexperimentaldesignandreasoning
about certainty in observed results. If we can now show that the null hypothesis is very
unlikely to be true, we may reject it with confidence.
To complete the story of how to work with hypothesis testing, we need to now introduce
some additional terminology and make some of our concepts above formal.
Statistical Significance
Thestatistical significance measures the probability of erroneously rejecting the null hy-
pothesis,ùêª0, when it should not be rejected, i.e.,
statistical significance =1 ùõº=1 ùëÉ¬πrejectùêª0jùêª0is true¬∫. (A.5)
It is also referred to as the typeIerror orfalsepositive . Theùõº, is called as the significance
leveland its commonly used value is 5%, i.e., 1 ùõº=95%. The significance level can
be explained as the level of risk that we are willing to take, when we reject a true null
hypothesis.
Fig. A.1 shows the observations‚Äô values and probability of a given normal distribution
in a two-sample hypothesis test. If the observation data example is located outsides the
95%threshold, it will be a very unlikely observation under the null hypothesis assump-
tion. Hence, there might be something wrong with the null hypothesis and we will reject
it.
Statistical Power
Thestatisticalpower (orsensitivity ) measures the probability of reject the null hypothesis,
ùêª0, when it should be rejected, i.e.,
statistical power =1 ùõΩ=1 ùëÉ¬πfail to reject ùêª0jùêª0is false¬∫. (A.6)
1014 Mathematics for Deep Learning
tFig. A.1 Statistical signiÔ¨Åcance.
Recall that a type I error is error caused by rejecting the null hypothesis when it is true,
whereas a typeIIerror is resulted from failing to reject the null hypothesis when it is false.
A type II error is usually denoted as ùõΩ, and hence the corresponding statistical power is
1 ùõΩ.
Intuitively, statistical power can be interpreted as how likely our test will detect a real dis-
crepancy of some minimum magnitude at a desired statistical significance level. 80%is
a commonly used statistical power threshold. The higher the statistical power, the more
likely we are to detect true differences.
One of the most common uses of statistical power is in determining the number of samples
needed. Theprobabilityyourejectthenullhypothesiswhenitisfalsedependsonthedegree
to which it is false (known as the effect size ) and the number of samples you have. As you
mightexpect,smalleffectsizeswillrequireaverylargenumberofsamplestobedetectable
with high probability. While beyond the scope of this brief appendix to derive in detail, as
an example, want to be able to reject a null hypothesis that our sample came from a mean
zerovarianceoneGaussian, andwebelievethatoursample‚Äôsmeanisactuallyclosetoone,
we can do so with acceptable error rates with a sample size of only 8. However, if we think
our sample population true mean is close to 0.01, then we‚Äôd need a sample size of nearly
80000to detect the difference.
We can imagine the power as a water filter. In this analogy, a high power hypothesis test is
like a high quality water filtration system that will reduce harmful substances in the water
as much as possible. On the other hand, a smaller discrepancy is like a low quality water
filter, where some relative small substances may easily escape from the gaps. Similarly, if
the statistical power is not of enough high power, then the test may not catch the smaller
discrepancy.
TestStatistic
Atest statisticùëá¬πùë•¬∫is a scalar which summarizes some characteristic of the sample data.
Thegoalofdefiningsuchastatisticisthatitshouldallowustodistinguishbetweendifferent
distributions and conduct our hypothesis test. Thinking back to our chemist example, if we
wish to show that one population performs better than the other, it could be reasonable to
1015 Statistics
take the mean as the test statistic. Different choices of test statistic can lead to statistical
test with drastically different statistical power.
Often,ùëá¬πùëã¬∫(the distribution of the test statistic under our null hypothesis) will follow, at
least approximately, a common probability distribution such as a normal distribution when
considered under the null hypothesis. If we can derive explicitly such a distribution, and
then measure our test statistic on our dataset, we can safely reject the null hypothesis if our
statistic is far outside the range that we would expect. Making this quantitative leads us to
the notion of ùëù-values.
ùëù-value
Theùëù-value (or the probabilityvalue ) is the probability that ùëá¬πùëã¬∫is at least as extreme as
the observed test statistic ùëá¬πùë•¬∫assuming that the null hypothesis is true, i.e.,
ùëù-value =ùëÉùêª0¬πùëá¬πùëã¬∫ùëá¬πùë•¬∫¬∫. (A.7)
If theùëù-valueis smaller than or equal to a predefined and fixed statistical significance level
ùõº, we may reject the null hypothesis. Otherwise, we will conclude that we are lack of
evidence to reject the null hypothesis. For a given population distribution, the region of
rejection will be the interval contained of all the points which has a ùëù-value smaller than
the statistical significance level ùõº.
One-side Testand Two-sidedTest
Normally there are two kinds of significance test: the one-sided test and the two-sided
test. The one-sided test (orone-tailed test ) is applicable when the null hypothesis and the
alternative hypothesis only have one direction. For example, the null hypothesis may state
that the true parameter ùúÉis less than or equal to a value ùëê. The alternative hypothesis
would be that ùúÉis greater than ùëê. That is, the region of rejection is on only one side of the
sampling distribution. Contrary to the one-sided test, the two-sidedtest (ortwo-tailedtest )
is applicable when the region of rejection is on both sides of the sampling distribution. An
example in this case may have a null hypothesis state that the true parameter ùúÉis equal to a
valueùëê. The alternative hypothesis would be that ùúÉis not equal to ùëê.
General Steps of HypothesisTesting
Aftergettingfamiliarwiththeaboveconcepts, let‚Äôsgothroughthegeneralstepsofhypoth-
esis testing.
1.State the question and establish a null hypotheses ùêª0.
2.Set the statistical significance level ùõºand a statistical power ( 1 ùõΩ).
3.Obtain samples through experiments. The number of samples needed will depend on
the statistical power, and the expected effect size.
4.Calculate the test statistic and the ùëù-value.
1016 Mathematics for Deep Learning
5.Make the decision to keep or reject the null hypothesis based on the ùëù-value and the
statistical significance level ùõº.
To conduct a hypothesis test, we start by defining a null hypothesis and a level of risk that
we are willing to take. Then we calculate the test statistic of the sample, taking an extreme
value of the test statistic as evidence against the null hypothesis. If the test statistic falls
withintherejectregion,wemayrejectthenullhypothesisinfavorofthealternative.
Hypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B
testing.
A.10.3Constructing Confidence Intervals
When estimating the value of a parameter ùúÉ, point estimators like ÀÜùúÉare of limited utility
since they contain no notion of uncertainty. Rather, it would be far better if we could
produce an interval that would contain the true parameter ùúÉwith high probability. If you
were interested in such ideas a century ago, then you would have been excited to read
‚ÄúOutlineofaTheoryofStatisticalEstimationBasedontheClassicalTheoryofProbability‚Äù
by Jerzy Neyman ( Neyman, 1937 ), who first introduced the concept of confidence interval
in 1937.
To be useful, a confidence interval should be as small as possible for a given degree of
certainty. Let‚Äôs see how to derive it.
Definition
Mathematically, a confidence interval for the true parameter ùúÉis an interval ùê∂ùëõthat com-
puted from the sample data such that
ùëÉùúÉ¬πùê∂ùëõ3ùúÉ¬∫1 ùõº,8ùúÉ. (A.8)
Hereùõº2¬π0,1¬∫, and 1 ùõºis called the confidencelevel orcoverage of the interval. This is
the sameùõºas the significance level as we discussed about above.
Note that (A.8)is about variable ùê∂ùëõ, not about the fixed ùúÉ. To emphasize this, we write
ùëÉùúÉ¬πùê∂ùëõ3ùúÉ¬∫rather thanùëÉùúÉ¬πùúÉ2ùê∂ùëõ¬∫.
Interpretation
It is very tempting to interpret a 95%confidence interval as an interval where you can be
95%surethetrueparameterlies,howeverthisissadlynottrue. Thetrueparameterisfixed,
and it is the interval that is random. Thus a better interpretation would be to say that if you
generated a large number of confidence intervals by this procedure, 95%of the generated
intervals would contain the true parameter.
Thismayseempedantic,butitcanhaverealimplicationsfortheinterpretationoftheresults.
In particular, we may satisfy (A.8)by constructing intervals that we are almost certain do
not contain the true value, as long as we only do so rarely enough. We close this section by
1017 Statistics
providing three tempting but false statements. An in-depth discussion of these points can
be found in Morey etal.(2016).
Fallacy1 . Narrow confidence intervals mean we can estimate the parameter precisely.
Fallacy2 . The values inside the confidence interval are more likely to be the true value
than those outside the interval.
Fallacy 3 . The probability that a particular observed 95%confidence interval contains
the true value is 95%.
Sufficed to say, confidence intervals are subtle objects. However, if you keep the interpre-
tation clear, they can be powerful tools.
A Gaussian Example
Let‚Äôsdiscussthemostclassicalexample,theconfidenceintervalforthemeanofaGaussian
of unknown mean and variance. Suppose we collect ùëõsamplesfùë•ùëñgùëõ
ùëñ=1from our Gaussian
N¬πùúá,ùúé2¬∫. We can compute estimators for the mean and variance by taking
ÀÜùúáùëõ=1
ùëõùëõ√ï
ùëñ=1ùë•ùëñand ÀÜùúé2
ùëõ=1
ùëõ 1ùëõ√ï
ùëñ=1¬πùë•ùëñ ÀÜùúá¬∫2. (A.9)
If we now consider the random variable
ùëá=ÀÜùúáùëõ ùúá
ÀÜùúéùëõ¬ùpùëõ, (A.10)
we obtain a random variable following a well-known distribution called the Student‚Äôs t-
distributionon ùëõ 1degreesof freedom .
This distribution is very well studied, and it is known, for instance, that as ùëõ!1, it is
approximately a standard Gaussian, and thus by looking up values of the Gaussian c.d.f. in
a table, we may conclude that the value of ùëáis in the interval¬ª 1.96,1.96¬ºat least 95%
of the time. For finite values of ùëõ, the interval needs to be somewhat larger, but are well
known and precomputed in tables.
Thus, we may conclude that for large ùëõ,
ùëÉÀÜùúáùëõ ùúá
ÀÜùúéùëõ¬ùpùëõ2¬ª  1.96,1.96¬º
0.95. (A.11)
Rearrangingthisbymultiplyingbothsidesby ÀÜùúéùëõ¬ùpùëõandthenadding ÀÜùúáùëõ,weobtain
ùëÉ
ùúá2
ÀÜùúáùëõ 1.96ÀÜùúéùëõpùëõ,ÀÜùúáùëõ¬∏1.96ÀÜùúéùëõpùëõ
0.95. (A.12)
Thus we know that we have found our 95%confidence interval:

ÀÜùúáùëõ 1.96ÀÜùúéùëõpùëõ,ÀÜùúáùëõ¬∏1.96ÀÜùúéùëõpùëõ
. (A.13)
It is safe to say that (A.13 )is one of the most used formula in statistics. Let‚Äôs close our
1018 Mathematics for Deep Learning
discussion of statisticsbyimplementing it. Forsimplicity, weassume we are in the asymp-
totic regime. Small values of ùëÅshould include the correct value of t_starobtained either
programmatically or from a ùë°-table.
# PyTorch uses Bessel's correction by default, which means the use of ddof=1
# instead of default ddof=0 in numpy. We can use unbiased=False to imitate
# ddof=0.
# Number of samples
N=1000
# Sample dataset
samples =torch .normal( 0,1, size =(N,))
# Lookup Students's t-distribution c.d.f.
t_star =1.96
# Construct interval
mu_hat =torch .mean(samples)
sigma_hat =samples .std(unbiased =True )
(mu_hat -t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)),\
mu_hat +t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)))
(tensor( -0.0568 ), tensor( 0.0704 ))
A.10.4Summary
Statistics focuses on inference problems, whereas deep learning emphasizes on making
accurate predictions without explicitly programming and understanding.
Therearethreecommonstatisticsinferencemethods: evaluatingandcomparingestima-
tors, conducting hypothesis tests, and constructing confidence intervals.
There are three most common estimators: statistical bias, standard deviation, and mean
square error.
A confidence interval is an estimated range of a true population parameter that we can
construct by given the samples.
Hypothesis testing is a way of evaluating some evidence against the default statement
about a population.
A.10.5Exercises
1.Letùëã1,ùëã2,...,ùëãùëõiidUnif¬π0,ùúÉ¬∫, where ‚Äúiid‚Äù stands for independent and identically
distributed . Consider the following estimators of ùúÉ:
ÀÜùúÉ=maxfùëã1,ùëã2,...,ùëãùëõg; (A.14)
ÀúùúÉ=2¬Øùëãùëõ=2
ùëõùëõ√ï
ùëñ=1ùëãùëñ. (A.15)
1019 Information Theory
289Find the statistical bias, standard deviation, and mean square error of ÀÜùúÉ.
Find the statistical bias, standard deviation, and mean square error of ÀúùúÉ.
Which estimator is better?
2.For our chemist example in introduction, can you derive the 5 steps to conduct a two-
sided hypothesis testing? Given the statistical significance level ùõº=0.05and the sta-
tistical power 1 ùõΩ=0.8.
3.Run the confidence interval code with ùëÅ=2andùõº=0.5for100independently gener-
ated dataset, and plot the resulting intervals (in this case t_star = 1.0 ). You will see
several very short intervals which are very far from containing the true mean 0. Does
this contradict the interpretation of the confidence interval? Do you feel comfortable
using short intervals to indicate high precision estimates?
Discussions289.
A.11InformationTheory
The universe is overflowing with information. Information provides a common language
acrossdisciplinaryrifts: fromShakespeare‚ÄôsSonnettoresearchers‚ÄôpaperonCornellArXiv,
from Van Gogh‚Äôs printing Starry Night to Beethoven‚Äôs music Symphony No. 5, from the
firstprogramminglanguagePlankalk√ºltothestate-of-the-artmachinelearningalgorithms.
Everything must follow the rules of information theory, no matter the format. With infor-
mation theory, we can measure and compare how much information is present in different
signals. Inthissection, wewillinvestigatethefundamentalconceptsofinformationtheory
and applications of information theory in machine learning.
Before we get started, let‚Äôs outline the relationship between machine learning and informa-
tiontheory. Machinelearningaimstoextractinterestingsignalsfromdataandmakecritical
predictions. On the other hand, information theory studies encoding, decoding, transmit-
ting, and manipulating information. As a result, information theory provides fundamental
languagefordiscussingtheinformationprocessinginmachinelearnedsystems. Forexam-
ple, many machine learning applications use the cross-entropy loss as described in Section
4.1. This loss can be directly derived from information theoretic considerations.
A.11.1Information
Let‚Äôs start with the ‚Äúsoul‚Äù of information theory: information. Information can be encoded
in anything with a particular sequence of one or more encoding formats. Suppose that we
task ourselves with trying to define a notion of information. What could be our starting
point?
Consider the following thought experiment. We have a friend with a deck of cards. They
1020 Mathematics for Deep Learning
will shuffle the deck, flip over some cards, and tell us statements about the cards. We will
try to assess the information content of each statement.
First, they flip over a card and tell us, ‚ÄúI see a card.‚Äù This provides us with no information
at all. We were already certain that this was the case so we hope the information should be
zero.
Next, they flip over a card and say, ‚ÄúI see a heart.‚Äù This provides us some information,
but in reality there are only 4different suits that were possible, each equally likely, so we
are not surprised by this outcome. We hope that whatever the measure of information, this
event should have low information content.
Next, they flip over a card and say, ‚ÄúThis is the 3of spades.‚Äù This is more information.
Indeed there were 52equally likely possible outcomes, and our friend told us which one it
was. This should be a medium amount of information.
Let‚Äôstakethistothelogicalextreme. Supposethatfinallytheyflipovereverycardfromthe
deck and read off the entire sequence of the shuffled deck. There are 52!different orders
to the deck, again all equally likely, so we need a lot of information to know which one it
is.
Any notion of information we develop must conform to this intuition. Indeed, in the next
sections we will learn how to compute that these events have 0bits, 2bits, 5.7bits, and
225.6bits of information respectively.
If we read through these thought experiments, we see a natural idea. As a starting point,
rather than caring about the knowledge, we may build off the idea that information repre-
sentsthedegreeofsurpriseortheabstractpossibilityoftheevent. Forexample, ifwewant
to describe an unusual event, we need a lot information. For a common event, we may not
need much information.
In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shan-
non, 1948 ) establishing the theory of information. In his article, Shannon introduced the
concept of information entropy for the first time. We will begin our journey here.
Self-information
Since information embodies the abstract possibility of an event, how do we map the pos-
sibility to the number of bits? Shannon introduced the terminology bitas the unit of in-
formation, which was originally created by John Tukey. So what is a ‚Äúbit‚Äù and why do we
use it to measure information? Historically, an antique transmitter can onlysend or receive
two types of code: 0and1. Indeed, binary encoding is still in common use on all modern
digital computers. In this way, any information is encoded by a series of 0and 1. And
hence, a series of binary digits of length ùëõcontainsùëõbits of information.
Now, suppose that for any series of codes, each 0or1occurs with a probability of1
2.
Hence, an event ùëãwith a series of codes of length ùëõ, occurs with a probability of1
2ùëõ. At
the same time, as we mentioned before, this series contains ùëõbits of information. So, can
1021 Information Theory
wegeneralizetoamathematicalfunctionwhichcantransfertheprobability ùëùtothenumber
of bits? Shannon gave the answer by defining self-information
ùêº¬πùëã¬∫= log2¬πùëù¬∫, (A.1)
as thebitsof information we have received for this event ùëã. Note that we will always use
base-2logarithmsinthissection. Forthesakeofsimplicity,therestofthissectionwillomit
the subscript 2 in the logarithm notation, i.e., log¬π.¬∫always refers to log2¬π.¬∫. For example,
the code ‚Äú0010‚Äù has a self-information
ùêº¬π‚Äù0010‚Äù¬∫= log¬πùëù¬π‚Äù0010‚Äù¬∫¬∫= log1
24
=4bits. (A.2)
We can calculate self information as shown below. Before that, let‚Äôs first import all the
necessary packages in this section.
import torch
from torch .nnimport NLLLoss
def nansum (x):
# Define nansum, as pytorch does not offer it inbuilt.
return x[~torch .isnan(x)] .sum()
def self_information (p):
return -torch .log2(torch .tensor(p)) .item()
self_information( 1/64)
6.0
A.11.2Entropy
As self-information only measures the information of a single discrete event, we need a
more generalized measure for any random variable of either discrete or continuous distri-
bution.
MotivatingEntropy
Let‚Äôstrytogetspecificaboutwhatwewant. Thiswillbeaninformalstatementofwhatare
known as the axioms of Shannon entropy . It will turn out that the following collection of
common-sense statements force us to a unique definition of information. A formal version
of these axioms, along with several others may be found in Csisz√°r ( 2008).
1.The information we gain by observing a random variable does not depend on what we
call the elements, or the presence of additional elements which have probability zero.
2.The information we gain by observing two random variables is no more than the sum
of the information we gain by observing them separately. If they are independent, then
it is exactly the sum.
1022 Mathematics for Deep Learning
3.The information gained when observing (nearly) certain events is (nearly) zero.
While proving this fact is beyond the scope of our text, it is important to know that this
uniquely determines the form that entropy must take. The only ambiguity that these allow
isinthechoiceoffundamentalunits, whichismostoftennormalizedbymakingthechoice
we saw before that the information provided by a single fair coin flip is one bit.
Definition
For any random variable ùëãthat follows a probability distribution ùëÉwith a probability den-
sityfunction(p.d.f.) oraprobabilitymassfunction(p.m.f.) ùëù¬πùë•¬∫,wemeasuretheexpected
amount of information through entropy(orShannonentropy )
ùêª¬πùëã¬∫= ùê∏ùë•ùëÉ¬ªlogùëù¬πùë•¬∫¬º. (A.3)
To be specific, if ùëãis discrete,
ùêª¬πùëã¬∫= √ï
ùëñùëùùëñlogùëùùëñ, whereùëùùëñ=ùëÉ¬πùëãùëñ¬∫.(A.4)
Otherwise, if ùëãis continuous, we also refer entropy as differentialentropy
ùêª¬πùëã¬∫= ¬π
ùë•ùëù¬πùë•¬∫logùëù¬πùë•¬∫ùëëùë•. (A.5)
We can define entropy as below.
def entropy (p):
entropy =-p*torch .log2(p)
# Operator `nansum` will sum up the non-nan number
out =nansum(entropy)
return out
entropy(torch .tensor([ 0.1,0.5,0.1,0.3]))
tensor( 1.6855 )
Interpretations
You may be curious: in the entropy definition (A.3), why do we use an expectation of a
negative logarithm? Here are some intuitions.
First, why do we use a logarithm function log? Suppose that ùëù¬πùë•¬∫=ùëì1¬πùë•¬∫ùëì2¬πùë•¬∫..., ùëìùëõ¬πùë•¬∫,
whereeachcomponentfunction ùëìùëñ¬πùë•¬∫isindependentfromeachother. Thismeansthateach
ùëìùëñ¬πùë•¬∫contributes independently to the total information obtained from ùëù¬πùë•¬∫. As discussed
above, we want the entropy formula to be additive over independent random variables.
Luckily, logcan naturally turn a product of probability distributions to a summation of the
individual terms.
Next, why do we use a negative log? Intuitively, more frequent events should contain less
1023 Information Theory
information than less common events, since we often gain more information from an un-
usual case than from an ordinary one. However, logis monotonically increasing with the
probabilities, and indeed negative for all values in ¬ª0,1¬º. We need to construct a monoton-
ically decreasing relationship between the probability of events and their entropy, which
will ideally be always positive (for nothing we observe should force us to forget what we
have known). Hence, we add a negative sign in front of logfunction.
Last, where does the expectation function come from? Consider a random variable ùëã. We
can interpret the self-information (  log¬πùëù¬∫) as the amount of surprise we have at seeing
a particular outcome. Indeed, as the probability approaches zero, the surprise becomes
infinite. Similarly, we can interpret the entropy as the average amount of surprise from
observingùëã. For example, imagine that a slot machine system emits statistical indepen-
dently symbols ùë†1,...,ùë†ùëòwith probabilities ùëù1,...,ùëùùëòrespectively. Then the entropy of
this system equals to the average self-information from observing each output, i.e.,
ùêª¬πùëÜ¬∫=√ï
ùëñùëùùëñùêº¬πùë†ùëñ¬∫= √ï
ùëñùëùùëñlogùëùùëñ.(A.6)
Propertiesof Entropy
By the above examples and interpretations, we can derive the following properties of en-
tropy (A.3). Here, we refer to ùëãas an event and ùëÉas the probability distribution of
ùëã.
ùêª¬πùëã¬∫0for all discrete ùëã(entropy can be negative for continuous ùëã).
IfùëãùëÉwith a p.d.f. or a p.m.f. ùëù¬πùë•¬∫, and we try to estimate ùëÉby a new probability
distributionùëÑwith a p.d.f. or a p.m.f. ùëû¬πùë•¬∫, then
ùêª¬πùëã¬∫= ùê∏ùë•ùëÉ¬ªlogùëù¬πùë•¬∫¬º ùê∏ùë•ùëÉ¬ªlogùëû¬πùë•¬∫¬º,with equality if and only if ùëÉ=ùëÑ.
(A.7)
Alternatively, ùêª¬πùëã¬∫gives a lower bound of the average number of bits needed to
encode symbols drawn from ùëÉ.
IfùëãùëÉ,thenùë•conveysthemaximumamountofinformationifitspreadsevenlyamong
all possible outcomes. Specifically, if the probability distribution ùëÉis discrete with
ùëò-classfùëù1,...,ùëùùëòg, then
ùêª¬πùëã¬∫log¬πùëò¬∫,with equality if and only if ùëùùëñ=1
ùëò,8ùëñ. (A.8)
IfùëÉisacontinuousrandomvariable,thenthestorybecomesmuchmorecomplicated.
However, if we additionally impose that ùëÉis supported on a finite interval (with all
valuesbetween 0and1),thenùëÉhasthehighestentropyifitistheuniformdistribution
on that interval.
A.11.3MutualInformation
Previously we defined entropy of a single random variable ùëã, how about the entropy of a
pair random variables ¬πùëã,ùëå¬∫? We can think of these techniques as trying to answer the
1024 Mathematics for Deep Learning
following type of question, ‚ÄúWhat information is contained in ùëãandùëåtogether compared
to each separately? Is there redundant information, or is it all unique?‚Äù
Forthefollowingdiscussion,wealwaysuse ¬πùëã,ùëå¬∫asapairofrandomvariablesthatfollows
a joint probability distribution ùëÉwith a p.d.f. or a p.m.f. ùëùùëã,ùëå¬πùë•,ùë¶¬∫, whileùëãandùëåfollow
probability distribution ùëùùëã¬πùë•¬∫andùëùùëå¬πùë¶¬∫, respectively.
JointEntropy
Similar to entropy of a single random variable (A.3), we define the joint entropy ùêª¬πùëã,ùëå¬∫
of a pair random variables ¬πùëã,ùëå¬∫as
ùêª¬πùëã,ùëå¬∫= ùê∏¬πùë•,ùë¶¬∫ùëÉ¬ªlogùëùùëã,ùëå¬πùë•,ùë¶¬∫¬º. (A.9)
Precisely, on the one hand, if ¬πùëã,ùëå¬∫is a pair of discrete random variables, then
ùêª¬πùëã,ùëå¬∫= √ï
ùë•√ï
ùë¶ùëùùëã,ùëå¬πùë•,ùë¶¬∫logùëùùëã,ùëå¬πùë•,ùë¶¬∫.(A.10)
On the other hand, if ¬πùëã,ùëå¬∫is a pair of continuous random variables, then we define the
differentialjoint entropy as
ùêª¬πùëã,ùëå¬∫= ¬π
ùë•,ùë¶ùëùùëã,ùëå¬πùë•,ùë¶¬∫logùëùùëã,ùëå¬πùë•,ùë¶¬∫ùëëùë• ùëëùë¶. (A.11)
We can think of (A.9)as telling us the total randomness in the pair of random variables.
As a pair of extremes, if ùëã=ùëåare two identical random variables, then the information in
the pair is exactly the information in one and we have ùêª¬πùëã,ùëå¬∫=ùêª¬πùëã¬∫=ùêª¬πùëå¬∫. On the
other extreme, if ùëãandùëåare independent then ùêª¬πùëã,ùëå¬∫=ùêª¬πùëã¬∫¬∏ùêª¬πùëå¬∫. Indeed we will
always have that the information contained in a pair of random variables is no smaller than
the entropy of either random variable and no more than the sum of both.
ùêª¬πùëã¬∫,ùêª¬πùëå¬∫ùêª¬πùëã,ùëå¬∫ùêª¬πùëã¬∫¬∏ùêª¬πùëå¬∫. (A.12)
Let‚Äôs implement joint entropy from scratch.
def joint_entropy (p_xy):
joint_ent =-p_xy *torch .log2(p_xy)
# Operator `nansum` will sum up the non-nan number
out =nansum(joint_ent)
return out
joint_entropy(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]))
tensor( 1.6855 )
Notice that this is the same codeas before, but now we interpret it differently as working
on the joint distribution of the two random variables.
1025 Information Theory
ConditionalEntropy
The joint entropy defined above the amount of information contained in a pair of random
variables. This is useful, but oftentimes it is not what we care about. Consider the setting
ofmachinelearning. Let‚Äôstake ùëãtobetherandomvariable(orvectorofrandomvariables)
that describes the pixel values of an image, and ùëåto be the random variable which is the
class label.ùëãshould contain substantial information‚Äîa natural image is a complex thing.
However, the information contained in ùëåonce the image has been show should be low.
Indeed, the image of a digit should already contain the information about what digit it is
unless the digit is illegible. Thus, to continue to extend our vocabulary of information
theory, we need to be able to reason about the information content in a random variable
conditional on another.
In the probability theory, we saw the definition of the conditional probability to measure
the relationship between variables. We now want to analogously define the conditional
entropyùêª¬πùëåjùëã¬∫. We can write this as
ùêª¬πùëåjùëã¬∫= ùê∏¬πùë•,ùë¶¬∫ùëÉ¬ªlogùëù¬πùë¶jùë•¬∫¬º, (A.13)
whereùëù¬πùë¶jùë•¬∫=ùëùùëã,ùëå¬πùë•,ùë¶¬∫
ùëùùëã¬πùë•¬∫is the conditional probability. Specifically, if ¬πùëã,ùëå¬∫is a pair of
discrete random variables, then
ùêª¬πùëåjùëã¬∫= √ï
ùë•√ï
ùë¶ùëù¬πùë•,ùë¶¬∫logùëù¬πùë¶jùë•¬∫.(A.14)
If¬πùëã,ùëå¬∫is a pair of continuous random variables, then the differentialconditionalentropy
is similarly defined as
ùêª¬πùëåjùëã¬∫= ¬π
ùë•¬π
ùë¶ùëù¬πùë•,ùë¶¬∫logùëù¬πùë¶jùë•¬∫ùëëùë• ùëëùë¶. (A.15)
It is now natural to ask, how does the conditional entropy ùêª¬πùëåjùëã¬∫relate to the entropy
ùêª¬πùëã¬∫and the joint entropy ùêª¬πùëã,ùëå¬∫? Using the definitions above, we can express this
cleanly:
ùêª¬πùëåjùëã¬∫=ùêª¬πùëã,ùëå¬∫ ùêª¬πùëã¬∫. (A.16)
This has an intuitive interpretation: the information in ùëågivenùëã(ùêª¬πùëåjùëã¬∫) is the same
as the information in both ùëãandùëåtogether (ùêª¬πùëã,ùëå¬∫) minus the information already con-
tained inùëã. This gives us the information in ùëåwhich is not also represented in ùëã.
Now, let‚Äôs implement conditional entropy (A.13 )from scratch.
def conditional_entropy (p_xy, p_x):
p_y_given_x =p_xy /p_x
cond_ent =-p_xy *torch .log2(p_y_given_x)
# Operator `nansum` will sum up the non-nan number
out =nansum(cond_ent)
return out
conditional_entropy(torch .tensor([[ 0.1,0.5], [ 0.2,0.3]]),
torch .tensor([ 0.2,0.8]))
1026 Mathematics for Deep Learning
tensor( 0.8635 )
MutualInformation
Given the previous setting of random variables ¬πùëã,ùëå¬∫, you may wonder: ‚ÄúNow that we
knowhowmuchinformationiscontainedin ùëåbutnotinùëã,canwesimilarlyaskhowmuch
information is shared between ùëãandùëå?‚Äù The answer will be the mutual information of
¬πùëã,ùëå¬∫, which we will write as ùêº¬πùëã,ùëå¬∫.
Rather than diving straight into the formal definition, let‚Äôs practice our intuition by first
trying to derive an expression for the mutual information entirely based on terms we have
constructed before. Wewish to find the information shared betweentwo random variables.
One way we could try to do this is to start with all the information contained in both ùëãand
ùëåtogether, and then we take off the parts that are not shared. The information contained in
bothùëãandùëåtogetheriswrittenas ùêª¬πùëã,ùëå¬∫. Wewanttosubtractfromthistheinformation
contained in ùëãbut not inùëå, and the information contained in ùëåbut not inùëã. As we saw in
the previous section, this is given by ùêª¬πùëãjùëå¬∫andùêª¬πùëåjùëã¬∫respectively. Thus, we have
that the mutual information should be
ùêº¬πùëã,ùëå¬∫=ùêª¬πùëã,ùëå¬∫ ùêª¬πùëåjùëã¬∫ ùêª¬πùëãjùëå¬∫. (A.17)
Indeed,thisisavaliddefinitionforthemutualinformation. Ifweexpandoutthedefinitions
of these terms and combine them, a little algebra shows that this is the same as
ùêº¬πùëã,ùëå¬∫=ùê∏ùë•ùê∏ùë¶
ùëùùëã,ùëå¬πùë•,ùë¶¬∫logùëùùëã,ùëå¬πùë•,ùë¶¬∫
ùëùùëã¬πùë•¬∫ùëùùëå¬πùë¶¬∫
. (A.18)
We can summarize all of these relationships in image Fig. A.1 . It is an excellent test of
intuition to see why the following statements are all also equivalent to ùêº¬πùëã,ùëå¬∫.
ùêª¬πùëã¬∫ ùêª¬πùëãjùëå¬∫
ùêª¬πùëå¬∫ ùêª¬πùëåjùëã¬∫
ùêª¬πùëã¬∫¬∏ùêª¬πùëå¬∫ ùêª¬πùëã,ùëå¬∫
tFig. A.1 Mutual information‚Äôs relationship with joint entropy and conditional entropy.
In many ways we can think of the mutual information (A.18 )as principled extension of
correlation coefficient we saw in Section A.6 . This allows us to ask not only for linear
1027 Information Theory
relationships between variables, but for the maximum information shared between the two
random variables of any kind.
Now, let‚Äôs implement mutual information from scratch.
def mutual_information (p_xy, p_x, p_y):
p=p_xy /(p_x *p_y)
mutual =p_xy *torch .log2(p)
# Operator `nansum` will sum up the non-nan number
out =nansum(mutual)
return out
mutual_information(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]),
torch .tensor([ 0.2,0.8]), torch .tensor([[ 0.75 ,0.25 ]]))
tensor( 0.7195 )
Properties of Mutual Information
Ratherthanmemorizingthedefinitionofmutualinformation (A.18 ),youonlyneedtokeep
in mind its notable properties:
Mutual information is symmetric, i.e., ùêº¬πùëã,ùëå¬∫=ùêº¬πùëå,ùëã¬∫.
Mutual information is non-negative, i.e., ùêº¬πùëã,ùëå¬∫0.
ùêº¬πùëã,ùëå¬∫=0if and only if ùëãandùëåare independent. For example, if ùëãandùëåare in-
dependent, then knowing ùëådoes not give any information about ùëãand vice versa, so
their mutual information is zero.
Alternatively, if ùëãis an invertible function of ùëå, thenùëåandùëãshare all information and
ùêº¬πùëã,ùëå¬∫=ùêª¬πùëå¬∫=ùêª¬πùëã¬∫. (A.19)
PointwiseMutual Information
When we worked with entropy at the beginning of this chapter, we were able to provide an
interpretation of log¬πùëùùëã¬πùë•¬∫¬∫as howsurprised we were with the particular outcome. We
may give a similar interpretation to the logarithmic term in the mutual information, which
is often referred to as the pointwise mutual information :
pmi¬πùë•,ùë¶¬∫=logùëùùëã,ùëå¬πùë•,ùë¶¬∫
ùëùùëã¬πùë•¬∫ùëùùëå¬πùë¶¬∫. (A.20)
We can think of (A.20 )as measuring how much more or less likely the specific combina-
tion of outcomes ùë•andùë¶are compared to what we would expect for independent random
outcomes. Ifitislargeandpositive,thenthesetwospecificoutcomesoccurmuchmorefre-
quentlythantheywouldcomparedtorandomchance( note: thedenominatoris ùëùùëã¬πùë•¬∫ùëùùëå¬πùë¶¬∫
which is the probability of the two outcomes were independent), whereas if it is large and
1028 Mathematics for Deep Learning
negativeitrepresentsthetwooutcomeshappeningfarlessthanwewouldexpectbyrandom
chance.
This allows us to interpret the mutual information (A.18 )as the average amount that we
were surprised to see two outcomes occurring together compared to what we would expect
if they were independent.
Applicationsof Mutual Information
Mutual information may be a little abstract in it pure definition, so how does it related to
machinelearning? Innaturallanguageprocessing, oneofthemostdifficultproblemsisthe
ambiguity resolution , or the issue of the meaning of a word being unclear from context.
For example, recently a headline in the news reported that ‚ÄúAmazon is on fire‚Äù. You may
wonder whether the company Amazon has a building on fire, or the Amazon rain forest is
on fire.
In this case, mutual information can help us resolve this ambiguity. We first find the group
of words that each has a relatively large mutual information with the company Amazon,
such as e-commerce, technology, and online. Second, we find another group of words that
each has a relatively large mutual information with the Amazon rain forest, such as rain,
forest, and tropical. When we need to disambiguate ‚ÄúAmazon‚Äù, we can compare which
group has more occurrence in the context of the word Amazon. In this case the article
would go on to describe the forest, and make the context clear.
A.11.4Kullback‚ÄìLeiblerDivergence
As what we have discussed in Section 2.3 , we can use norms to measure distance between
two points in space of any dimensionality. We would like to be able to do a similar task
withprobabilitydistributions. Therearemanywaystogoaboutthis,butinformationtheory
provides one of the nicest. We now explore the Kullback‚ÄìLeibler (KL) divergence , which
provides a way to measure if two distributions are close together or not.
Definition
Given a random variable ùëãthat follows the probability distribution ùëÉwith a p.d.f. or a
p.m.f.ùëù¬πùë•¬∫, and we estimate ùëÉby another probability distribution ùëÑwith a p.d.f. or a
p.m.f.ùëû¬πùë•¬∫. Then the Kullback‚ÄìLeibler (KL) divergence (orrelative entropy ) betweenùëÉ
andùëÑis
ùê∑KL¬πùëÉkùëÑ¬∫=ùê∏ùë•ùëÉ
logùëù¬πùë•¬∫
ùëû¬πùë•¬∫
. (A.21)
Aswiththepointwisemutualinformation (A.20 ),wecanagainprovideaninterpretationof
the logarithmic term:  logùëû¬πùë•¬∫
ùëù¬πùë•¬∫= log¬πùëû¬πùë•¬∫¬∫ ¬π  log¬πùëù¬πùë•¬∫¬∫¬∫will be large and positive
if we seeùë•far more often under ùëÉthan we would expect for ùëÑ, and large and negative if
we see the outcome far less than expected. In this way, we can interpret it as our relative
surprise at observing the outcome compared to how surprised we would be observing it
from our reference distribution.
1029 Information Theory
Let‚Äôs implement the KL divergence from Scratch.
def kl_divergence (p, q):
kl=p*torch .log2(p /q)
out =nansum(kl)
return out.abs() .item()
KL DivergenceProperties
Let‚Äôs take a look at some properties of the KL divergence (A.21 ).
KL divergence is non-symmetric, i.e., there are ùëÉ,ùëÑsuch that
ùê∑KL¬πùëÉkùëÑ¬∫‚â†ùê∑KL¬πùëÑkùëÉ¬∫. (A.22)
KL divergence is non-negative, i.e.,
ùê∑KL¬πùëÉkùëÑ¬∫0. (A.23)
Note that the equality holds only when ùëÉ=ùëÑ.
If there exists an ùë•such thatùëù¬πùë•¬∫>0andùëû¬πùë•¬∫=0, thenùê∑KL¬πùëÉkùëÑ¬∫=1.
There is a close relationship between KL divergence and mutual information. Besides
the relationship shown in Fig. A.1 ,ùêº¬πùëã,ùëå¬∫is also numerically equivalent with the
following terms:
1.ùê∑KL¬πùëÉ¬πùëã,ùëå¬∫kùëÉ¬πùëã¬∫ùëÉ¬πùëå¬∫¬∫;
2.ùê∏ùëåfùê∑KL¬πùëÉ¬πùëãjùëå¬∫kùëÉ¬πùëã¬∫¬∫g;
3.ùê∏ùëãfùê∑KL¬πùëÉ¬πùëåjùëã¬∫kùëÉ¬πùëå¬∫¬∫g.
For the first term, we interpret mutual information as the KL divergence between
ùëÉ¬πùëã,ùëå¬∫and the product of ùëÉ¬πùëã¬∫andùëÉ¬πùëå¬∫, and thus is a measure of how differ-
ent the joint distribution is from the distribution if they were independent. For the
second term, mutual information tells us the average reduction in uncertainty about ùëå
thatresultsfromlearningthevalueofthe ùëã‚Äôsdistribution. Similarlytothethirdterm.
Example
Let‚Äôs go through a toy example to see the non-symmetry explicitly.
First, let‚Äôs generate and sort three tensors of length 10,000: an objective tensor ùëùwhich
follows a normal distribution ùëÅ¬π0,1¬∫, and two candidate tensors ùëû1andùëû2which follow
normal distributions ùëÅ¬π 1,1¬∫andùëÅ¬π1,1¬∫respectively.
1030 Mathematics for Deep Learning
torch .manual_seed( 1)
tensor_len =10000
p=torch .normal( 0,1, (tensor_len, ))
q1=torch .normal( -1,1, (tensor_len, ))
q2=torch .normal( 1,1, (tensor_len, ))
p=torch .sort(p)[ 0]
q1=torch .sort(q1)[ 0]
q2=torch .sort(q2)[ 0]
Sinceùëû1andùëû2are symmetric with respect to the y-axis (i.e., ùë•=0), we expect a similar
value of KL divergencebetween ùê∑KL¬πùëùkùëû1¬∫andùê∑KL¬πùëùkùëû2¬∫. As youcan see below, there
is only a less than 3% off between ùê∑KL¬πùëùkùëû1¬∫andùê∑KL¬πùëùkùëû2¬∫.
kl_pq1 =kl_divergence(p, q1)
kl_pq2 =kl_divergence(p, q2)
similar_percentage =abs(kl_pq1 -kl_pq2) /((kl_pq1 +kl_pq2) /2)*100
kl_pq1, kl_pq2, similar_percentage
(8582.0341796875 ,8828.3095703125 ,2.8290698237936858 )
In contrast, you may find that ùê∑KL¬πùëû2kùëù¬∫andùê∑KL¬πùëùkùëû2¬∫are off a lot, with around 40%
off as shown below.
kl_q2p =kl_divergence(q2, p)
differ_percentage =abs(kl_q2p -kl_pq2) /((kl_q2p +kl_pq2) /2)*100
kl_q2p, differ_percentage
(14130.125 ,46.18621024399691 )
A.11.5Cross-Entropy
Ifyouarecuriousaboutapplicationsofinformationtheoryindeeplearning,hereisaquick
example. We define the true distribution ùëÉwith probability distribution ùëù¬πùë•¬∫, and the
estimated distribution ùëÑwith probability distribution ùëû¬πùë•¬∫, and we will use them in the
rest of this section.
Say we need to solve a binary classification problem based on given ùëõdata examples
{ùë•1,...,ùë•ùëõ}. Assume that we encode 1and 0as the positive and negative class label ùë¶ùëñ
respectively, and our neural network is parametrized by ùúÉ. If we aim to find a best ùúÉso
that ÀÜùë¶ùëñ=ùëùùúÉ¬πùë¶ùëñjùë•ùëñ¬∫, it is natural to apply the maximum log-likelihood approach as was
seen inSection A.7 . To be specific, for true labels ùë¶ùëñand predictions ÀÜùë¶ùëñ=ùëùùúÉ¬πùë¶ùëñjùë•ùëñ¬∫, the
probability to be classified as positive is ùúãùëñ=ùëùùúÉ¬πùë¶ùëñ=1jùë•ùëñ¬∫. Hence, the log-likelihood
1031 Information Theory
function would be
ùëô¬πùúÉ¬∫=logùêø¬πùúÉ¬∫
=logùëõ√ñ
ùëñ=1ùúãùë¶ùëñ
ùëñ¬π1 ùúãùëñ¬∫1 ùë¶ùëñ
=ùëõ√ï
ùëñ=1ùë¶ùëñlog¬πùúãùëñ¬∫¬∏¬π 1 ùë¶ùëñ¬∫log¬π1 ùúãùëñ¬∫.(A.24)
Maximizing the log-likelihood function ùëô¬πùúÉ¬∫is identical to minimizing  ùëô¬πùúÉ¬∫, and hence
we can find the best ùúÉfrom here. To generalize the above loss to any distributions, we also
called ùëô¬πùúÉ¬∫thecross-entropyloss CE¬πùë¶,ÀÜùë¶¬∫, whereùë¶follows the true distribution ùëÉand ÀÜùë¶
follows the estimated distribution ùëÑ.
This was all derived by working from the maximum likelihood point of view. However, if
welookcloselywecanseethattermslike log¬πùúãùëñ¬∫haveenteredintoourcomputationwhich
is a solid indication that we can understand the expression from an information theoretic
point of view.
FormalDefinition
LikeKLdivergence, forarandomvariable ùëã, wecanalsomeasurethedivergencebetween
the estimating distribution ùëÑand the true distribution ùëÉviacross-entropy ,
CE¬πùëÉ,ùëÑ¬∫= ùê∏ùë•ùëÉ¬ªlog¬πùëû¬πùë•¬∫¬∫¬º. (A.25)
By using properties of entropy discussed above, we can also interpret it as the summation
of the entropy ùêª¬πùëÉ¬∫and the KL divergence between ùëÉandùëÑ, i.e.,
CE¬πùëÉ,ùëÑ¬∫=ùêª¬πùëÉ¬∫¬∏ùê∑KL¬πùëÉkùëÑ¬∫. (A.26)
We can implement the cross-entropy loss as below.
def cross_entropy (y_hat, y):
ce=-torch .log(y_hat[ range (len(y_hat)), y])
return ce.mean()
Now define two tensors for the labels and predictions, and calculate the cross-entropy loss
of them.
labels =torch .tensor([ 0,2])
preds =torch .tensor([[ 0.3,0.6,0.1], [ 0.2,0.3,0.5]])
cross_entropy(preds, labels)
tensor( 0.9486 )
1032 Mathematics for Deep Learning
Properties
Asalludedinthebeginningofthissection,cross-entropy (A.25 )canbeusedtodefinealoss
function in the optimization problem. It turns out that the following are equivalent:
1.Maximizing predictive probability of ùëÑfor distribution ùëÉ, (i.e.,ùê∏ùë•ùëÉ¬ªlog¬πùëû¬πùë•¬∫¬∫¬º);
2.Minimizing cross-entropy CE ¬πùëÉ,ùëÑ¬∫;
3.Minimizing the KL divergence ùê∑KL¬πùëÉkùëÑ¬∫.
The definition of cross-entropy indirectly proves the equivalent relationship between ob-
jective 2 and objective 3, as long as the entropy of true data ùêª¬πùëÉ¬∫is constant.
Cross-Entropyas An ObjectiveFunction of Multi-class Classification
If we dive deep into the classification objective function with cross-entropy loss CE, we
willfind minimizing CE is equivalentto maximizing the log-likelihoodfunction ùêø.
To begin with, suppose that we are given a dataset with ùëõexamples, and it can be classified
intoùëò-classes. For each data example ùëñ, we represent any ùëò-class label yùëñ=¬πùë¶ùëñ1,...,ùë¶ùëñùëò¬∫
byone-hot encoding . To be specific, if the example ùëñbelongs to class ùëó, then we set the
ùëó-th entry to 1, and all other components to 0, i.e.,
ùë¶ùëñùëó=(
1ùëó2ùêΩ;
0otherwise.(A.27)
Forinstance,ifamulti-classclassificationproblemcontainsthreeclasses ùê¥,ùêµ,andùê∂,then
the labels yùëñcan be encoded in { ùê¥:¬π1,0,0¬∫;ùêµ:¬π0,1,0¬∫;ùê∂:¬π0,0,1¬∫}.
Assume that our neural network is parametrized by ùúÉ. For true label vectors yùëñand predic-
tions
ÀÜyùëñ=ùëùùúÉ¬πyùëñjxùëñ¬∫=ùëò√ï
ùëó=1ùë¶ùëñùëóùëùùúÉ¬πùë¶ùëñùëójxùëñ¬∫. (A.28)
Hence, the cross-entropyloss would be
CE¬πy,ÀÜy¬∫= ùëõ√ï
ùëñ=1yùëñlog ÀÜyùëñ= ùëõ√ï
ùëñ=1ùëò√ï
ùëó=1ùë¶ùëñùëólogùëùùúÉ¬πùë¶ùëñùëójxùëñ¬∫. (A.29)
On the other side, we can also approach the problem through maximum likelihood es-
timation. To begin with, let‚Äôs quickly introduce a ùëò-class multinoulli distribution. It is
an extension of the Bernoulli distribution from binary class to multi-class. If a random
variable z=¬πùëß1,...,ùëßùëò¬∫follows aùëò-classmultinoulli distribution with probabilities p=
(ùëù1,...,ùëùùëò), i.e.,
ùëù¬πz¬∫=ùëù¬πùëß1,...,ùëßùëò¬∫=Multi¬πùëù1,...,ùëùùëò¬∫,whereùëò√ï
ùëñ=1ùëùùëñ=1, (A.30)
1033 Information Theory
then the joint probability mass function(p.m.f.) of zis
pz=ùëò√ñ
ùëó=1ùëùùëßùëó
ùëó. (A.31)
It can be seen that the label of each data example, yùëñ, is following a ùëò-class multinoulli
distribution with probabilities ùùÖ=(ùúã1,...,ùúãùëò). Therefore, the joint p.m.f. of each data
example yùëñis√üyùëñ=√éùëò
ùëó=1ùúãùë¶ùëñ ùëó
ùëó.Hence, the log-likelihood function would be
ùëô¬πùúÉ¬∫=logùêø¬πùúÉ¬∫=logùëõ√ñ
ùëñ=1ùùÖyùëñ=logùëõ√ñ
ùëñ=1ùëò√ñ
ùëó=1ùúãùë¶ùëñ ùëó
ùëó=ùëõ√ï
ùëñ=1ùëò√ï
ùëó=1ùë¶ùëñùëólogùúãùëó. (A.32)
Since in maximum likelihood estimation, we maximizing the objective function ùëô¬πùúÉ¬∫by
havingùúãùëó=ùëùùúÉ¬πùë¶ùëñùëójxùëñ¬∫. Therefore, for any multi-class classification, maximizing the
abovelog-likelihoodfunction ùëô¬πùúÉ¬∫isequivalenttominimizingtheCElossCE ¬πùë¶,ÀÜùë¶¬∫.
To test the above proof, let‚Äôs apply the built-in measure NegativeLogLikelihood . Using
the same labelsandpredsas in the earlier example, we will get the same numerical loss
as the previous example up to the 5 decimal place.
# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`
# and `nn.NLLLoss()`
nll_loss =NLLLoss()
loss =nll_loss(torch .log(preds), labels)
loss
tensor( 0.9486 )
A.11.6Summary
Information theory is a field of study about encoding, decoding, transmitting, and ma-
nipulating information.
Entropy is the unit to measure how much information is presented in different signals.
KL divergence can also measure the divergence between two distributions.
Cross-entropy can be viewed as an objective function of multi-class classification. Min-
imizing cross-entropy loss is equivalent to maximizing the log-likelihood function.
A.11.7Exercises
1.Verify that the card examples from the first section indeed have the claimed entropy.
2.ShowthattheKLdivergence ùê∑¬πùëùkùëû¬∫isnonnegativeforalldistributions ùëùandùëû. Hint:
use Jensen‚Äôs inequality, i.e., use the fact that  logùë•is a convex function.
3.Let‚Äôs compute the entropy from a few data sources:
1034 Mathematics for Deep Learning
290Assume that you are watching the output generated by a monkey at a typewriter. The
monkey presses any of the 44keys of the typewriter at random (you can assume
that it has not discovered any special keys or the shift key yet). How many bits of
randomness per character do you observe?
Being unhappy with the monkey, you replaced it by a drunk typesetter. It is able
to generate words, albeit not coherently. Instead, it picks a random word out of
a vocabulary of 2,000words. Let‚Äôs assume that the average length of a word is
4.5letters in English. How many bits of randomness per character do you observe
now?
Still being unhappy with the result, you replace the typesetter by a high quality lan-
guage model. The language model can currently obtain a perplexity as low as 15
points per word. The character perplexity of a language model is defined as the
inverse of the geometric mean of a set of probabilities, each probability is corre-
sponding to a character in the word. To be specific, if the length of a given word is
ùëô, then PPL¬πword¬∫=¬ª√é
ùëñùëù¬πcharacterùëñ¬∫¬º 1
ùëô=exp
 1
ùëô√ç
ùëñlogùëù¬πcharacterùëñ¬∫
.As-
sume that the test word has 4.5 letters, how many bits of randomness per character
do you observe now?
4.Explain intuitively why ùêº¬πùëã,ùëå¬∫=ùêª¬πùëã¬∫ ùêª¬πùëãjùëå¬∫. Then, show this is true by
expressing both sides as an expectation with respect to the joint distribution.
5.What is the KL Divergence between the two Gaussian distributions N¬πùúá1,ùúé2
1¬∫and
N¬πùúá2,ùúé2
2¬∫?
Discussions290.
291
B Tools for Deep Learning
To get the most out of DiveintoDeepLearning , we will talk you through different tools in
thisappendix,suchasforrunningandcontributingtothisinteractiveopen-sourcebook.
B.1UsingJupyterNotebooks
This section describes how to edit and run the code in each section of this book using
the Jupyter Notebook. Make sure you have installed Jupyter and downloaded the code as
described in Installation (page xxxiv). If you want to know more about Jupyter see the
excellent tutorial in their documentation291.
B.1.1Editingand Runningthe Code Locally
Suppose that the local path of the book‚Äôs code is xx/yy/d2l-en/ . Use the shell to change
the directory to this path ( cd xx/yy/d2l-en ) and run the command jupyter notebook .
If your browser does not do this automatically, open http://localhost:8888 and you will see
theinterfaceofJupyterandallthefolderscontainingthecodeofthebook, asshownin Fig.
B.1.
tFig. B.1 The folders containing the code of this book.
Youcanaccessthenotebookfilesbyclickingonthefolderdisplayedonthewebpage. They
usuallyhavethesuffix‚Äú.ipynb‚Äù. Forthesakeofbrevity,wecreateatemporary‚Äútest.ipynb‚Äù
file. The content displayed after you click it is shown in Fig. B.2. This notebook includes a
1035
1036 Tools for Deep Learning
markdowncellandacodecell. Thecontentinthemarkdowncellincludes‚ÄúThisIsaTitle‚Äù
and ‚ÄúThis is text.‚Äù. The code cell contains two lines of Python code.
tFig. B.2 Markdown and code cells in the ‚Äútext.ipynb‚Äù Ô¨Åle.
Doubleclickonthemarkdowncelltoentereditmode. Addanewtextstring‚ÄúHelloworld.‚Äù
at the end of the cell, as shown in Fig. B.3.
tFig. B.3 Edit the markdown cell.
As demonstrated in Fig. B.4, click ‚ÄúCell‚Äù!‚ÄúRun Cells‚Äù in the menu bar to run the edited
cell.
After running, the markdown cell is shown in Fig. B.5.
Next, clickonthecodecell. Multiplytheelementsby2afterthelastlineofcode, asshown
inFig. B.6.
You can also run the cell with a shortcut (‚ÄúCtrl + Enter‚Äù by default) and obtain the output
result from Fig. B.7.
Whenanotebookcontainsmorecells,wecanclick‚ÄúKernel‚Äù !‚ÄúRestart&RunAll‚Äùinthe
menu bar to run all the cells in the entire notebook. By clicking ‚ÄúHelp‚Äù !‚ÄúEdit Keyboard
Shortcuts‚Äùinthemenubar,youcanedittheshortcutsaccordingtoyourpreferences.
1037 B.1 Using Jupyter Notebooks
tFig. B.4 Run the cell.
tFig. B.5 The markdown cell after running.
tFig. B.6 Edit the code cell.
1038 Tools for Deep Learning
tFig. B.7 Run the code cell to obtain the output.
B.1.2AdvancedOptions
Beyondlocaleditingtwothingsarequiteimportant: editingthenotebooksinthemarkdown
formatandrunningJupyterremotely. Thelattermatterswhenwewanttorunthecodeona
fasterserver. TheformermatterssinceJupyter‚Äôsnativeipynbformatstoresalotofauxiliary
data that is irrelevant to the content, mostly related to how and where the code is run. This
is confusing for Git, making reviewing contributions very difficult. Fortunately there is an
alternative‚Äînative editing in the markdown format.
MarkdownFiles in Jupyter
Ifyouwishtocontributetothecontentofthisbook,youneedtomodifythesourcefile(md
file, notipynbfile)onGitHub. Usingthenotedownpluginwecanmodifynotebooksinthe
md format directly in Jupyter.
First, install the notedown plugin, run the Jupyter Notebook, and load the plugin:
pip install d2l -notedown # You may need to uninstall the original notedown.
jupyter notebook --NotebookApp .contents_manager_class ='notedown.
‚Ü©!NotedownContentsManager '
You may also turn on the notedown plugin by default whenever you run the Jupyter Note-
book. First,generateaJupyterNotebookconfigurationfile(ifithasalreadybeengenerated,
you can skip this step).
jupyter notebook --generate -config
Then,addthefollowinglinetotheendoftheJupyterNotebookconfigurationfile(forLinux
or macOS, usually in the path ~/.jupyter/jupyter_notebook_config.py ):
1039 Using Jupyter Notebooks
292c.NotebookApp .contents_manager_class ='notedown.NotedownContentsManager '
After that, you only need to run the jupyter notebook command to turn on the notedown
plugin by default.
RunningJupyterNotebookson a RemoteServer
Sometimes,youmaywanttorunJupyternotebooksonaremoteserverandaccessitthrough
a browser on your local computer. If Linux or macOS is installed on your local machine
(Windowscanalsosupportthisfunctionthroughthird-partysoftwaresuchasPuTTY),you
can use port forwarding:
ssh myserver -L8888 :localhost: 8888
The above string myserver is the address of the remote server. Then we can use http:
//localhost:8888 to access the remote server myserver that runs Jupyter notebooks. We
willdetailonhowtorunJupyternotebooksonAWSinstanceslaterinthisappendix.
Timing
We can use the ExecuteTime plugin to time the execution of each code cell in Jupyter
notebooks. Use the following commands to install the plugin:
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
jupyter nbextension enable execute_time /ExecuteTime
B.1.3Summary
Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the
book.
We can run Jupyter notebooks on remote servers using port forwarding.
B.1.4Exercises
1.Edit and run the code in this book with the Jupyter Notebook on your local machine.
2.EditandrunthecodeinthisbookwiththeJupyterNotebook remotely viaportforward-
ing.
3.Compare the running time of the operations A>BandABfor two square matrices in
R10241024. Which one is faster?
Discussions292.
1040 Tools for Deep Learning
293B.2Using Amazon SageMaker
Deep learning applications may demand so much computational resource that easily goes
beyond what your local machine can offer. Cloud computing services allow you to run
GPU-intensivecodeofthisbookmoreeasilyusingmorepowerfulcomputers. Thissection
will introduce how to use Amazon SageMaker to run the code of this book.
B.2.1SigningUp
First, we need to sign up an account at https://aws.amazon.com/ . For additional security,
usingtwo-factorauthenticationisencouraged. Itisalsoagoodideatosetupdetailedbilling
and spending alerts to avoid any surprise, e.g., when forgetting to stop running instances.
After logging into your AWS account, go to your console293and search for ‚ÄúAmazon
SageMaker‚Äù (see Fig. B.1), then click it to open the SageMaker panel.
tFig. B.1 Search for and open the SageMaker panel.
B.2.2Creatinga SageMakerInstance
Next, let‚Äôs create a notebook instance as described in Fig. B.2.
tFig. B.2 Create a SageMaker instance.
1041 Using Amazon SageMaker
294SageMaker provides multiple instance types294with varying computational power and
prices. When creating a notebook instance, we can specify its name and type. In Fig. B.3,
we choose ml.p3.2xlarge : with one Tesla V100 GPU and an 8-core CPU, this instance is
powerful enough for most of the book.
tFig. B.3 Choose the instance type.
The entire book in the ipynb format for running with SageMaker is available at https://
github.com/d2l-ai/d2l-pytorch-sagemaker . We can specify this GitHub repository URL
(Fig. B.4) to allow SageMaker to clone it when creating the instance.
tFig. B.4 Specify the GitHub repository.
B.2.3Runningand Stopping an Instance
Creatinganinstancemaytakeafewminutes. Whenitisready,clickonthe‚ÄúOpenJupyter‚Äù
link next to it ( Fig. B.5) so you can edit and run all the Jupyter notebooks of this book on
this instance (similar to steps in Section B.1 ).
tFig. B.5 Open Jupyter on the created SageMaker instance.
After finishing your work, do not forget to stop the instance to avoid being charged further
(Fig. B.6).
1042 Tools for Deep Learning
tFig. B.6 Stop a SageMaker instance.
295B.2.4UpdatingNotebooks
Notebooks of this open-source book will be regularly updated in the d2l-ai/d2l-pytorch-
sagemaker295repository on GitHub. To update to the latest version, you may open a
terminal on the SageMaker instance ( Fig. B.7).
tFig. B.7 Open a terminal on the SageMaker instance.
Youmaywish to commit yourlocal changesbeforepulling updates fromthe remote repos-
itory. Otherwise, simply discard all your local changes with the following commands in
the terminal:
cdSageMaker/d2l-pytorch-sagemaker/
git reset --hard
git pull
B.2.5Summary
WecancreateanotebookinstanceusingAmazonSageMakertorunGPU-intensivecode
of this book.
We can update notebooks via the terminal on the Amazon SageMaker instance.
B.2.6Exercises
1.Edit and run any section that requires a GPU using Amazon SageMaker.
1043 Using AWS EC2 Instances
2962.Open a terminal to access the local directory that hosts all the notebooks of this book.
Discussions296.
B.3Using AWSEC2 Instances
Inthissection,wewillshowyouhowtoinstallalllibrariesonarawLinuxmachine. Recall
thatinSectionB.2 wediscussedhowtouseAmazonSageMaker,whilebuildinganinstance
by yourself costs less on AWS. The walkthrough includes three steps:
1.Request for a GPU Linux instance from AWS EC2.
2.Install CUDA (or use an Amazon Machine Image with preinstalled CUDA).
3.Installthedeeplearningframeworkandotherlibrariesforrunningthecodeofthebook.
This process applies to other instances (and other clouds), too, albeit with some minor
modifications. Before going forward, you need to create an AWS account, see Section B.2
for more details.
B.3.1Creating and Runningan EC2 Instance
AfterloggingintoyourAWSaccount,click‚ÄúEC2‚Äù( Fig.B.1)togototheEC2panel.
tFig. B.1 Open the EC2 console.
Fig. B.2 shows the EC2 panel.
PresettingLocation
Select a nearby data center to reduce latency, e.g., ‚ÄúOregon‚Äù (marked by the red box in the
top-right of Fig. B.2). If you are located in China, you can select a nearby Asia Pacific
1044 Tools for Deep Learning
tFig. B.2 The EC2 panel.
region, such as Seoul or Tokyo. Please note that some data centers may not have GPU
instances.
IncreasingLimits
Before choosing an instance, check if there are quantity restrictions by clicking the ‚ÄúLim-
its‚Äù label in the bar on the left as shown in Fig. B.2.Fig. B.3 shows an example of such
a limitation. The account currently cannot open ‚Äúp2.xlarge‚Äù instances according to the re-
gion. If you need to open one or more instances, click on the ‚ÄúRequest limit increase‚Äù link
to apply for a higher instance quota. Generally, it takes one business day to process an
application.
tFig. B.3 Instance quantity restrictions.
Launchingan Instance
Next, click the ‚ÄúLaunch Instance‚Äù button marked by the red box in Fig. B.2 to launch your
instance.
WebeginbyselectingasuitableAmazonMachineImage(AMI).SelectanUbuntuinstance
(Fig. B.4).
EC2 provides many different instance configurations to choose from. This can sometimes
feel overwhelming to a beginner. tab_ec2 lists different suitable machines.
:Different EC2 instance types
1045 Using AWS EC2 Instances
tFig. B.4 Choose an AMI.
297
298Table B.1: label: tab_ec2
Name GPU Notes
g2 Grid K520 ancient
p2 Kepler K80 old but often cheap as spot
g3 Maxwell M60 good trade-off
p3 Volta V100 high performance for FP16
p4 Ampere A100 high performance for large-scale training
g4 Turing T4 inference optimized FP16/INT8
All these servers come in multiple flavors indicating the number of GPUs used. For exam-
ple, a p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more
details, see the AWS EC2 documentation297or asummary page298. For the purpose of
illustration, a p2.xlarge will suffice (marked in the red box of Fig. B.5).
tFig. B.5 Choose an instance.
1046 Tools for Deep Learning
Note that youshould use a GPU-enabled instancewith suitable driversand a GPU-enabled
deeplearningframework. OtherwiseyouwillnotseeanybenefitfromusingGPUs.
We go on to select the key pair used to access the instance. If you do not have a key pair,
click ‚ÄúCreate new key pair‚Äù in Fig. B.6 to generate a key pair. Subsequently, you can select
the previously generated key pair. Make sure that you download the key pair and store
it in a safe location if you generated a new one. This is your only way to SSH into the
server.
tFig. B.6 Select a key pair.
In this example, we will keep the default configurations for ‚ÄúNetwork settings‚Äù (click the
‚ÄúEdit‚Äù button to configure items such as the subnet and security groups). We just increase
the default hard disk size to 64 GB ( Fig. B.7). Note that CUDA by itself already takes up
4 GB.
tFig. B.7 Modify the hard disk size.
Click‚ÄúLaunchInstance‚Äùtolaunchthecreatedinstance. ClicktheinstanceIDshownin Fig.
B.8to view the status of this instance.
Connecting to the Instance
Asshownin Fig.B.9,aftertheinstancestateturnsgreen,right-clicktheinstanceandselect
Connect to view the instance access method.
Ifthisisanewkey,itmustnotbepubliclyviewableforSSHtowork. Gotothefolderwhere
1047 Using AWS EC2 Instances
tFig. B.8 Click the instance ID.
tFig. B.9 View the instance access method.
you store D2L_key.pem and execute the following command to make the key not publicly
viewable:
chmod 400 D2L_key.pem
tFig. B.10 View instance access and startup method.
Now,copytheSSHcommandinthelowerredboxof Fig.B.10 andpasteontothecommand
line:
1048 Tools for Deep Learning
299ssh -i"D2L_key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com
Whenthecommandlineprompts‚ÄúAreyousureyouwanttocontinueconnecting(yes/no)‚Äù,
enter ‚Äúyes‚Äù and press Enter to log into the instance.
Your server is ready now.
B.3.2Installing CUDA
Before installing CUDA, be sure to update the instance with the latest drivers.
sudo apt-get update &&sudo apt-get install -ybuild-essential git libgfortran3
HerewedownloadCUDA12.1. VisitNVIDIA‚Äôs officialrepository299tofindthedownload
link as shown in Fig. B.11 .
tFig. B.11 Find the CUDA 12.1 download address.
Copy the instructions and paste them onto the terminal to install CUDA 12.1.
# The link and file name are subject to changes
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_
‚Ü©!64/cuda-ubuntu2204.pin
sudo mvcuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_
‚Ü©!installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -icuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
(continues on next page)
1049 Using AWS EC2 Instances
(continued from previous page)
sudo cp/var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/
‚Ü©!keyrings/
sudo apt-get update
sudo apt-get -yinstall cuda
After installing the program, run the following command to view the GPUs:
nvidia-smi
Finally, add CUDA to the library path to help other libraries find it, such as appending the
following lines to the end of ~/.bashrc .
export PATH ="/usr/local/cuda-12.1/bin: $PATH "
export LD_LIBRARY_PATH =${LD_LIBRARY_PATH }:/usr/local/cuda-12.1/lib64
B.3.3Installing Libraries forRunningthe Code
Torunthecodeofthisbook,justfollowstepsin Installation (pagexxxiv)forLinuxuserson
the EC2 instance and use the following tips for working on a remote Linux server:
TodownloadthebashscriptontheMinicondainstallationpage,rightclickthedownload
link and select ‚ÄúCopy Link Address‚Äù, then execute wget [copied link address] .
After running ~/miniconda3/bin/conda init , you may execute source ~/.bashrc
instead of closing and reopening your current shell.
B.3.4Runningthe JupyterNotebookremotely
To run the Jupyter Notebook remotely you need to use SSH port forwarding. After all, the
server in the cloud does not havea monitor or keyboard. Forthis, log into your serverfrom
your desktop (or laptop) as follows:
# This command must be run in the local command line
ssh -i"/path/to/key.pem "ubuntu @ec2 -xx-xxx-xxx-xxx.y.compute .amazonaws .com -L‚ê£
‚Ü©!8889 :localhost: 8888
Next, go to the location of the downloaded code of this book on the EC2 instance, then
run:
conda activate d2l
jupyter notebook
Fig.B.12 showsthepossibleoutputafteryouruntheJupyterNotebook. Thelastrowisthe
URL for port 8888.
Since you used port forwarding to port 8889, copy the last row in the red box of Fig. B.12 ,
replace ‚Äú8888‚Äù with ‚Äú8889‚Äù in the URL, and open it in your local browser.
1050 Tools for Deep Learning
tFig. B.12 Output after running the Jupyter Notebook. The last row is the URL for port 8888.
300B.3.5Closing UnusedInstances
Ascloudservicesarebilledbythetimeofuse,youshouldcloseinstancesthatarenotbeing
used. Note that there are alternatives:
‚ÄúStopping‚Äù an instance means that you will be able to start it again. This is akin to
switching off the power for your regular server. However, stopped instances will still
be billed a small amount for the hard disk space retained.
‚ÄúTerminating‚Äù an instance will delete all data associated with it. This includes the disk,
hence you cannot start it again. Only do this if you know that you will not need it in
the future.
If you want to use the instance as a template for many more instances, right-click on the
examplein Fig.B.9 andselect‚ÄúImage‚Äù!‚ÄúCreate‚Äùtocreateanimageoftheinstance. Once
thisiscomplete,select‚ÄúInstanceState‚Äù !‚ÄúTerminate‚Äùtoterminatetheinstance. Thenext
time you want to use this instance, you can follow the steps in this section to create an
instance based on the saved image. The only difference is that, in ‚Äú1. Choose AMI‚Äù shown
inFig.B.4,youmustusethe‚ÄúMyAMIs‚Äùoptiononthelefttoselectyoursavedimage. The
created instance will retain the information stored on the image hard disk. For example,
you will not have to reinstall CUDA and other runtime environments.
B.3.6Summary
We can launch and stop instances on demand without having to buy and build our own
computer.
We need to install CUDA before using the GPU-enabled deep learning framework.
We can use port forwarding to run the Jupyter Notebook on a remote server.
B.3.7Exercises
1.The cloud offers convenience, but it does not come cheap. Find out how to launch spot
instances300to see how to reduce costs.
1051 Using Google Colab
301
3022.Experiment with different GPU servers. How fast are they?
3.Experiment with multi-GPU servers. How well can you scale things up?
Discussions301.
B.4UsingGoogle Colab
We introduced how to run this book on AWS in Section B.2 andSection B.3 . Another
option is running this book on Google Colab302if you have a Google account.
To run the code of a section on Colab, simply click the Colabbutton as shown in Fig.
B.1.
tFig. B.1 Run the code of a section on Colab
If it is your first time to run a code cell, you will receive a warning message as shown in
Fig. B.2. Just click ‚ÄúRUN ANYWAY‚Äù to ignore it.
tFig. B.2 Ignore the warning message by clicking ‚ÄúRUN ANYWAY‚Äù.
Next, Colab will connect you to an instance to run the code of this section. Specifically,
if a GPU is needed, Colab will be automatically requested for connecting to a GPU in-
stance.
B.4.1Summary
You can use Google Colab to run each section‚Äôs code in this book.
Colab will be requested to connect to a GPU instance if a GPU is needed in any section
of this book.
1052 Tools for Deep Learning
303
304
305B.4.2Exercises
1.Open any section of this book using Google Colab.
2.Edit and run any section that requires a GPU using Google Colab.
Discussions303.
B.5SelectingServersand GPUs
Deep learning training generally requires large amounts of computation. At present GPUs
arethemostcost-effectivehardwareacceleratorsfordeeplearning. Inparticular,compared
with CPUs, GPUs are cheaper and offer higher performance, often by over an order of
magnitude. Furthermore, a single server can support multiple GPUs, up to 8 for high end
servers. More typical numbers are up to 4 GPUs for an engineering workstation, since
heat, cooling, and power requirements escalate quickly beyond what an office building can
support. For larger deployments, cloud computing (e.g., Amazon‚Äôs P3304andG4305
instances) is a much more practical solution.
B.5.1Selecting Servers
There is typically no need to purchase high-end CPUs with many threads since much of
the computation occurs on the GPUs. That said, due to the global interpreter lock (GIL)
in Python single-thread performance of a CPU can matter in situations where we have 4‚Äì8
GPUs. AllthingsequalthissuggeststhatCPUswithasmallernumberofcoresbutahigher
clockfrequencymightbeamoreeconomicalchoice. Forexample,whenchoosingbetween
a 6-core 4 GHz and an 8-core 3.5 GHz CPU, the former is much preferable, even though
its aggregate speed is less. An important consideration is that GPUs use lots of power and
thus dissipate lots of heat. This requires very good cooling and a large enough chassis to
use the GPUs. Follow the guidelines below if possible:
1.Power Supply . GPUs use significant amounts of power. Budget with up to 350W per
device(checkforthe peakdemand ofthegraphicscardratherthantypicaldemand,since
efficient code can use lots of energy). If your power supply is not up to the demand you
will find that your system becomes unstable.
2.ChassisSize . GPUsarelargeandtheauxiliarypowerconnectorsoftenneedextraspace.
Also, large chassis are easier to cool.
3.GPU Cooling . If you have a large number of GPUs you might want to invest in water
cooling. Also, aim for reference designs even if they have fewer fans, since they are
thin enough to allow for air intake between the devices. If you buy a multi-fan GPU it
might be too thick to get enough air when installing multiple GPUs and you will run
into thermal throttling.
1053 Selecting Servers and GPUs
4.PCIe Slots . Moving data to and from the GPU (and exchanging it between GPUs)
requires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount
multipleGPUs,besuretocarefullyreadthemotherboarddescriptiontoensurethat16 
bandwidth is still available when multiple GPUs are used at the same time and that you
aregettingPCIe3.0asopposedtoPCIe2.0fortheadditionalslots. Somemotherboards
downgradeto8oreven4bandwidthwithmultipleGPUsinstalled. Thisispartlydue
to the number of PCIe lanes that the CPU offers.
In short, here are some recommendations for building a deep learning server:
Beginner . Buy a low end GPU with low power consumption (cheap gaming GPUs suit-
ablefordeeplearninguse150‚Äì200W).Ifyouareluckyyourcurrentcomputersupports
it.
1 GPU. A low-end CPU with 4 cores will be sufficient and most motherboards suffice.
Aim for at least 32 GB DRAM and invest into an SSD for local data access. A power
supply with 600W should be sufficient. Buy a GPU with lots of fans.
2GPUs. A low-end CPU with 4-6 cores will suffice. Aim for 64 GB DRAM and invest
into an SSD. You will need in the order of 1000W for two high-end GPUs. In terms
of mainboards, make sure that they have twoPCIe 3.0 x16 slots. If you can, get a
mainboard that has two free spaces (60mm spacing) between the PCIe 3.0 x16 slots
for extra air. In this case, buy two GPUs with lots of fans.
4GPUs. MakesurethatyoubuyaCPUwithrelativelyfastsingle-threadspeed(i.e.,high
clock frequency). You will probably need a CPU with a larger number of PCIe lanes,
such as an AMD Threadripper. You will likely need relatively expensive mainboards
toget4PCIe3.0x16slotssincetheyprobablyneedaPLXtomultiplexthePCIelanes.
BuyGPUswithreferencedesignthatarenarrowandletairinbetweentheGPUs. You
needa1600‚Äì2000Wpowersupplyandtheoutletinyourofficemightnotsupportthat.
This server will probably run loud and hot . You do not want it under your desk. 128
GB of DRAM is recommended. Get an SSD (1‚Äì2 TB NVMe) for local storage and a
bunch of hard disks in RAID configuration to store your data.
8GPUs. Youneedtobuyadedicatedmulti-GPUserverchassiswithmultipleredundant
power supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket
server CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended),
and you will need to check whether the servers support the physical form factor of
the GPUs. Airflow and wiring placement differ significantly between consumer and
server GPUs (e.g., RTX 2080 vs. Tesla V100). This means that you might not be able
toinstalltheconsumerGPUinaserverduetoinsufficientclearanceforthepowercable
or lack of a suitable wiring harness (as one of the coauthors painfully discovered).
B.5.2SelectingGPUs
Atpresent,AMDandNVIDIAarethetwomainmanufacturersofdedicatedGPUs. NVIDIA
was the first to enter the deep learning field and provides better support for deep learning
frameworks via CUDA. Therefore, most buyers choose NVIDIA GPUs.
1054 Tools for Deep Learning
NVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and
RTX series) and enterprise users (via its Tesla series). The two types of GPUs provide
comparable compute power. However, the enterprise user GPUs generally use (passive)
forced cooling, more memory, and ECC (error correcting) memory. These GPUs are more
suitable for data centers and usually cost ten times more than consumer GPUs.
Ifyouarealargecompanywith100+serversyoushouldconsidertheNVIDIATeslaseries
oralternativelyuseGPUserversinthecloud. Foralaborasmalltomediumcompanywith
10+ servers the NVIDIA RTX series is likely most cost effective. You can buy preconfig-
ured servers with Supermicro or Asus chassis that hold 4‚Äì8 GPUs efficiently.
GPU vendors typically release a new generation every one to two years, such as the GTX
1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019.
Each series offers several different models that provide different performance levels. GPU
performance is primarily a combination of the following three parameters:
1.Compute Power . Generally we look for 32-bit floating-point compute power. 16-bit
floatingpointtraining(FP16)isalsoenteringthemainstream. Ifyouareonlyinterested
inprediction,youcanalsouse8-bitinteger. ThelatestgenerationofTuringGPUsoffers
4-bit acceleration. Unfortunately at the time of writing the algorithms for training low-
precision networks are not yet widespread.
2.MemorySize . As your models become larger or the batches used during training grow
bigger,youwillneedmoreGPUmemory. CheckforHBM2(HighBandwidthMemory)
vs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive.
3.MemoryBandwidth . You can only get the most out of your compute power when you
have sufficient memory bandwidth. Look for wide memory buses if using GDDR6.
Formostusers,itisenoughtolookatcomputepower. NotethatmanyGPUsofferdifferent
types of acceleration. For example, NVIDIA‚Äôs TensorCores accelerate a subset of opera-
tors by 5. Ensure that your libraries support this. The GPU memory should be no less
than 4 GB (8 GB is much better). Try to avoid using the GPU also for displaying a GUI
(use the built-in graphics instead). If you cannot avoid it, add an extra 2 GB of RAM for
safety.
Fig. B.1 compares the 32-bit floating-point compute power and price of the various GTX
900, GTX 1000 and RTX 2000 series models. The prices suggested are those found on
Wikipedia at the time of writing.
We can see a number of things:
1.Withineachseries,priceandperformanceareroughlyproportional. Titanmodelscom-
mand a significant premium for the benefit of larger amounts of GPU memory. How-
ever, the newer models offer better cost effectiveness, as can be seen by comparing the
980Tiand1080Ti. ThepricedoesnotappeartoimprovemuchfortheRTX2000series.
However, this is due to the fact that they offer far superior low precision performance
(FP16, INT8, and INT4).
1055 Selecting Servers and GPUs
tFig. B.1 Floating-point compute power and price comparison.
2.The performance-to-cost ratio of the GTX 1000 series is about two times greater than
the 900 series.
3.FortheRTX2000seriestheperformance(inGFLOPs)isan aÔ¨Äinefunctionoftheprice.
tFig. B.2 Floating-point compute power and energy consumption.
Fig. B.2 shows how energy consumption scales mostly linearly with the amount of com-
1056 Tools for Deep Learning
306
307
308
309
310putation. Second, later generations are more efficient. This seems to be contradicted by
the graph corresponding to the RTX 2000 series. However, this is a consequence of the
TensorCores that draw disproportionately much energy.
B.5.3Summary
Watch out for power, PCIe bus lanes, CPU single thread speed, and cooling when build-
ing a server.
You should purchase the latest GPU generation if possible.
Use the cloud for large deployments.
High density servers may not be compatible with all GPUs. Check the mechanical and
cooling specifications before you buy.
Use FP16 or lower precision for high efficiency.
Discussions306.
B.6Contributing to This Book
Contributionsby readers307helpusimprovethisbook. Ifyoufindatypo,anoutdatedlink,
something where you think we missed a citation, where the code does not look elegant or
whereanexplanationisunclear,pleasecontributebackandhelpushelpourreaders. While
in regular books the delay between print runs (and thus between typo corrections) can be
measured in years, it typically takes hours to days to incorporate an improvement in this
book. Thisisallpossibleduetoversioncontrolandcontinuousintegration(CI)testing. To
dosoyouneedtosubmita pullrequest308totheGitHubrepository. Whenyourpullrequest
is merged into the code repository by the authors, you will become a contributor.
B.6.1Submitting Minor Changes
The most common contributions are editing one sentence or fixing typos. We recommend
that you find the source file in the GitHub repository309and edit the file directly. For
example, you can search the file through the Find file310button (Fig. B.1) to locate the
source file (a markdown file). Then you click the ‚ÄúEdit this file‚Äù button on the upper-right
corner to make your changes in the markdown file.
After you are done, fill in your change descriptions in the ‚ÄúPropose file change‚Äù panel on
the page bottom and then click the ‚ÄúPropose file change‚Äù button. It will redirect you to a
new page to review your changes ( Fig. B.7). If everything is good, you can submit a pull
request by clicking the ‚ÄúCreate pull request‚Äù button.
1057 Contributing to This Book
tFig. B.1 Edit the Ô¨Åle on Github.
311
312B.6.2ProposingMajorChanges
If you plan to update a large portion of text or code, then you need to know a little bit more
about the format this book is using. The source file is based on the markdown format311
with a set of extensions through the D2L-Book312package such as referring to equations,
images, chapters, and citations. You can use any markdown editors to open these files and
make your changes.
If you would like to change the code, we recommend that you use the Jupyter Notebook to
open these markdown files as described in Section B.1 , so that you can run and test your
changes. Please remember to clear all outputs before submitting your changes since our CI
system will execute the sections you updated to generate outputs.
Some sections may support multiple framework implementations. If you add a new code
block, please use %%tabto mark this block on the beginning line. For example, %%tab
pytorch for a PyTorch code block, %%tab tensorflow for a TensorFlow code block, or
%%tab all a shared code block for all implementations. You may refer to the d2lbook
package for more information.
B.6.3Submitting MajorChanges
We suggest you to use the standard Git process to submit a major change. In a nutshell the
process works as described in Fig. B.2.
tFig. B.2 Contributing to the book.
We will walk you through the steps in detail. If you are already familiar with Git you
can skip this section. For concreteness we assume that the contributor‚Äôs user name is ‚Äúas-
tonzhang‚Äù.
1058 Tools for Deep Learning
313
314
315Installing Git
The Git open-source book describes how to install Git313. This typically works via apt
install git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by
using GitHub‚Äôs desktop client314. If you do not have a GitHub account, you need to sign
up for one.
Logging in to GitHub
Enter the address315of the book‚Äôs code repository in your browser. Click on the Fork
button in the red box at the upper-right of Fig. B.3, to make a copy of the repository of this
book. This is now yourcopy and you can change it any way you want.
tFig. B.3 The code repository page.
Now, the code repository of this book will be forked (i.e., copied) to your username, such
asastonzhang/d2l-en shown at the upper-left of Fig. B.4.
tFig. B.4 The forked code repository.
Cloningthe Repository
To clone the repository (i.e., to make a local copy) we need to get its repository address.
The green button in Fig. B.5 displays this. Make sure that your local copy is up to date
with the main repository if you decide to keep this fork around for longer. For now simply
follow the instructions in Installation (page xxxiv) to get started. The main difference is
that you are now downloading yourownfork of the repository.
tFig. B.5 Cloning the repository.
1059 Contributing to This Book
# Replace your_github_username with your GitHub username
git clone https: //github .com/your_github_username /d2l-en.git
Editing and Pushing
Nowitistimetoeditthebook. ItisbesttoedititintheJupyterNotebookfollowinginstruc-
tions inSection B.1 . Make the changes and check that they are OK. Assume that we have
modified a typo in the file ~/d2l-en/chapter_appendix-tools-for-deep-learning/
contributing.md . You can then check which files you have changed.
At this point Git will prompt that the chapter_appendix-tools-for-deep-learning/
contributing.md file has been modified.
mylaptop:d2l-en me$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
(use "git add <file>..." to update what will be committed)
(use "git checkout -- <file>..." to discard changes in working directory)
modified: chapter_appendix-tools-for-deep-learning/contributing.md
After confirming that this is what you want, execute the following command:
git add chapter_appendix -tools -for-deep -learning /contributing .md
git commit -m'Fix a typo in git documentation '
git push
The changed code will then be in your personal fork of the repository. To request the
addition of your change, you have to create a pull request for the official repository of the
book.
SubmittingPull Requests
As shown in Fig. B.6, go to your fork of the repository on GitHub and select ‚ÄúNew pull
request‚Äù. This will open up a screen that shows you the changes between your edits and
what is current in the main repository of the book.
tFig. B.6 New pull request.
1060 Tools for Deep Learning
316
317Finally, submit a pull request by clicking the button as shown in Fig. B.7. Make sure to
describe the changes you have made in the pull request. This will make it easier for the
authors to review it and to merge it with the book. Depending on the changes, this might
getacceptedrightaway,rejected,ormorelikely,youwillgetsomefeedbackonthechanges.
Once you have incorporated them, you are good to go.
tFig. B.7 Create pull request.
B.6.4Summary
You can use GitHub to contribute to this book.
You can edit the file on GitHub directly for minor changes.
For a major change, please fork the repository, edit things locally, and only contribute
back once you are ready.
Pull requests are how contributions are being bundled up. Try not to submit huge pull
requestssincethismakesthemhardtounderstandandincorporate. Bettersendseveral
smaller ones.
B.6.5Exercises
1.Star and fork the d2l-ai/d2l-en repository.
2.If you spot anything that needs improvement (e.g., missing a reference), submit a pull
request.
3.It is usually a better practice to create a pull request using a new branch. Learn how to
do it with Git branching316.
Discussions317.
B.7UtilityFunctions and Classes
This section contains the implementations of utility functions and classes used in this
book.
1061 Utility Functions and Classes
import collections
import inspect
from IPython import display
from torch import nn
from d2l import torch asd2l
Hyperparameters.
@d2l .add_to_class(d2l .HyperParameters) #@save
def save_hyperparameters (self , ignore =[]):
"""Save function arguments into class attributes."""
frame =inspect .currentframe() .f_back
_, _, _, local_vars =inspect .getargvalues(frame)
self .hparams ={k:v for k, v inlocal_vars .items()
ifknot inset(ignore +['self '])and not k.startswith( '_')}
for k, v inself .hparams .items():
setattr (self , k, v)
Progress bar.
@d2l .add_to_class(d2l .ProgressBoard) #@save
def draw (self , x, y, label, every_n =1):
Point =collections .namedtuple( 'Point ', ['x','y'])
ifnot hasattr (self ,'raw_points '):
self .raw_points =collections .OrderedDict()
self .data =collections .OrderedDict()
iflabel not inself .raw_points:
self .raw_points[label] =[]
self .data[label] =[]
points =self .raw_points[label]
line =self .data[label]
points .append(Point(x, y))
iflen(points) !=every_n:
return
mean =lambda x:sum(x) /len(x)
line .append(Point(mean([p .xfor pinpoints]),
mean([p .yfor pinpoints])))
points .clear()
ifnot self .display:
return
d2l.use_svg_display()
ifself .fig isNone :
self .fig =d2l.plt.figure(figsize =self .figsize)
plt_lines, labels =[], []
for (k, v), ls, color inzip(self .data .items(), self .ls, self .colors):
plt_lines .append(d2l .plt.plot([p .xfor pinv], [p .yfor pinv],
linestyle =ls, color =color)[ 0])
labels .append(k)
axes =self .axes ifself .axes else d2l.plt.gca()
ifself .xlim: axes .set_xlim( self .xlim)
ifself .ylim: axes .set_ylim( self .ylim)
ifnot self .xlabel: self .xlabel =self .x
axes .set_xlabel( self .xlabel)
axes .set_ylabel( self .ylabel)
(continues on next page)
1062 Tools for Deep Learning
(continued from previous page)
axes .set_xscale( self .xscale)
axes .set_yscale( self .yscale)
axes .legend(plt_lines, labels)
display .display( self .fig)
display .clear_output(wait =True )
Add FrozenLake enviroment
def frozen_lake (seed): #@save
# See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ to ‚ê£
‚Ü©!learn more about this env
# How to process env.P.items is adpated from https://sites.google.com/view/
‚Ü©!deep-rl-bootcamp/labs
import gym
env =gym.make( 'FrozenLake-v1 ', is_slippery =False )
env.seed(seed)
env.action_space .np_random .seed(seed)
env.action_space .seed(seed)
env_info ={}
env_info[ 'desc ']=env.desc # 2D array specifying what each grid item ‚ê£
‚Ü©!means
env_info[ 'num_states ']=env.nS # Number of observations/states or obs/
‚Ü©!state dim
env_info[ 'num_actions ']=env.nA # Number of actions or action dim
# Define indices for (transition probability, nextstate, reward, done) ‚ê£
‚Ü©!tuple
env_info[ 'trans_prob_idx ']=0# Index of transition probability entry
env_info[ 'nextstate_idx ']=1# Index of next state entry
env_info[ 'reward_idx ']=2# Index of reward entry
env_info[ 'done_idx ']=3# Index of done entry
env_info[ 'mdp']={}
env_info[ 'env']=env
for (s, others) inenv.P.items():
# others(s) = {a0: [ (p(s'|s,a0), s', reward, done),...], a1:[...], ...
‚Ü©!}
for (a, pxrds) inothers .items():
# pxrds is [(p1,next1,r1,d1),(p2,next2,r2,d2),..].
# e.g. [(0.3, 0, 0, False), (0.3, 0, 0, False), (0.3, 4, 1, False)]
env_info[ 'mdp'][(s,a)] =pxrds
return env_info
Create enviroment
def make_env (name ='', seed =0):#@save
# Input parameters:
# name: specifies a gym environment.
# For Value iteration, only FrozenLake-v1 is supported.
ifname =='FrozenLake-v1 ':
return frozen_lake(seed)
(continues on next page)
1063 Utility Functions and Classes
(continued from previous page)
else :
raise ValueError ("%senv is not supported in this Notebook ")
Show value function
def show_value_function_progress (env_desc, V, pi): #@save
# This function visualizes how value and policy changes over time.
# V: [num_iters, num_states]
# pi: [num_iters, num_states]
# How to visualize value function is adapted (but changed) from: https://
‚Ü©!sites.google.com/view/deep-rl-bootcamp/labs
num_iters =V.shape[ 0]
fig, ax =plt.subplots(figsize =(15,15))
for kinrange (V.shape[ 0]):
plt.subplot( 4,4, k +1)
plt.imshow(V[k] .reshape( 4,4), cmap ="bone ")
ax=plt.gca()
ax.set_xticks(np .arange( 0,5)-.5, minor =True )
ax.set_yticks(np .arange( 0,5)-.5, minor =True )
ax.grid(which ="minor ", color ="w", linestyle ='-', linewidth =3)
ax.tick_params(which ="minor ", bottom =False , left =False )
ax.set_xticks([])
ax.set_yticks([])
# LEFT action: 0, DOWN action: 1
# RIGHT action: 2, UP action: 3
action2dxdy ={0:(-.25,0),1: (0,.25),
2:(0.25 ,0),3: (-.25,0)}
for yinrange (4):
for xinrange (4):
action =pi[k] .reshape( 4,4)[y, x]
dx, dy =action2dxdy[action]
ifenv_desc[y,x] .decode() =='H':
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="y",
size =20, fontweight ='bold ')
elif env_desc[y,x] .decode() =='G':
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="w",
size =20, fontweight ='bold ')
else :
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="g",
size =15, fontweight ='bold ')
# No arrow for cells with G and H labels
ifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=
(continues on next page)
1064 Tools for Deep Learning
(continued from previous page)
‚Ü©!'H':
ax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_
‚Ü©!length =0.15 )
ax.set_title( "Step = "+str(k+1), fontsize =20)
fig.tight_layout()
plt.show()
Show Q function
def show_Q_function_progress (env_desc, V_all, pi_all): #@save
# This function visualizes how value and policy changes over time.
# V: [num_iters, num_states]
# pi: [num_iters, num_states]
# We want to only shows few values
num_iters_all =V_all .shape[ 0]
num_iters =num_iters_all //10
vis_indx =np.arange( 0, num_iters_all, num_iters) .tolist()
vis_indx .append(num_iters_all -1)
V=np.zeros(( len(vis_indx), V_all .shape[ 1]))
pi=np.zeros(( len(vis_indx), V_all .shape[ 1]))
for c, i inenumerate (vis_indx):
V[c] =V_all[i]
pi[c] =pi_all[i]
num_iters =V.shape[ 0]
fig, ax =plt.subplots(figsize =(15,15))
for kinrange (V.shape[ 0]):
plt.subplot( 4,4, k +1)
plt.imshow(V[k] .reshape( 4,4), cmap ="bone ")
ax=plt.gca()
ax.set_xticks(np .arange( 0,5)-.5, minor =True )
ax.set_yticks(np .arange( 0,5)-.5, minor =True )
ax.grid(which ="minor ", color ="w", linestyle ='-', linewidth =3)
ax.tick_params(which ="minor ", bottom =False , left =False )
ax.set_xticks([])
ax.set_yticks([])
# LEFT action: 0, DOWN action: 1
# RIGHT action: 2, UP action: 3
action2dxdy ={0:(-.25,0),1:(0,.25),
2:(0.25 ,0),3:(-.25,0)}
for yinrange (4):
for xinrange (4):
action =pi[k] .reshape( 4,4)[y, x]
dx, dy =action2dxdy[action]
ifenv_desc[y,x] .decode() =='H':
(continues on next page)
1065 Utility Functions and Classes
(continued from previous page)
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="y",
size =20, fontweight ='bold ')
elif env_desc[y,x] .decode() =='G':
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="w",
size =20, fontweight ='bold ')
else :
ax.text(x, y, str(env_desc[y,x] .decode()),
ha="center ", va ="center ", color ="g",
size =15, fontweight ='bold ')
# No arrow for cells with G and H labels
ifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=
‚Ü©!'H':
ax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_
‚Ü©!length =0.15 )
ax.set_title( "Step = "+str(vis_indx[k] +1), fontsize =20)
fig.tight_layout()
plt.show()
Trainer
A bunch of functions that will be deprecated:
def load_array (data_arrays, batch_size, is_train =True ): #@save
"""Construct a PyTorch data iterator."""
dataset =torch .utils .data .TensorDataset( *data_arrays)
return torch .utils .data .DataLoader(dataset, batch_size, shuffle =is_train)
def synthetic_data (w, b, num_examples): #@save
"""Generate y = Xw + b + noise."""
X=torch .normal( 0,1, (num_examples, len(w)))
y=torch .matmul(X, w) +b
y+=torch .normal( 0,0.01 , y.shape)
return X, y .reshape(( -1,1))
def sgd(params, lr, batch_size): #@save
"""Minibatch stochastic gradient descent."""
with torch .no_grad():
for param inparams:
param -=lr*param .grad /batch_size
param .grad .zero_()
def get_dataloader_workers (): #@save
"""Use 4 processes to read the data."""
return 4
def load_data_fashion_mnist (batch_size, resize =None ): #@save
"""Download the Fashion-MNIST dataset and then load it into memory."""
(continues on next page)
1066 Tools for Deep Learning
(continued from previous page)
trans =[transforms .ToTensor()]
ifresize:
trans .insert( 0, transforms .Resize(resize))
trans =transforms .Compose(trans)
mnist_train =torchvision .datasets .FashionMNIST(
root ="../data ", train =True , transform =trans, download =True )
mnist_test =torchvision .datasets .FashionMNIST(
root ="../data ", train =False , transform =trans, download =True )
return (torch .utils .data .DataLoader(mnist_train, batch_size, shuffle =True ,
num_workers =get_dataloader_workers()),
torch .utils .data .DataLoader(mnist_test, batch_size, shuffle =False ,
num_workers =get_dataloader_workers()))
def evaluate_accuracy_gpu (net, data_iter, device =None ):#@save
"""Compute the accuracy for a model on a dataset using a GPU."""
ifisinstance (net, nn .Module):
net.eval() # Set the model to evaluation mode
ifnot device:
device =next (iter (net .parameters())) .device
# No. of correct predictions, no. of predictions
metric =d2l.Accumulator( 2)
with torch .no_grad():
for X, y indata_iter:
ifisinstance (X, list ):
# Required for BERT Fine-tuning (to be covered later)
X=[x.to(device) for xinX]
else :
X=X.to(device)
y=y.to(device)
metric .add(d2l .accuracy(net(X), y), y .numel())
return metric[ 0]/metric[ 1]
#@save
def train_ch6 (net, train_iter, test_iter, num_epochs, lr, device):
"""Train a model with a GPU (defined in Chapter 6)."""
def init_weights (m):
iftype (m) ==nn.Linear ortype (m) ==nn.Conv2d:
nn.init .xavier_uniform_(m .weight)
net.apply(init_weights)
print ('training on ', device)
net.to(device)
optimizer =torch .optim .SGD(net .parameters(), lr =lr)
loss =nn.CrossEntropyLoss()
animator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],
legend =['train loss ','train acc ','test acc '])
timer, num_batches =d2l.Timer(), len(train_iter)
for epoch inrange (num_epochs):
# Sum of training loss, sum of training accuracy, no. of examples
metric =d2l.Accumulator( 3)
net.train()
for i, (X, y) inenumerate (train_iter):
timer .start()
optimizer .zero_grad()
(continues on next page)
1067 Utility Functions and Classes
(continued from previous page)
X, y =X.to(device), y .to(device)
y_hat =net(X)
l=loss(y_hat, y)
l.backward()
optimizer .step()
with torch .no_grad():
metric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])
timer .stop()
train_l =metric[ 0]/metric[ 2]
train_acc =metric[ 1]/metric[ 2]
if(i+1)%(num_batches //5)==0ori==num_batches -1:
animator .add(epoch +(i+1)/num_batches,
(train_l, train_acc, None ))
test_acc =evaluate_accuracy_gpu(net, test_iter)
animator .add(epoch +1, (None ,None , test_acc))
print (f'loss {train_l :.3f}, train acc {train_acc :.3f},'
f'test acc {test_acc :.3f}')
print (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec '
f'on{str(device) }')
def show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save
"""Plot a list of images."""
figsize =(num_cols *scale, num_rows *scale)
_, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize)
axes =axes .flatten()
for i, (ax, img) inenumerate (zip(axes, imgs)):
try:
img =img.detach() .numpy()
except :
pass
ax.imshow(img)
ax.axes .get_xaxis() .set_visible( False )
ax.axes .get_yaxis() .set_visible( False )
iftitles:
ax.set_title(titles[i])
return axes
def linreg (X, w, b): #@save
"""The linear regression model."""
return torch .matmul(X, w) +b
def squared_loss (y_hat, y): #@save
"""Squared loss."""
return (y_hat -y.reshape(y_hat .shape)) **2/2
def get_fashion_mnist_labels (labels): #@save
"""Return text labels for the Fashion-MNIST dataset."""
text_labels =['t-shirt ','trouser ','pullover ','dress ','coat ',
'sandal ','shirt ','sneaker ','bag','ankle boot ']
return [text_labels[ int(i)] for iinlabels]
class Animator :#@save
"""For plotting data in animation."""
(continues on next page)
1068 Tools for Deep Learning
(continued from previous page)
def __init__ (self , xlabel =None , ylabel =None , legend =None , xlim =None ,
ylim =None , xscale ='linear ', yscale ='linear ',
fmts =('-','m--','g-.','r:'), nrows =1, ncols =1,
figsize =(3.5,2.5)):
# Incrementally plot multiple lines
iflegend isNone :
legend =[]
d2l.use_svg_display()
self .fig, self .axes =d2l.plt.subplots(nrows, ncols, figsize =figsize)
ifnrows *ncols ==1:
self .axes =[self .axes, ]
# Use a lambda function to capture arguments
self .config_axes =lambda : d2l .set_axes(
self .axes[ 0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self .X,self .Y,self .fmts =None ,None , fmts
def add(self , x, y):
# Add multiple data points into the figure
ifnot hasattr (y, "__len__ "):
y=[y]
n=len(y)
ifnot hasattr (x, "__len__ "):
x=[x] *n
ifnot self .X:
self .X=[[] for _inrange (n)]
ifnot self .Y:
self .Y=[[] for _inrange (n)]
for i, (a, b) inenumerate (zip(x, y)):
ifaisnot None and bisnot None :
self .X[i] .append(a)
self .Y[i] .append(b)
self .axes[ 0].cla()
for x, y, fmt inzip(self .X,self .Y,self .fmts):
self .axes[ 0].plot(x, y, fmt)
self .config_axes()
display .display( self .fig)
display .clear_output(wait =True )
class Accumulator :#@save
"""For accumulating sums over `n` variables."""
def __init__ (self , n):
self .data =[0.0]*n
def add(self ,*args):
self .data =[a+float (b) for a, b inzip(self .data, args)]
def reset (self ):
self .data =[0.0]*len(self .data)
def __getitem__ (self , idx):
return self .data[idx]
def accuracy (y_hat, y): #@save
"""Compute the number of correct predictions."""
(continues on next page)
1069 Utility Functions and Classes
(continued from previous page)
iflen(y_hat .shape) >1and y_hat .shape[ 1]>1:
y_hat =y_hat .argmax(axis =1)
cmp =y_hat .type(y .dtype) ==y
return float (cmp .type(y .dtype) .sum())
import hashlib
import os
import tarfile
import zipfile
import requests
def download (url, folder ='../data ', sha1_hash =None ): #@save
"""Download a file to folder and return the local filepath."""
ifnot url.startswith( 'http '):
# For back compatability
url, sha1_hash =DATA_HUB[url]
os.makedirs(folder, exist_ok =True )
fname =os.path .join(folder, url .split( '/')[-1])
# Check if hit cache
ifos.path .exists(fname) and sha1_hash:
sha1 =hashlib .sha1()
with open (fname, 'rb')asf:
while True :
data =f.read( 1048576 )
ifnot data:
break
sha1 .update(data)
ifsha1 .hexdigest() ==sha1_hash:
return fname
# Download
print (f'Downloading {fname }from {url}...')
r=requests .get(url, stream =True , verify =True )
with open (fname, 'wb')asf:
f.write(r .content)
return fname
def extract (filename, folder =None ): #@save
"""Extract a zip/tar file into folder."""
base_dir =os.path .dirname(filename)
_, ext =os.path .splitext(filename)
assert ext in('.zip ','.tar ','.gz'),'Only support zip/tar files. '
ifext =='.zip ':
fp=zipfile .ZipFile(filename, 'r')
else :
fp=tarfile .open(filename, 'r')
iffolder isNone :
folder =base_dir
fp.extractall(folder)
def download_extract (name, folder =None ): #@save
"""Download and extract a zip/tar file."""
fname =download(name)
(continues on next page)
1070 Tools for Deep Learning
(continued from previous page)
base_dir =os.path .dirname(fname)
data_dir, ext =os.path .splitext(fname)
ifext =='.zip ':
fp=zipfile .ZipFile(fname, 'r')
elif ext in('.tar ','.gz'):
fp=tarfile .open(fname, 'r')
else :
assert False ,'Only zip/tar files can be extracted. '
fp.extractall(base_dir)
return os.path .join(base_dir, folder) iffolder else data_dir
def tokenize (lines, token ='word '): #@save
"""Split text lines into word or character tokens."""
assert token in('word ','char '),'Unknown token type: '+token
return [line .split() iftoken =='word 'else list (line) for line inlines]
def evaluate_loss (net, data_iter, loss): #@save
"""Evaluate the loss of a model on the given dataset."""
metric =d2l.Accumulator( 2)# Sum of losses, no. of examples
for X, y indata_iter:
out =net(X)
y=y.reshape(out .shape)
l=loss(out, y)
metric .add(l .sum(), l .numel())
return metric[ 0]/metric[ 1]
def grad_clipping (net, theta): #@save
"""Clip the gradient."""
ifisinstance (net, nn .Module):
params =[pfor pinnet.parameters() ifp.requires_grad]
else :
params =net.params
norm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))
ifnorm >theta:
for param inparams:
param .grad[:] *=theta /norm
More for the attention chapter.
#@save
d2l.DATA_HUB[ 'fra-eng ']=(d2l .DATA_URL +'fra-eng.zip ',
'94646ad1522d915e7b0f9296181140edcf86a4f5 ')
#@save
def read_data_nmt ():
"""Load the English-French dataset."""
data_dir =d2l.download_extract( 'fra-eng ')
with open (os.path .join(data_dir, 'fra.txt '),'r', encoding ='utf-8 ')asf:
return f.read()
#@save
(continues on next page)
1071 Utility Functions and Classes
(continued from previous page)
def preprocess_nmt (text):
"""Preprocess the English-French dataset."""
def no_space (char, prev_char):
return char inset(',.!? ')and prev_char !=''
# Replace non-breaking space with space, and convert uppercase letters to
# lowercase ones
text =text .replace( '\u202f ','').replace( '\xa0 ','').lower()
# Insert space between words and punctuation marks
out =[''+char ifi>0and no_space(char, text[i -1])else char
for i, char inenumerate (text)]
return ''.join(out)
#@save
def tokenize_nmt (text, num_examples =None ):
"""Tokenize the English-French dataset."""
source, target =[], []
for i, line inenumerate (text .split( '\n')):
ifnum_examples and i>num_examples:
break
parts =line .split( '\t')
iflen(parts) ==2:
source .append(parts[ 0].split( ''))
target .append(parts[ 1].split( ''))
return source, target
#@save
def truncate_pad (line, num_steps, padding_token):
"""Truncate or pad sequences."""
iflen(line) >num_steps:
return line[:num_steps] # Truncate
return line +[padding_token] *(num_steps -len(line)) # Pad
#@save
def build_array_nmt (lines, vocab, num_steps):
"""Transform text sequences of machine translation into minibatches."""
lines =[vocab[l] for linlines]
lines =[l+[vocab[ '<eos> ']]for linlines]
array =torch .tensor([truncate_pad(
l, num_steps, vocab[ '<pad> '])for linlines])
valid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)
return array, valid_len
#@save
def load_data_nmt (batch_size, num_steps, num_examples =600):
"""Return the iterator and the vocabularies of the translation dataset."""
text =preprocess_nmt(read_data_nmt())
source, target =tokenize_nmt(text, num_examples)
src_vocab =d2l.Vocab(source, min_freq =2,
reserved_tokens =['<pad> ','<bos> ','<eos> '])
tgt_vocab =d2l.Vocab(target, min_freq =2,
reserved_tokens =['<pad> ','<bos> ','<eos> '])
(continues on next page)
1072 Tools for Deep Learning
(continued from previous page)
src_array, src_valid_len =build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len =build_array_nmt(target, tgt_vocab, num_steps)
data_arrays =(src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter =d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab
#@save
def sequence_mask (X, valid_len, value =0):
"""Mask irrelevant entries in sequences."""
maxlen =X.size( 1)
mask =torch .arange((maxlen), dtype =torch .float32,
device =X.device)[ None , :] <valid_len[:, None ]
X[~mask] =value
return X
#@save
class MaskedSoftmaxCELoss (nn.CrossEntropyLoss):
"""The softmax cross-entropy loss with masks."""
# `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)
# `label` shape: (`batch_size`, `num_steps`)
# `valid_len` shape: (`batch_size`,)
def forward (self , pred, label, valid_len):
weights =torch .ones_like(label)
weights =sequence_mask(weights, valid_len)
self .reduction ='none '
unweighted_loss =super (MaskedSoftmaxCELoss, self ).forward(
pred .permute( 0,2,1), label)
weighted_loss =(unweighted_loss *weights) .mean(dim =1)
return weighted_loss
#@save
def train_seq2seq (net, data_iter, lr, num_epochs, tgt_vocab, device):
"""Train a model for sequence to sequence."""
def xavier_init_weights (m):
iftype (m) ==nn.Linear:
nn.init .xavier_uniform_(m .weight)
iftype (m) ==nn.GRU:
for param inm._flat_weights_names:
if"weight "inparam:
nn.init .xavier_uniform_(m ._parameters[param])
net.apply(xavier_init_weights)
net.to(device)
optimizer =torch .optim .Adam(net .parameters(), lr =lr)
loss =MaskedSoftmaxCELoss()
net.train()
animator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',
xlim =[10, num_epochs])
for epoch inrange (num_epochs):
timer =d2l.Timer()
metric =d2l.Accumulator( 2)# Sum of training loss, no. of tokens
for batch indata_iter:
optimizer .zero_grad()
X, X_valid_len, Y, Y_valid_len =[x.to(device) for xinbatch]
(continues on next page)
1073 Utility Functions and Classes
(continued from previous page)
bos =torch .tensor([tgt_vocab[ '<bos> ']]*Y.shape[ 0],
device =device) .reshape( -1,1)
dec_input =torch .cat([bos, Y[:, : -1]], 1)# Teacher forcing
Y_hat, _ =net(X, dec_input, X_valid_len)
l=loss(Y_hat, Y, Y_valid_len)
l.sum() .backward() # Make the loss scalar for `backward`
d2l.grad_clipping(net, 1)
num_tokens =Y_valid_len .sum()
optimizer .step()
with torch .no_grad():
metric .add(l .sum(), num_tokens)
if(epoch +1)%10==0:
animator .add(epoch +1, (metric[ 0]/metric[ 1],))
print (f'loss {metric[ 0]/metric[ 1]:.3f},{metric[ 1]/timer .stop() :.1f}'
f'tokens/sec on {str(device) }')
#@save
def predict_seq2seq (net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights =False ):
"""Predict for sequence to sequence."""
# Set `net` to eval mode for inference
net.eval()
src_tokens =src_vocab[src_sentence .lower() .split( '')]+[
src_vocab[ '<eos> ']]
enc_valid_len =torch .tensor([ len(src_tokens)], device =device)
src_tokens =d2l.truncate_pad(src_tokens, num_steps, src_vocab[ '<pad> '])
# Add the batch axis
enc_X =torch .unsqueeze(
torch .tensor(src_tokens, dtype =torch .long, device =device), dim =0)
enc_outputs =net.encoder(enc_X, enc_valid_len)
dec_state =net.decoder .init_state(enc_outputs, enc_valid_len)
# Add the batch axis
dec_X =torch .unsqueeze(torch .tensor(
[tgt_vocab[ '<bos> ']], dtype =torch .long, device =device), dim =0)
output_seq, attention_weight_seq =[], []
for _inrange (num_steps):
Y, dec_state =net.decoder(dec_X, dec_state)
# We use the token with the highest prediction likelihood as input
# of the decoder at the next time step
dec_X =Y.argmax(dim =2)
pred =dec_X .squeeze(dim =0).type(torch .int32) .item()
# Save attention weights (to be covered later)
ifsave_attention_weights:
attention_weight_seq .append(net .decoder .attention_weights)
# Once the end-of-sequence token is predicted, the generation of the
# output sequence is complete
ifpred ==tgt_vocab[ '<eos> ']:
break
output_seq .append(pred)
return ''.join(tgt_vocab .to_tokens(output_seq)), attention_weight_seq
1074 Tools for Deep Learning
318B.8The d2lAPI Document
This section displays classes and functions (sorted alphabetically) in the d2lpackage,
showing where they are defined in the book so you can find more detailed implementa-
tions and explanations. See also the source code on the GitHub repository318.
B.8.1Classes
class d2l.torch.AdditiveAttention( num_hiddens ,dropout,**kwargs )
Bases: Module
Additive attention.
Defined in Section 11.3.2
forward( queries,keys,values,valid_lens )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.AddNorm( norm_shape ,dropout )
Bases: Module
The residual connection followed by layer normalization.
Defined in Section 11.7.2
forward( X,Y)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.AttentionDecoder
Bases: Decoder (page 1075)
The base attention-based decoder interface.
1075 Thed2lAPI Document
Defined in Section 11.4
property attention_weights
class d2l.torch.Classifier( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )
Bases: Module(page 1078)
The base class of classification models.
Defined in Section 4.3
accuracy( Y_hat,Y,averaged=True )
Compute the number of correct predictions.
Defined in Section 4.3
layer_summary( X_shape )
Defined in Section 7.6
loss(Y_hat,Y,averaged=True )
Defined in Section 4.5
validation_step( batch )
class d2l.torch.DataModule( root=‚Äô../data‚Äô ,num_workers=4 )
Bases: HyperParameters (page 1077)
The base class of data.
Defined in Section 3.2.2
get_dataloader( train )
get_tensorloader( tensors,train,indices=slice(0,None,None) )
Defined in Section 3.3
train_dataloader()
val_dataloader()
class d2l.torch.Decoder
Bases: Module
The base decoder interface for the encoder‚Äìdecoder architecture.
Defined in Section 10.6
forward( X,state)
Defines the computation performed at every call.
Should be overridden by all subclasses.
1076 Tools for Deep Learning
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
init_state( enc_all_outputs ,*args )
class d2l.torch.DotProductAttention( dropout )
Bases: Module
Scaled dot product attention.
Defined in Section 11.3.2
forward( queries,keys,values,valid_lens=None )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.Encoder
Bases: Module
The base encoder interface for the encoder‚Äìdecoder architecture.
Defined in Section 10.6
forward( X,*args )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.EncoderDecoder( encoder,decoder )
Bases: Classifier (page 1075)
The base class for the encoder‚Äìdecoder architecture.
Defined in Section 10.6
1077 Thed2lAPI Document
forward( enc_X,dec_X,*args )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
predict_step( batch,device,num_steps ,save_attention_weights=False )
Defined in Section 10.7.6
class d2l.torch.FashionMNIST( batch_size=64 ,resize=(28, 28) )
Bases: DataModule (page 1075)
The Fashion-MNIST dataset.
Defined in Section 4.2
get_dataloader( train )
Defined in Section 4.2
text_labels( indices )
Return text labels.
Defined in Section 4.2
visualize( batch,nrows=1 ,ncols=8,labels=[] )
Defined in Section 4.2
class d2l.torch.GRU( num_inputs ,num_hiddens ,num_layers ,dropout=0 )
Bases: RNN(page 1081)
The multilayer GRU model.
Defined in Section 10.3
class d2l.torch.HyperParameters
Bases: object
The base class of hyperparameters.
save_hyperparameters( ignore=[] )
Save function arguments into class attributes.
Defined in Section B.7
1078 Tools for Deep Learning
class d2l.torch.LeNet( lr=0.1,num_classes=10 )
Bases: Classifier (page 1075)
The LeNet-5 model.
Defined in Section 7.6
class d2l.torch.LinearRegression( lr)
Bases: Module(page 1078)
The linear regression model implemented with high-level APIs.
Defined in Section 3.5
configure_optimizers()
Defined in Section 3.5
forward( X)
Defined in Section 3.5
get_w_b()
Defined in Section 3.5
loss(y_hat,y)
Defined in Section 3.5
class d2l.torch.LinearRegressionScratch( num_inputs ,lr,sigma=0.01 )
Bases: Module(page 1078)
The linear regression model implemented from scratch.
Defined in Section 3.4
configure_optimizers()
Defined in Section 3.4
forward( X)
Defined in Section 3.4
loss(y_hat,y)
Defined in Section 3.4
class d2l.torch.Module( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )
Bases: Module,HyperParameters (page 1077)
The base class of models.
Defined in Section 3.2
apply_init( inputs,init=None )
Defined in Section 6.4
1079 Thed2lAPI Document
configure_optimizers()
Defined in Section 4.3
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
loss(y_hat,y)
plot(key,value,train )
Plot a point in animation.
training_step( batch )
validation_step( batch )
class d2l.torch.MTFraEng( batch_size ,num_steps=9 ,num_train=512 ,
num_val=128 )
Bases: DataModule (page 1075)
The English-French dataset.
Defined in Section 10.5
build(src_sentences ,tgt_sentences )
Defined in Section 10.5.3
get_dataloader( train )
Defined in Section 10.5.3
class d2l.torch.MultiHeadAttention( num_hiddens ,num_heads ,dropout,
bias=False ,**kwargs )
Bases: Module(page 1078)
Multi-head attention.
Defined in Section 11.5
forward( queries,keys,values,valid_lens )
Defines the computation performed at every call.
Should be overridden by all subclasses.
1080 Tools for Deep Learning
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
transpose_output( X)
Reverse the operation of transpose_qkv.
Defined in Section 11.5
transpose_qkv( X)
Transposition for parallel computation of multiple attention heads.
Defined in Section 11.5
class d2l.torch.PositionalEncoding( num_hiddens ,dropout,max_len=1000 )
Bases: Module
Positional encoding.
Defined in Section 11.6
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.PositionWiseFFN( ffn_num_hiddens ,ffn_num_outputs )
Bases: Module
The positionwise feed-forward network.
Defined in Section 11.7
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
1081 Thed2lAPI Document
class d2l.torch.ProgressBoard( xlabel=None ,ylabel=None ,xlim=None ,
ylim=None ,xscale=‚Äôlinear‚Äô ,yscale=‚Äôlinear‚Äô ,
ls=[‚Äô-‚Äô,‚Äô--‚Äô,‚Äô-.‚Äô,‚Äô:‚Äô] ,colors=[‚ÄôC0‚Äô,‚ÄôC1‚Äô,‚ÄôC2‚Äô,
‚ÄôC3‚Äô],fig=None ,axes=None ,figsize=(3.5, 2.5) ,
display=True )
Bases: HyperParameters (page 1077)
The board that plots data points in animation.
Defined in Section 3.2
draw(x,y,label,every_n=1 )
Defined in Section B.7
class d2l.torch.Residual( num_channels ,use_1x1conv=False ,strides=1 )
Bases: Module
The Residual block of ResNet models.
Defined in Section 8.6
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.ResNeXtBlock( num_channels ,groups,bot_mul,
use_1x1conv=False ,strides=1 )
Bases: Module
The ResNeXt block.
Defined in Section 8.6.2
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
1082 Tools for Deep Learning
class d2l.torch.RNN( num_inputs ,num_hiddens )
Bases: Module(page 1078)
The RNN model implemented with high-level APIs.
Defined in Section 9.6
forward( inputs,H=None )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.RNNLM( rnn,vocab_size ,lr=0.01 )
Bases: RNNLMScratch (page 1082)
The RNN-based language model implemented with high-level APIs.
Defined in Section 9.6
init_params()
output_layer( hiddens )
Defined in Section 9.5
class d2l.torch.RNNLMScratch( rnn,vocab_size ,lr=0.01 )
Bases: Classifier (page 1075)
The RNN-based language model implemented from scratch.
Defined in Section 9.5
forward( X,state=None )
Defined in Section 9.5
init_params()
one_hot( X)
Defined in Section 9.5
output_layer( rnn_outputs )
Defined in Section 9.5
predict( prefix,num_preds ,vocab,device=None )
Defined in Section 9.5
1083 Thed2lAPI Document
training_step( batch )
validation_step( batch )
class d2l.torch.RNNScratch( num_inputs ,num_hiddens ,sigma=0.01 )
Bases: Module(page 1078)
The RNN model implemented from scratch.
Defined in Section 9.5
forward( inputs,state=None )
Defined in Section 9.5
class d2l.torch.Seq2Seq( encoder,decoder,tgt_pad,lr)
Bases: EncoderDecoder (page 1076)
The RNN encoder‚Äìdecoder for sequence to sequence learning.
Defined in Section 10.7.3
configure_optimizers()
Defined in Section 4.3
validation_step( batch )
class d2l.torch.Seq2SeqEncoder( vocab_size ,embed_size ,num_hiddens ,
num_layers ,dropout=0 )
Bases: Encoder (page 1076)
The RNN encoder for sequence-to-sequence learning.
Defined in Section 10.7
forward( X,*args )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.SGD( params,lr)
Bases: HyperParameters (page 1077)
Minibatch stochastic gradient descent.
Defined in Section 3.4
1084 Tools for Deep Learning
step()
zero_grad()
class d2l.torch.SoftmaxRegression( num_outputs ,lr)
Bases: Classifier (page 1075)
The softmax regression model.
Defined in Section 4.5
forward( X)
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.SyntheticRegressionData( w,b,noise=0.01 ,num_train=1000 ,
num_val=1000 ,batch_size=32 )
Bases: DataModule (page 1075)
Synthetic data for linear regression.
Defined in Section 3.3
get_dataloader( train )
Defined in Section 3.3
class d2l.torch.TimeMachine( batch_size ,num_steps ,num_train=10000 ,
num_val=5000 )
Bases: DataModule (page 1075)
The Time Machine dataset.
Defined in Section 9.2
build(raw_text ,vocab=None )
Defined in Section 9.2
get_dataloader( train )
Defined in Section 9.3.3
class d2l.torch.Trainer( max_epochs ,num_gpus=0 ,gradient_clip_val=0 )
Bases: HyperParameters (page 1077)
The base class for training models with data.
1085 Thed2lAPI Document
Defined in Section 3.2.2
clip_gradients( grad_clip_val ,model )
Defined in Section 9.5
fit(model,data)
fit_epoch()
Defined in Section 3.4
prepare_batch( batch )
Defined in Section 6.7
prepare_data( data)
prepare_model( model )
Defined in Section 6.7
class d2l.torch.TransformerEncoder( vocab_size ,num_hiddens ,ffn_num_hiddens ,
num_heads ,num_blks ,dropout,
use_bias=False )
Bases: Encoder (page 1076)
The Transformer encoder.
Defined in Section 11.7.4
forward( X,valid_lens )
Defines the computation performed at every call.
Should be overridden by all subclasses.
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.TransformerEncoderBlock( num_hiddens ,ffn_num_hiddens ,
num_heads ,dropout,use_bias=False )
Bases: Module
The Transformer encoder block.
Defined in Section 11.7.2
forward( X,valid_lens )
Defines the computation performed at every call.
Should be overridden by all subclasses.
1086 Tools for Deep Learning
Note:Although the recipe for forward pass needs to be defined within this function,
one should call the Module(page 1078) instance afterwards instead of this since the
formertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.
class d2l.torch.Vocab( tokens=[] ,min_freq=0 ,reserved_tokens=[] )
Bases: object
Vocabulary for text.
to_tokens( indices )
property unk
B.8.2Functions
d2l.torch.add_to_class( Class )
Register functions as methods in created class.
Defined in Section 3.2
d2l.torch.bleu( pred_seq ,label_seq ,k)
Compute the BLEU.
Defined in Section 10.7.6
d2l.torch.check_len( a,n)
Check the length of a list.
Defined in Section 9.5
d2l.torch.check_shape( a,shape )
Check the shape of a tensor.
Defined in Section 9.5
d2l.torch.corr2d( X,K)
Compute 2D cross-correlation.
Defined in Section 7.2
d2l.torch.cpu()
Get the CPU device.
Defined in Section 6.7
d2l.torch.gpu( i=0)
Get a GPU device.
Defined in Section 6.7
1087 Thed2lAPI Document
d2l.torch.init_cnn( module )
Initialize weights for CNNs.
Defined in Section 7.6
d2l.torch.init_seq2seq( module )
Initialize weights for sequence-to-sequence learning.
Defined in Section 10.7
d2l.torch.masked_softmax( X,valid_lens )
Perform softmax operation by masking elements on the last axis.
Defined in Section 11.3
d2l.torch.num_gpus()
Get the number of available GPUs.
Defined in Section 6.7
d2l.torch.plot( X,Y=None,xlabel=None ,ylabel=None ,legend=[] ,xlim=None ,
ylim=None ,xscale=‚Äôlinear‚Äô ,yscale=‚Äôlinear‚Äô ,fmts=(‚Äô-‚Äô,‚Äôm--‚Äô,‚Äôg-.‚Äô,
‚Äôr:‚Äô),figsize=(3.5,2.5) ,axes=None )
Plot data points.
Defined in Section 2.4
d2l.torch.set_axes( axes,xlabel,ylabel,xlim,ylim,xscale,yscale,legend )
Set the axes for matplotlib.
Defined in Section 2.4
d2l.torch.set_figsize( figsize=(3.5,2.5) )
Set the figure size for matplotlib.
Defined in Section 2.4
d2l.torch.show_heatmaps( matrices ,xlabel,ylabel,titles=None ,figsize=(2.5, 2.5) ,
cmap=‚ÄôReds‚Äô )
Show heatmaps of matrices.
Defined in Section 11.1
d2l.torch.show_list_len_pair_hist( legend,xlabel,ylabel,xlist,ylist)
Plot the histogram for list length pairs.
Defined in Section 10.5
d2l.torch.try_all_gpus()
Return all available GPUs, or [cpu(),] if no GPU exists.
Defined in Section 6.7
1088 Tools for Deep Learning
d2l.torch.try_gpu( i=0)
Return gpu(i) if exists, otherwise return cpu().
Defined in Section 6.7
d2l.torch.use_svg_display()
Use the svg format to display a plot in Jupyter.
Defined in Section 2.4
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ‚Ä¶ et al. (2016). Tensor-
Flow: asystemforlarge-scalemachinelearning. 12thUSENIXSymposiumonOperating
SystemsDesign and Implementation (OSDI 16) (pp. 265‚Äì283).
Abdel-Hamid, O., Mohamed, A.-R., Jiang, H., Deng, L., Penn, G., & Yu, D. (2014). Con-
volutional neural networks for speech recognition. IEEE/ACM Transactions on Audio,
Speech,and LanguageProcessing ,22(10), 1533‚Äì1545.
Ahmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable
inference in latent variable models. ProceedingsoftheFifthACMInternationalConfer-
enceon WebSearchand Data Mining (pp. 123‚Äì132).
Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: a next-generation
hyperparameter optimization framework. Proceedings of the 25th ACM SIGKDD Inter-
nationalConferenceon KnowledgeDiscovery & Data Mining .
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ‚Ä¶ et al. (2022).
Flamingo: a visual language model for few-shot learning. ArXiv:2204.14198 .
Alsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., & Reblitz-Richardson, O. (2020).
Mind the PAD ‚Äì CNNs can develop blind spots. ArXiv:2010.02178 .
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., ‚Ä¶ et al. (2023).
PaLM 2 Technical Report. ArXiv:2305.10403 .
Anil, R., Gupta, V., Koren, T., Regan, K., & Singer, Y. (2020). Scalable second-order
optimization for deep learning. ArXiv:2002.09018 .
Aronszajn, N. (1950). Theory of reproducing kernels. TransactionsoftheAmericanMath-
ematicalSociety ,68(3), 337‚Äì404.
Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. ArXiv:1607.06450 .
Baevski, A., & Auli, M. (2018). Adaptive input representations for neural language mod-
eling.International Conferenceon Learning Representations .
Bahdanau,D.,Cho,K.,&Bengio,Y.(2014).Neuralmachinetranslationbyjointlylearning
to align and translate. ArXiv:1409.0473 .
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., ‚Ä¶ et al. (2022). Con-
stitutional AI: harmlessness from AI feedback. ArXiv:2212.08073 .
Baptista, R., & Poloczek, M. (2018). Bayesian optimization of combinatorial structures.
Proceedingsof the35thInternational Conferenceon MachineLearning .
Bardenet, R., Brendel, M., K√©gl, B., & Sebag, M. (2013). Collaborative hyperparam-
eter tuning. Proceedings of the 30th International Conference on Machine Learning
(ICML‚Äô13) .
Bay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded up robust features. Euro-
peanConferenceon ComputerVision (pp. 404‚Äì417).
1089
1090 REFERENCES
Bellman, R. (1966). Dynamic programming. Science,153, 34‚Äì37.
Bellman, R. (1952). On the theory of dynamic programming. ProceedingsoftheNational
Academyof Sciences ,38(8), 716‚Äì719.
Bellman, R. (1957). A Markovian decision process. JournalofMathematicsandMechan-
ics,6(5), 679‚Äì684. URL: http://www.jstor.org/stable/24900506
Bellman, R. (1957). Dynamic Programming . Dover Publications.
Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: the long-document trans-
former.ArXiv:2004.05150 .
Bengio,Y.,Ducharme,R.,Vincent,P.,&Jauvin,C.(2003).Aneuralprobabilisticlanguage
model.Journal of MachineLearning Research ,3(Feb), 1137‚Äì1155.
Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gra-
dient descent is difficult. IEEE Transactionson NeuralNetworks ,5(2), 157‚Äì166.
Bergstra, J., Bardenet, R., Bengio, Y., & K√©gl, B. (2011). Algorithms for hyper-parameter
optimization. Advancesin NeuralInformation ProcessingSystems ,24.
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., ‚Ä¶ Ben-
gio, Y. (2010). Theano: a CPU and GPU math compiler in Python. Proc. 9th Python in
ScienceConference (pp. 3‚Äì10).
Beutel, A., Murray, K., Faloutsos, C., & Smola, A. J. (2014). CoBaFi: collaborative
Bayesianfiltering. Proceedingsofthe23rdInternationalConferenceonWorldWideWeb
(pp. 97‚Äì108).
Bishop, C.M.(1995).TrainingwithnoiseisequivalenttoTikhonovregularization. Neural
Computation ,7(1), 108‚Äì116.
Bishop, C. M. (2006). Pattern Recognitionand MachineLearning . Springer.
Black, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal
ofPoliticalEconomy ,81, 637‚Äì654.
Bodla, N., Singh, B., Chellappa, R., & Davis, L. S. (2017). Soft-NMS-improving object
detection with one line of code. Proceedings of the IEEE International Conference on
ComputerVision (pp. 5561‚Äì5569).
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with
subword information. Transactions of the Association for Computational Linguistics ,5,
135‚Äì146.
Bollob√°s, B. (1999). Linear Analysis . Cambridge University Press.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ‚Ä¶ et al.
(2021). On the opportunities and risks of foundation models. ArXiv:2108.07258 .
Bottou,L.(2010).Large-scalemachinelearningwithstochasticgradientdescent. Proceed-
ingsof COMPSTAT‚Äô2010 (pp. 177‚Äì186). Springer.
Bottou, L., & Le Cun, Y. (1988). SN: a simulator for connectionist models. Proceedings
of NeuroNimes 88 (pp. 371‚Äì382). Nimes, France. URL: http://leon.bottou.org/papers/
bottou-lecun-88
Boucheron, S., Bousquet, O., & Lugosi, G. (2005). Theory of classification: a survey of
some recent advances. ESAIM: Probabilityand Statistics ,9, 323‚Äì375.
Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus
for learning natural language inference. ArXiv:1508.05326 .
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization . Cambridge, England:
Cambridge University Press.
1091 REFERENCES
Bradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The
method of paired comparisons. Biometrika ,39(3/4), 324‚Äì345.
Brown, N., & Sandholm, T. (2017). Libratus: the superhuman AI for no-limit poker. IJCAI
(pp. 5226‚Äì5228).
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J.,
‚Ä¶ Roossin, P. S. (1990). A statistical approach to machine translation. Computational
Linguistics ,16(2), 79‚Äì85.
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L.,
& Roossin, P. (1988). A statistical approach to language translation. COLING Budapest
1988Volume1: International Conferenceon Computational Linguistics .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ‚Ä¶ et al. (2020).
Languagemodelsarefew-shotlearners. AdvancesinNeuralInformationProcessingSys-
tems,33, 1877‚Äì1901.
Buslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., Druzhinin, M., & Kalinin,
A. A. (2020). Albumentations: Fast and flexible image augmentations. Information ,
11(2), 125.
Campbell, M., Hoane Jr, A. J., & Hsu, F.-h. (2002). Deep blue. Artificial Intelligence ,
134(1-2), 57‚Äì83.
Canny, J. (1987). A computational approach to edge detection. Readings in Computer Vi-
sion(pp. 184‚Äì203). Elsevier.
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). SemEval-2017 Task
1: semantictextualsimilaritymultilingualandcrosslingualfocusedevaluation. Proceed-
ingsofthe11thInternationalWorkshoponSemanticEvaluation(SemEval-2017) (pp.1‚Äì
14).
Chan, W., Jaitly, N., Le, Q. V., & Vinyals, O. (2015). Listen, attend and spell.
ArXiv:1508.01211 .
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., ‚Ä¶ Mordatch, I. (2021).
Decision transformer: reinforcement learning via sequence modeling. AdvancesinNeu-
ralInformation ProcessingSystems ,34, 15084‚Äì15097.
Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., ‚Ä¶ Zhang, Z. (2015). MXNET:
a flexible and efficient machine learning library for heterogeneous distributed systems.
ArXiv:1512.01274 .
Cheng, J., Dong, L., &Lapata, M.(2016).Longshort-termmemory-networksformachine
reading.Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguage
Processing (pp. 551‚Äì561).
Chetlur,S.,Woolley,C.,Vandermersch,P.,Cohen,J.,Tran,J.,Catanzaro,B.,&Shelhamer,
E. (2014). CuDNN: Efficient primitives for deep learning. ArXiv:1410.0759 .
Cho, K., Van Merri√´nboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of
neural machine translation: Encoder‚Äìdecoder approaches. ArXiv:1409.1259 .
Cho, K., Van Merri√´nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.,
& Bengio, Y. (2014). Learning phrase representations using RNN encoder‚Äìdecoder for
statistical machine translation. ArXiv:1406.1078 .
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,G.,Roberts,A.,‚Ä¶etal.(2022).
PaLM: scaling language modeling with pathways. ArXiv:2204.02311 .
1092 REFERENCES
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated re-
current neural networks on sequence modeling. ArXiv:1412.3555 .
Clark,K.,Luong,M.-T.,Le, Q.V.,&Manning,C.D.(2020).ELECTRA:pre-trainingtext
encodersasdiscriminatorsratherthangenerators. InternationalConferenceonLearning
Representations .
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search,12, 2493‚Äì2537.
Cordonnier, J.-B., Loukas, A., & Jaggi, M. (2020). On the relationship between self-
attention and convolutional layers. International Conference on Learning Representa-
tions.
Cover, T., & Thomas, J. (1999). Elementsof Information Theory . John Wiley & Sons.
Csisz√°r, I. (2008). Axiomatic characterizations of information measures. Entropy,10(3),
261‚Äì273.
Cybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction. Mathemat-
icsof Control,Signals and Systems ,2(4), 303‚Äì314.
Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection.
2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR‚Äô05) (pp. 886‚Äì893).
De Cock, D. (2011). Ames, Iowa: alternative to the Boston housing data as an end of
semester regression project. Journal of StatisticsEducation ,19(3).
Dean,J.,Corrado,G.S.,Monga,R.,Chen,K.,Devin,M.,Le,Q.V.,‚Ä¶etal.(2012).Large
scale distributed deep networks. Proceedings of the 25th International Conference on
NeuralInformation ProcessingSystems,Volume1 (pp. 1223‚Äì1231).
DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., ‚Ä¶
Vogels, W. (2007). Dynamo: Amazon‚Äôs highly available key-value store. ACM SIGOPS
OperatingSystemsReview (pp. 205‚Äì220).
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: a large-
scale hierarchical image database. 2009IEEEConferenceonComputerVisionandPat-
tern Recognition (pp. 248‚Äì255).
Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? does it matter? Struc-
turalSafety ,31(2), 105‚Äì112.
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep
bidirectional transformers for language understanding. ArXiv:1810.04805 .
Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: non-linear independent components
estimation. ArXiv:1410.8516 .
Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real NVP. In-
ternational Conferenceon Learning Representations .
Doersch,C.,Gupta,A.,&Efros,A.A.(2015).Unsupervisedvisualrepresentationlearning
by context prediction. Proceedings of the IEEE International Conference on Computer
Vision(pp. 1422‚Äì1430).
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ‚Ä¶
et al. (2021). An image is worth 16 x 16 words: transformers for image recognition at
scale.International Conferenceon Learning Representations .
1093 REFERENCES
Duchi,J.,Hazan,E.,&Singer,Y.(2011).Adaptivesubgradientmethodsforonlinelearning
and stochastic optimization. Journal of MachineLearning Research ,12, 2121‚Äì2159.
Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning.
ArXiv:1603.07285 .
Dwivedi, V. P., & Bresson, X. (2020). A generalization of transformer networks to graphs.
ArXiv:2012.09699 .
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., & Roth, A. L. (2015). Pre-
servingstatisticalvalidityinadaptivedataanalysis. Proceedingsofthe47thAnnualACM
Symposiumon Theory of Computing (pp. 117‚Äì126).
Elman, J. L. (1990). Finding structure in time. CognitiveScience ,14(2), 179‚Äì211.
Elsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: a ssurvey.
ArXiv:1808.05377[stat.ML] .
Fechner, G. T. (1860). Elementeder Psychophysik . Vol. 2. Breitkopf u. H√§rtel.
Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: scaling to trillion param-
eter models with simple and efficient sparsity. Journal of Machine Learning Research ,
23(120), 1‚Äì39.
Fernando,R.(2004). GPUGems: ProgrammingTechniques,Tips,andTricksforReal-Time
Graphics . Addison-Wesley.
Feurer, M., & Hutter, F. (2018). Hyperparameter ptimization. Automatic Machine Learn-
ing: Methods,Systems,Challenges . Springer.
Feurer, M., Letham, B., Hutter, F., & Bakshy, E. (2022). Practical transfer learning for
Bayesian optimization. ArXiv:1802.02219[stat.ML] .
Field, D. J. (1987). Relations between the statistics of natural images and the response
properties of cortical cells. JOSA A,4(12), 2379‚Äì2394.
Fisher, R. A. (1925). Statistical MethodsforResearchWorkers. Oliver & Boyd.
Flammarion, N., & Bach, F. (2015). From averaging to acceleration, there is only a step-
size.Conferenceon Learning Theory (pp. 658‚Äì695).
Forrester, A. I., S√≥bester, A., & Keane, A. J. (2007). Multi-fidelity optimization via surro-
gate modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engi-
neeringSciences ,463(2088), 3251‚Äì3269.
Franceschi,L.,Donini,M.,Frasconi,P.,&Pontil,M.(2017).Forwardandreversegradient-
based hyperparameter optimization. Proceedings of the 34th International Conference
onMachineLearning (ICML‚Äô17) .
Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: finding sparse, trainable
neural networks. ArXiv:1803.03635 .
Frazier, P. I. (2018). A tutorial on Bayesian optimization. ArXiv:1807.02811 .
Freund, Y., & Schapire, R. E. (1996). Experiments with a new boosting algorithm. Pro-
ceedingsof theInternational Conferenceon MachineLearning (pp. 148‚Äì156).
Friedman, J.H.(1987).Exploratoryprojectionpursuit. JournaloftheAmericanStatistical
Association ,82(397), 249‚Äì266.
Frostig, R., Johnson, M. J., & Leary, C. (2018). Compiling machine learning programs via
high-level tracing. Proceedingsof SystemsforMachineLearning .
Fukushima, K. (1982). Neocognitron: a self-organizing neural network model for a
mechanism of visual pattern recognition. Competition and Cooperation in Neural Nets
(pp. 267‚Äì285). Springer.
1094 REFERENCES
Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., & Wilson, A. G. (2018). GPyTorch:
blackbox matrix‚Äìmatrix Gaussian process inference with GPU acceleration. Advances
inNeuralInformation ProcessingSystems .
Garg, S., Balakrishnan, S., Kolter, Z., & Lipton, Z. (2021). RATT: leveraging unla-
beled data to guarantee generalization. International Conference on Machine Learning
(pp. 3598‚Äì3609).
Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional
neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (pp. 2414‚Äì2423).
Gauss, C. F. (1809). Theoria motus corporum coelestum. Werke. K√∂niglich Preussische
Akademie der Wissenschaften.
Gibbs, J. W. (1902). Elementary Principles of StatisticalMhanics . Scribner‚Äôs.
Ginibre,J.(1965).Statisticalensemblesofcomplex,quaternion,andrealmatrices. Journal
ofMathematicalPhysics ,6(3), 440‚Äì449.
Girshick, R. (2015). Fast R-CNN. Proceedings of the IEEE International Conference on
ComputerVision (pp. 1440‚Äì1448).
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for ac-
curateobjectdetectionandsemanticsegmentation. ProceedingsoftheIEEEConference
onComputerVisionand Pattern Recognition (pp. 580‚Äì587).
Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward
neural networks. Proceedings of the 13th International Conference on Artificial Intelli-
genceand Statistics (pp. 249‚Äì256).
Goh, G. (2017). Why momentum really works. Distill. URL: http://distill.pub/2017/
momentum
Goldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative filtering to
weave an information tapestry. Communicationsof theACM ,35(12), 61‚Äì71.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations . Johns Hopkins University
Press.
Goodfellow,I.,Bengio,Y.,&Courville,A.(2016). DeepLearning .MITPress. http://www.
deeplearningbook.org .
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ‚Ä¶ Ben-
gio, Y. (2014). Generative adversarial nets. Advances in Neural Information Processing
Systems (pp. 2672‚Äì2680).
Gotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A closer look at deep learning
heuristics: learning rate restarts, warmup and distillation. ArXiv:1810.13243 .
Goyal, A., Bochkovskiy, A., Deng, J., & Koltun, V. (2021). Non-deep networks.
ArXiv:2110.07641 .
Graham, B. (2014). Fractional max-pooling. ArXiv:1412.6071 .
Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks. ArXiv:1308.0850 .
Graves, A., Liwicki, M., Fern√°ndez, S., Bertolami, R., Bunke, H., & Schmidhuber, J.
(2008). A novel connectionist system for unconstrained handwriting recognition. IEEE
Transactionson Pattern Analysisand MachineIntelligence ,31(5), 855‚Äì868.
Graves,A.,&Schmidhuber,J.(2005).Framewisephonemeclassificationwithbidirectional
LSTM and other neural network architectures. Neural Networks ,18(5-6), 602‚Äì610.
1095 REFERENCES
Griewank, A. (1989). On automatic differentiation. Mathematical Programming: Recent
Developmentsand Applications (pp. 83‚Äì107). Kluwer.
Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., ‚Ä¶ et al. (2020). Con-
former: convolution-augmented transformer for speech recognition. Proc. Interspeech
2020, pp. 5036‚Äì5040.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. A. (2008). Feature Extraction: Founda-
tionsand Applications . Springer.
Hadjis, S., Zhang, C., Mitliagkas, I., Iter, D., & R√©, C. (2016). Omnivore: an optimizer for
multi-device deep learning on CPUs and GPUs. ArXiv:1606.04487 .
Hartley, R., & Zisserman, A. (2000). Multiple View Geometry in Computer Vision .
Cambridge University Press.
Hartley, R. I., & Kahl, F. (2009). Global optimization through rotation space search. Inter-
nationalJournal of ComputerVision ,82(1), 64‚Äì79.
He,K.,Chen,X.,Xie,S.,Li,Y.,Doll√°r,P.,&Girshick,R.(2022).Maskedautoencodersare
scalable vision learners. ProceedingsoftheIEEE/CVFConferenceonComputerVision
andPattern Recognition (pp. 16000‚Äì16009).
He, K., Gkioxari, G., Doll√°r, P., & Girshick, R. (2017). Mask R-CNN. Proceedings of the
IEEEInternational Conferenceon ComputerVision (pp. 2961‚Äì2969).
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: surpassing
human-level performance on ImageNet classification. ProceedingsoftheIEEEInterna-
tionalConferenceon ComputerVision (pp. 1026‚Äì1034).
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recogni-
tion.ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(pp. 770‚Äì778).
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks.
EuropeanConferenceon ComputerVision (pp. 630‚Äì645).
Hebb, D. O. (1949). The Organizationof Behavior . Wiley.
Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (GELUs).
ArXiv:1606.08415 .
Hennessy, J. L., & Patterson, D. A. (2011). Computer Architecture: A Quantitative Ap-
proach. Elsevier.
Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances
inNeuralInformation ProcessingSystems ,33, 6840‚Äì6851.
Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recur-
rentnets: thedifficultyoflearninglong-termdependencies. AFieldGuidetoDynamical
RecurrentNeuralNetworks . IEEE Press.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. NeuralComputation ,
9(8), 1735‚Äì1780.
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ‚Ä¶ et al.
(2022). Training compute-optimal large language models. ArXiv:2203.15556 .
Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., ‚Ä¶ Adam, H. (2019).
SearchingforMobileNetV3. ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision (pp. 1314‚Äì1324).
1096 REFERENCES
Hoyer,P.O.,Janzing,D.,Mooij,J.M.,Peters,J.,&Sch√∂lkopf,B.(2009).Nonlinearcausal
discovery with additive noise models. Advances in Neural Information Processing Sys-
tems(pp. 689‚Äì696).
Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. Proceedings of the
IEEEConferenceon ComputerVisionand Pattern Recognition (pp. 7132‚Äì7141).
Hu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback
datasets.20088thIEEE International Conferenceon Data Mining (pp. 263‚Äì272).
Hu, Z., Lee, R. K.-W., Aggarwal, C. C., & Zhang, A. (2022). Text style transfer: a review
and experimental evaluation. SIGKDD Explor. Newsl. ,24(1). URL: https://doi.org/10.
1145/3544903.3544906
Huang,C.-Z.A.,Vaswani,A.,Uszkoreit,J.,Simon,I.,Hawthorne,C.,Shazeer,N.,‚Ä¶Eck,
D. (2018). Music transformer: generating music with long-term structure. International
Conferenceon Learning Representations .
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected
convolutional networks. Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (pp. 4700‚Äì4708).
Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM‚ÄìCRF models for sequence tag-
ging.ArXiv:1508.01991 .
Hubel,D.H.,&Wiesel,T.N.(1959).Receptivefieldsofsingleneuronesinthecat‚Äôsstriate
cortex.Journal of Physiology ,148(3), 574‚Äì591.
Hubel,D.H.,&Wiesel,T.N.(1962).Receptivefields,binocularinteractionandfunctional
architecture in the cat‚Äôs visual cortex. Journal of Physiology ,160(1), 106‚Äì154.
Hubel, D. H., & Wiesel, T. N. (1968). Receptive fields and functional architecture of mon-
key striate cortex. Journal of Physiology ,195(1), 215‚Äì243.
Hutter,F.,Hoos,H.,&Leyton-Brown,K.(2011).Sequentialmodel-basedoptimizationfor
general algorithm configuration. Proceedings of the Fifth International Conference on
Learning and IntelligentOptimization(LION‚Äô11) .
Hutter, F., Kotthoff, L., & Vanschoren, J. (Eds.) (2019). Automated Machine Learning:
Methods,Systems,Challenges . Springer.
Ioffe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batch-
normalized models. Advances in Neural Information Processing Systems (pp. 1945‚Äì
1953).
Ioffe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training
by reducing internal covariate shift. ArXiv:1502.03167 .
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging
weights leads to wider optima and better generalization. ArXiv:1803.05407 .
Jacot, A., Gabriel, F., & Hongler, C. (2018). Neural tangent kernel: convergence and gen-
eralization in neural networks. Advancesin NeuralInformation ProcessingSystems .
Jaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL,
EKF and the ‚Äúecho state network‚Äù approach . GMD-Forschungszentrum Information-
stechnik Bonn.
Jamieson, K., & Talwalkar, A. (2016). Non-stochastic best arm identification and hyper-
parameter optimization. Proceedings of the 17th International Conference on Artificial
Intelligenceand Statistics .
1097 REFERENCES
Jenatton, R., Archambeau, C., Gonz√°lez, J., & Seeger, M. (2017). Bayesian optimization
with tree-structured dependencies. Proceedingsofthe34thInternationalConferenceon
MachineLearning (ICML‚Äô17) .
Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., ‚Ä¶ et al. (2018). Highly scalable
deep learning training system with mixed-precision: training ImageNet in four minutes.
ArXiv:1807.11205 .
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., ‚Ä¶ Darrell, T.
(2014). Caffe: convolutional architecture for fast feature embedding. Proceedingsofthe
22ndACMInternational Conferenceon Multimedia (pp. 675‚Äì678).
Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., & Levy, O. (2020). SpanBERT:
improving pre-training by representing and predicting spans. Transactions of the Asso-
ciationforComputational Linguistics ,8, 64‚Äì77.
Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., ‚Ä¶ et al. (2017).
In-datacenter performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th
AnnualInternational Symposium on ComputerArchitecture(ISCA) (pp. 1‚Äì12).
Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A convolutional neural network
for modelling sentences. ArXiv:1404.2188 .
Kalman, B. L., & Kwasny, S. C. (1992). Why tanh: choosing a sigmoidal function. Pro-
ceedings of the International Joint Conference on Neural Networks (IJCNN) (pp. 578‚Äì
581).
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ‚Ä¶ Amodei,
D. (2020). Scaling laws for neural language models. ArXiv:2001.08361 .
Karnin, Z., Koren, T., & Somekh, O. (2013). Almost optimal exploration in multi-
armed bandits. Proceedings of the 30th International Conference on Machine Learning
(ICML‚Äô13) .
Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of GANs for
improved quality, stability, and variation. ArXiv:1710.10196 .
Kim, J., El-Khamy, M., & Lee, J. (2017). Residual LSTM: design of a deep recurrent
architecture for distant speech recognition. ArXiv:1701.03360 .
Kim, Y. (2014). Convolutional neural networks for sentence classification.
ArXiv:1408.5882 .
Kimeldorf, G. S., & Wahba, G. (1971). Some results on Tchebycheffian spline functions.
J.Math.Anal. Appl. ,33, 82‚Äì95.
Kingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization.
ArXiv:1412.6980 .
Kingma,D.P.,&Welling,M.(2014).Auto-encodingvariationalBayes. InternationalCon-
ferenceon Learning Representations(ICLR) .
Kipf,T.N.,&Welling,M.(2016).Semi-supervisedclassificationwithgraphconvolutional
networks. ArXiv:1609.02907 .
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models
are zero-shot reasoners. arxiv.org/abs/2205.11916 .
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press.
Kolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Inst.
Ital.Attuari, Giorn. ,4, 83‚Äì91.
1098 REFERENCES
Kolter, Z. (2008). Linear algebra review and reference. Available online:
http://cs229.stanford.edu/section/cs229-linalg.pdf .
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep
convolutional neural networks. Advances in Neural Information Processing Systems
(pp. 1097‚Äì1105).
Kung, S. Y. (1988). VLSI Array Processors. PrenticeHall .
Kuzovkin, I., Vicente, R., Petton, M., Lachaux, J.-P., Baciu, M., Kahane, P., ‚Ä¶ Aru, J.
(2018).Activationsofdeepconvolutionalneuralnetworksarealignedwithgammaband
activity of human visual cortex. CommunicationsBiology ,1(1), 1‚Äì12.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). ALBERT:
aliteBERTforself-supervisedlearningoflanguagerepresentations. ArXiv:1909.11942 .
Lavin, A., & Gray, S. (2016). Fast algorithms for convolutional neural networks. Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4013‚Äì
4021).
Le,Q.V.(2013).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning. Pro-
ceedingsoftheIEEEInternationalConferenceonAcoustics,SpeechandSignalProcess-
ing(pp. 8595‚Äì8598).
LeCun, Y., Bengio, Y., & et al. (1995). Convolutional networks for images, speech, and
time series. TheHandbook of BrainTheory and NeuralNetworks (p. 3361). MIT Press.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel,
L.D.(1989).Backpropagationappliedtohandwrittenzipcoderecognition. NeuralCom-
putation ,1(4), 541‚Äì551.
LeCun, Y., Bottou, L., Orr, G., & Muller, K.-R. (1998). Efficient backprop. Neural Net-
works: Tricksof theTrade . Springer.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied
to document recognition. Proceedingsof theIEEE ,86(11), 2278‚Äì2324.
LeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., ‚Ä¶ et al. (1995). Com-
parison of learning algorithms for handwritten digit recognition. International Confer-
enceon Artificial NeuralNetworks (pp. 53‚Äì60).
Legendre, A.M.(1805). M√©moiresurlesOp√©rationsTrigonom√©triques: dontlesR√©sultats
D√©pendentde la Figurede la Terre . F. Didot.
Lewis,M.,Liu,Y.,Goyal,N.,Ghazvininejad,M.,Mohamed,A.,Levy,O.,‚Ä¶Zettlemoyer,
L.(2019).BART:denoisingsequence-to-sequencepre-trainingfornaturallanguagegen-
eration, translation, and comprehension. ArXiv:1910.13461 .
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh,
V., ‚Ä¶ et al. (2022). Solving quantitative reasoning problems with language models.
ArXiv:2206.14858 .
Li, L., Jamieson, K., Rostamizadeh, A., Gonina, K., Hardt, M., Recht, B., & Talwalkar, A.
(2018). Massively parallel hyperparameter tuning. ArXiv:1810.05934 .
Li,M.(2017). ScalingDistributedMachineLearningwithSystemandAlgorithmCo-design
(Doctoral dissertation). PhD Thesis, CMU.
Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., ‚Ä¶ Su, B.-Y.
(2014).Scalingdistributedmachinelearningwiththeparameterserver. 11thSymposium
onOperatingSystemsDesign and Implementation (OSDI 14) (pp. 583‚Äì598).
1099 REFERENCES
Li, M., Zhang, T., Chen, Y., & Smola, A. J. (2014). Efficient mini-batch training for
stochastic optimization. Proceedings of the 20th ACM SIGKDD International Confer-
enceon KnowledgeDiscovery and Data Mining (pp. 661‚Äì670).
Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J., & Stoica, I. (2018). Tune: a
research platform for distributed model selection and training. ArXiv:1807.05118 .
Lin, M., Chen, Q., & Yan, S. (2013). Network in network. ArXiv:1312.4400 .
Lin, T.-Y., Goyal, P., Girshick, R., He, K., & Doll√°r, P. (2017). Focal loss for dense ob-
ject detection. Proceedings of the IEEE International Conference on Computer Vision
(pp. 2980‚Äì2988).
Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., ‚Ä¶ others. (2010). ImageNet classifi-
cation: fast descriptor coding and large-scale SVM training. Large Scale Visual Recog-
nitionChallenge .
Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A
structured self-attentive sentence embedding. ArXiv:1703.03130 .
Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural
networks for sequence learning. ArXiv:1506.00019 .
Lipton, Z. C., Kale, D. C., Elkan, C., & Wetzel, R. (2016). Learning to diagnose with
LSTM recurrent neural networks. International Conference on Learning Representa-
tions(ICLR) .
Lipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship.
Communicationsof theACM ,17, 45‚Äì77.
Liu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale
optimization. MathematicalProgramming ,45(1), 503‚Äì528.
Liu, H., Simonyan, K., & Yang, Y. (2018). DARTS: differentiable architecture search.
ArXiv:1806.09055 .
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016).
SSD: single shot multibox detector. EuropeanConferenceonComputerVision (pp. 21‚Äì
37).
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ‚Ä¶ Stoyanov, V. (2019). RoBERTa:
a robustly optimized BERT pretraining approach. ArXiv:1907.11692 .
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ‚Ä¶ Guo, B. (2021). Swin transformer:
hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF
International Conferenceon ComputerVision (pp. 10012‚Äì10022).
Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convNet
for the 2020s. ArXiv:2201.03545 .
Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic
segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (pp. 3431‚Äì3440).
Loshchilov, I., & Hutter, F. (2016). SGDR: stochastic gradient descent with warm restarts.
ArXiv:1608.03983 .
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. Interna-
tionalJournal of ComputerVision ,60(2), 91‚Äì110.
Luo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in
batch normalization. ArXiv:1809.00846 .
1100 REFERENCES
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learn-
ing word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, Volume 1
(pp. 142‚Äì150).
Mack, Y.-P., & Silverman, B. W. (1982). Weak and strong uniform consistency of kernel
regression estimates. Zeitschrift f√ºr Wahrscheinlichkeitstheorie und verwandte Gebiete ,
61(3), 405‚Äì415.
MacKay,D.J.(2003). InformationTheory,InferenceandLearningAlgorithms .Cambridge
University Press.
Maclaurin, D., Duvenaud, D., & Adams, R. (2015). Gradient-based hyperparameter opti-
mization through reversible learning. Proceedingsofthe32ndInternationalConference
onMachineLearning (ICML‚Äô15) .
Mangasarian, O. L. (1965). Linear and nonlinear separation of patterns by linear program-
ming.Oper.Res. ,13, 444-452.
Mangram,M.E.(2013).AsimplifiedperspectiveoftheMarkowitzportfoliotheory. Global
Journal of Business Research ,7(1), 59‚Äì70.
Matthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., & Ghahramani, Z. (2018).
Gaussian process behaviour in wide deep neural networks. ArXiv:1804.11271 .
McCann,B.,Bradbury,J.,Xiong,C.,&Socher,R.(2017).Learnedintranslation: Contex-
tualized word vectors. Advances in Neural Information Processing Systems (pp. 6294‚Äì
6305).
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous
activity.Bulletinof MathematicalBiophysics ,5(4), 115‚Äì133.
Mead, C. (1980). Introduction to VLSI systems. IEE Proceedings I-Solid-State and Elec-
tronDevices ,128(1), 18.
Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models.
ArXiv:1609.07843 .
Micchelli,C.A.(1984).Interpolationofscattereddata: distancematricesandconditionally
positive definite functions. Approximation Theory and Spline Functions (pp. 143‚Äì145).
Springer.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word repre-
sentations in vector space. ArXiv:1301.3781 .
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed rep-
resentations of words and phrases and their compositionality. AdvancesinNeuralInfor-
mationProcessingSystems (pp. 3111‚Äì3119).
Miller,G.A.(1995).WordNet: alexicaldatabaseforEnglish. CommunicationsoftheACM ,
38(11), 39‚Äì41.
Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., ‚Ä¶ Dean, J. (2017).
Device placement optimization with reinforcement learning. Proceedings of the 34th
International Conferenceon MachineLearning (pp. 2430‚Äì2439).
Mnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention.
Advancesin NeuralInformation ProcessingSystems (pp. 2204‚Äì2212).
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Ried-
miller, M. (2013). Playing Atari with deep reinforcement learning. ArXiv:1312.5602 .
1101 REFERENCES
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ‚Ä¶ et al.
(2015). Human-level control through deep reinforcement learning. Nature,518(7540),
529‚Äì533.
Moon,T.,Smola,A.,Chang,Y.,&Zheng,Z.(2010).Intervalrank: isotonicregressionwith
listwise and pairwise constraints. Proceedingsofthe3rdACMInternationalConference
onWebSearchand Data Mining (pp. 151‚Äì160).
Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The
fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review ,
23(1), 103‚Äì123.
Morozov, V. A. (1984). MethodsforSolvingIncorrectlyPosedProblems . Springer.
Nadaraya,E.A.(1964).Onestimatingregression. TheoryofProbability&itsApplications ,
9(1), 141‚Äì142.
Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann ma-
chines.ICML.
Nakkiran,P.,Kaplun,G.,Bansal,Y.,Yang,T.,Barak,B.,&Sutskever,I.(2021).Deepdou-
ble descent: where bigger models and more data hurt. JournalofStatisticalMechanics:
Theory and Experiment ,2021(12), 124003.
Naor, M., & Reingold, O. (1999). On the construction of pseudorandom permutations:
Luby‚ÄìRackoff revisited. Journal of Cryptology ,12(1), 29‚Äì66.
Neal, R. M. (1996). BayesianLearning forNeuralNetworks . Springer.
Nesterov, Y. (2018). Lectureson ConvexOptimization . Springer.
Nesterov, Y., & Vial, J.-P. (2000). Confidence level solutions for stochastic programming.
Automatica ,44(6), 1559‚Äì1568.
Neyman, J. (1937). Outline of a theory of statistical estimation based on the classical the-
ory of probability. PhilosophicalTransactionsoftheRoyalSocietyofLondon.SeriesA,
Mathematicaland PhysicalSciences ,236(767), 333‚Äì380.
Norelli, A., Fumero, M., Maiorca, V., Moschella, L., Rodol√†, E., & Locatello, F.
(2022). ASIF: coupled data turns unimodal models to multimodal without training.
ArXiv:2210.01738 .
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., ‚Ä¶ Sohl-Dickstein, J. (2018).
Bayesian deep convolutional networks with many channels are Gaussian processes.
ArXiv:1810.05148 .
Novikoff, A. B. J. (1962). On convergence proofs on perceptrons. ProceedingsoftheSym-
posiumon theMathematicalTheory of Automata (pp. 615‚Äì622).
Olshausen,B.A.,&Field,D.J.(1996).Emergenceofsimple-cellreceptivefieldproperties
by learning a sparse code for natural images. Nature,381(6583), 607‚Äì609.
Ong, C. S., Smola, A., & Williamson, R. (2005). Learning the kernel with hyperkernels.
Journal of MachineLearning Research ,6, 1043‚Äì1071.
OpenAI. (2023). GPT-4 Technical Report. ArXiv:2303.08774 .
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ‚Ä¶
et al. (2022). Training language models to follow instructions with human feedback.
ArXiv:2203.02155 .
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic
evaluation of machine translation. Proceedings of the 40th Annual Meeting of the Asso-
ciationforComputational Linguistics (pp. 311‚Äì318).
1102 REFERENCES
Parikh, A. P., T√§ckstr√∂m, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention
model for natural language inference. ArXiv:1606.01933 .
Park, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with
spatially-adaptive normalization. ProceedingsoftheIEEEConferenceonComputerVi-
sionand Pattern Recognition (pp. 2337‚Äì2346).
Parzen, E. (1957). On consistent estimates of the spectrum of a stationary time series. An-
nalsof MathematicalStatistics ,28, 329‚Äì348.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ‚Ä¶ et al. (2019). Py-
Torch: an imperative style, high-performance deep learning library. AdvancesinNeural
Information ProcessingSystems ,32, 8026‚Äì8037.
Paulus, R., Xiong, C., & Socher, R. (2017). A deep reinforced model for abstractive sum-
marization. ArXiv:1705.04304 .
Penedo,G.,Malartic,Q.,Hesslow,D.,Cojocaru,R.,Cappelli,A.,Alobeidli,H.,‚Ä¶Launay,
J.(2023).TheRefinedWebdatasetforFalconLLM:outperformingcuratedcorporawith
web data, and web data only. ArXiv:2306.01116 .
Pennington, J., Schoenholz, S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep
learningthroughdynamicalisometry: theoryandpractice. AdvancesinNeuralInforma-
tionProcessingSystems (pp. 4785‚Äì4795).
Pennington,J.,Socher,R.,&Manning,C.(2014).GloVe: globalvectorsforwordrepresen-
tation.Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP) (pp. 1532‚Äì1543).
Peters,J.,Janzing,D.,&Sch√∂lkopf,B.(2017). ElementsofCausalInference: Foundations
andLearning Algorithms . MIT Press.
Peters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence
tagging with bidirectional language models. Proceedingsofthe55thAnnualMeetingof
theAssociation forComputational Linguistics,Volume1 (pp. 1756‚Äì1765).
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.
(2018). Deep contextualized word representations. Proceedingsofthe2018Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human
LanguageTechnologies,Volume1 (pp. 2227‚Äì2237).
Petersen, K. B., & Pedersen, M. S. (2008). The Matrix Cookbook . Technical University of
Denmark.
Pleiss, G., Chen, D., Huang, G., Li, T., Van Der Maaten, L., & Weinberger, K. Q. (2017).
Memory-efficient implementation of densenets. ArXiv:1707.06990 .
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.
USSRComputational Mathematicsand MathematicalPhysics ,4(5), 1‚Äì17.
Prakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016). Neural
paraphrase generation with stacked residual LSTM networks. ArXiv:1610.03098 .
Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. (2023). Is ChatGPT a
general-purpose natural language processing task solver? ArXiv:2302.06476 .
Quadrana, M., Cremonesi, P., & Jannach, D. (2018). Sequence-aware recommender sys-
tems.ACMComputing Surveys ,51(4), 66.
Quinlan, J. R. (1993). C4.5: ProgramsforMachineLearning . Elsevier.
Rabiner, L., & Juang, B.-H. (1993). Fundamentalsof SpeechRecognition . Prentice-Hall.
1103 REFERENCES
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ‚Ä¶ et al. (2021).
Learning transferable visual models from natural language supervision. International
Conferenceon MachineLearning (pp. 8748‚Äì8763).
Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with
deep convolutional generative adversarial networks. ArXiv:1511.06434 .
Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language
understanding by generative pre-training. OpenAI.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language
models are unsupervised multitask learners. OpenAI Blog ,1(8), 9.
Radosavovic, I., Johnson, J., Xie, S., Lo, W.-Y., & Doll√°r, P. (2019). On network design
spaces for visual recognition. Proceedings of the IEEE/CVF International Conference
onComputerVision (pp. 1882‚Äì1890).
Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Doll√°r, P. (2020). Designing net-
work design spaces. ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition (pp. 10428‚Äì10436).
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ‚Ä¶ et al.
(2021). Scaling language models: methods, analysis & insights from training gopher.
ArXiv:2112.11446 .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ‚Ä¶ Liu, P. J. (2020).
Exploring the limits of transfer learning with a unified text-to-text transformer. Journal
ofMachineLearning Research ,21, 1‚Äì67.
Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ questions for
machine comprehension of text. ArXiv:1606.05250 .
Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019).
Stand-alone self-attention in vision models. AdvancesinNeuralInformationProcessing
Systems,32.
Ramachandran, P., Zoph, B., & Le, Q. V. (2017). Searching for activation functions.
ArXiv:1710.05941 .
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-
conditional image generation with clip latents. ArXiv:2204.06125 .
Ram√≥n y Cajal, Santiago, & Azoulay, L. (1894). Les Nouvelles Id√©es sur la Structure du
Syst√®meNerveuxchezl‚ÄôHomme etchezles Vert√©br√©s . Paris, C. Reinwald & Cie.
Ranzato, M.-A., Boureau, Y.-L., Chopra, S., & LeCun, Y. (2007). A unified energy-based
frameworkforunsupervisedlearning. ArtificialIntelligenceandStatistics (pp.371‚Äì379).
Rasmussen, C. E., & Williams, C. K. (2006). Gaussian Processes for Machine Learning .
MIT Press.
Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of Adam and beyond.
ArXiv:1904.09237 .
Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look once: unified,
real-timeobjectdetection. ProceedingsoftheIEEEConferenceonComputerVisionand
Pattern Recognition (pp. 779‚Äì788).
Redmon, J., & Farhadi, A. (2018). YOLOv3: an incremental improvement.
ArXiv:1804.02767 .
Reed, S., & De Freitas, N. (2015). Neural programmer-interpreters. ArXiv:1511.06279 .
1104 REFERENCES
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., ‚Ä¶
et al. (2022). A generalist agent. ArXiv:2205.06175 .
Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: towards real-time object
detection with region proposal networks. Advances in Neural Information Processing
Systems (pp. 91‚Äì99).
Revels, J., Lubin, M., & Papamarkou, T. (2016). Forward-mode automatic differentiation
in Julia.ArXiv:1607.07892 .
Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and ap-
proximate inference in deep generative models. International Conference on Machine
Learning (pp. 1278‚Äì1286).
Riesenhuber, M., &Poggio, T.(1999).Hierarchicalmodelsofobjectrecognitionincortex.
NatureNeuroscience ,2(11), 1019‚Äì1025.
Rockafellar, R. T. (1970). ConvexAnalysis . Princeton University Press.
Rolnick, D., Veit, A., Belongie, S., &Shavit, N.(2017).Deeplearningisrobusttomassive
label noise. ArXiv:1705.10694 .
Rudin, W. (1973). FunctionalAnalysis . McGraw-Hill.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by
back-propagating errors. CognitiveModeling ,5(3), 1.
Russakovsky, O., Deng, J., Huang, Z., Berg, A. C., & Fei-Fei, L. (2013). Detecting avoca-
dostozucchinis: whathavewedone,andwherearewegoing? InternationalConference
onComputerVision(ICCV) .
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ‚Ä¶ et al. (2015). Ima-
geNetlargescalevisualrecognitionchallenge. InternationalJournalofComputerVision ,
115(3), 211‚Äì252.
Russell, S. J., & Norvig, P. (2016). Artificial Intelligence: A Modern Approach . Pearson
Education Limited.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., ‚Ä¶ et al. (2022).
Photorealistic text-to-image diffusion models with deep language understanding.
ArXiv:2205.11487 .
Salinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., & Archambeau, C. (2022).
Syne Tune: a library for large scale hyperparameter tuning and reproducible research.
FirstConferenceon AutomatedMachineLearning .
Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of
BERT: smaller, faster, cheaper and lighter. ArXiv:1910.01108 .
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., ‚Ä¶ et al. (2021).
Multitask prompted training enables zero-shot task generalization. ArXiv:2110.08207 .
Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normaliza-
tion help optimization? AdvancesinNeuralInformationProcessingSystems (pp. 2483‚Äì
2493).
Sarwar, B. M., Karypis, G., Konstan, J. A., & Riedl, J. (2001). Item-based collaborative
filtering recommendation algorithms. Proceedings of 10th International Conference on
WorldWideWeb (pp. 285‚Äì295).
Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Iliƒá, S., Hesslow, D., ‚Ä¶ et al. (2022). BLOOM:
a 176B-parameter open-access multilingual language model. ArXiv:2211.05100 .
1105 REFERENCES
Schein,A.I.,Popescul,A.,Ungar,L.H.,&Pennock,D.M.(2002).Methodsandmetricsfor
cold-start recommendations. Proceedings of the 25th Annual International ACM SIGIR
Conferenceon Researchand Developmentin Information Retrieval (pp. 253‚Äì260).
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., ‚Ä¶ et al.
(2022). LAION-5B: an open large-scale dataset for training next generation image-text
models.ArXiv:2210.08402 .
Schuster,M.,&Paliwal,K.K.(1997).Bidirectionalrecurrentneuralnetworks. IEEETrans-
actionson Signal Processing ,45(11), 2673‚Äì2681.
Sch√∂lkopf, B., Herbrich, R., & Smola, A. J. (2001). Helmbold, D. P., & Williamson, B.
(Eds.). A generalized representer theorem. Proceedings of the Annual Conference on
ComputationalLearning Theory (pp. 416‚Äì426). Springer-Verlag.
Sch√∂lkopf,B.,Burges,C.,&Vapnik,V.(1996).Incorporatinginvariancesinsupportvector
learning machines. InternationalConferenceonArtificialNeuralNetworks (pp. 47‚Äì52).
Sch√∂lkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector Machines,
Regularization,Optimization,and Beyond . MIT Press.
Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words
with subword units. ArXiv:1508.07909 .
Sergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in
TensorFlow. ArXiv:1802.05799 .
Shannon, C. E. (1948). A mathematical theory of communication. TheBellSystemTechni-
calJournal ,27(3), 379‚Äì423.
Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., ‚Ä¶ Abdelzaher, T. (2020). Con-
trolVAE: controllable variational autoencoder. Proceedings of the 37th International
Conferenceon MachineLearning .
Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position repre-
sentations. ArXiv:1803.02155 .
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019).
Megatron-LM: training multi-billion parameter language models using model paral-
lelism.ArXiv:1909.08053 .
Silver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,‚Ä¶etal.
(2016). Mastering the game of Go with deep neural networks and tree search. Nature,
529(7587), 484.
Silverman, B. W. (1986). Density Estimation for Statistical and Data Analysis . Chapman
and Hall.
Simard, P. Y., LeCun, Y. A., Denker, J. S., & Victorri, B. (1998). Transformation invari-
anceinpatternrecognition‚Äìtangentdistanceandtangentpropagation. NeuralNetworks:
Tricksof theTrade (pp. 239‚Äì274). Springer.
Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale
image recognition. ArXiv:1409.1556 .
Sindhwani, V., Sainath, T. N., & Kumar, S. (2015). Structured transforms for small-
footprint deep learning. ArXiv:1510.01722 .
Sivic,J.,&Zisserman,A.(2003).VideoGoogle: atextretrievalapproachtoobjectmatch-
ing in videos. Proceedings of the IEEE International Conference on Computer Vision
(pp. 1470‚Äì1470).
1106 REFERENCES
Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., ‚Ä¶ et al.
(2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-
scale generative language model. ArXiv:2201.11990 .
Smola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Pro-
ceedingsof theVLDB Endowment ,3(1-2), 703‚Äì710.
Snoek,J.,Larochelle,H.,&Adams,R.(2012).PracticalBayesianoptimizationofmachine
learning algorithms. AdvancesinNeuralInformationProcessingSystems25 (pp. 2951‚Äì
2959).
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsu-
pervised learning using nonequilibrium thermodynamics. International Conference on
MachineLearning (pp. 2256‚Äì2265).
Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data
distribution. Advancesin NeuralInformation ProcessingSystems ,32.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021).
Score-basedgenerativemodelingthroughstochasticdifferentialequations. International
Conferenceon Learning Representations .
Speelpenning, B. (1980). Compiling fast partial derivatives of functions given by algo-
rithms(Doctoral dissertation). University of Illinois at Urbana-Champaign.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., ‚Ä¶ et al. (2022).
Beyond the imitation game: quantifying and extrapolating the capabilities of language
models.ArXiv:2206.04615 .
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).
Dropout: a simple way to prevent neural networks from overfitting. JournalofMachine
Learning Research ,15(1), 1929‚Äì1958.
Srivastava, R. K., Greff, K., & Schmidhuber, J. (2015). Highway networks.
ArXiv:1505.00387 .
Strang, G. (1993). IntroductiontoLinear Algebra . Wellesley‚ÄìCambridge Press.
Su, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Ad-
vancesin Artificial Intelligence ,2009.
Sukhbaatar, S., Weston, J., & Fergus, R. (2015). End-to-end memory networks. Advances
inNeuralInformation ProcessingSystems (pp. 2440‚Äì2448).
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initializa-
tion and momentum in deep learning. International Conference on Machine Learning
(pp. 1139‚Äì1147).
Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural
networks. Advancesin NeuralInformation ProcessingSystems (pp. 3104‚Äì3112).
Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, Inception-
ResNet and the impact of residual connections on learning. 31st AAAI Conference on
Artificial Intelligence .
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ‚Ä¶ Rabinovich, A.
(2015). Going deeper with convolutions. Proceedings of the IEEE Conference on Com-
puterVisionand Pattern Recognition (pp. 1‚Äì9).
Szegedy,C.,Vanhoucke,V.,Ioffe,S.,Shlens,J.,&Wojna,Z.(2016).RethinkingtheIncep-
tionarchitectureforcomputervision. ProceedingsoftheIEEEConferenceonComputer
Visionand Pattern Recognition (pp. 2818‚Äì2826).
1107 REFERENCES
Tallec, C., & Ollivier, Y. (2017). Unbiasing truncated backpropagation through time.
ArXiv:1705.08209 .
Tan, M., & Le, Q. (2019). EfficientNet: rethinking model scaling for convolutional neural
networks. International Conferenceon MachineLearning (pp. 6105‚Äì6114).
Taskar, B., Guestrin, C., & Koller, D. (2004). Max-margin Markov networks. Advances in
NeuralInformation ProcessingSystems ,16, 25.
Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). Efficient transformers: a survey.
ArXiv:2009.06732 .
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., ‚Ä¶Stojnic, R.
(2022). Galactica: a large language model for science. ArXiv:2211.09085 .
Teye, M., Azizpour, H., & Smith, K. (2018). Bayesian uncertainty estimation for batch
normalized deep networks. ArXiv:1802.06455 .
Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., ‚Ä¶ Li, L.-J.
(2016). Yfcc100m: the new data in multimedia research. Communications of the ACM ,
59(2), 64‚Äì73.
Tieleman, T., & Hinton, G. (2012). Divide the gradient by a running average of its recent
magnitude. COURSERA:NeuralNetworksforMachineLearning,Lecture6.5-rmsprop .
Tikhonov, A. N., & Arsenin, V. Y. (1977). SolutionsofIll-PosedProblems . W.H. Winston.
Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ‚Ä¶ et al.
(2021).MLP-mixer: anall-MLParchitectureforvision. AdvancesinNeuralInformation
ProcessingSystems ,34.
Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: a large data set
for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis
andMachineIntelligence ,30(11), 1958‚Äì1970.
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & J√©gou, H. (2021). Train-
ingdata-efficientimagetransformers&distillationthroughattention. InternationalCon-
ferenceon MachineLearning (pp. 10347‚Äì10357).
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., ‚Ä¶ et al.
(2023a). LLaMA: open and efficient foundation language models. ArXiv:2302.13971 .
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ‚Ä¶ et al. (2023b).
LLaMA 2: open foundation and fine-tuned chat models. ArXiv:2307.09288 .
Tsoumakas, G., &Katakis, I.(2007).Multi-labelclassification: anoverview. International
Journal of Data Warehousingand Mining ,3(3), 1‚Äì13.
Turing, A. (1950). Computing machinery and intelligence. Mind,59(236), 433.
Uijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective
search for object recognition. International Journal of Computer Vision ,104(2), 154‚Äì
171.
Vapnik, V. (1995). The Natureof StatisticalLearning Theory . New York: Springer.
Vapnik, V. (1998). StatisticalLearning Theory . New York: John Wiley and Sons.
Vapnik, V., & Chervonenkis, A. (1964). A note on one class of perceptrons. Automation
andRemoteControl ,25.
Vapnik, V., & Chervonenkis, A. (1968). Uniform convergence of frequencies of occurence
of events to their probabilities. Dokl. Akad. NaukSSSR ,181, 915-918.
Vapnik,V.,&Chervonenkis,A.(1971).Ontheuniformconvergenceofrelativefrequencies
of events to their probabilities. Theory Probab.Appl. ,16(2), 264-281.
1108 REFERENCES
Vapnik, V., & Chervonenkis, A. (1981). The necessary and sufficient conditions for the
uniform convergence of averages to their expected values. Teoriya Veroyatnostei i Ee
Primeneniya ,26(3), 543-564.
Vapnik,V.,&Chervonenkis,A.(1991).Thenecessaryandsufficientconditionsforconsis-
tency in the empirical risk minimization method. Pattern Recognition and Image Anal-
ysis,1(3), 283-305.
Vapnik, V. N., & Chervonenkis, A. Y. (1974). Ordered risk minimization. Automationand
RemoteControl ,35, 1226‚Äì1235, 1403‚Äì1412.
Vapnik, V. (1992). Principles of risk minimization for learning theory. AdvancesinNeural
Information ProcessingSystems (pp. 831‚Äì838).
Vapnik, V., Levin, E., & Le Cun, Y. (1994). Measuring the VC-dimension of a learning
machine. NeuralComputation ,6(5), 851‚Äì876.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,‚Ä¶Polosukhin,
I. (2017). Attention is all you need. AdvancesinNeuralInformationProcessingSystems
(pp. 5998‚Äì6008).
Wahba, G. (1990). Spline Models forObservationalData . SIAM.
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. J. (1989). Phoneme recog-
nition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and
SignalProcessing ,37(3), 328‚Äì339.
Wang, H., Zhang, A., Zheng, S., Shi, X., Li, M., & Wang, Z. (2022). Removing batch nor-
malization boosts adversarial training. International Conference on Machine Learning
(pp. 23433‚Äì23445).
Wang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for
aggregation. Networks ,2(3), 2‚Äì3.
Wang,Q.,Li,B.,Xiao,T.,Zhu,J.,Li,C.,Wong,D.F.,&Chao,L.S.(2019).Learningdeep
transformer models for machine translation. Proceedingsofthe57thAnnualMeetingof
theAssociation forComputational Linguistics (pp. 1810‚Äì1822).
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., & Zhou, D. (2023). Self-consistency
improves chain of thought reasoning in language models. International Conference on
Learning Representations .
Wang, Y., Davidson, A., Pan, Y., Wu, Y., Riffel, A., & Owens, J. D. (2016). Gunrock: a
high-performancegraphprocessinglibraryontheGPU. ACMSIGPLANNotices (p.11).
Warstadt,A.,Singh,A.,&Bowman,S.R.(2019).Neuralnetworkacceptabilityjudgments.
Transactionsof theAssociation forComputational Linguistics ,7, 625‚Äì641.
Wasserman,L.(2013). AllofStatistics: AConciseCourseinStatisticalInference .Springer.
Watkins, C. J., & Dayan, P. (1992). Q-learning. MachineLearning ,8(3‚Äì4), 279‚Äì292.
Watson, G. S. (1964). Smooth regression analysis. SankhyƒÅ: TheIndianJournalofStatis-
tics,Series A , pp. 359‚Äì372.
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., ‚Ä¶ Le, Q. V. (2021).
Finetuned language models are zero-shot learners. ArXiv:2109.01652 .
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ‚Ä¶ et al. (2022). Emer-
gent abilities of large language models. ArXiv:2206.07682 .
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain
of thought prompting elicits reasoning in large language models. ArXiv:2201.11903 .
1109 REFERENCES
Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dy-
namics.Proceedingsofthe28thInternationalConferenceonMachineLearning(ICML-
11)(pp. 681‚Äì688).
Wengert,R.E.(1964).Asimpleautomaticderivativeevaluationprogram. Communications
oftheACM ,7(8), 463‚Äì464.
Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Pro-
ceedingsof theIEEE ,78(10), 1550‚Äì1560.
Wigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann.
Math.(pp. 325‚Äì327).
Wilson,A.G.,&Izmailov,P.(2020).Bayesiandeeplearningandaprobabilisticperspective
of generalization. Advances in Neural Information ProcessingSystems ,33, 4697‚Äì4708.
Wistuba, M., Rawat, A., & Pedapati, T. (2019). A survey on neural architecture search.
ArXiv:1905.01392[cs.LG] .
Wistuba, M., Schilling, N., & Schmidt-Thieme, L. (2018). Scalable Gaussian process-
based transfer surrogates for hyperparameter optimization. MachineLearning ,108, 43‚Äì
78.
Wolpert, D. H., & Macready, W. G. (1995). No free lunch theorems for search . Technical
Report SFI-TR-95-02-010, Santa Fe Institute.
Wood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence
memoizer. Communicationsof theACM ,54(2), 91‚Äì98.
Wu, B., Wan, A., Yue, X., Jin, P., Zhao, S., Golmant, N., ‚Ä¶ Keutzer, K. (2018). Shift: a
zero flop, zero parameter alternative to spatial convolutions. Proceedings of the IEEE
Conferenceon ComputerVisionand Pattern Recognition (pp. 9127‚Äì9135).
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ‚Ä¶ et al. (2016).
Google‚Äôs neural machine translation system: bridging the gap between human and ma-
chine translation. ArXiv:1609.08144 .
Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for
benchmarking machine learning algorithms. ArXiv:1708.07747 .
Xiao,L.,Bahri,Y.,Sohl-Dickstein,J.,Schoenholz,S.,&Pennington,J.(2018).Dynamical
isometry and a mean field theory of CNNs: how to train 10,000-layer vanilla convolu-
tionalneuralnetworks. InternationalConferenceonMachineLearning (pp.5393‚Äì5402).
Xie, S., Girshick, R., Doll√°r, P., Tu, Z., & He, K. (2017). Aggregated residual transforma-
tionsfordeepneuralnetworks. ProceedingsoftheIEEEConferenceonComputerVision
andPattern Recognition (pp. 1492‚Äì1500).
Xiong,R.,Yang,Y.,He,D.,Zheng,K.,Zheng,S.,Xing,C.,‚Ä¶Liu,T.(2020).Onlayernor-
malization in the transformer architecture. InternationalConferenceonMachineLearn-
ing(pp. 10524‚Äì10533).
Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., & Stolcke, A. (2018). The Microsoft
2017conversationalspeechrecognitionsystem. 2018IEEEInternationalConferenceon
Acoustics,Speechand Signal Processing(ICASSP) (pp. 5934‚Äì5938).
Yamaguchi, K., Sakamoto, K., Akabane, T., & Fujimoto, Y. (1990). A neural network
for speaker-independent isolated word recognition. First International Conference on
SpokenLanguageProcessing .
Yang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016). Neural machine translation with
recurrent attention modeling. ArXiv:1607.05108 .
1110 REFERENCES
Yang, Z., Moczulski, M., Denil, M., De Freitas, N., Smola, A., Song, L., & Wang, Z.
(2015).Deepfriedconvnets. ProceedingsoftheIEEEInternationalConferenceonCom-
puterVision (pp. 1476‚Äì1483).
Ye,M.,Yin,P.,Lee,W.-C.,&Lee,D.-L.(2011).Exploitinggeographicalinfluenceforcol-
laborativepoint-of-interestrecommendation. Proceedingsofthe34thInternationalACM
SIGIR Conference on Research and Development in Information Retrieval (pp. 325‚Äì
334).
You,Y.,Gitman,I.,&Ginsburg,B.(2017).Largebatchtrainingofconvolutionalnetworks.
ArXiv:1708.03888 .
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., ‚Ä¶ Wu, Y. (2022). Scaling au-
toregressive models for content-rich text-to-image generation. ArXiv:2206.10789 .
Zaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for
nonconvexoptimization. AdvancesinNeuralInformationProcessingSystems (pp.9793‚Äì
9803).
Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. ArXiv:1212.5701 .
Zeiler, M. D., & Fergus, R. (2013). Stochastic pooling for regularization of deep convolu-
tional neural networks. ArXiv:1301.3557 .
Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., & Fu, J. (2021). Beyond
fully-connected layers with quaternions: parameterization of hypercomplex multiplica-
tions with 1/n parameters. International Conferenceon Learning Representations .
Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2021). Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM ,64(3),
107‚Äì115.
Zhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system:
a survey and new perspectives. ACMComputing Surveys ,52(1), 5.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., ‚Ä¶ et al. (2022). OPT:
open pre-trained transformer language models. ArXiv:2205.01068 .
Zhang, W., Tanida, J., Itoh, K., & Ichioka, Y. (1988). Shift-invariant pattern recognition
neural network and its optical architecture. Proceedings of Annual Conference of the
JapanSocietyof Applied Physics .
Zhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., ‚Ä¶ Wang, X. (2021). ByteTrack:
multi-object tracking by associating every detection box. ArXiv:2110.06864 .
Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting
in large language models. International Conference on Learning Representations .
Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2023). Multimodal
chain-of-thought reasoning in language models. ArXiv:2302.00923 .
Zhao, Z.-Q., Zheng, P., Xu, S.-t., & Wu, X. (2019). Object detection with deep learning:
a review. IEEE Transactions on Neural Networks and Learning Systems ,30(11), 3212‚Äì
3232.
Zhou, D., Sch√§rli, N., Hou, L., Wei, J., Scales, N., Wang, X., ‚Ä¶ Chi, E. (2023). Least-
to-most prompting enables complex reasoning in large language models. International
Conferenceon Learning Representations .
Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image transla-
tion using cycle-consistent adversarial networks. ProceedingsoftheIEEEInternational
Conferenceon ComputerVision (pp. 2223‚Äì2232).
1111 REFERENCES
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.
(2015). Aligning books and movies: towards story-like visual explanations by watch-
ing movies and reading books. Proceedings of the IEEE International Conference on
ComputerVision (pp. 19‚Äì27).
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning.
ArXiv:1611.01578 .
